<?xml version="1.0" encoding="utf-8"?>
<raweb xmlns:xlink="http://www.w3.org/1999/xlink" xml:lang="en" year="2016">
  <identification id="lagadic" isproject="true">
    <shortname>LAGADIC</shortname>
    <projectName>Visual servoing in robotics, computer vision, and augmented reality</projectName>
    <theme-de-recherche>Robotics and Smart environments</theme-de-recherche>
    <domaine-de-recherche>Perception, Cognition and Interaction</domaine-de-recherche>
    <urlTeam>http://team.inria.fr/lagadic/welcome-eng.html</urlTeam>
    <structure_exterieure type="Labs">
      <libelle>Institut de recherche en informatique et syst√®mes al√©atoires (IRISA)</libelle>
    </structure_exterieure>
    <structure_exterieure type="Organism">
      <libelle>CNRS</libelle>
    </structure_exterieure>
    <structure_exterieure type="Organism">
      <libelle>Institut national des sciences appliqu√©es de Rennes</libelle>
    </structure_exterieure>
    <structure_exterieure type="Organism">
      <libelle>Universit√© Rennes 1</libelle>
    </structure_exterieure>
    <header_dates_team>Creation of the Project-Team: 2004 December 06</header_dates_team>
    <LeTypeProjet>Project-Team</LeTypeProjet>
    <keywordsSdN>
      <term>5.4.4. - 3D and spatio-temporal reconstruction</term>
      <term>5.4.5. - Object tracking and motion analysis</term>
      <term>5.4.6. - Object localization</term>
      <term>5.4.7. - Visual servoing</term>
      <term>5.6. - Virtual reality, augmented reality</term>
      <term>5.10.2. - Perception</term>
      <term>5.10.4. - Robot control</term>
      <term>5.10.5. - Robot interaction (with the environment, humans, other robots)</term>
      <term>5.10.6. - Swarm robotics</term>
    </keywordsSdN>
    <keywordsSecteurs>
      <term>2.4.3. - Surgery</term>
      <term>2.5. - Handicap and personal assistances</term>
      <term>5.1. - Factory of the future</term>
      <term>5.6. - Robotic systems</term>
      <term>7.2.1. - Smart vehicles</term>
      <term>8.4. - Security and personal assistance</term>
    </keywordsSecteurs>
    <UR name="Rennes"/>
    <UR name="Sophia"/>
  </identification>
  <team id="uid1">
    <person key="lagadic-2014-idp72528">
      <firstname>Fabien</firstname>
      <lastname>Spindler</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Inria, Senior engineer</moreinfo>
    </person>
    <person key="lagadic-2014-idm27984">
      <firstname>Fran√ßois</firstname>
      <lastname>Chaumette</lastname>
      <categoryPro>Chercheur</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Team leader, Inria, Senior Researcher</moreinfo>
      <hdr>oui</hdr>
    </person>
    <person key="lagadic-2014-idm26496">
      <firstname>Alexandre</firstname>
      <lastname>Krupa</lastname>
      <categoryPro>Chercheur</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Inria, Researcher</moreinfo>
      <hdr>oui</hdr>
    </person>
    <person key="lagadic-2016-idp162944">
      <firstname>Claudio</firstname>
      <lastname>Pacchierotti</lastname>
      <categoryPro>Chercheur</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>CNRS, Researcher, from Dec 2016</moreinfo>
    </person>
    <person key="mimetic-2014-idm25552">
      <firstname>Julien</firstname>
      <lastname>Pettr√©</lastname>
      <categoryPro>Chercheur</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Inria, Researcher, from April 2016</moreinfo>
      <hdr>oui</hdr>
    </person>
    <person key="lagadic-2014-idp65680">
      <firstname>Patrick</firstname>
      <lastname>Rives</lastname>
      <categoryPro>Chercheur</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria, Senior Researcher</moreinfo>
      <hdr>oui</hdr>
    </person>
    <person key="lagadic-2014-idp67120">
      <firstname>Paolo</firstname>
      <lastname>Robuffo Giordano</lastname>
      <categoryPro>Chercheur</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>CNRS, Senior Researcher</moreinfo>
      <hdr>oui</hdr>
    </person>
    <person key="lagadic-2015-idp68520">
      <firstname>Paolo</firstname>
      <lastname>Salaris</lastname>
      <categoryPro>Chercheur</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria, Researcher</moreinfo>
    </person>
    <person key="lagadic-2014-idp68368">
      <firstname>Marie</firstname>
      <lastname>Babel</lastname>
      <categoryPro>Enseignant</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>INSA Rennes, Associate Professor</moreinfo>
      <hdr>oui</hdr>
    </person>
    <person key="lagadic-2014-idp69816">
      <firstname>Vincent</firstname>
      <lastname>Drevelle</lastname>
      <categoryPro>Enseignant</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Univ. Rennes I, Associate Professor</moreinfo>
    </person>
    <person key="lagadic-2014-idp71088">
      <firstname>Eric</firstname>
      <lastname>Marchand</lastname>
      <categoryPro>Enseignant</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Univ. Rennes I, Professor</moreinfo>
      <hdr>oui</hdr>
    </person>
    <person key="lagadic-2016-idp184672">
      <firstname>Don Joven</firstname>
      <lastname>Agravante</lastname>
      <categoryPro>PostDoc</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Inria, granted by H2020 Comanoid project</moreinfo>
    </person>
    <person key="lagadic-2015-idp76432">
      <firstname>Thomas</firstname>
      <lastname>Bellavoir</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>CNRS, granted by ANR SenseFly project</moreinfo>
    </person>
    <person key="lagadic-2014-idp75040">
      <firstname>Giovanni</firstname>
      <lastname>Claudio</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Inria, granted by BPI Romeo 2 project and H2020 Comanoid project</moreinfo>
    </person>
    <person key="lagadic-2016-idp192208">
      <firstname>Pierre-Marie</firstname>
      <lastname>Kerzerho</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Inria, granted by ANR VisioLand, from Nov 2016</moreinfo>
    </person>
    <person key="lagadic-2015-idp80232">
      <firstname>Nicol√≤</firstname>
      <lastname>Pedemonte</lastname>
      <categoryPro>PostDoc</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>CNRS, granted by H2020 Romans project, until Sep 2016</moreinfo>
    </person>
    <person key="lagadic-2015-idp81504">
      <firstname>Andrea</firstname>
      <lastname>Peruffo</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>CNRS, granted by H2020 RoMans, until Apr 2016</moreinfo>
    </person>
    <person key="lagadic-2016-idp199760">
      <firstname>Marc</firstname>
      <lastname>Pouliquen</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Inria, granted by ADT UsTk, from Sep 2016</moreinfo>
    </person>
    <person key="lagadic-2014-idp111520">
      <firstname>Riccardo</firstname>
      <lastname>Spica</lastname>
      <categoryPro>PostDoc</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>CNRS, granted by ANR SenseFly</moreinfo>
    </person>
    <person key="lagadic-2014-idp77584">
      <firstname>Souriya</firstname>
      <lastname>Trinh</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Inria, granted by H2020 Comanoid project</moreinfo>
    </person>
    <person key="mimetic-2014-idp109304">
      <firstname>David</firstname>
      <lastname>Wolinski</lastname>
      <categoryPro>PostDoc</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Inria, until Nov 2016, granted by ANR Percolation</moreinfo>
    </person>
    <person key="lagadic-2014-idp78864">
      <firstname>Aur√©lien</firstname>
      <lastname>Yol</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Inria, granted by ANR VisioLand project</moreinfo>
    </person>
    <person key="lagadic-2015-idp85296">
      <firstname>Firas</firstname>
      <lastname>Abi Farraj</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>CNRS, granted by H2020 Romans project</moreinfo>
    </person>
    <person key="lagadic-2014-idp90248">
      <firstname>Quentin</firstname>
      <lastname>Bateux</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Univ. Rennes I, granted by MESR</moreinfo>
    </person>
    <person key="lagadic-2016-idp217152">
      <firstname>Aline</firstname>
      <lastname>Baudry</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>INSA Rennes, granted by MESR and Brittany Council, from Oct 2016</moreinfo>
    </person>
    <person key="lagadic-2014-idp91504">
      <firstname>Suman Raj</firstname>
      <lastname>Bista</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Inria, granted by BPI Romeo 2 and Brittany Council</moreinfo>
    </person>
    <person key="phoenix-2014-idp112224">
      <firstname>Julien</firstname>
      <lastname>Bruneau</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Univ. Rennes I, granted by MESR</moreinfo>
    </person>
    <person key="lagadic-2014-idp92760">
      <firstname>Nicolas</firstname>
      <lastname>Cazy</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Inria, granted by BPI Romeo 2 project</moreinfo>
    </person>
    <person key="lagadic-2014-idp93992">
      <firstname>Pierre</firstname>
      <lastname>Chatelain</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Univ. Rennes 1, granted by ENS Cachan</moreinfo>
    </person>
    <person key="lagadic-2014-idp95240">
      <firstname>Jason</firstname>
      <lastname>Chevrie</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Univ. Rennes I, granted by ENS Cachan</moreinfo>
    </person>
    <person key="lagadic-2014-idp96504">
      <firstname>Le</firstname>
      <lastname>Cui</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Univ. Rennes I, granted by ANR Nanorobust, until Jan 2016</moreinfo>
    </person>
    <person key="lagadic-2016-idp234448">
      <firstname>Quentin</firstname>
      <lastname>Delamare</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Univ. Rennes I, granted by ENS Rennes, from Sep 2016</moreinfo>
    </person>
    <person key="lagadic-2015-idp94120">
      <firstname>Louise</firstname>
      <lastname>Devigne</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>P√¥le Saint H√©lier, granted by Cifre</moreinfo>
    </person>
    <person key="lagadic-2014-idp100248">
      <firstname>Lesley-Ann</firstname>
      <lastname>Duflot</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Inria, granted by Femto-ST and Brittany Council</moreinfo>
    </person>
    <person key="lagadic-2016-idp241920">
      <firstname>Hadrien</firstname>
      <lastname>Gurnel</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Inst. de Recherche Technologique b<formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mo>&lt;</mo><mo>&gt;</mo></mrow></math></formula>com, from Oct 2016</moreinfo>
    </person>
    <person key="lagadic-2016-idp246352">
      <firstname>Dayana</firstname>
      <lastname>Hassan</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria, granted by Cifre, from Nov 2016</moreinfo>
    </person>
    <person key="lagadic-2015-idp98008">
      <firstname>Salma</firstname>
      <lastname>Jiddi</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Technicolor, granted by Cifre</moreinfo>
    </person>
    <person key="lagadic-2014-idp102752">
      <firstname>Vishnu</firstname>
      <lastname>Karakkat Narayanan</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Inria, granted by AEN Pal</moreinfo>
    </person>
    <person key="lagadic-2015-idp100496">
      <firstname>Ide Flore</firstname>
      <lastname>Kenmogne Fokam</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Inria, granted in part by Brittany Council</moreinfo>
    </person>
    <person key="lagadic-2016-idp256192">
      <firstname>Axel</firstname>
      <lastname>Lopez Gandia</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Univ. Rennes I, granted by MESR, from Oct 2016</moreinfo>
    </person>
    <person key="lagadic-2014-idp103992">
      <firstname>Aly</firstname>
      <lastname>Magassouba</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Univ. Rennes I, granted by MESR</moreinfo>
    </person>
    <person key="lagadic-2014-idp105232">
      <firstname>Renato Jos√©</firstname>
      <lastname>Martins</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>CNPq,</moreinfo>
    </person>
    <person key="lagadic-2014-idp106480">
      <firstname>No√´l</firstname>
      <lastname>M√©riaux</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Inria, granted by ANR VisioLand</moreinfo>
    </person>
    <person key="lagadic-2014-idp107744">
      <firstname>Pedro Alfonso</firstname>
      <lastname>Patlan Rosales</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Conacyt</moreinfo>
    </person>
    <person key="lagadic-2015-idp106832">
      <firstname>Bryan</firstname>
      <lastname>Penin</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Inria, granted in part by DGA</moreinfo>
    </person>
    <person key="lagadic-2014-idp109024">
      <firstname>Lucas</firstname>
      <lastname>Royer</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Inst. de Recherche Technologique b<formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mo>&lt;</mo><mo>&gt;</mo></mrow></math></formula>com</moreinfo>
    </person>
    <person key="lagadic-2014-idp110272">
      <firstname>Fabrizio</firstname>
      <lastname>Schiano</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="lagadic-2015-idp111800">
      <firstname>Muhammad</firstname>
      <lastname>Usman</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>CNRS, granted by ANR SenseFly</moreinfo>
    </person>
    <person key="mimetic-2014-idp86848">
      <firstname>Panayiotis</firstname>
      <lastname>Charalambous</lastname>
      <categoryPro>PostDoc</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Inria, granted by ANR Percolation, until May 2016</moreinfo>
    </person>
    <person key="lagadic-2014-idp80120">
      <firstname>Eduardo</firstname>
      <lastname>Fernandez Moral</lastname>
      <categoryPro>PostDoc</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria, granted by ANR Platinum</moreinfo>
    </person>
    <person key="lagadic-2014-idp86512">
      <firstname>H√©l√®ne</firstname>
      <lastname>de La Ru√©e</lastname>
      <categoryPro>Assistant</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Inria, in common with I4S group</moreinfo>
    </person>
    <person key="lagadic-2014-idp87784">
      <firstname>Christine</firstname>
      <lastname>Riehl</lastname>
      <categoryPro>Assistant</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria, in common with Hephaistos group and Focus group at Univ. of Bologna</moreinfo>
    </person>
  </team>
  <presentation id="uid2">
    <bodyTitle>Overall Objectives</bodyTitle>
    <subsection id="uid3" level="1">
      <bodyTitle>Overall Objectives</bodyTitle>
      <p>Historically, research activities of the Lagadic team are concerned with visual servoing, visual tracking, and active vision.
Visual servoing consists in using the information provided by a vision sensor
to control the movements of a dynamic system.
This research topic is at the intersection of the fields of robotics,
automatic control, and computer vision. These fields are the subject of
profitable research since many years and are particularly interesting by their
very broad scientific and application spectrum. Within this spectrum, we
focus on the interaction between visual perception and action.
This topic is significant because it provides an alternative to the
traditional Perception-Decision-Action cycle.
It is indeed possible to link the perception and action aspects more closely,
by directly integrating the measurements provided by a vision sensor in
closed loop control laws.
Our objective is thus to design strategies
of coupling perception and action from images for applications in robotics,
computer vision, virtual reality and augmented reality.</p>
      <p>This objective is significant, first of all because of the variety and the
great number of potential applications to which our work can lead (see Section¬†<ref xlink:href="#uid13" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>).
Secondly, it
is also significant to be able to raise the scientific aspects associated
with these
problems, namely modeling of visual features representing the interaction
between action and perception in an optimal way, taking into account of
complex environments and the specification of high level tasks. We also
work to treat new problems provided by imagery systems such as those resulting
from an omnidirectional vision sensor or echographic probes. We are finally
interested in revisiting traditional problems in computer vision
(3D¬†localization) through the visual servoing approach.</p>
      <p>Thanks to the arrival of Patrick Rives and his students in the group in April¬†2012, which makes Lagadic now localized both in Rennes and Sophia Antipolis, the group now also focuses on building consistent representations of the environment that can be used to trigger and execute the robot actions. In its broadest sense, perception requires detecting, recognizing, and localizing elements of the environment, given the limited sensing and computational resources available on the embedded system. Perception is a fundamental issue for both the implementation of reactive behaviors, as is traditionaly studied in the group, and the construction of the representations that are used at the task level. Simultaneous Localization and Mapping (Slam) is thus now one of our research areas.</p>
      <p>Among the sensory modalities, computer vision, range finder and odometry are of particular importance and interest for mobile robots due to their availability and extended range of applicability, while ultrasound images and force measurements are both required for our medical robotics applications. The fusion of complementary information provided by different sensors is thus also a central issue for modeling the environment, robot localization, control, and navigation.</p>
      <p>Much of the processing must be performed in real time, with a good degree of robustness so as to accommodate with the large variability of the physical world. Computational efficiency and well-posedness of the methods developed are thus constant preoccupations of the group.</p>
    </subsection>
  </presentation>
  <fondements id="uid4">
    <bodyTitle>Research Program</bodyTitle>
    <subsection id="uid5" level="1">
      <bodyTitle>Visual servoing</bodyTitle>
      <p>Basically, visual servoing techniques consist in using the data provided by
one or several cameras in order to control the motions of a dynamic
system¬†<ref xlink:href="#lagadic-2016-bid0" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.
Such systems are usually robot arms, or mobile robots, but can also be
virtual robots, or even a virtual camera.
A large variety of positioning tasks, or mobile target
tracking, can be implemented by controlling from one to
all the degrees of freedom of the system. Whatever the sensor
configuration, which can vary from one on-board camera on the robot
end-effector to several free-standing cameras,
a set of visual features has to be selected at best from the
image measurements available, allowing to control
the desired degrees of freedom. A control law has also to be designed
so that these visual features <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mi>ùê¨</mi><mo>(</mo><mi>t</mi><mo>)</mo></mrow></math></formula> reach a desired
value¬†<formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msup><mi>ùê¨</mi><mo>*</mo></msup></math></formula>, defining a correct realization of the task.
A desired planned trajectory¬†<formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><msup><mi>ùê¨</mi><mo>*</mo></msup><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></mrow></math></formula> can also be
tracked. The control
principle is thus to regulate
the error vector <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mi>ùê¨</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow><mo>-</mo><msup><mi>ùê¨</mi><mo>*</mo></msup><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></mrow></math></formula> to zero.
With a vision sensor providing 2D measurements, potential visual features
are numerous, since 2D data (coordinates of feature points
in the image, moments, ...) as well as 3D data provided
by a localization algorithm exploiting
the extracted 2D features can be considered. It is also possible to
combine 2D and 3D visual features to take the advantages of each approach
while avoiding their respective drawbacks.</p>
      <p>More precisely, a set <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>ùê¨</mi></math></formula> of <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>k</mi></math></formula> visual features can be taken into
account in a visual servoing scheme if it can be written:</p>
      <formula id-text="1" id="uid6" textype="equation" type="display">
        <math xmlns="http://www.w3.org/1998/Math/MathML" mode="display" overflow="scroll">
          <mrow>
            <mi>ùê¨</mi>
            <mo>=</mo>
            <mi>ùê¨</mi>
            <mo>(</mo>
            <mi>ùê±</mi>
            <mo>(</mo>
            <mi>ùê©</mi>
            <mo>(</mo>
            <mi>t</mi>
            <mo>)</mo>
            <mo>)</mo>
            <mo>,</mo>
            <mi>ùêö</mi>
            <mo>)</mo>
          </mrow>
        </math>
      </formula>
      <p noindent="true">where <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mi>ùê©</mi><mo>(</mo><mi>t</mi><mo>)</mo></mrow></math></formula> describes the pose at the instant <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>t</mi></math></formula>
between the camera frame and the target frame, <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>ùê±</mi></math></formula> the image
measurements, and <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>ùêö</mi></math></formula> a set of parameters encoding a potential
additional knowledge, if available (such as for instance a coarse
approximation of the camera calibration parameters,
or the 3D model of the target in some cases).</p>
      <p>The time variation of <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>ùê¨</mi></math></formula> can be linked
to the relative instantaneous velocity <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>ùêØ</mi></math></formula> between the camera and
the scene:</p>
      <formula id-text="2" id="uid7" textype="equation" type="display">
        <math xmlns="http://www.w3.org/1998/Math/MathML" mode="display" overflow="scroll">
          <mrow>
            <mover accent="true">
              <mi>ùê¨</mi>
              <mo>Àô</mo>
            </mover>
            <mo>=</mo>
            <mfrac>
              <mrow>
                <mi>‚àÇ</mi>
                <mi>ùê¨</mi>
              </mrow>
              <mrow>
                <mi>‚àÇ</mi>
                <mi>ùê©</mi>
              </mrow>
            </mfrac>
            <mspace width="0.277778em"/>
            <mover accent="true">
              <mi>ùê©</mi>
              <mo>Àô</mo>
            </mover>
            <mo>=</mo>
            <msub>
              <mi>ùêã</mi>
              <mi>ùê¨</mi>
            </msub>
            <mspace width="0.277778em"/>
            <mi>ùêØ</mi>
          </mrow>
        </math>
      </formula>
      <p noindent="true">where
<formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msub><mi>ùêã</mi><mi>ùê¨</mi></msub></math></formula>
is the interaction matrix related to <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>ùê¨</mi></math></formula>.
This interaction matrix
plays an essential role. Indeed, if we consider for instance an eye-in-hand
system and the camera velocity as input of the robot controller, we
obtain when the control law is designed
to try to obtain an exponential decoupled decrease of the error:</p>
      <formula id-text="3" id="uid8" textype="equation" type="display">
        <math xmlns="http://www.w3.org/1998/Math/MathML" mode="display" overflow="scroll">
          <mrow>
            <msub>
              <mi>ùêØ</mi>
              <mi>c</mi>
            </msub>
            <mo>=</mo>
            <mo>-</mo>
            <mi>Œª</mi>
            <msup>
              <mover accent="true">
                <msub>
                  <mi>ùêã</mi>
                  <mi>ùê¨</mi>
                </msub>
                <mo>^</mo>
              </mover>
              <mo>+</mo>
            </msup>
            <mrow>
              <mo>(</mo>
              <mi>ùê¨</mi>
              <mo>-</mo>
              <msup>
                <mi>ùê¨</mi>
                <mo>*</mo>
              </msup>
              <mo>)</mo>
            </mrow>
            <mo>-</mo>
            <msup>
              <mover accent="true">
                <msub>
                  <mi>ùêã</mi>
                  <mi>ùê¨</mi>
                </msub>
                <mo>^</mo>
              </mover>
              <mo>+</mo>
            </msup>
            <mover accent="true">
              <mfrac>
                <mrow>
                  <mi>‚àÇ</mi>
                  <mi>ùê¨</mi>
                </mrow>
                <mrow>
                  <mi>‚àÇ</mi>
                  <mi>t</mi>
                </mrow>
              </mfrac>
              <mo>^</mo>
            </mover>
          </mrow>
        </math>
      </formula>
      <p noindent="true">where <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>Œª</mi></math></formula> is a proportional gain that has to be tuned to minimize the
time-to-convergence, <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msup><mover accent="true"><msub><mi>ùêã</mi><mi>ùê¨</mi></msub><mo>^</mo></mover><mo>+</mo></msup></math></formula> is the
pseudo-inverse of a model or an approximation
of the interaction matrix, and <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mover accent="true"><mfrac><mrow><mi>‚àÇ</mi><mi>ùê¨</mi></mrow><mrow><mi>‚àÇ</mi><mi>t</mi></mrow></mfrac><mo>^</mo></mover></math></formula>
an estimation of the features velocity due to a possible own
object motion.</p>
      <p>From the selected
visual features and the corresponding interaction matrix, the behavior of
the system will have particular properties as for stability, robustness
with respect to noise or to calibration errors, robot 3D trajectory, etc.
Usually, the interaction matrix is composed of highly non linear terms
and does not present
any decoupling properties. This is generally the case when <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>ùê¨</mi></math></formula> is
directly chosen as <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>ùê±</mi></math></formula>.
In some cases, it may lead to inadequate robot
trajectories or even motions impossible to realize, local minimum, tasks
singularities, etc.
It is thus extremely important to design
adequate visual features for each robot task or application, the ideal case
(very difficult to obtain) being when the corresponding interaction matrix
is constant, leading to a simple linear control system. To conclude in a
few words, <b>visual servoing is basically a non linear control problem.
Our Holy Grail quest is to transform it into a linear control problem.</b></p>
      <p>Furthermore, embedding visual servoing in the task function
approach allows solving efficiently the redundancy
problems that appear when the visual task does not constrain all the
degrees of freedom of the system.
It is then possible to realize
simultaneously the visual task and secondary tasks such as visual inspection,
or joint limits or singularities avoidance. This formalism can also be used
for tasks sequencing purposes in order to deal with high level complex
applications.</p>
    </subsection>
    <subsection id="uid9" level="1">
      <bodyTitle>Visual tracking</bodyTitle>
      <p>Elaboration of object tracking algorithms in
image sequences is
an important issue for researches and applications related to visual
servoing and more generally for robot vision. A robust extraction and
real time spatio-temporal tracking process of visual cues is indeed one
of the keys to success of a visual servoing task.
If fiducial markers may
still be useful to validate theoretical aspects
in modeling and control, natural scenes with non-cooperative objects and subject to various illumination conditions have to be considered
for addressing large scale realistic applications.</p>
      <p>Most of the available tracking methods can be divided into two
main classes: feature-based and model-based. The former approach
focuses on tracking 2D features such as geometrical primitives
(points, segments, circles,...), object contours, regions of
interest, etc. The latter explicitly uses a model of the tracked
objects. This can be either a 3D model or a 2D template of the object. This
second class of methods usually provides a more robust
solution. Indeed, the main advantage of the model-based methods is
that the knowledge about the scene
allows improving tracking robustness and performance, by being able to
predict hidden movements of the object, detect partial occlusions and
acts to reduce the effects of outliers. The challenge is to build
algorithms that are fast and robust
enough to meet our application requirements. Therefore, even if we still
consider 2D feature tracking in some cases, our researches
mainly focus on real-time 3D model-based tracking, since
these approaches are very accurate, robust, and well adapted to any
class of visual servoing schemes. Furthermore, they also meet
the requirements of other classes of application, such as augmented reality.</p>
    </subsection>
    <subsection id="uid10" level="1">
      <bodyTitle>Slam</bodyTitle>
      <p>Most of the applications involving mobile robotic systems (ground vehicles, aerial robots, automated submarines,...) require a reliable localization of the robot in its environment. A challenging problem is when neither the robot localization nor the map is known. Localization and mapping must then be considered concurrently. This problem is known as Simultaneous Localization And Mapping (Slam). In this case, the robot moves from an unknown location in an unknown environment and proceeds to incrementally build up a navigation map of the environment, while simultaneously using this map to update its estimated position.</p>
      <p>Nevertheless, solving the Slam problem is not sufficient for guaranteeing an autonomous and safe navigation. The choice of the representation of the map is, of course, essential. The representation has to support the different levels of the navigation process: motion planning, motion execution and collision avoidance and, at the global level, the definition of an optimal strategy of displacement. The original formulation of the Slam problem is purely metric (since it basically consists in estimating the Cartesian situations of the robot and a set of landmarks), and it does not involve complex representations of the environment. However, it is now well recognized that <b>several complementary representations are needed to perform exploration, navigation, mapping, and control tasks successfully. We propose to use composite models of the environment that mix topological, metric, and grid-based representations.</b> Each type of representation is well adapted to a particular aspect of autonomous navigation¬†<ref xlink:href="#lagadic-2016-bid1" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>: the metric model allows one to locate the robot precisely and plan Cartesian paths, the topological model captures the accessibility of different sites in the environment and allows a coarse localization, and finally the grid representation is useful to characterize the free space and design potential functions used for reactive obstacle avoidance. However, ensuring the consistency of these various representations during the robot exploration, and merging observations acquired from different viewpoints by several cooperative robots, are difficult problems. This is particularly true when different sensing modalities are involved. New studies to derive efficient algorithms for manipulating the hybrid representations (merging, updating, filtering...) while preserving their consistency are needed.</p>
    </subsection>
    <subsection id="uid11" level="1">
      <bodyTitle>Scene modeling and understanding</bodyTitle>
      <p>Long-term mapping has received an increasing amount of attention during last years, largely motivated by the growing need to integrate robots into the real world wherein dynamic objects constantly change the appearance of the scene. A mobile robot evolving in such a dynamic world should not only be able to build a map of the observed environment at a specific moment, but also to maintain this map consistent over a long period of time. It has to deal with dynamic changes that can cause the navigation process to fail. However updating the map is particularly challenging in large-scale environments. To identify changes, robots have to keep a memory of the previous states of the environment and the more dynamic it is, the higher will be the number of states to manage and the more computationally intensive will be the updating process. Mapping large-scale dynamic environments is then particularly difficult as the map size can be arbitrary large. Additionally, mapping many times the whole environment is not always possible or convenient and it is useful to take advantages of methods using only a small number of observations.</p>
      <p>A recent trend in robotic mapping is to augment low-level maps with semantic interpretation of their content, which allows to improve the robot‚Äôs environmental awareness through the use of high-level concepts. In mobile robot navigation, the so-called semantic maps have already been used to improve path planning methods, mainly by providing the robot with the ability to deal with human-understandable targets.</p>
    </subsection>
  </fondements>
  <domaine id="uid12">
    <bodyTitle>Application Domains</bodyTitle>
    <subsection id="uid13" level="1">
      <bodyTitle>Application Domains</bodyTitle>
      <p>The natural applications of our research are obviously in robotics.
In fact, researches undertaken in the Lagadic group can apply to all
the fields of robotics implying a vision sensor. They are indeed conceived
to be independent of the system considered (and the robot and the vision
sensor can even be virtual for some applications).</p>
      <p>Currently, we are mostly interested in using visual servoing for
aerial and space application, micromanipulation, autonomous vehicle navigation
in large urban environments or for disabled or elderly people.</p>
      <p>We also address the field of medical robotics. The applications we consider
turn around new functionalities of assistance to
the clinician during a medical examination: visual servoing on echographic
images, needle insertion, compensation of organ motion, etc.</p>
      <p>Robotics is not the only possible application field to our researches.
In the past, we were interested in applying visual servoing in computer
animation, either for controlling the motions of virtual
humanoids according to their pseudo-perception, or for controlling
the point of view of visual restitution of an animation. In both cases,
potential applications are in the field of virtual reality, for example
for the design of video games, or virtual cinematography.</p>
      <p>Applications also exist in computer vision and augmented reality. It is
then a question of carrying out a virtual visual servoing for the
3D¬†localization of a tool with respect to the vision sensor, or for the
estimation of its 3D¬†motion.
This field of application is very promising, because it is in full rise for the
realization of special effects in the multi-media field or for the design and
the inspection of objects manufactured in the industrial world.</p>
    </subsection>
  </domaine>
  <highlights id="uid14">
    <bodyTitle>Highlights of the Year</bodyTitle>
    <subsection id="uid15" level="1">
      <bodyTitle>Highlights of the Year</bodyTitle>
      <simplelist>
        <li id="uid16">
          <p noindent="true">Eric Marchand and Fabien Spindler co-authored with Prof. Hideaki Uchiyama (Kyushu Univ., Japan) a survey on pose estimation for augmented reality published in IEEE Trans. on Visualization and Computer Graphics¬†<ref xlink:href="#lagadic-2016-bid2" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
        </li>
        <li id="uid17">
          <p noindent="true">The second edition of the Springer Handbook of Robotics has been released this year. It contains an extended version of the chapter on visual servoing
co-authored by Fran√ßois Chaumette, Prof. Seth Hutchinson (UIUC, Illinois)
and Prof. Peter Corke (QUT, Brisbane, Australia)¬†<ref xlink:href="#lagadic-2016-bid3" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
        </li>
      </simplelist>
      <subsection id="uid18" level="2">
        <bodyTitle>Awards</bodyTitle>
        <simplelist>
          <li id="uid19">
            <p noindent="true">The ANR project ENTRACTE, of which Julien Pettr√© is partner, has received the ‚ÄúANR Grand Prix du Num√©rique 2016‚Äù. The project is about anthropomorphic action planning and understanding: <ref xlink:href="http://www.agence-nationale-recherche.fr/?Project=ANR-13-CORD-0002" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>www.<allowbreak/>agence-nationale-recherche.<allowbreak/>fr/<allowbreak/>?Project=ANR-13-CORD-0002</ref> (see also Section¬†<ref xlink:href="#uid125" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>).</p>
          </li>
          <li id="uid20">
            <p noindent="true">Paper¬†<ref xlink:href="#lagadic-2016-bid4" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> has been selected has one of the five finalists for the ICARCV'2016 Best Paper Award.</p>
          </li>
          <li id="uid21">
            <p noindent="true">Lagadic is a member of the five finalist teams for the KUKA Innovation Award (<ref xlink:href="https://www.kuka.com/en-de/press/events/kuka-innovation-award" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>www.<allowbreak/>kuka.<allowbreak/>com/<allowbreak/>en-de/<allowbreak/>press/<allowbreak/>events/<allowbreak/>kuka-innovation-award</ref>), together with the RIS group at LAAS (coordinator), the University of Siena, Italy, and the Seoul National University, South Korea. The goal is to address search and rescue operations in regions which are difficult to access or dangerous following disasters. For this, the team will explore the collaboration between a quadrotor UAV and a KUKA lightweight arm for cooperative transportation and manipulation of rigid objects (e.g., long bards), with a final peg-in-hole task to be demonstrated live at the Hannover fair during spring 2017.</p>
          </li>
        </simplelist>
      </subsection>
    </subsection>
  </highlights>
  <logiciels id="uid22">
    <bodyTitle>New Software and Platforms</bodyTitle>
    <subsection id="uid23" level="1">
      <bodyTitle>DESlam</bodyTitle>
      <p>Dense Egocentric SLAM</p>
      <p noindent="true"><span class="smallcap" align="left">Keywords:</span> Deph Perception - Robotics - Localization</p>
      <p noindent="true">
        <span class="smallcap" align="left">Functional Description</span>
      </p>
      <p>This software proposes a full and self content solution to the dense Slam problem. Based on a generic RGB-D representation valid for various types of sensors (stereovision, multi-cameras, RGB-D sensors...), it provides a 3D textured representation of complex large indoor and outdoor environments and it allows localizing in real time (45Hz) a robot or a person carrying out a mobile camera.</p>
      <simplelist>
        <li id="uid24">
          <p noindent="true">Participants: Maxime Meilland, Andrew Ian Comport and Patrick Rives</p>
        </li>
        <li id="uid25">
          <p noindent="true">Contact: Patrick Rives</p>
        </li>
        <li id="uid26">
          <p noindent="true">URL: <ref xlink:href="http://team.inria.fr/lagadic" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>team.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>lagadic</ref></p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid27" level="1">
      <bodyTitle>HandiViz</bodyTitle>
      <p><span class="smallcap" align="left">Keywords:</span> Health - Persons attendant - Handicap</p>
      <p noindent="true">
        <span class="smallcap" align="left">Functional Description</span>
      </p>
      <p>The HandiViz software proposes a semi-autonomous navigation framework of a wheelchair relying on visual servoing.</p>
      <p>It has been registered to the APP (‚ÄúAgence de Protection des Programmes‚Äù) as an INSA software (IDDN.FR.001.440021.000.S.P.2013.000.10000) and is under GPL license.</p>
      <simplelist>
        <li id="uid28">
          <p noindent="true">Participants: Francois Pasteau and Marie Babel</p>
        </li>
        <li id="uid29">
          <p noindent="true">Contact: Marie Babel</p>
        </li>
        <li id="uid30">
          <p noindent="true">URL: <ref xlink:href="https://team.inria.fr/lagadic/" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>team.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>lagadic/</ref></p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid31" level="1">
      <bodyTitle>Perception360</bodyTitle>
      <p>Robot vision and 3D mapping with omnidirectional RGB-D sensors.</p>
      <p noindent="true"><span class="smallcap" align="left">Keywords:</span> Depth Perception - 3D rendering - Computer vision - Robotics - Image registration - Sensors - Realistic rendering - 3D reconstruction - Localization</p>
      <p noindent="true">
        <span class="smallcap" align="left">Functional Description</span>
      </p>
      <p>This software is a collection of libraries and applications for robot vision and 3D mapping with omnidirectional RGB-D sensors or standard perspective cameras. It provides the functionalities to do image acquisition, semantic annotation, dense registration, localization and 3D mapping. The omnidirectional RGB-D sensors used within this software have been developed at Inria Sophia Antipolis.</p>
      <simplelist>
        <li id="uid32">
          <p noindent="true">Participants: Eduardo Fernandez Moral, Renato Jos√© Martins and Patrick Rives</p>
        </li>
        <li id="uid33">
          <p noindent="true">Contact: Patrick Rives</p>
        </li>
        <li id="uid34">
          <p noindent="true">URL: <ref xlink:href="https://team.inria.fr/lagadic" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>team.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>lagadic</ref></p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid35" level="1">
      <bodyTitle>Sinatrack</bodyTitle>
      <p><span class="smallcap" align="left">Keywords:</span> Computer vision - Robotics</p>
      <p noindent="true">
        <span class="smallcap" align="left">Functional Description</span>
      </p>
      <p>Sinatrack is a tracking software that performs the 3D localization (translation and rotation) of an object with respect to a monocular camera. It allows considering objects with complex shape. The underlying approach is a model-based tracking technique. It has been developed for satellite localization and on-orbit service applications but is also suitable for augmented reality purpose.</p>
      <simplelist>
        <li id="uid36">
          <p noindent="true">Participants: Antoine Petit, Eric Marchand and Francois Chaumette</p>
        </li>
        <li id="uid37">
          <p noindent="true">Contact: Eric Marchand</p>
        </li>
        <li id="uid38">
          <p noindent="true">URL: <ref xlink:href="http://team.inria.fr/lagadic" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>team.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>lagadic</ref></p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid39" level="1">
      <bodyTitle>UsTk</bodyTitle>
      <p>Ultrasound Toolkit</p>
      <p noindent="true"><span class="smallcap" align="left">Keywords:</span> Echographic imagery - Image reconstruction - Active contours - Medical robotics</p>
      <p noindent="true">
        <span class="smallcap" align="left">Functional Description</span>
      </p>
      <p>UsTk, standing for Ultrasound Toolkit, is a cross-platform library for two- and three-dimensional ultrasound image processing and visual servoing based on ultrasound images. Written in C++, UsTk provides tools for ultrasound image acquisition, processing, and display, as well as control of ultrasound probe motion by ultrasound visual servoing. This year we started the development of a new version. The objective is first to consolidate existing developments, to improve the quality of the software, to add new state-of-the-art algorithms, and then to disseminate them within the community as an open-source software.</p>
      <simplelist>
        <li id="uid40">
          <p noindent="true">Participants: Marc Pouliquen, Alexandre Krupa, Pierre Chatelain and Fabien Spindler</p>
        </li>
        <li id="uid41">
          <p noindent="true">Contact: Alexandre Krupa</p>
        </li>
        <li id="uid42">
          <p noindent="true">URL: <ref xlink:href="https://team.inria.fr/lagadic/" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>team.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>lagadic/</ref></p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid43" level="1">
      <bodyTitle>ViSP</bodyTitle>
      <p><span class="smallcap" align="left">Keywords:</span> Computer vision - Robotics - Augmented reality - Visual servoing</p>
      <p noindent="true">
        <span class="smallcap" align="left">Scientific Description</span>
      </p>
      <p>Since 2005, we have been developing and releasing ViSP¬†<ref xlink:href="#lagadic-2016-bid5" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, an open source library available from <ref xlink:href="http://visp.inria.fr" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>visp.<allowbreak/>inria.<allowbreak/>fr</ref>. ViSP standing for Visual Servoing Platform allows prototyping and developing applications using visual tracking and visual servoing techniques at the heart of the Lagadic research. ViSP was designed to be independent from the hardware, to be simple to use, expandable and cross-platform. ViSP allows to design vision-based tasks for eye-in-hand and eye-to-hand visual servoing that contains the most classical visual features that are used in practice. It involves a large set of elementary positioning tasks with respect to various visual features (points, segments, straight lines, circles, spheres, cylinders, image moments, pose...) that can be combined together, and image processing algorithms that allow tracking of visual cues (dots, segments, ellipses...) or 3D model-based tracking of known objects or template tracking. Simulation capabilities are also available.</p>
      <p>
        <span class="smallcap" align="left">Functional Description</span>
      </p>
      <p>ViSP provides simple ways to integrate and validate new algorithms with already existing tools. It follows a module-based software engineering design where data types, algorithms, sensors, viewers and user interaction are made available. Written in C++, ViSP is based on open-source cross-platform libraries (such as OpenCV) and builds with CMake. Several platforms are supported, including OSX, Windows and Linux. ViSP online documentation allows to ease learning. More than 250 fully documented classes organized in 16 different modules, with more than 200 examples and 35 tutorials are proposed to the user. ViSP is released under a dual licensing model. It is open-source with a GNU GPLv2 license. A professional edition license that replaces GNU GPLv2 is also available.</p>
      <simplelist>
        <li id="uid44">
          <p noindent="true">Participants: Fran√ßois Chaumette, Eric Marchand, Fabien Spindler, Aur√©lien Yol and Souriya Trinh</p>
        </li>
        <li id="uid45">
          <p noindent="true">Partner: Inria, Universit√© de Rennes 1</p>
        </li>
        <li id="uid46">
          <p noindent="true">Contact: Fabien Spindler</p>
        </li>
        <li id="uid47">
          <p noindent="true">URL: <ref xlink:href="http://visp.inria.fr" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>visp.<allowbreak/>inria.<allowbreak/>fr</ref></p>
        </li>
      </simplelist>
      <object id="uid48">
        <table>
          <tr>
            <td>
              <ressource xlink:href="IMG/visp1.png" type="inline" width="320.25pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
            </td>
          </tr>
          <tr>
            <td>
              <ressource xlink:href="IMG/visp2.png" type="inline" width="320.25pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
            </td>
          </tr>
        </table>
        <caption>This figure highlights ViSP main capabilities for visual tracking, visual servoing, and augmented reality that may benefit from computer vision algorithms. ViSP allows controlling specific platforms through hardware abstraction or in simulation. ViSP provides also bridges over other frameworks such as OpenCV and ROS. All these capabilities are cross-platform. Moreover, for easing the prototyping of applications, ViSP provides tools for image manipulation, mathematics, data plotting, camera calibration, and many other features. ViSP powerful API is fully documented and available on Github as an open source software under GPLv2 license.</caption>
      </object>
      <p>In December 2015, ViSP 3.0.0 new modular architecture was released. The corresponding source code tarball was downloaded 2138¬†times, much more than the previous 2.10.0 release that was downloaded 1412¬†times. This confirms that ViSP popularity is increasing and motivates the efforts we are doing since more than 10 years to improve the software. ViSP 3.0.0 last release was packaged for Debian, Ubuntu 16.04 LTS, Arch Linux, OSX and ROS. ViSP 3.0.1 next release is in preparation and should be released at the beginning of 2017. This release will be also packaged for iOS devices. In this new version we introduced new wrapper for USB-3 or GigE PointGrey cameras, Haption haptic device, ATI force/torque sensors, Intel RealSense RGB-D devices. We also make an effort to optimize some critical code sections using SSE and make possible cross-compilation for Raspberry PI and iOS targets, and also Nao, Romeo and Pepper robots from SoftBank Robotics. We also introduce a new version of the 3D model-based tracker dedicated to stereo tracking, fixed some issues, improved the documentation by providing new tutorials and by updating the existing ones.</p>
      <p>Concerning ROS community, all the existing packages in ‚Äú<tt>vision_visp</tt>‚Äù ROS stack (see <ref xlink:href="http://wiki.ros.org/vision_visp" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>wiki.<allowbreak/>ros.<allowbreak/>org/<allowbreak/>vision_visp</ref>) were updated and ported to kinetic build system. To ease ViSP usage in the ROS framework, the releases of the year were packaged for ROS.</p>
      <p>ViSP is used in research labs in France, USA, Canada,
Japan, Korea, India, China, Italy, Spain, Portugal, etc. For instance, it
is used as a support in graduate courses
at IFMA Clermont-Ferrand, University of Picardie in Amiens, T√©l√©com Physique in Strasbourg and ESIR in Rennes. Last August, during the Intel Developer Forum opening keynote, Intel CEO Brian Krzanich introduced the Intel Joule compute module. Using an Intel Joule with glasses from French company PivotHead, Intel demonstrated an augmented reality application that was using ViSP (<ref xlink:href="https://www.youtube.com/watch?v=QRBofzL4MDY" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>www.<allowbreak/>youtube.<allowbreak/>com/<allowbreak/>watch?v=QRBofzL4MDY</ref>).</p>
    </subsection>
    <subsection id="uid49" level="1">
      <bodyTitle>WarpDriver</bodyTitle>
      <p><span class="smallcap" align="left">Keywords:</span> Crowd Simulation - Pedestrian Simulation - Collision Avoidance - Reactive Navigation</p>
      <p noindent="true">
        <span class="smallcap" align="left">Functional Description</span>
      </p>
      <p>WarpDriver is a microscopic crowd simulation software, which simulates the collision-free locomotion of many individual agents among the obstacles of a given environment. The originality of the algorithm relies on motion prediction mechanism which allows each agent to predict the probability of colliding other agents with respect to their current motion, their past motion, and the presence of obstacles forcing agents to follow some paths in the environment. Agents then move to their goal whilst they minimize their probability of colliding obstacles.</p>
      <simplelist>
        <li id="uid50">
          <p noindent="true">Participants: David Wolinski and Julien Pettr√©</p>
        </li>
        <li id="uid51">
          <p noindent="true">Contact: Julien Pettr√©</p>
        </li>
        <li id="uid52">
          <p noindent="true">URL: <ref xlink:href="http://team.inria.fr/lagadic" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>team.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>lagadic</ref></p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid53" level="1">
      <bodyTitle>bib2html</bodyTitle>
      <p>
        <span class="smallcap" align="left">Functional Description</span>
      </p>
      <p>The purpose of this software is to automatically produce html pages from BibTEX files, and to provide access to the BibTEX entries by several criteria: year of publication, category of publication, keywords, author name. Moreover cross-linking is generating between pages to provide an easy navigation through the pages without going back to the index.</p>
      <simplelist>
        <li id="uid54">
          <p noindent="true">Contact: Eric Marchand</p>
        </li>
        <li id="uid55">
          <p noindent="true">URL: <ref xlink:href="http://www.irisa.fr/lagadic/soft/bib2html/bib2html.html" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>www.<allowbreak/>irisa.<allowbreak/>fr/<allowbreak/>lagadic/<allowbreak/>soft/<allowbreak/>bib2html/<allowbreak/>bib2html.<allowbreak/>html</ref></p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid56" level="1">
      <bodyTitle>Platforms</bodyTitle>
      <subsection id="uid57" level="2">
        <bodyTitle>Robot vision platform</bodyTitle>
        <participants>
          <person key="lagadic-2014-idp72528">
            <firstname>Fabien</firstname>
            <lastname>Spindler</lastname>
            <moreinfo>contact</moreinfo>
          </person>
        </participants>
        <p>We exploit two industrial robotic systems built by Afma Robots in the
nineties to validate our researches in visual servoing and active
vision. The first one is a Gantry robot with six degrees of freedom,
the other one is a cylindrical robot with four degrees of freedom (see
Fig.¬†<ref xlink:href="#uid58" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.a). These robots are equipped with cameras. The
Gantry robot also allows embedding grippers on its end-effector.</p>
        <p>This year we completed the platform with a haptic Virtuose¬†6D device from Haption company (see Fig.¬†<ref xlink:href="#uid58" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.b). This device is used for
visual-based shared control (see Section¬†<ref xlink:href="#uid148" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>).</p>
        <p>Note that 3 papers published by Lagadic in 2016 enclose results validated on
this platform¬†<ref xlink:href="#lagadic-2016-bid6" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/><ref xlink:href="#lagadic-2016-bid7" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/><ref xlink:href="#lagadic-2016-bid8" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
        <object id="uid58">
          <table rend="inline">
            <tr style="">
              <td style="text-align:center;" halign="center">
                <ressource xlink:href="IMG/afma.png" type="inline" width="256.2026pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
              <td style="text-align:center;" halign="center">¬†¬†</td>
              <td style="text-align:center;" halign="center">
                <ressource xlink:href="IMG/virtuose.jpg" type="inline" width="128.1013pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
            <tr style="">
              <td style="text-align:center;" halign="center">(a)</td>
              <td style="text-align:center;" halign="center">¬†¬†</td>
              <td style="text-align:center;" halign="center">(b)</td>
            </tr>
            <caption/>
          </table>
          <caption>a) Lagadic robotics platform for vision-based manipulation, b) Virtuose 6D haptic device</caption>
        </object>
      </subsection>
      <subsection id="uid59" level="2">
        <bodyTitle>Mobile robots</bodyTitle>
        <participants>
          <person key="lagadic-2014-idp72528">
            <firstname>Fabien</firstname>
            <lastname>Spindler</lastname>
            <moreinfo>contact</moreinfo>
          </person>
          <person key="lagadic-2014-idp68368">
            <firstname>Marie</firstname>
            <lastname>Babel</lastname>
          </person>
          <person key="lagadic-2014-idp65680">
            <firstname>Patrick</firstname>
            <lastname>Rives</lastname>
          </person>
        </participants>
        <subsection id="uid60" level="3">
          <bodyTitle>Indoor mobile robots</bodyTitle>
          <p>For fast prototyping of algorithms in perception, control and autonomous navigation, the team uses Hannibal in Sophia Antipolis, a cart-like platform built by Neobotix (see Fig.¬†<ref xlink:href="#uid62" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.a), and, in Rennes, a Pioneer 3DX from Adept (see Fig.¬†<ref xlink:href="#uid62" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.b). These platforms are equipped with various sensors needed for Slam purposes, autonomous navigation and sensor-based control.</p>
          <p>Moreover, to validate the researches in personally assisted living topic (see Section¬†<ref xlink:href="#uid100" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>), we have three electric wheelchairs in Rennes, one from Permobil, one from Sunrise and the last from YouQ (see Fig.¬†<ref xlink:href="#uid62" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.c). The control of the wheelchair is performed using a plug and play system between the joystick and the low level control of the wheelchair. Such a system lets us acquire the user intention through the joystick position and control the wheelchair by applying corrections to its motion. The wheelchairs have been fitted with cameras and ultrasound sensors to perform the required servoing for assisting handicapped people.</p>
          <p>Note that 11 papers exploiting the indoors mobile robots were published this year¬†<ref xlink:href="#lagadic-2016-bid9" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/><ref xlink:href="#lagadic-2016-bid10" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/><ref xlink:href="#lagadic-2016-bid11" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/><ref xlink:href="#lagadic-2016-bid12" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/><ref xlink:href="#lagadic-2016-bid13" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/><ref xlink:href="#lagadic-2016-bid14" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> <ref xlink:href="#lagadic-2016-bid15" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/><ref xlink:href="#lagadic-2016-bid16" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/><ref xlink:href="#lagadic-2016-bid4" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/><ref xlink:href="#lagadic-2016-bid17" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/><ref xlink:href="#lagadic-2016-bid18" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
        </subsection>
        <subsection id="uid61" level="3">
          <bodyTitle>Outdoor vehicles</bodyTitle>
          <p>A camera rig has been developed in Sophia Antipolis. It can be fixed to a standard car (see Fig. <ref xlink:href="#uid63" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>), which is driven at a variable speed depending on the road/traffic conditions, with an average of 30 km/h and a maximum speed of 80 km/h. The sequences are recorded at a frame rate of 20 Hz, where the six global shutter cameras of the stereo system are synchronized, producing spherical images with a resolution of 2048x665 (see Fig. <ref xlink:href="#uid63" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>). Such sequences are fused offline to obtain maps that can be used later for localization or for scene rendering (in a similar fashion to Google Street View) as shown in the video <ref xlink:href="http://www-sop.inria.fr/members/Renato-Jose.Martins/iros15.html" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>www-sop.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>members/<allowbreak/>Renato-Jose.<allowbreak/>Martins/<allowbreak/>iros15.<allowbreak/>html</ref>.</p>
          <p>Paper¬†<ref xlink:href="#lagadic-2016-bid19" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> contains experimental results obtained
with this camera rig.</p>
          <object id="uid62">
            <table rend="inline">
              <tr style="">
                <td style="text-align:center;" halign="center">
                  <ressource xlink:href="IMG/hannibal-top.jpg" type="inline" width="128.1013pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
                </td>
                <td style="text-align:center;" halign="center">
                  <ressource xlink:href="IMG/pioneer.png" type="inline" width="128.1013pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
                </td>
              </tr>
              <tr style="">
                <td style="text-align:center;" halign="center">(a)</td>
                <td style="text-align:center;" halign="center">(b)</td>
              </tr>
              <caption/>
            </table>
            <table rend="inline">
              <tr style="">
                <td style="text-align:center;" halign="center">
                  <ressource xlink:href="IMG/fauteuil-tous.jpg" type="inline" width="256.2026pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
                </td>
              </tr>
              <tr style="">
                <td style="text-align:center;" halign="center">(c)</td>
              </tr>
              <caption/>
            </table>
            <caption>a) Hannibal platform, b) Pioneer P3-DX robot, c) wheelchairs from Permobil, Sunrise and YouQ.</caption>
          </object>
          <object id="uid63">
            <table rend="inline">
              <tr style="">
                <td style="text-align:center;" halign="center">
                  <ressource xlink:href="IMG/globeye.jpg" type="inline" width="149.4526pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
                </td>
                <td style="text-align:center;" halign="center">
                  <ressource xlink:href="IMG/lagadic_car.jpg" type="inline" width="213.5pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
                </td>
              </tr>
              <caption/>
            </table>
            <table rend="inline">
              <tr style="">
                <td style="text-align:center;" halign="center">
                  <ressource xlink:href="IMG/figex.png" type="inline" width="298.8987pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
                </td>
              </tr>
              <caption/>
            </table>
            <caption>Globeye stereo sensor and acquisition system.</caption>
          </object>
        </subsection>
      </subsection>
      <subsection id="uid64" level="2">
        <bodyTitle>Medical robots</bodyTitle>
        <participants>
          <person key="lagadic-2014-idp72528">
            <firstname>Fabien</firstname>
            <lastname>Spindler</lastname>
            <moreinfo>contact</moreinfo>
          </person>
          <person key="lagadic-2014-idm26496">
            <firstname>Alexandre</firstname>
            <lastname>Krupa</lastname>
          </person>
        </participants>
        <p>This testbed is of primary interest for researches and experiments concerning ultrasound visual servoing applied to probe positioning, soft tissue tracking or robotic needle insertion tasks (see Section¬†<ref xlink:href="#uid89" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>).</p>
        <p>This platform is composed by two Adept Viper six degrees of freedom arms (see
Fig.¬†<ref xlink:href="#uid65" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.a). Ultrasound probes connected either to a
SonoSite 180¬†Plus or an Ultrasonix SonixTouch imaging system can be
mounted on a force torque sensor attached to each robot end-effector.</p>
        <p>This year we replaced the F/T sensor attached to one of the Viper robot in order to use a DAQ acquisition board able to provide measures at a higher frame rate (up to 1¬†kHz). This feature is especially useful for flexible needle steering by ultrasound visual servoing¬†(see Fig.¬†<ref xlink:href="#uid65" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.b).</p>
        <p>Notice that 10 papers published this year include experimental results obtained with this platform¬†<ref xlink:href="#lagadic-2016-bid20" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/><ref xlink:href="#lagadic-2016-bid21" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/><ref xlink:href="#lagadic-2016-bid22" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/><ref xlink:href="#lagadic-2016-bid23" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/><ref xlink:href="#lagadic-2016-bid24" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/><ref xlink:href="#lagadic-2016-bid25" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/><ref xlink:href="#lagadic-2016-bid26" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/><ref xlink:href="#lagadic-2016-bid27" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> <ref xlink:href="#lagadic-2016-bid28" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/><ref xlink:href="#lagadic-2016-bid29" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
        <object id="uid65">
          <table rend="inline">
            <tr style="">
              <td style="text-align:center;" halign="center">
                <ressource xlink:href="IMG/viper-twin.png" type="inline" width="256.2026pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
              <td style="text-align:center;" halign="center">
                <ressource xlink:href="IMG/needle_setup.png" type="inline" width="136.64313pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
            <tr style="">
              <td style="text-align:center;" halign="center">(a)</td>
              <td style="text-align:center;" halign="center">(b)</td>
            </tr>
            <caption/>
          </table>
          <caption>a) Lagadic medical robotics platforms. On the right Viper S850 robot arm equipped with a SonixTouch 3D ultrasound probe. On the left Viper S650 equipped with a tool changer that allows to attach a classical camera or biopsy needles. b) Robotic setup for autonomous needle insertion by visual servoing.</caption>
        </object>
      </subsection>
      <subsection id="uid66" level="2">
        <bodyTitle>Humanoid robots</bodyTitle>
        <participants>
          <person key="lagadic-2014-idp75040">
            <firstname>Giovanni</firstname>
            <lastname>Claudio</lastname>
          </person>
          <person key="lagadic-2014-idp72528">
            <firstname>Fabien</firstname>
            <lastname>Spindler</lastname>
            <moreinfo>contact</moreinfo>
          </person>
        </participants>
        <p>Romeo is a humanoid robot from SoftBank Robotics which is intended to be a genuine personal assistant and companion. For the moment only the upper part of the body (trunk, arms, neck, head, eyes) is working. This research platform is used to validate our researches in visual servoing and visual tracking for object manipulation (see Fig.¬†<ref xlink:href="#uid67" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.a).</p>
        <p>In July, this platform was extended with Pepper, another human-shaped robot designed by SoftBank Robotics to be a genuine day-to-day companion (see Fig.¬†<ref xlink:href="#uid67" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.b). It has 17 degrees of freedom mounted on a wheeled holonomic base and a set of sensors (cameras, laser, ultrasound, inertial) that makes this platform interesting for researches in vision-based manipulation and navigation. Our first developments were devoted to visual servoing for controlling the gaze of the robot exploiting the redundancy of the head and mobile base and adding the capability to follow a person.</p>
        <p>Note that 4 papers published this year include experimental results obtained with these platforms¬†<ref xlink:href="#lagadic-2016-bid30" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/><ref xlink:href="#lagadic-2016-bid31" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/><ref xlink:href="#lagadic-2016-bid32" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/><ref xlink:href="#lagadic-2016-bid33" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
        <object id="uid67">
          <table rend="inline">
            <tr style="">
              <td style="text-align:center;" halign="center">
                <ressource xlink:href="IMG/Romeo.png" type="inline" width="128.1013pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
              <td style="text-align:center;" halign="center">
                <ressource xlink:href="IMG/romeo-grasping.png" type="inline" width="209.23235pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
              <td style="text-align:center;" halign="center">
                <ressource xlink:href="IMG/pepper.jpg" type="inline" width="76.85687pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
            <tr style="">
              <td style="text-align:center;" cols="2" halign="center">(a)</td>
              <td style="text-align:center;" halign="center">(b)</td>
            </tr>
            <caption/>
          </table>
          <caption>a) Romeo experimental platform, b) Pepper human-shaped robot</caption>
        </object>
      </subsection>
      <subsection id="uid68" level="2">
        <bodyTitle>Unmanned Aerial Vehicles (UAVs)</bodyTitle>
        <participants>
          <person key="lagadic-2015-idp76432">
            <firstname>Thomas</firstname>
            <lastname>Bellavoir</lastname>
          </person>
          <person key="lagadic-2014-idp67120">
            <firstname>Paolo</firstname>
            <lastname>Robuffo Giordano</lastname>
            <moreinfo>contact</moreinfo>
          </person>
        </participants>
        <p>From 2014, Lagadic also started some activities involving perception and control for single and multiple quadrotor UAVs, especially thanks to a grant from ‚ÄúRennes M√©tropole‚Äù (see Section¬†<ref xlink:href="#uid119" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>) and the ANR project ‚ÄúSenseFly‚Äù (see Section¬†<ref xlink:href="#uid127" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>). To this end, we purchased four quadrotors from Mikrokopter Gmbh, Germany (see Fig.¬†<ref xlink:href="#uid69" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.a), and one quadrotor from 3DRobotics, USA (see Fig.¬†<ref xlink:href="#uid69" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.b). The Mikrokopter quadrotors have been heavily customized by: <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></math></formula> reprogramming from scratch the low-level attitude controller onboard the microcontroller of the quadrotors, <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mo>(</mo><mi>i</mi><mi>i</mi><mo>)</mo></mrow></math></formula> equipping each quadrotor with an Odroid XU4 board (see Fig.¬†<ref xlink:href="#uid69" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.d) running Linux Ubuntu and the TeleKyb software (the middleware used for managing the experiment flows and the communication among the UAVs and the base station), and <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mo>(</mo><mi>i</mi><mi>i</mi><mi>i</mi><mo>)</mo></mrow></math></formula> purchasing the Flea Color USB3 cameras together with the gimbal needed to mount them on the UAVs (see Fig.¬†<ref xlink:href="#uid69" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.c). The quadrotor group is used as robotic platforms for testing a number of single and multiple flight control schemes with a special attention on the use of onboard vision as main sensory modality.</p>
        <p>Two papers published this year enclose experimental results obtained with this platform¬†<ref xlink:href="#lagadic-2016-bid34" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/><ref xlink:href="#lagadic-2016-bid17" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
        <object id="uid69">
          <table rend="inline">
            <tr style="">
              <td style="text-align:center;" halign="center">
                <ressource xlink:href="IMG/QC.jpg" type="inline" width="128.1013pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
              <td style="text-align:center;" halign="center">
                <ressource xlink:href="IMG/Iris.png" type="inline" width="128.1013pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
            <tr style="">
              <td style="text-align:center;" halign="center">(a)</td>
              <td style="text-align:center;" halign="center">(b)</td>
            </tr>
            <tr style="">
              <td style="text-align:center;" halign="center">
                <ressource xlink:href="IMG/flea_camera.jpg" type="inline" width="128.1013pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
              <td style="text-align:center;" halign="center">
                <ressource xlink:href="IMG/odroid.jpg" type="inline" width="128.1013pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
            <tr style="">
              <td style="text-align:center;" halign="center">(c)</td>
              <td style="text-align:center;" halign="center">(d)</td>
            </tr>
            <caption/>
          </table>
          <caption>a) Quadrotor XL1 from Mikrokopter, b) Quadrotor Iris from 3DRobotics,
c) Flea Color USB3 camera, d) Odroid XU4 board</caption>
        </object>
      </subsection>
    </subsection>
  </logiciels>
  <resultats id="uid70">
    <bodyTitle>New Results</bodyTitle>
    <subsection id="uid71" level="1">
      <bodyTitle>Visual Perception</bodyTitle>
      <subsection id="uid72" level="2">
        <bodyTitle>Micro/nano Manipulation</bodyTitle>
        <participants>
          <person key="lagadic-2014-idp96504">
            <firstname>Le</firstname>
            <lastname>Cui</lastname>
          </person>
          <person key="lagadic-2014-idp71088">
            <firstname>Eric</firstname>
            <lastname>Marchand</lastname>
          </person>
        </participants>
        <p>Le Cui's Ph.D. <ref xlink:href="#lagadic-2016-bid35" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> ended with a contribution related to
visual tracking and estimation of the 3D pose of a micro/nano-object. It is indeed a key issue in the development of automated manipulation tasks using visual feedback. The 3D pose of the micro object can be estimated based on a template matching algorithm. Nevertheless, a key challenge for visual tracking in a scanning electron microscope (SEM) was the difficulty to observe the motion along the depth direction. We then proposed a template-based hybrid visual tracking scheme that uses luminance information to estimate the object displacement on <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>x</mi></math></formula>-<formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>y</mi></math></formula> plane and uses defocus information to estimate object depth¬†<ref xlink:href="#lagadic-2016-bid36" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
      </subsection>
      <subsection id="uid73" level="2">
        <bodyTitle>3D Localization for Space Debris Removal</bodyTitle>
        <participants>
          <person key="lagadic-2014-idp78864">
            <firstname>Aur√©lien</firstname>
            <lastname>Yol</lastname>
          </person>
          <person key="lagadic-2014-idp71088">
            <firstname>Eric</firstname>
            <lastname>Marchand</lastname>
          </person>
          <person key="lagadic-2014-idm27984">
            <firstname>Fran√ßois</firstname>
            <lastname>Chaumette</lastname>
          </person>
        </participants>
        <p>This study is realized in the scope of the FP7 Removedebris project (see Section¬†<ref xlink:href="#uid133" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>)¬†<ref xlink:href="#lagadic-2016-bid37" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.
We compared two vision-based navigation methods for tracking space debris in a low Earth orbit environment. The proposed approaches rely on a frame to frame model-based tracking in order to obtain the complete 3D pose of the camera with respect to the target¬†<ref xlink:href="#lagadic-2016-bid38" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. The proposed algorithms robustly combine points of interest and edge features, as well as color-based features if needed. Experimental results have been obtained demonstrating the robustness of the approaches on synthetic image sequences simulating a CubeSat satellite orbiting the Earth¬†<ref xlink:href="#lagadic-2016-bid39" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
      </subsection>
      <subsection id="uid74" level="2">
        <bodyTitle>3D Localization for Airplane Landing</bodyTitle>
        <participants>
          <person key="lagadic-2014-idp106480">
            <firstname>No√´l</firstname>
            <lastname>M√©riaux</lastname>
          </person>
          <person key="lagadic-2014-idm27984">
            <firstname>Fran√ßois</firstname>
            <lastname>Chaumette</lastname>
          </person>
          <person key="lagadic-2014-idp65680">
            <firstname>Patrick</firstname>
            <lastname>Rives</lastname>
          </person>
          <person key="lagadic-2014-idp71088">
            <firstname>Eric</firstname>
            <lastname>Marchand</lastname>
          </person>
        </participants>
        <p>This study is realized in the scope of the ANR VisioLand project (see Section¬†<ref xlink:href="#uid124" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>). In a first step, we have considered and adapted our model-based tracker¬†<ref xlink:href="#lagadic-2016-bid38" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> to localize the aircraft with respect to the airport surroundings. Satisfactory results have been obtained from real image sequences provided by Airbus. In a second step, we are now considering to perform this localization from a set of keyframe images corresponding to the landing trajectory.</p>
      </subsection>
      <subsection id="uid75" level="2">
        <bodyTitle>Scene Registration based on Planar Patches</bodyTitle>
        <participants>
          <person key="lagadic-2014-idp105232">
            <firstname>Renato Jos√©</firstname>
            <lastname>Martins</lastname>
          </person>
          <person key="lagadic-2014-idp80120">
            <firstname>Eduardo</firstname>
            <lastname>Fernandez Moral</lastname>
          </person>
          <person key="lagadic-2014-idp65680">
            <firstname>Patrick</firstname>
            <lastname>Rives</lastname>
          </person>
        </participants>
        <p>Image registration has been a major problem in computer vision over the past decades. It implies searching an image in a database of previously acquired images to find one (or several) that fulfill some degree of similarity, e.g. an image of the same scene from a similar viewpoint. This problem is interesting in mobile robotics for topological mapping, re-localization, loop closure and object identification. Scene registration can be seen as a generalization of the above problem where the representation to match is not necessarily defined by a single image (i.e. the information may come from different images and/or sensors), attempting to exploit all information available to pursue higher performance and flexibility. This problem is ubiquitous in robot localization and navigation. We propose a probabilistic framework to improve the accuracy and efficiency of a previous solution for structure registration based on planar representation. Our solution consists of matching graphs where the nodes represent planar patches and the edges describe geometric relationships. The maximum likelihood estimation of the registration is estimated by computing the graph similarity from a series of geometric properties (areas, angles, proximity, etc.) to maximize the global consistency of the graph. Our technique has been validated on different RGB-D sequences, both perspective and spherical¬†<ref xlink:href="#lagadic-2016-bid10" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
      </subsection>
      <subsection id="uid76" level="2">
        <bodyTitle>Direct RGB-D Registration</bodyTitle>
        <participants>
          <person key="lagadic-2014-idp105232">
            <firstname>Renato Jos√©</firstname>
            <lastname>Martins</lastname>
          </person>
          <person key="lagadic-2014-idp80120">
            <firstname>Eduardo</firstname>
            <lastname>Fernandez Moral</lastname>
          </person>
          <person key="lagadic-2014-idp65680">
            <firstname>Patrick</firstname>
            <lastname>Rives</lastname>
          </person>
        </participants>
        <p>Dense direct RGB-D registration methods are widely used in tasks ranging from localisation and tracking to 3D scene reconstruction <ref xlink:href="#lagadic-2016-bid1" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. This work addresses a peculiar aspect which drastically limits the applicability of direct registration, namely the weakness of the convergence domain. In general, registration is performed only between close frames (small displacements), since dense registration tasks are particularly sensible to the local convexity of the cost error function. The main contribution of this work is an adaptive RGB-D error cost function that has a larger convergence domain and a faster convergence in both simulated and real data <ref xlink:href="#lagadic-2016-bid9" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, <ref xlink:href="#lagadic-2016-bid19" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. This formulation employs the relative condition number metric to update the weighting of the RGB and depth costs. This approach is performed within a multi-resolution framework, where an efficient pixel selection for both RGB and ICP costs reduces the computational cost whilst preserving the precision. The formulation results in a larger region of attraction and faster convergence than classical RGB, ICP and RGB-D costs. Experiments was conducted using real sequences of indoor and outdoor images using perspective and spherical RGB-D sensors. Significant improvements was denoted in terms of the convergence stability and the speed of convergence in comparison with state-of-the-art methods.</p>
      </subsection>
      <subsection id="uid77" level="2">
        <bodyTitle>Online localization and mapping for UAVs</bodyTitle>
        <participants>
          <person key="lagadic-2015-idp111800">
            <firstname>Muhammad</firstname>
            <lastname>Usman</lastname>
          </person>
          <person key="lagadic-2014-idp67120">
            <firstname>Paolo</firstname>
            <lastname>Robuffo Giordano</lastname>
          </person>
          <person key="lagadic-2014-idp71088">
            <firstname>Eric</firstname>
            <lastname>Marchand</lastname>
          </person>
        </participants>
        <p>Localization and mapping in unknown environments is still an open problem, in particular for what concerns UAVs because of the typical limited memory and processing power available onboard. In order to provide our quadrotor UAVs with high autonomy, we started studying how to exploit onboard cameras for an accurate (but fast) localization and mapping in unknown indoor environments. We chose to base both processes on the newly available Semi-Direct Visual Odometry (SVO) library (<ref xlink:href="http://rpg.ifi.uzh.ch/software" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>rpg.<allowbreak/>ifi.<allowbreak/>uzh.<allowbreak/>ch/<allowbreak/>software</ref>) which has gained considerable attention over the last years in the robotics community. The idea is to exploit dense images (i.e., with little image pre-processing) for obtaining an incremental update of the camera pose which, when integrated over time, can provide the camera localization (pose) w.r.t. the initial frame. In order to reduce drifts during motion, a concurrent mapping thread is also used for comparing the current view with a set of keyframes (taken at regular steps during motion) which constitute a ‚Äúmap‚Äù of the environment. We have started porting the SVO library to our UAVs and the preliminary results showed good performance of the localization accuracy against the Vicon ground truth. We are now planning to close the loop and base the UAV flight on the reconstructed pose from the SVO algorithm.</p>
      </subsection>
      <subsection id="uid78" level="2">
        <bodyTitle>Reflectance and Illumination Estimation for Realistic Augmented Reality</bodyTitle>
        <participants>
          <person key="lagadic-2015-idp98008">
            <firstname>Salma</firstname>
            <lastname>Jiddi</lastname>
          </person>
          <person key="lagadic-2014-idp71088">
            <firstname>Eric</firstname>
            <lastname>Marchand</lastname>
          </person>
        </participants>
        <p>The acquisition of surface material properties and lighting conditions is a fundamental step for photo-realistic Augmented Reality. Human visual cues remain sensitive to the global coherence within a computer-generated image. Absence or bad rendered virtual shadows, unconsidered specular reflections and/or occlusions, confused color perception such as an exuberantly bright virtual object are all elements which may not help an AR user interact and commit to a target application. In this work, we studied a new method for the estimation of the diffuse and specular reflectance properties of an indoor real static scene. Using an RGB-D sensor, we further estimate the 3D position of light sources responsible for specular phenomena and propose a novel photometry-based classification for all the 3D points. The resulting algorithm allows convincing AR results such as realistic virtual shadows as well as proper illumination and specularity occlusions
<ref xlink:href="#lagadic-2016-bid40" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
      </subsection>
      <subsection id="uid79" level="2">
        <bodyTitle>Optimal Active Sensing Control</bodyTitle>
        <participants>
          <person key="lagadic-2015-idp68520">
            <firstname>Paolo</firstname>
            <lastname>Salaris</lastname>
          </person>
          <person key="lagadic-2014-idp111520">
            <firstname>Riccardo</firstname>
            <lastname>Spica</lastname>
          </person>
          <person key="lagadic-2014-idp67120">
            <firstname>Paolo</firstname>
            <lastname>Robuffo Giordano</lastname>
          </person>
        </participants>
        <p>This study concerns the problem of active sensing control. The objective is to improve the estimation accuracy of an observer by determining the inputs of the system that maximize the amount of information gathered by the outputs. In <ref xlink:href="#lagadic-2016-bid41" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> this problem has been solved within the Structure from Motion (SfM) framework for 3D structure estimation problems, i.e. a point, a sphere and a cylinder, in the particular case where the observability property is instantaneously guaranteed. The optimal estimation strategy is hence given in terms of the instantaneous velocity direction of the camera velocities.</p>
        <p>Recently, we have extended the optimal active sensing control to the case where the observability property is not instantaneously guaranteed. To simplify the analysis, we considered nonlinear differentially flat systems. Moreover, to quantify the richness of the acquired information the Observability Gramian (OG) has been used. We have hence defined a trajectory for the flat outputs of the system by using B-Spline curves and then, we have exploited an online gradient descent strategy to move the control points of such B-Spline in order to actively maximise the smallest eigenvalue of the OG over the whole fixed planning time horizon. While the system travels along its planned (optimized) trajectory, an Extended Kalman Filter (EKF) is used to estimate the system state. In order to keep memory of the past acquired sensory data for online re-planning, the OG is also computed on the past estimated state trajectories. This is then used for an online replanning of the optimal trajectory during the robot motion which is continuously refined by exploiting the estimated system state by the EKF. In order to show the effectiveness of our method we have considered a simple but significant case of a planar robot with a single range measurement. The simulation results show that, along the optimal path, the EKF converges faster and provides a more accurate estimate than along any other possible (non-optimal) paths. These results have been submitted to ICRA'2017.</p>
      </subsection>
    </subsection>
    <subsection id="uid80" level="1">
      <bodyTitle>Sensor-based Robot Control</bodyTitle>
      <subsection id="uid81" level="2">
        <bodyTitle>Determining Singularity Configurations in IBVS</bodyTitle>
        <participants>
          <person key="lagadic-2014-idm27984">
            <firstname>Fran√ßois</firstname>
            <lastname>Chaumette</lastname>
          </person>
        </participants>
        <p>This theoretical study has been achieved through an informal collaboration with
S√©bastien Briot and Philippe Martinet from IRCCyN in Nantes, France.
It concerns the determination of the singularity configurations of image-based
visual servoing using tools from the mechanical engineering community and the
concept of ‚Äúhidden‚Äù robot. In a first step, we have revisited the welknown
case of using three image points as visual feature, and then solved the
general case of <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>n</mi></math></formula> image points¬†<ref xlink:href="#lagadic-2016-bid42" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. The case of three
image straight lines has also been solved for the first time¬†<ref xlink:href="#lagadic-2016-bid43" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
      </subsection>
      <subsection id="uid82" level="2">
        <bodyTitle>Interval-based IBVS convergence domain computation</bodyTitle>
        <participants>
          <person key="lagadic-2014-idp69816">
            <firstname>Vincent</firstname>
            <lastname>Drevelle</lastname>
          </person>
        </participants>
        <p>This work aims to compute the set of camera poses from which IBVS will converge to the desired pose (that corresponds to the reference image).
Starting from a (small) initial attraction domain of the desired pose (obtained using Lyapunov theory), we employ subpavings and guaranteed integration to iteratively increase the proven convergence domain, using a viability-based approach. Image-domain and pose-domain constraints are also enforced, like feature points visibility or workspace boundaries.
First results have been obtained for a 3DOF line-scan camera IBVS case¬†<ref xlink:href="#lagadic-2016-bid44" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
      </subsection>
      <subsection id="uid83" level="2">
        <bodyTitle>Visual Servoing of Humanoid Robots</bodyTitle>
        <participants>
          <person key="lagadic-2014-idp75040">
            <firstname>Giovanni</firstname>
            <lastname>Claudio</lastname>
          </person>
          <person key="lagadic-2016-idp184672">
            <firstname>Don Joven</firstname>
            <lastname>Agravante</lastname>
          </person>
          <person key="lagadic-2014-idp72528">
            <firstname>Fabien</firstname>
            <lastname>Spindler</lastname>
          </person>
          <person key="lagadic-2014-idm27984">
            <firstname>Fran√ßois</firstname>
            <lastname>Chaumette</lastname>
          </person>
        </participants>
        <p>This study is realized in the scope of the BPI Romeo¬†2 and H2020 Comanoid projects (see Sections¬†<ref xlink:href="#uid129" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> and¬†<ref xlink:href="#uid140" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>).</p>
        <p>In a first step, we have considered classical kinematic visual servoing schemes
for gaze control and manipulation tasks, such as can or box grasping. Two-hand
manipulation has also been achieved using a master/slave approach¬†<ref xlink:href="#lagadic-2016-bid30" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>,¬†<ref xlink:href="#lagadic-2016-bid31" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. In a second step, we have designed the modeling of the visual features at the acceleration level to embed visual tasks and visual constraints in an existing QP controller¬†<ref xlink:href="#lagadic-2016-bid33" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/><ref xlink:href="#lagadic-2016-bid45" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. Experimental results have been obtained on Romeo (see Section¬†<ref xlink:href="#uid66" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>).</p>
      </subsection>
      <subsection id="uid84" level="2">
        <bodyTitle>Model Predictive Visual Servoing</bodyTitle>
        <participants>
          <person key="lagadic-2014-idp92760">
            <firstname>Nicolas</firstname>
            <lastname>Cazy</lastname>
          </person>
          <person key="lagadic-2014-idp67120">
            <firstname>Paolo</firstname>
            <lastname>Robuffo Giordano</lastname>
          </person>
          <person key="lagadic-2014-idm27984">
            <firstname>Fran√ßois</firstname>
            <lastname>Chaumette</lastname>
          </person>
        </participants>
        <p>This study is realized in collaboration with Pierre-Brice Wieber, from Bipop group at Inria Rh√¥ne Alpes.</p>
        <p>Model Predictive Control (MPC) is a powerful control framework able to take explicitly into account the presence of constraints in the controlled system (e.g., actuator saturations, sensor limitations, and so on). In this research activity, we studied the possibility of using MPC for tackling one of the most classical constraints of visual servoing applications, that is, the possibility to lose tracking of features because of occlusions, limited camera field of view, or imperfect image processing/tracking. The MPC framework depends upon the possibility to predict the future evolution of the controlled system over some time horizon, for correcting the current state of the modeled system whenever new information (e.g., new measurements) become available. We have also explored the possibility of applying these ideas in a multi-robot collaboration scenario where a UAV with a downfacing camera (with limited field of view) needs to provide localization services to a team of ground robots¬†<ref xlink:href="#lagadic-2016-bid46" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
      </subsection>
      <subsection id="uid85" level="2">
        <bodyTitle>Model Predictive Control for Visual Servoing of a UAV</bodyTitle>
        <participants>
          <person key="lagadic-2015-idp106832">
            <firstname>Bryan</firstname>
            <lastname>Penin</lastname>
          </person>
          <person key="lagadic-2014-idp111520">
            <firstname>Riccardo</firstname>
            <lastname>Spica</lastname>
          </person>
          <person key="lagadic-2014-idm27984">
            <firstname>Fran√ßois</firstname>
            <lastname>Chaumette</lastname>
          </person>
          <person key="lagadic-2014-idp67120">
            <firstname>Paolo</firstname>
            <lastname>Robuffo Giordano</lastname>
          </person>
        </participants>
        <p>Visual servoing is a welknown class of techniques meant to control the pose of a robot from visual input by considering an error function directly defined in the image (sensor) space. These techniques are particularly appealing since they do not require, in general, a full state reconstruction, thus granting more robustness and lower computational loads. However, because of the quadrotor underaction and inherent sensor limitations (mainly limited camera field of view), extending the classical visual servoing framework to the quadrotor flight control is not straightforward. For instance, for realizing a horizontal displacement the quadrotor needs to tilt in the desired direction. This tilting, however, will cause any downlooking camera to point in the opposite direction with, e.g., possible loss of feature tracking because of the limited camera field of view.</p>
        <p>In order to cope with these difficulties and achieve a high-performance visual servoing of quadrotor UAVs, we are exploring the possibility of using techniques borrowed from Model-Predictive Control (MPC) for explicitly dealing with this kind of constraints during flight. Indeed, MPC is a class of (numerical) optimal control techniques able to explicitly take into account state and input constraints, as well as complex (and underactuated) nonlinear dynamics of the controlled system. In particular, the ability to predict, over some future time window, the behavior of the visual features on the image plane will allow the quadrotor to fly ‚Äúblindly‚Äù for some limited phases, for then regaining tracking of any lost feature. This possibility will be crucial for allowing quick maneuvering guided by a direct visual feedback. We have started addressing the case of a simulated planar UAV as a representative case study, and we are now working towards an experimental validation with a real quadrotor UAV equipped with an onboard camera.</p>
      </subsection>
      <subsection id="uid86" level="2">
        <bodyTitle>Visual-based shared control</bodyTitle>
        <participants>
          <person key="lagadic-2015-idp85296">
            <firstname>Firas</firstname>
            <lastname>Abi Farraj</lastname>
          </person>
          <person key="lagadic-2015-idp80232">
            <firstname>Nicol√≤</firstname>
            <lastname>Pedemonte</lastname>
          </person>
          <person key="lagadic-2014-idp67120">
            <firstname>Paolo</firstname>
            <lastname>Robuffo Giordano</lastname>
          </person>
        </participants>
        <p>This work concerns our activities in the context of the RoMaNS H2020 project (see Section¬†<ref xlink:href="#uid148" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. Our main goal is to allow a human operator to be interfaced in an intuitive way with a two-arm system, one arm carrying a gripper (for grasping an object), and the other one carrying a camera for looking at the scene (gripper + object) and providing the needed visual feedback. The operator should be allowed to control the two-arm system in an easy way for letting the gripper approaching the target object, and she/he should also receive force cues informative of how feasible her/his commands are w.r.t. the constraints of the system (e.g., joint limits, singularities, limited camera fov, and so on).</p>
        <p>We have started working on this topic by proposing a shared control architecture in which the operator could provide instantaneous velocity commands along four suitable task-space directions not interfering with the main task of keeping the gripper aligned towards the target object (this main task was automatically regulated). The operator was also receiving force cues informative of how much her/his commands were conflicting with the system constraints, in our case joint limits of both manipulators. Finally, the camera was always moving so as to keep both the gripper and the target object at two fixed locations on the image plane¬†<ref xlink:href="#lagadic-2016-bid8" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
        <p>We have then extended this framework in two directions: first, by allowing the possibility of controlling a whole future trajectory for both arms (gripper+camera) while coping with the system constraints. The operator was then receiving an `integral' force feedback along the whole planned trajectory: in this way, the operator's actions and the corresponding force cues were function of a planned trajectory (thus, carrying information over a future time window) that could be manipulated at runtime. Second, we studied how to integrate learning from demonstration into our framework by first using learning techniques for extracting statistical regularities of `expert users' executing successful trajectories for the gripper towards the target object. Then, these learned trajectories were used for generating force cues able to guide novice users during their teleoperation task by the `hands' of the expert users who demonstrated the trajectories in the first place. Both works have been submitted to ICRA'2017.</p>
      </subsection>
      <subsection id="uid87" level="2">
        <bodyTitle>Direct Visual Servoing</bodyTitle>
        <participants>
          <person key="lagadic-2014-idp90248">
            <firstname>Quentin</firstname>
            <lastname>Bateux</lastname>
          </person>
          <person key="lagadic-2014-idp71088">
            <firstname>Eric</firstname>
            <lastname>Marchand</lastname>
          </person>
        </participants>
        <p>In the direct visual servoing methods such as photometric framework, the images as a whole are used to define the control law. This can be opposed to the classical visual servoing approaches that relies on geometric features and where image processing algorithms that extract and track visual features are necessary. In <ref xlink:href="#lagadic-2016-bid6" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>,
we proposed a generic framework to consider histograms as visual features. A histogram is an estimate of the probability distribution of a variable (for example the probability of occurrence in an intensity, color, or gradient orientation in an image). We demonstrated that the framework we proposed applies, but is not limited to, a wide set of histograms and allows the definition of efficient control laws.</p>
        <p>Nevertheless, the main drawback for the direct visual servoing class of methods comparing to the classical geometric visual servoing methods is their comparatively limited convergence range. We then proposed in <ref xlink:href="#lagadic-2016-bid7" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> a new direct visual servoing control law that relies on a particle filter to perform non-local and non-linear optimization in order to increase the convergence domain. To each particle considered we associate a virtual camera that predicts the image it should capture by using image transfer techniques. This new control law has been validated on a 6 DOF positioning task performed on our Gantry
robot (see Section¬†<ref xlink:href="#uid57" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>).</p>
      </subsection>
      <subsection id="uid88" level="2">
        <bodyTitle>Audio-based Control</bodyTitle>
        <participants>
          <person key="lagadic-2014-idp103992">
            <firstname>Aly</firstname>
            <lastname>Magassouba</lastname>
          </person>
          <person key="lagadic-2014-idm27984">
            <firstname>Fran√ßois</firstname>
            <lastname>Chaumette</lastname>
          </person>
        </participants>
        <p>This study is concerned with the application of sensor-based control approach to audio sensors. It is made
in collaboration with Nancy Bertin from Panama group at Irisa and Inria Rennes-Bretagne Atlantique. Auditory features such as Interaural Time Difference (ITD),
Interaural Level Difference (ILD), and sound energy have been modeled and
integrated in various control schemes to control the motion of a mobile robot
with two microphones onboard¬†<ref xlink:href="#lagadic-2016-bid18" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, <ref xlink:href="#lagadic-2016-bid17" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. Experiments with Romeo and Pepper (see Section¬†<ref xlink:href="#uid66" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>) have also been achieved¬†<ref xlink:href="#lagadic-2016-bid32" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. They show the
robustness of closed loop sensor-based control with respect to coarse modeling
and that explicit sound source localization is not a mandatory step for aural
servoing.</p>
      </subsection>
    </subsection>
    <subsection id="uid89" level="1">
      <bodyTitle>Medical Robotics</bodyTitle>
      <subsection id="uid90" level="2">
        <bodyTitle>Non-rigid Target Tracking in Ultrasound Images</bodyTitle>
        <participants>
          <person key="lagadic-2014-idp109024">
            <firstname>Lucas</firstname>
            <lastname>Royer</lastname>
          </person>
          <person key="lagadic-2014-idm26496">
            <firstname>Alexandre</firstname>
            <lastname>Krupa</lastname>
          </person>
        </participants>
        <p>We pursued our work concerning the development of a real-time approach that allows tracking deformable soft tissue structures in 3D ultrasound sequences. In previous work we proposed a method which consists in estimating the target deformation by combining robust dense motion estimation and mechanical model simulation. This year we improved the robustness of our method to several image artefacts as the presence of large shadows, local illumination changes and image occlusions that occur due to the modification of the imaging gain and re-orientation of the ultrasound beam induced by probe motion. To achieve this, we proposed a new dissimilarity criterion between the current and reference images based on the Sum of Conditional Variance (SCV). Our new criterion, that we named Sum of Confident Conditional Variance (SCCV), consists in discriminating unconfident voxels thanks to the use of a pixel-wise quality measurement of the ultrasound images. This improved approach was experimentally validated on organic soft tissues and the obtained results were published in¬†<ref xlink:href="#lagadic-2016-bid20" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
      </subsection>
      <subsection id="uid91" level="2">
        <bodyTitle>Optimization of Ultrasound Image Quality by Visual Servoing</bodyTitle>
        <participants>
          <person key="lagadic-2014-idp93992">
            <firstname>Pierre</firstname>
            <lastname>Chatelain</lastname>
          </person>
          <person key="lagadic-2014-idm26496">
            <firstname>Alexandre</firstname>
            <lastname>Krupa</lastname>
          </person>
        </participants>
        <p>This study is realized in collaboration with Prof. Nassir Navab from the Technical University of Munich (TUM).</p>
        <p>In previous work, we have developed ultrasound-based visual servoing methods to fulfill various tasks, such as compensating for physiological motion, maintaining the visibility of an anatomic target during ultrasound probe teleoperation, or tracking a surgical instrument. However, due to the specific nature of ultrasound images, guaranteeing a good image quality during the procedure remains a challenge. Therefore we pursued our study on the use of ultrasound confidence maps as a new modality for automatically positioning an ultrasound probe in order to improve the image quality. In addition to our visual servoing approach that optimizes the global quality of the image, this year we proposed a control fusion to optimize the acoustic window for a specific anatomical target which is tracked in the ultrasound images¬†<ref xlink:href="#lagadic-2016-bid28" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. Recently, we extended our confidence-driven control to the out-of-plane motion of a 3D ultrasound probe and experimentally validated it on a human volunteer at TUM¬†<ref xlink:href="#lagadic-2016-bid47" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
      </subsection>
      <subsection id="uid92" level="2">
        <bodyTitle>Visual Servoing using Shearlet Transform</bodyTitle>
        <participants>
          <person key="lagadic-2014-idp100248">
            <firstname>Lesley-Ann</firstname>
            <lastname>Duflot</lastname>
          </person>
          <person key="lagadic-2014-idm26496">
            <firstname>Alexandre</firstname>
            <lastname>Krupa</lastname>
          </person>
        </participants>
        <p>In collaboration with the Femto-ST lab in Besan√ßon, we proposed in a first-hand a solution to reduce the acquisition time of an Optical Coherence Tomography (OCT) 3D imaging scanner. This latter consists in sweeping a laser beam on a tissue sample of interest. To increase the frame rate of this imaging device we proposed to apply an optimal trajectory to the laser that covers entirely the image but without performing all the OCT measurements. The reconstruction of the missing data is then achieved by applying an updated Fast Iterative Soft-Thresholding Algorithm (FISTA) on a sparse representation of the image that is based on the shearlet transform <ref xlink:href="#lagadic-2016-bid24" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. In a second hand, we studied the feasibility of using the subsampled shearlet coefficients of an ultrasound image as the visual features of an image-based visual servoing. In a preliminary study we estimated numerically the interaction matrix that links the variation of the shearlet coarsest coefficients to the 6 degrees of freedom motion of the ultrasound probe and uses it in the visual servoing framework. The results obtained in cases of automatic probe positioning and phantom motion compensation demonstrated the efficiency of the shearlet-based features in terms of accuracy, repeatability, robustness and convergence behavior <ref xlink:href="#lagadic-2016-bid27" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. Then we proposed to consider a more efficient and adequate shearlet implementation that consists in a non-subsampled representation of the image. In this case the shearlet coefficients represent different images, focused on different singularities of the initial image, and we consider directly their pixel intensity values in the visual feature vector similarly to the photometry-based visual servoing approach. The modeling of the interaction matrix was analytically derived and experimental results demonstrated the reliability of the new method and its robustness to speckle noise <ref xlink:href="#lagadic-2016-bid23" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
      </subsection>
      <subsection id="uid93" level="2">
        <bodyTitle>3D Steering of Flexible Needle by Ultrasound Visual Servoing</bodyTitle>
        <participants>
          <person key="lagadic-2014-idp95240">
            <firstname>Jason</firstname>
            <lastname>Chevrie</lastname>
          </person>
          <person key="lagadic-2014-idp68368">
            <firstname>Marie</firstname>
            <lastname>Babel</lastname>
          </person>
          <person key="lagadic-2014-idm26496">
            <firstname>Alexandre</firstname>
            <lastname>Krupa</lastname>
          </person>
        </participants>
        <p>The objective of this work is to provide robotic assistance during needle insertion procedures such as biopsy or ablation of localized tumor. In the past we only considered the control of the insertion and needle rotation along and around its main axis by the use of a duty-cycling control strategy. This latter consists in adapting online from visual feedback the orientation of a beveled-tip flexible needle during its insertion for controlling the needle curvature in 3D space that is induced by asymmetrical forces exerted on the bevel. However, such strategy limits the workspace of the needle tip. Therefore we proposed a new control method for flexible needle steering that combines direct base manipulation and needle tip based control. The direct base manipulation control is generated thanks to the use of a 3D model of a flexible beveled tip needle that gives the adequate motion of the needle base to obtain a given motion of the needle tip. This 3D model is based on virtual springs that characterize the needle mechanical interaction with soft tissue and is adapted online from visual tracking of the needle shape. From this model, a measure of the controllability of the needle tip degrees of freedom was proposed in order to mix the control between the direct base manipulation and the duty cycling technique <ref xlink:href="#lagadic-2016-bid29" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. Preliminary results of an automatic needle tip positioning in a translucent gelatine phantom, observed by 2 orthogonal cameras, demonstrated the feasibility of the combination between direct base manipulation and needle tip control for reaching a desired target. This hybrid control allows better targeting capabilities in terms of larger needle workspace and reduced needle bending. In order to predict the trajectory of a needle during insertion under lateral motion of the tissue, we also improved our 3D model of the flexible needle to take into account the effect of the motion of the tissues on the needle shape. This was achieved thanks to the design of an algorithm based on an unscented Kalman filter that estimates the tissue motion. Results obtained from several needle insertions in a moving soft tissue phantom showed that our model gives good performance in terms of needle trajectory prediction. This model was also considered in a closed-loop control approach to allow automatic reaching of a target in case of tissue lateral displacement <ref xlink:href="#lagadic-2016-bid26" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. Future work will address the consideration of 3D ultrasound as visual feedback.</p>
      </subsection>
      <subsection id="uid94" level="2">
        <bodyTitle>Enhancement of Ultrasound Elastography by Visual Servoing and Force Control</bodyTitle>
        <participants>
          <person key="lagadic-2014-idp107744">
            <firstname>Pedro Alfonso</firstname>
            <lastname>Patlan Rosales</lastname>
          </person>
          <person key="lagadic-2014-idm26496">
            <firstname>Alexandre</firstname>
            <lastname>Krupa</lastname>
          </person>
        </participants>
        <p>Elastography imaging is performed by applying continuous stress variation on soft tissues in order to estimate a strain map of the observed tissues. It is obtained by estimating, from the RF (radio-frequency) signal along each scan line of the probe transducer, the echo time delays between pre- and post-compressed tissue. Usually, this continuous stress variation is performed manually by the user who manipulates the US probe and it results therefore in an user-dependent quality of the elastography image. To improve the ultrasound elastography imaging and provide quantitative measurement, we developed an assistant robotic palpation system that automatically moves a 2D ultrasound probe for optimizing ultrasound elastography <ref xlink:href="#lagadic-2016-bid25" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.
The main originality of this work is the use of the elastography modality directly as input of the robot controller. Force measures are also considered in the probe control in order to automatically induce soft tissue deformation needed for real-time elastography imaging process.</p>
      </subsection>
    </subsection>
    <subsection id="uid95" level="1">
      <bodyTitle>Navigation of Mobile Robots</bodyTitle>
      <subsection id="uid96" level="2">
        <bodyTitle>Visual Navigation from an Image Memory</bodyTitle>
        <participants>
          <person key="lagadic-2014-idp91504">
            <firstname>Suman Raj</firstname>
            <lastname>Bista</lastname>
          </person>
          <person key="lagadic-2014-idp67120">
            <firstname>Paolo</firstname>
            <lastname>Robuffo Giordano</lastname>
          </person>
          <person key="lagadic-2014-idm27984">
            <firstname>Fran√ßois</firstname>
            <lastname>Chaumette</lastname>
          </person>
        </participants>
        <p>This study is concerned with visual autonomous navigation in indoor
environments. As in our previous works concerning navigation outdoors¬†<ref xlink:href="#lagadic-2016-bid48" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, the approach is based on a topological localization of the current image with respect to a set of keyframe images, but the visual features used
for this localization as well as for the visual servoing are not composed of points of interest, but either on mutual information¬†<ref xlink:href="#lagadic-2016-bid4" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> following the idea proposed in¬†<ref xlink:href="#lagadic-2016-bid49" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, or
straight lines that are more common indoors¬†<ref xlink:href="#lagadic-2016-bid16" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, or finally on a combination of points of interest and straight lines¬†<ref xlink:href="#lagadic-2016-bid50" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. Satisfactory experimental results have been obtained using the Pioneer mobile robot (see Section¬†<ref xlink:href="#uid59" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>).</p>
      </subsection>
      <subsection id="uid97" level="2">
        <bodyTitle>Robot-Human Interactions during Locomotion</bodyTitle>
        <participants>
          <person key="mimetic-2014-idm25552">
            <firstname>Julien</firstname>
            <lastname>Pettr√©</lastname>
          </person>
        </participants>
        <p>In collaboration with the Gepetto team of Laas in Toulouse and the Mimetic group in Rennes, we have studied how humans avoid collision with a robot. Understanding how humans achieve such avoidance is crucial to better anticipate humans' reactions to the presence of a robot and to control the robot to adapt its trajectory accordingly. It is generally assumed that humans avoid a robot just like they avoid another human. In this work, we bring the empirical demonstration that humans actually set a specific strategy to avoid robots, and that, more precisely, they show a preference to give way to a robot which is on a collision course with them¬†<ref xlink:href="#lagadic-2016-bid51" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. This results brings useful insight about human-robot interactions during locomotion, and provides useful guidelines to design reactive navigation techniques for mobile robots aimed at moving among humans.</p>
      </subsection>
      <subsection id="uid98" level="2">
        <bodyTitle>Scene Mapping based on Intelligent Human/Robot Interactions</bodyTitle>
        <participants>
          <person key="lagadic-2014-idp65680">
            <firstname>Patrick</firstname>
            <lastname>Rives</lastname>
          </person>
        </participants>
        <p>For mobile robots to operate in compliance with human presence, interpreting the impact of human activities and responding constructively is a challenging goal. Towards this objective, mapping an environment allows robots to be deployed in diverse workspaces, marking this skill as a primary element in the integration of robots into human-populated environments. We proposed an effective approach for using human activity cues in order to enhance robot mapping and navigation and in particular in filtering noisy human detections, detecting passages, inferring space occupancy and allowing navigation within unexplored areas. Our contributions <ref xlink:href="#lagadic-2016-bid52" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> are based on the development of intelligent interactions among conceptually different mapping levels, namely, the metric, social and semantic levels. Experiments, using the Hannibal platform (see Section¬†<ref xlink:href="#uid59" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>), highlighted a number of strong dependences among these levels and the way in which they can be used to enhance individual performances and in turn the global robot operation.</p>
      </subsection>
      <subsection id="uid99" level="2">
        <bodyTitle>Autonomous Social Navigation of a Wheelchair </bodyTitle>
        <participants>
          <person key="lagadic-2014-idp102752">
            <firstname>Vishnu</firstname>
            <lastname>Karakkat Narayanan</lastname>
          </person>
          <person key="lagadic-2014-idp68368">
            <firstname>Marie</firstname>
            <lastname>Babel</lastname>
          </person>
        </participants>
        <p>This work is realized in collaboration with Anne Spalanzani (Chroma team - Inria Grenoble).</p>
        <p>A key issue that hinders the adoption of assistive robotic technologies such as robotized wheelchair, in the real world, is that they need to operate in mostly human environments and among human crowds. Indeed intelligent wheelchairs need to be deployed in a human environment thereby making it essential for such robots to incorporate a sense of human-awareness. Simply put, humans are special objects that have to be perceived
and acted on in a special manner by robots that interact with us humans. Thus one can define Human-aware Navigation as an intersection between human-robot interaction and robotic motion
planning.</p>
        <p>In this context we introduced a low-level velocity controller that could be employed by a social robot like a robotic wheelchair for approaching a group of interacting humans, in order to become a part of the interaction. Taking into account an interaction space that is created when at least two humans
interact, a meeting point can be calculated where the robot should reach in order to equitably share space among the interacting group. We then proposed a sensor-based control law which uses the position and orientation of the humans with respect to the sensor as inputs, to reach the
meeting point while respecting spatial social constraints¬†<ref xlink:href="#lagadic-2016-bid11" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. Experiments using a mobile robot equipped with a single
laser scanner, realized in collaboration with Ren Luo (Taiwan) within the Sampen Inria associated team, also proved the success of the algorithm in a noisy real world scenario¬†<ref xlink:href="#lagadic-2016-bid13" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
        <p>In addition, a semi-autonomous framework for human-aware navigation in an intelligent wheelchair has been designed. A generalized linear control sharing framework was proposed that was able to progressively correct the user teleoperation in order to avoid obstacles and in order to avoid disturbance to humans. Meanwhile, we also proposed a Bayesian approach for user intention estimation. The formulation was partly inferred from the design of the controller for assisted doorway passing, wherein we hypothesized that predicting short term goals is sufficient for eliminating user intention uncertainty¬†<ref xlink:href="#lagadic-2016-bid53" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
      </subsection>
      <subsection id="uid100" level="2">
        <bodyTitle>Semi-autonomous Control of a Wheelchair for Navigation Assistance</bodyTitle>
        <participants>
          <person key="lagadic-2015-idp94120">
            <firstname>Louise</firstname>
            <lastname>Devigne</lastname>
          </person>
          <person key="lagadic-2014-idp102752">
            <firstname>Vishnu</firstname>
            <lastname>Karakkat Narayanan</lastname>
          </person>
          <person key="lagadic-2014-idp68368">
            <firstname>Marie</firstname>
            <lastname>Babel</lastname>
          </person>
        </participants>
        <p>To address the wheelchair driving assistance issue, we proposed a unified shared control framework able to smoothly correct the trajectory of the electrical wheelchair¬†<ref xlink:href="#lagadic-2016-bid53" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. The system integrates the manual control with sensor-based constraints by means of a dedicated optimization strategy. The resulting low-complex and low-cost embedded system is easily plugged onto on-the-shelf wheelchairs <ref xlink:href="#lagadic-2016-bid15" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. The robotic solution is currently under validation process with volunteering patients of P√¥le Saint H√©lier (France) who present different disabling neuro-pathologies preventing them to drive non-assisted wheelchairs.</p>
        <p>Within the frame of ISI4NAVE associated team (see Section¬†<ref xlink:href="#uid177" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>), this shared-control solution has been then coupled with first experimental biofeedback devices such as haptic devices. Preliminary tests have been conducted within the PAMELA facility at University College of London and within the rehabilitation center of P√¥le Saint H√©lier in Rennes (see Section¬†<ref xlink:href="#uid111" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>). They involved regular wheelchair users as well as medical staff. We have demonstrated the ability of the framework to provide relevant assistance and now need to focus on methods to fine-tune parameters and customize/calibrate to the individual and evolving needs of each user.</p>
      </subsection>
    </subsection>
    <subsection id="uid101" level="1">
      <bodyTitle>Multi-robot and Crowd Motion Control</bodyTitle>
      <subsection id="uid102" level="2">
        <bodyTitle>Advanced multi-robot control and estimation</bodyTitle>
        <participants>
          <person key="lagadic-2014-idp67120">
            <firstname>Paolo</firstname>
            <lastname>Robuffo Giordano</lastname>
          </person>
        </participants>
        <p>The challenge of coordinating the actions of multiple robots is inspired by the idea that proper coordination of many simple robots can lead to the fulfillment of arbitrarily complex tasks in a robust (to single robot failures) and highly flexible way. Teams of multi-robots can take advantage of their number to perform, for example, complex manipulation and assembly tasks, or to obtain rich spatial awareness by suitably distributing themselves in the environment. Within the scope of robotics, autonomous search and rescue, firefighting, exploration and intervention in dangerous or inaccessible areas are the most promising applications.</p>
        <p>In the context of multi-robot (and multi-UAV) coordinated control, <i>connectivity</i> of the underlying graph is perhaps the most fundamental requirement in order to allow a group of robots accomplishing common goals by means of <i>decentralized</i> solutions. In fact, graph connectivity ensures the needed continuity in the data flow among all the robots in the group which, over time, makes it possible to share and distribute the needed information. We gave two contributions in this field: in the first one¬†<ref xlink:href="#lagadic-2016-bid54" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, we proposed a decentralized exploration strategy for a team of 3D agents able to guarantee exploration of a finite space in a finite amount of time while coping with the constraints of a connected sensing/communication graph for the robot group against sensing/communication constraints (limited range, occluded line-of-sight), and of obstacle and inter-robot collision avoidance. The strategy exploits a suitable state machine for assigning dynamic roles to the agents in the group for allowing completion of the exploration in finite time. Second, in¬†<ref xlink:href="#lagadic-2016-bid55" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> we studied how the choice of a leader agent in a leader-follower scenario could affect the performance of the group when tracking a desired formation (shape and gross motion). The proposed strategy allows selecting the ‚Äúbest leader‚Äù online as a function of the current group state (relative positions and velocities) and of the group topology (assumed connected). By cycling among several connected topologies during motion, we could show that our proposed leader selection algorithm provides the best performance among other possible choices (including the random one) while coping with the constraint of a connected (but possibly time-varying) topology.</p>
        <p>These works were realized in collaboration with the robotics group at the Max Planck Institute for Biological Cybernetics, T√ºbingen, Germany, and the RIS group at Laas in Toulouse.</p>
      </subsection>
      <subsection id="uid103" level="2">
        <bodyTitle>Rigidity-based methods for formation control</bodyTitle>
        <participants>
          <person key="lagadic-2014-idp110272">
            <firstname>Fabrizio</firstname>
            <lastname>Schiano</lastname>
          </person>
          <person key="lagadic-2014-idp111520">
            <firstname>Riccardo</firstname>
            <lastname>Spica</lastname>
          </person>
          <person key="lagadic-2015-idp81504">
            <firstname>Andrea</firstname>
            <lastname>Peruffo</lastname>
          </person>
          <person key="lagadic-2014-idp67120">
            <firstname>Paolo</firstname>
            <lastname>Robuffo Giordano</lastname>
          </person>
        </participants>
        <p>Most multi-robot applications must rely on <i>relative sensing</i> among the robot pairs (rather than absolute/external sensing such as, e.g., GPS). For these systems, the concept of <i>rigidity</i> provides the correct framework for defining an appropriate sensing and communication topology architecture. Rigidity is a combinatorial theory for characterizing the ‚Äústiffness‚Äù or ‚Äúflexibility‚Äù of structures formed by rigid bodies connected by flexible linkages or hinges. In a broader context, rigidity turns out to be an important architectural property of many multi-agent systems when a common inertial reference frame is unavailable. Applications that rely on sensor fusion for localization, exploration, mapping and cooperative tracking of a target, all can benefit from notions in rigidity theory. The concept of rigidity, therefore, provides the theoretical foundation for approaching decentralized solutions to the aforementioned problems using distance measurement sensors, and thus establishing an appropriate framework for relating system level architectural requirements to the sensing and communication capabilities of the system.</p>
        <p>In the recent past, we have proposed a decentralized gradient-based rigidity maintenance action for a group of quadrotor UAVs¬†<ref xlink:href="#lagadic-2016-bid56" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. By starting in a rigid configuration, the group of UAVs was able to estimate their relative position from sole relative distance measurements, and then use these estimated relative positions in a control action able to preserve rigidity of the whole formation despite presence of sensor limitations (maximum range and line-of-sight occlusions), possible collisions with obstacles and inter-robot collisions. This (rigidity-based) control/estimation framework has now been extended to the case of <i>bearing rigidity</i> for directed graphs: here, rather than distances the measurements are the 3D bearing vectors expressed in the local body-frame of each agent. The theory has been extended to the case of 3D agents evolving in <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><msup><mi>‚Ñù</mi><mn>3</mn></msup><mo>√ó</mo><msup><mi>ùíÆ</mi><mn>1</mn></msup></mrow></math></formula> by proposing a decentralized bearing controller/localization algorithm that only requires one single distance measurement (among an arbitrary pair of agents) for a correct convergence¬†<ref xlink:href="#lagadic-2016-bid34" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. The proposed algorithm ensures stabilization towards a desired bearing formation, and allows for the possibility of actuating the motion directions in the null-space of the bearing constraints (that is, collective translations in 3D, expansion/retraction, and coordinated rotation about a vertical axis).</p>
        <p>The need of a single distance measurement (for fixing the formation scale) has also been relaxed in¬†<ref xlink:href="#lagadic-2016-bid57" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> where an <i>active</i> scale estimation scheme has been proposed for allowing the (distributed) estimation of the various inter-agent distances online by processing the measured bearings and the known agent ego-motion (body-frame linear and angular velocities). Finally, we have also proposed an extension of the ‚Äúdistance‚Äù rigidity maintenance controller proposed in¬†<ref xlink:href="#lagadic-2016-bid56" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> to the case of bearing measurements (and bearing rigidity), by considering the typical sensing constraints of onboard cameras, that is, limited range, limited field of view, of possible mutual occlusions when two or more agents lie on the same line-of-sight. This work has been experimentally validated with 5 quadrotor UAVs, and has been submitted to ICRA'2017.</p>
        <p>These works were realized in collaboration with the RIS group at Laas, Toulouse, and with Technion, Israel.</p>
      </subsection>
      <subsection id="uid104" level="2">
        <bodyTitle>Cooperative localization using interval analysis</bodyTitle>
        <participants>
          <person key="lagadic-2015-idp100496">
            <firstname>Ide Flore</firstname>
            <lastname>Kenmogne Fokam</lastname>
          </person>
          <person key="lagadic-2014-idp69816">
            <firstname>Vincent</firstname>
            <lastname>Drevelle</lastname>
          </person>
        </participants>
        <p>In the context of multi-robot fleets, cooperative localization consists in
gaining better position estimate through measurements and data exchange with
neighboring robots.
Positioning integrity (i.e., providing reliable position uncertainty information)
is also a key point for mission-critical tasks, like collision avoidance.
The goal of this work is to compute position uncertainty volumes for each robot
of the fleet, using a decentralized method (i.e., using only local communication
with the neighbors).
The problem is addressed in a bounded-error framework, with interval analysis
and constraint propagation methods. These methods enable to provide guaranteed
position error bounds, assuming bounded-error measurements. They are not
affected by over-convergence due to data incest, which makes them a well sound
framework for decentralized estimation.
Ongoing work focuses on position uncertainty domain computation in image-based UAV
localization¬†<ref xlink:href="#lagadic-2016-bid58" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, and its extension to cooperative localization in a multi-UAV fleet.</p>
      </subsection>
      <subsection id="uid105" level="2">
        <bodyTitle>Numerical Models of Local Interactions during Locomotion</bodyTitle>
        <participants>
          <person key="phoenix-2014-idp112224">
            <firstname>Julien</firstname>
            <lastname>Bruneau</lastname>
          </person>
          <person key="mimetic-2014-idp86848">
            <firstname>Panayiotis</firstname>
            <lastname>Charalambous</lastname>
          </person>
          <person key="mimetic-2014-idp109304">
            <firstname>David</firstname>
            <lastname>Wolinski</lastname>
          </person>
          <person key="mimetic-2014-idm25552">
            <firstname>Julien</firstname>
            <lastname>Pettr√©</lastname>
          </person>
        </participants>
        <p>The numerical models of local interactions are core components of reactive navigation techniques (which allows a robot to avoid dynamic obstacles) and of microscopic crowd simulation algorithms (which allows to simulate a crowd motion as a collection of agent trajectories). We have pursued our efforts to design local models of interactions which capture humans pedestrian behavior, to simulate how they adapt their trajectory so as to perform interactions with their neighbors¬†<ref xlink:href="#lagadic-2016-bid59" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. This year, our efforts were focused on the simulation of grouping behaviors <ref xlink:href="#lagadic-2016-bid60" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, and mid-term strategies human set to perform energy-efficient sequences of successive avoidance adaptations <ref xlink:href="#lagadic-2016-bid61" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. These two situations deal with complex situations of interactions, where several interactions of different kinds need to be combined to compute agents trajectories. For example, when moving in groups, agents have to keep close to the other members of their group while they should not collide with them, as well as they should avoid collision with any other agent or obstacle out of this group.</p>
        <p>We also revisited the foundation of velocity-based models of local interaction for collision avoidance. Using a velocity-based model, a collision-free motion is computed for one agent by extrapolating the future motion of neighbor agents with respect to their current position and velocity. From this information, each agent can deduce the set of velocities (called admissible velocities) that lead to a collision-free motion in the near future. The extrapolation is generally simply based on a linear extrapolation of the future position along the current velocity vector. This is simplistic as it assumes that the current velocity vector is representative of the future motion, while it is often false when, for instance, the agent is currently performing adaptations due to ongoing collision avoidance, or when the agent is following a curvy path. To improve the accuracy of motion prediction and the resulting simulation, we have introduced a probabilistic representation of future position, that can be computed from a set of context elements such as the layout of the environment or the agents past motion <ref xlink:href="#lagadic-2016-bid62" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. We demonstrate in this work the high impact on the level of realism of resulting simulations. This work is implemented in the WarpDriver software (see Section¬†<ref xlink:href="#uid49" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>).</p>
        <p>Finally, we address applications of our simulators to the Computer Animation. Crowd simulation agents generally have a simplistic geometrical and kinematics models, typically, an oriented 2D circle moving on a flat surface. In Computer Animation, an animation of a crowd of 3D realistic characters can be computed on top of the agents simulation by computing their internal joints trajectories so as to perform walking motion along computed agents trajectories. However, the discrepancies between the 2D model of agents and 3D full body characters may result into residual collisions between character shapes. In this collaboration with the Mimetic team, we demonstrate that simple secondary animations for characters, such as local shoulder motions, can be efficiently triggered to camouflage those artefacts, with a very low computational overhead¬†<ref xlink:href="#lagadic-2016-bid63" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
      </subsection>
      <subsection id="uid106" level="2">
        <bodyTitle>Motion Planning for Digital Characters</bodyTitle>
        <participants>
          <person key="mimetic-2014-idm25552">
            <firstname>Julien</firstname>
            <lastname>Pettr√©</lastname>
          </person>
        </participants>
        <p>Motion planning is an important component for agents and robot navigation and control, providing them the ability to perform geometrical reasoning over their environment to transform a high-level distant goal in their environment into a sequence of local motions and sub-goals to reach. This year, we have been involved into two collaborations dealing with motion planning. First collaboration was with the University of Utrecht in the Netherlands. We have proposed a method to evaluate and compare various environment decomposition techniques <ref xlink:href="#lagadic-2016-bid64" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. Environment decomposition is an important step to perform navigation planning in large static environments. Second collaboration was with the University of North Carolina in Chapel Hill (see Section¬†<ref xlink:href="#uid165" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>). We have coupled a contact planner for virtual characters with ITOMP, a motion optimization technique to achieve complex motion in cluttered environment
<ref xlink:href="#lagadic-2016-bid65" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
      </subsection>
    </subsection>
  </resultats>
  <contrats id="uid107">
    <bodyTitle>Bilateral Contracts and Grants with Industry</bodyTitle>
    <subsection id="uid108" level="1">
      <bodyTitle>Bilateral Grants with Industry</bodyTitle>
      <subsection id="uid109" level="2">
        <bodyTitle>Technicolor</bodyTitle>
        <participants>
          <person key="lagadic-2015-idp98008">
            <firstname>Salma</firstname>
            <lastname>Jiddi</lastname>
          </person>
          <person key="lagadic-2014-idp71088">
            <firstname>Eric</firstname>
            <lastname>Marchand</lastname>
          </person>
        </participants>
        <p>
          <i>no Univ. Rennes 1 15CC310-02D, duration: 36 months.</i>
        </p>
        <p>This project funded by Technicolor started in October 2015.
It supports Salma Jiddi's Ph.D. about augmented reality (see Section¬†<ref xlink:href="#uid78" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>).</p>
      </subsection>
      <subsection id="uid110" level="2">
        <bodyTitle>Realyz</bodyTitle>
        <participants>
          <person key="lagadic-2014-idp71088">
            <firstname>Eric</firstname>
            <lastname>Marchand</lastname>
          </person>
        </participants>
        <p>
          <i>no Inria Rennes 10822, duration: 36 months.</i>
        </p>
        <p>This project funded by Realiz started in October 2015. It is realized
in cooperation with Anatole Lecuyer, Hybrid group at Irisa and Inria
Rennes-Bretagne Atlantique to support Guillaume Cortes Ph.D. about motion
capture.</p>
      </subsection>
      <subsection id="uid111" level="2">
        <bodyTitle>P√¥le Saint H√©lier</bodyTitle>
        <participants>
          <person key="lagadic-2015-idp94120">
            <firstname>Louise</firstname>
            <lastname>Devigne</lastname>
          </person>
          <person key="lagadic-2014-idp68368">
            <firstname>Marie</firstname>
            <lastname>Babel</lastname>
          </person>
        </participants>
        <p>
          <i>no. Insa Rennes 2015/0890, duration: 36 months.</i>
        </p>
        <p>This project started in November 2015. It addresses the following two issues. First, the idea is to design a low-cost indoor / outdoor efficient obstacle avoidance system that respects the user intention, and does not alter user perception. This involves embedding innovative sensors to tackle the outdoor wheelchair navigation problem. The second objective is to take advantage of the proposed assistive tool to enhance the user Quality of Experience by means of biofeedback as well as the understanding of the evolution of the pathology.</p>
      </subsection>
      <subsection id="uid112" level="2">
        <bodyTitle>Axyn</bodyTitle>
        <participants>
          <person key="lagadic-2016-idp246352">
            <firstname>Dayana</firstname>
            <lastname>Hassan</lastname>
          </person>
          <person key="lagadic-2015-idp68520">
            <firstname>Paolo</firstname>
            <lastname>Salaris</lastname>
          </person>
          <person key="lagadic-2014-idp65680">
            <firstname>Patrick</firstname>
            <lastname>Rives</lastname>
          </person>
        </participants>
        <p>
          <i>no Inria Sophia 10874-1, duration: 36 months.</i>
        </p>
        <p>The objective of this project that started in November 2016 is to explore new methodologies for the interaction between humans and robots, autonomous navigation and mapping and to transfer the results obtained on the robotic platform developed by AXYN for assisting disabled/elderly people at home or in hospital structures. Cost limits, good accessibility to aged people, robustness and safety related to the applications are at the heart of the project. This contract (ANRT-CIFRE) support Dayana Hassan's Ph.D.
</p>
      </subsection>
    </subsection>
  </contrats>
  <partenariat id="uid113">
    <bodyTitle>Partnerships and Cooperations</bodyTitle>
    <subsection id="uid114" level="1">
      <bodyTitle>Regional Initiatives</bodyTitle>
      <subsection id="uid115" level="2">
        <bodyTitle>ARED NavRob</bodyTitle>
        <participants>
          <person key="lagadic-2014-idp91504">
            <firstname>Suman Raj</firstname>
            <lastname>Bista</lastname>
          </person>
          <person key="lagadic-2014-idp67120">
            <firstname>Paolo</firstname>
            <lastname>Robuffo Giordano</lastname>
          </person>
          <person key="lagadic-2014-idm27984">
            <firstname>Fran√ßois</firstname>
            <lastname>Chaumette</lastname>
          </person>
        </participants>
        <p>
          <i>no Inria Rennes 8033, duration: 36 months.</i>
        </p>
        <p>This project funded by the Brittany council ended in October 2016.
It supported in part Suman Raj Bista's Ph.D. about visual navigation
(see Section¬†<ref xlink:href="#uid96" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>).</p>
      </subsection>
      <subsection id="uid116" level="2">
        <bodyTitle>ARED DeSweep</bodyTitle>
        <participants>
          <person key="lagadic-2014-idp100248">
            <firstname>Lesley-Ann</firstname>
            <lastname>Duflot</lastname>
          </person>
          <person key="lagadic-2014-idm26496">
            <firstname>Alexandre</firstname>
            <lastname>Krupa</lastname>
          </person>
        </participants>
        <p>
          <i>no Inria Rennes 8033, duration: 36 months.</i>
        </p>
        <p>This project funded by the Brittany council started in October 2014.
It supports in part Lesley-Ann Duflot's Ph.D. about visual servoing based
on the shearlet transform (see Section¬†<ref xlink:href="#uid92" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>).</p>
      </subsection>
      <subsection id="uid117" level="2">
        <bodyTitle>ARED Locoflot</bodyTitle>
        <participants>
          <person key="lagadic-2015-idp100496">
            <firstname>Ide Flore</firstname>
            <lastname>Kenmogne Fokam</lastname>
          </person>
          <person key="lagadic-2014-idp69816">
            <firstname>Vincent</firstname>
            <lastname>Drevelle</lastname>
          </person>
          <person key="lagadic-2014-idp71088">
            <firstname>Eric</firstname>
            <lastname>Marchand</lastname>
          </person>
        </participants>
        <p>
          <i>no Inria Rennes 9944, duration: 36 months.</i>
        </p>
        <p>This project funded by the Brittany council started in October 2015.
It supports in part Ide Flore Kenmogne Fokam's Ph.D. about cooperative
localization in multi-robot fleets using interval analysis
(see Section¬†<ref xlink:href="#uid104" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>).</p>
      </subsection>
      <subsection id="uid118" level="2">
        <bodyTitle>ARED Mod4Nav</bodyTitle>
        <participants>
          <person key="lagadic-2016-idp217152">
            <firstname>Aline</firstname>
            <lastname>Baudry</lastname>
          </person>
          <person key="lagadic-2014-idp68368">
            <firstname>Marie</firstname>
            <lastname>Babel</lastname>
          </person>
        </participants>
        <p>
          <i>no INSA Rennes 2016/01, duration: 36 months.</i>
        </p>
        <p>This project funded by the Brittany council started in October 2016.
It supports in part Aline Baudry's Ph.D. about wheelchair modeling.</p>
      </subsection>
      <subsection id="uid119" level="2">
        <bodyTitle>‚ÄúEquipement mi-lourd Rennes M√©tropole‚Äù</bodyTitle>
        <participants>
          <person key="lagadic-2014-idp67120">
            <firstname>Paolo</firstname>
            <lastname>Robuffo Giordano</lastname>
          </person>
        </participants>
        <p>
          <i>no CNRS Rennes 14C0481, duration: 36 months.</i>
        </p>
        <p>A grant from ‚ÄúRennes M√©tropole‚Äù has been obtained in June 2014 and
supports the activities related to the use of drones (quadrotor UAVs). The
platform described in Section¬†<ref xlink:href="#uid68" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> has been purchased in part
thanks to this grant.</p>
      </subsection>
      <subsection id="uid120" level="2">
        <bodyTitle>IRT Jules Verne Mascot</bodyTitle>
        <participants>
          <person key="lagadic-2014-idm27984">
            <firstname>Fran√ßois</firstname>
            <lastname>Chaumette</lastname>
          </person>
        </participants>
        <p>
          <i>no Inria Rennes 10361, duration: 36 months.</i>
        </p>
        <p>This project started in October 2015. It is managed by IRT Jules Verne in Nantes and realized in cooperation with IRCCyN, Airbus, Renault, Faurecia and Alsthom. Its goal is to perform screwing for various industrial applications.</p>
      </subsection>
      <subsection id="uid121" level="2">
        <bodyTitle>IRT b<formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mo>&lt;</mo><mo>&gt;</mo></mrow></math></formula>com NeedleWare</bodyTitle>
        <participants>
          <person key="lagadic-2016-idp241920">
            <firstname>Hadrien</firstname>
            <lastname>Gurnel</lastname>
          </person>
          <person key="lagadic-2014-idm26496">
            <firstname>Alexandre</firstname>
            <lastname>Krupa</lastname>
          </person>
        </participants>
        <p>
          <i>no Inria Rennes 9072, duration: 36 months.</i>
        </p>
        <p>This project started in October 2016. It supports Hadrien Gurnel's Ph.D. about the study of a shared control strategy fusing haptic and ultrasound visual control for assisting manual steering of needles for biopsies or therapy purposes in a synergetic way.
</p>
      </subsection>
    </subsection>
    <subsection id="uid122" level="1">
      <bodyTitle>National Initiatives</bodyTitle>
      <subsection id="uid123" level="2">
        <bodyTitle>France Life Imaging WP3-FLI ANFEET</bodyTitle>
        <participants>
          <person key="lagadic-2014-idm26496">
            <firstname>Alexandre</firstname>
            <lastname>Krupa</lastname>
          </person>
        </participants>
        <p>
          <i>duration: 24 months. </i>
        </p>
        <p>This project started in January 2016. Its objective is to initiate collaborative research with the ICube laboratory (Strasbourg) on the control and supervision of flexible endoscopes in the digestive tube using ultrasound images.</p>
      </subsection>
      <subsection id="uid124" level="2">
        <bodyTitle>ANR Contint Visioland</bodyTitle>
        <participants>
          <person key="lagadic-2014-idp106480">
            <firstname>No√´l</firstname>
            <lastname>M√©riaux</lastname>
          </person>
          <person key="lagadic-2014-idp65680">
            <firstname>Patrick</firstname>
            <lastname>Rives</lastname>
          </person>
          <person key="lagadic-2014-idm27984">
            <firstname>Fran√ßois</firstname>
            <lastname>Chaumette</lastname>
          </person>
        </participants>
        <p>
          <i>no Inria Rennes 8304, duration: 48 months.</i>
        </p>
        <p>This project started in November 2013. It is composed of a consortium managed
by Onera in Toulouse with Airbus, Spikenet Technology, IRCCyN, and Lagadic.
Its aim is to develop vision-based localization and navigation techniques for autonomous landing on a runway (see Section¬†<ref xlink:href="#uid74" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>).</p>
      </subsection>
      <subsection id="uid125" level="2">
        <bodyTitle>ANR Contint Entracte</bodyTitle>
        <participants>
          <person key="mimetic-2014-idm25552">
            <firstname>Julien</firstname>
            <lastname>Pettr√©</lastname>
          </person>
        </participants>
        <p>
          <i>no Inria Rennes 8013, duration: 42 months.</i>
        </p>
        <p>This project started in November 2013. It is realized in collaboration with the
Gepetto group at Laas, Toulouse, and the Mimetic group at Irisa and Inria Rennes Bretagne Atlantique. It addresses the problem of motion planning for
anthropomorphic systems, and more generally, the problem of manipulation
path planning. ENTRACTE proposes to study in parallel both the mathematical
foundation of artificial motion and the neurocognitive structures used by
humans to quickly solve motion problems.</p>
      </subsection>
      <subsection id="uid126" level="2">
        <bodyTitle>ANR JCJC Percolation</bodyTitle>
        <participants>
          <person key="mimetic-2014-idm25552">
            <firstname>Julien</firstname>
            <lastname>Pettr√©</lastname>
          </person>
        </participants>
        <p>
          <i>no Inria Rennes 7991, duration: 42 months.</i>
        </p>
        <p>The ANR ‚ÄúJeune Chercheur‚Äù Percolation project started on January 2014. It aims at designing perception-based crowd simulation algorithms. We develop agents which are capable of perceiving their virtual environment through virtual sensors, and which are able to navigate in it, as well as to interact with the other agents.</p>
      </subsection>
      <subsection id="uid127" level="2">
        <bodyTitle>ANR JCJC SenseFly</bodyTitle>
        <participants>
          <person key="lagadic-2015-idp76432">
            <firstname>Thomas</firstname>
            <lastname>Bellavoir</lastname>
          </person>
          <person key="lagadic-2015-idp111800">
            <firstname>Muhammad</firstname>
            <lastname>Usman</lastname>
          </person>
          <person key="lagadic-2014-idp111520">
            <firstname>Riccardo</firstname>
            <lastname>Spica</lastname>
          </person>
          <person key="lagadic-2014-idp67120">
            <firstname>Paolo</firstname>
            <lastname>Robuffo Giordano</lastname>
          </person>
        </participants>
        <p>
          <i>no Irisa CNRS 50476, duration: 36 months.</i>
        </p>
        <p>The ANR ‚ÄúJeune Chercheur‚Äù SenseFly project started in August 2015. Its goal is to advance the state-of-the-art in multi-UAV in the design and implementation of fully decentralized and sensor-based group behaviors by only resorting to onboard sensing (mainly cameras and IMU) and local communication (e.g., Bluetooth communication, wireless networks). Topics such as individual flight control, formation control robust against sensor limitations (e.g., limited field of view, occlusions), distributed estimation of relative positions/bearings from local sensing, maintenance of architectural properties of a multi-UAV formation will be touched by the project. Part of the platforms described in Section¬†<ref xlink:href="#uid68" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> has been purchased thanks to this grant.</p>
      </subsection>
      <subsection id="uid128" level="2">
        <bodyTitle>ANR PLaTINUM</bodyTitle>
        <participants>
          <person key="lagadic-2014-idp80120">
            <firstname>Eduardo</firstname>
            <lastname>Fernandez Moral</lastname>
          </person>
          <person key="lagadic-2014-idp69816">
            <firstname>Vincent</firstname>
            <lastname>Drevelle</lastname>
          </person>
          <person key="lagadic-2014-idp65680">
            <firstname>Patrick</firstname>
            <lastname>Rives</lastname>
          </person>
        </participants>
        <p>
          <i>no Inria Sophia 10204, duration: 42 months.</i>
        </p>
        <p>This project started in November 2015. It is composed of a consortium managed
by Litis in Rouen with IGN Matis (Paris), Le2i (Le Creusot) and Lagadic group.
It aims at proposing novel solutions to robust long-term mapping of urban environments.</p>
      </subsection>
      <subsection id="uid129" level="2">
        <bodyTitle>BPI Romeo 2</bodyTitle>
        <participants>
          <person key="lagadic-2014-idp75040">
            <firstname>Giovanni</firstname>
            <lastname>Claudio</lastname>
          </person>
          <person key="lagadic-2014-idp92760">
            <firstname>Nicolas</firstname>
            <lastname>Cazy</lastname>
          </person>
          <person key="lagadic-2014-idp91504">
            <firstname>Suman Raj</firstname>
            <lastname>Bista</lastname>
          </person>
          <person key="lagadic-2014-idp72528">
            <firstname>Fabien</firstname>
            <lastname>Spindler</lastname>
          </person>
          <person key="lagadic-2014-idm27984">
            <firstname>Fran√ßois</firstname>
            <lastname>Chaumette</lastname>
          </person>
        </participants>
        <p>
          <i>no Inria Rennes 7114, duration: 60 months.</i>
        </p>
        <p>This project started in November 2012. It is composed of a large consortium managed
by Softbank Robotics (ex Aldebaran Robotics) with Laas in Toulouse, Isir in Paris, Lirmm in Montpellier, Inria groups Lagadic, Bipop (Pierre-Brice Wieber), Flowers (Pierre-Yves Oudeyer), and many other partners. It aims at developing advanced control and perception functionalities to a humanoid robot. In this project,
we are in charge of visual manipulation and navigation with Romeo and
Pepper. It supports in part Suman Raj Bista's Ph.D. about visual navigation (see Section¬†<ref xlink:href="#uid96" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>), as well as Nicolas Cazy's Ph.D. about model-based predictive control for visual servoing (see Section¬†<ref xlink:href="#uid84" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>).</p>
      </subsection>
      <subsection id="uid130" level="2">
        <bodyTitle>Equipex Robotex</bodyTitle>
        <participants>
          <person key="lagadic-2014-idp72528">
            <firstname>Fabien</firstname>
            <lastname>Spindler</lastname>
          </person>
          <person key="lagadic-2014-idm27984">
            <firstname>Fran√ßois</firstname>
            <lastname>Chaumette</lastname>
          </person>
        </participants>
        <p>
          <i>no Inria Rennes 6388, duration: 9 years.</i>
        </p>
        <p>Lagadic is one of the 15 French academic partners involved in the Equipex Robotex network thats started in February 2011. It is devoted to get and manage significative equipment in the main robotics labs in France. In the scope of this project,
we have got the humanoid robot Romeo (see Section¬†<ref xlink:href="#uid66" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>).</p>
      </subsection>
    </subsection>
    <subsection id="uid131" level="1">
      <bodyTitle>European Initiatives</bodyTitle>
      <subsection id="uid132" level="2">
        <bodyTitle>FP7 &amp; H2020 Projects</bodyTitle>
        <subsection id="uid133" level="3">
          <bodyTitle>FP7 Space RemoveDEBRIS</bodyTitle>
          <participants>
            <person key="lagadic-2014-idp78864">
              <firstname>Aur√©lien</firstname>
              <lastname>Yol</lastname>
            </person>
            <person key="lagadic-2014-idp71088">
              <firstname>Eric</firstname>
              <lastname>Marchand</lastname>
            </person>
            <person key="lagadic-2014-idm27984">
              <firstname>Fran√ßois</firstname>
              <lastname>Chaumette</lastname>
            </person>
          </participants>
          <sanspuceslist>
            <li id="uid134">
              <p noindent="true">Instrument: Specific Targeted Research Project</p>
            </li>
            <li id="uid135">
              <p noindent="true">Duration: October 2013 - September 2017</p>
            </li>
            <li id="uid136">
              <p noindent="true">Coordinator: University of Surrey (United Kingdom)</p>
            </li>
            <li id="uid137">
              <p noindent="true">Partners: Surrey Satellite Technology (United Kingdom), Airbus (Toulouse, France and Bremen, Germany), Isis (Delft, The Netherlands), CSEM (Neuch√¢tel, Switzerland), Stellenbosch University (South Africa).</p>
            </li>
            <li id="uid138">
              <p noindent="true">Inria contact: Fran√ßois Chaumette</p>
            </li>
            <li id="uid139">
              <p noindent="true">Abstract: The goal of this project is to validate model-based tracking algorithms on images acquired during an actual space debris removal mission (see
Section¬†<ref xlink:href="#uid73" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>).</p>
            </li>
          </sanspuceslist>
        </subsection>
        <subsection id="uid140" level="3">
          <bodyTitle>H2020 Comanoid</bodyTitle>
          <participants>
            <person key="lagadic-2016-idp184672">
              <firstname>Don Joven</firstname>
              <lastname>Agravante</lastname>
            </person>
            <person key="lagadic-2014-idp75040">
              <firstname>Giovanni</firstname>
              <lastname>Claudio</lastname>
            </person>
            <person key="lagadic-2014-idp77584">
              <firstname>Souriya</firstname>
              <lastname>Trinh</lastname>
            </person>
            <person key="lagadic-2014-idp72528">
              <firstname>Fabien</firstname>
              <lastname>Spindler</lastname>
            </person>
            <person key="lagadic-2014-idm27984">
              <firstname>Fran√ßois</firstname>
              <lastname>Chaumette</lastname>
            </person>
          </participants>
          <sanspuceslist>
            <li id="uid141">
              <p noindent="true">Title: Multi-contact Collaborative Humanoids in Aircraft Manufacturing</p>
            </li>
            <li id="uid142">
              <p noindent="true">Programm: H2020</p>
            </li>
            <li id="uid143">
              <p noindent="true">Duration: January 2015 - December 2018</p>
            </li>
            <li id="uid144">
              <p noindent="true">Coordinator: CNRS (Lirmm)</p>
            </li>
            <li id="uid145">
              <p noindent="true">Partners: Airbus Group (France), DLR (Germany), Universit√† Degli Studi di Roma La Sapienza (Italy), CNRS (I3S)</p>
            </li>
            <li id="uid146">
              <p noindent="true">Inria contact: Francois Chaumette</p>
            </li>
            <li id="uid147">
              <p noindent="true">COMANOID investigates the deployment of robotic solutions in well-identified Airbus airliner assembly operations that are laborious or tedious for human workers and for which access is impossible for wheeled or rail-ported robotic platforms. As a solution to these constraints a humanoid robot is proposed to achieve the described tasks in real-use cases provided by Airbus Group. At a first glance, a humanoid robotic solution appears extremely risky, since the operations to be conducted are in highly constrained aircraft cavities with non-uniform (cargo) structures. Furthermore, these tight spaces are to be shared with human workers. Recent developments, however, in multi-contact planning and control suggest that this is a much more plausible solution than current alternatives such as a manipulator mounted on multi-legged base. Indeed, if humanoid robots can efficiently exploit their surroundings in order to support themselves during motion and manipulation, they can ensure balance and stability, move in non-gaited (acyclic) ways through narrow passages, and also increase operational forces by creating closed-kinematic chains. Bipedal robots are well suited to narrow environments specifically because they are able to perform manipulation using only small support areas. Moreover, the stability benefits of multi-legged robots that have larger support areas are largely lost when the manipulator must be brought close, or even beyond, the support borders. COMANOID aims at assessing clearly how far the state-of-the-art stands from such novel technologies. In particular the project focuses on implementing a real-world humanoid robotics solution using the best of research and innovation. The main challenge will be to integrate current scientific and technological advances including multi-contact planning and control; advanced visual-haptic servoing; perception and localization; human-robot safety and the operational efficiency of cobotics solutions in airliner manufacturing.</p>
            </li>
          </sanspuceslist>
        </subsection>
        <subsection id="uid148" level="3">
          <bodyTitle>H2020 Romans</bodyTitle>
          <participants>
            <person key="lagadic-2015-idp80232">
              <firstname>Nicol√≤</firstname>
              <lastname>Pedemonte</lastname>
            </person>
            <person key="lagadic-2015-idp85296">
              <firstname>Firas</firstname>
              <lastname>Abi Farraj</lastname>
            </person>
            <person key="lagadic-2014-idp72528">
              <firstname>Fabien</firstname>
              <lastname>Spindler</lastname>
            </person>
            <person key="lagadic-2014-idm27984">
              <firstname>Fran√ßois</firstname>
              <lastname>Chaumette</lastname>
            </person>
            <person key="lagadic-2014-idp67120">
              <firstname>Paolo</firstname>
              <lastname>Robuffo Giordano</lastname>
            </person>
          </participants>
          <sanspuceslist>
            <li id="uid149">
              <p noindent="true">Title: Robotic Manipulation for Nuclear Sort and Segregation</p>
            </li>
            <li id="uid150">
              <p noindent="true">Programm: H2020</p>
            </li>
            <li id="uid151">
              <p noindent="true">Duration: May 2015 - April 2018</p>
            </li>
            <li id="uid152">
              <p noindent="true">Coordinator: University of Birmingham</p>
            </li>
            <li id="uid153">
              <p noindent="true">Partners: NLL (UK), CEA (France), Univ. Darmstadt (Germany)</p>
            </li>
            <li id="uid154">
              <p noindent="true">CNRS contact: Paolo Robuffo Giordano</p>
            </li>
            <li id="uid155">
              <p noindent="true">The RoMaNS (Robotic Manipulation for Nuclear Sort and Segregation) project will advance the state of the art in mixed autonomy for tele-manipulation, to solve a challenging and safety-critical ‚Äúsort and segregate‚Äù industrial problem, driven by urgent market and societal needs. Cleaning up the past half century of nuclear waste, in the UK alone (mostly at the Sellafield site), represents the largest environmental remediation project in the whole of Europe. Most EU countries face related challenges. Nuclear waste must be ‚Äúsorted and segregated‚Äù, so that low-level waste is placed in low-level storage containers, rather than occupying extremely expensive and resource intensive high-level storage containers and facilities. Many older nuclear sites (&gt;60 years in UK) contain large numbers of legacy storage containers, some of which have contents of mixed contamination levels, and sometimes unknown contents. Several million of these legacy waste containers must now be cut open, investigated, and their contents sorted. This can only be done remotely using robots, because of the high levels of radioactive material. Current state-of-the-art practice in the industry, consists of simple tele-operation (e.g. by joystick or teach-pendant). Such an approach is not viable in the long-term, because it is prohibitively slow for processing the vast quantity of material required. The project will: 1) Develop novel hardware and software solutions for advanced bi-lateral master-slave tele-operation. 2) Develop advanced autonomy methods for highly adaptive automatic grasping and manipulation actions. 3) Combine autonomy and tele-operation methods using state-of-the-art understanding of mixed initiative planning, variable autonomy and shared control approaches. 4) Deliver a TRL 6 demonstration in an industrial plant-representative environment at the UK National Nuclear Lab Workington test facility.</p>
            </li>
          </sanspuceslist>
        </subsection>
      </subsection>
      <subsection id="uid156" level="2">
        <bodyTitle>Collaborations with European Partners</bodyTitle>
        <participants>
          <person key="lagadic-2014-idp72528">
            <firstname>Fabien</firstname>
            <lastname>Spindler</lastname>
          </person>
          <person key="lagadic-2014-idm26496">
            <firstname>Alexandre</firstname>
            <lastname>Krupa</lastname>
          </person>
          <person key="lagadic-2014-idm27984">
            <firstname>Fran√ßois</firstname>
            <lastname>Chaumette</lastname>
          </person>
        </participants>
        <sanspuceslist>
          <li id="uid157">
            <p noindent="true">Project acronym: i-Process</p>
          </li>
          <li id="uid158">
            <p noindent="true">Project title: Innovative and Flexible Food Processing Technology in Norway</p>
          </li>
          <li id="uid159">
            <p noindent="true">Duration: January 2016 - December 2019</p>
          </li>
          <li id="uid160">
            <p noindent="true">Coordinator: Sintef (Norway)</p>
          </li>
          <li id="uid161">
            <p noindent="true">Other partners: Nofima, Univ. of Stavanger, NMBU, NTNU (Norway), DTU (Denmark), KU Leuven (Belgium), and about 10 Norwegian companies.</p>
          </li>
          <li id="uid162">
            <p noindent="true">Abstract:
This project is granted by the Norwegian Government. Its main objective is to develop novel concepts and methods for flexible and sustainable food processing in Norway. In the scope of this project, the Lagadic group is involved for visual tracking and visual servoing of generic and potentially deformable objects.
Prof. Pal Johan from the Norwegian University of Life Sciences (NMBU), and Ekrem Misimi from Sintef spent a short visit in June and October respectively.</p>
          </li>
        </sanspuceslist>
      </subsection>
    </subsection>
    <subsection id="uid163" level="1">
      <bodyTitle>International Initiatives</bodyTitle>
      <subsection id="uid164" level="2">
        <bodyTitle>Inria Associate Teams</bodyTitle>
        <subsection id="uid165" level="3">
          <bodyTitle>
            <ref xlink:href="http://people.rennes.inria.fr/Julien.Pettre/EASIMS/easims.html" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">SIMS </ref>
          </bodyTitle>
          <sanspuceslist>
            <li id="uid166">
              <p noindent="true">Title: Realistic and Efficient Simulation of Complex Systems</p>
            </li>
            <li id="uid167">
              <p noindent="true">International Partners:</p>
              <sanspuceslist>
                <li id="uid168">
                  <p noindent="true">University of North Carolina at Chapel Hill (USA) - GAMMA Group - Ming C. Lin, Dinesh Manocha</p>
                </li>
                <li id="uid169">
                  <p noindent="true">University of Minnesota (USA) - Motion Lab - Stephen Guy</p>
                </li>
                <li id="uid170">
                  <p noindent="true">Brown University (USA) - VenLab - William Warren</p>
                </li>
              </sanspuceslist>
            </li>
            <li id="uid171">
              <p noindent="true">Start year: 2012</p>
            </li>
            <li id="uid172">
              <p noindent="true">See <ref xlink:href="http://people.rennes.inria.fr/Julien.Pettre/EASIMS/easims.html" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>people.<allowbreak/>rennes.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>Julien.<allowbreak/>Pettre/<allowbreak/>EASIMS/<allowbreak/>easims.<allowbreak/>html</ref></p>
            </li>
            <li id="uid173">
              <p noindent="true">The general goal of SIMS is to make significant progress toward realistic and efficient simulation of highly complex systems, which raise combinatory explosive problems. This proposal is focused on human motion and interaction, and covers 3 active topics with wide application range:</p>
              <orderedlist>
                <li id="uid174">
                  <p noindent="true">Crowd simulation: virtual human interacting with other virtual humans,</p>
                </li>
                <li id="uid175">
                  <p noindent="true">Autonomous virtual humans interacting with their environment,</p>
                </li>
                <li id="uid176">
                  <p noindent="true">Physical simulation: real humans interacting with virtual environments.</p>
                </li>
              </orderedlist>
              <p>SIMS is orthogonally structured by transversal questions: the evaluation of the level of realism reached by a simulation (which is a problem by itself in the considered topics), considering complex systems at various scales (micro, meso and macroscopic ones), and facing combinatory explosion of simulation algorithms.</p>
            </li>
          </sanspuceslist>
        </subsection>
        <subsection id="uid177" level="3">
          <bodyTitle>
            <ref xlink:href="http://www.irisa.fr/lagadic/team/MarieBabel/ISI4NAVE/ISI4NAVE.html" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">ISI4NAVE </ref>
          </bodyTitle>
          <sanspuceslist>
            <li id="uid178">
              <p noindent="true">Title: Innovative Sensors and adapted Interfaces for assistive NAVigation and pathology Evaluation</p>
            </li>
            <li id="uid179">
              <p noindent="true">International Partner:</p>
              <sanspuceslist>
                <li id="uid180">
                  <p noindent="true">University College London (United Kingdom)
- Aspire CREATe - Tom Carlson</p>
                </li>
              </sanspuceslist>
            </li>
            <li id="uid181">
              <p noindent="true">Start year: 2016</p>
            </li>
            <li id="uid182">
              <p noindent="true">See <ref xlink:href="http://www.irisa.fr/lagadic/team/MarieBabel/ISI4NAVE/ISI4NAVE.html" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>www.<allowbreak/>irisa.<allowbreak/>fr/<allowbreak/>lagadic/<allowbreak/>team/<allowbreak/>MarieBabel/<allowbreak/>ISI4NAVE/<allowbreak/>ISI4NAVE.<allowbreak/>html</ref></p>
            </li>
            <li id="uid183">
              <p noindent="true">The global ageing population, along with disability compensation constitute major challenging societal and economic issues. In particular, achieving autonomy remains a fundamental need that contributes to the individual‚Äôs wellness and well-being. In this context, innovative and smart technologies are designed to achieve independence while matching user‚Äôs individual needs and desires.</p>
              <p>Hence, designing a robotic assistive solution related to wheelchair navigation remains of major importance as soon as it compensates partial incapacities. This project will then address the following two issues. First, the idea is to design an indoor / outdoor efficient obstacle avoidance system that respects the user intention, and does not alter user perception. This involves embedding innovative sensors to tackle the outdoor wheelchair navigation problem. The second objective is to take advantage of the proposed assistive tool to enhance the user Quality of Experience by means of biofeedback. Indeed, adapted interfaces should improve the understanding of people that suffer from cognitive and/or visual impairments.</p>
              <p>The originality of the project is to continuously integrate medical validation as well as clinical trials during the scientific research work in order to match user needs and acceptation.</p>
            </li>
          </sanspuceslist>
        </subsection>
      </subsection>
      <subsection id="uid184" level="2">
        <bodyTitle>Inria International Partners</bodyTitle>
        <subsection id="uid185" level="3">
          <bodyTitle>Informal International Partners</bodyTitle>
          <simplelist>
            <li id="uid186">
              <p noindent="true">Alexandre Krupa has a collaboration with Prof. Nassir Navab from the Technical University of Munich concerning the joint supervision of Pierre Chatelain's Ph.D. (see Section¬†<ref xlink:href="#uid91" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>).</p>
            </li>
          </simplelist>
        </subsection>
      </subsection>
      <subsection id="uid187" level="2">
        <bodyTitle>Participation in International Programs</bodyTitle>
        <p>The Lagadic group is one of the few external partners of the Australian Center for Robotic Vision (see¬†<ref xlink:href="http://roboticvision.org" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>roboticvision.<allowbreak/>org</ref>). It groups QUT in Brisbane, ANU in Canberra, Monash University and Adelaide University. In the scope of this project,
Peter Corke and Ben Upcroft spent a short visit in May 2016 while
Jurgen Leitner spent a 1-month visit in October 2016.</p>
      </subsection>
    </subsection>
    <subsection id="uid188" level="1">
      <bodyTitle>International Research Visitors</bodyTitle>
      <subsection id="uid189" level="2">
        <bodyTitle>Visits of International Scientists</bodyTitle>
        <simplelist>
          <li id="uid190">
            <p noindent="true">Nicolas Alt, senior researcher at Technical University of Munich (TUM) was a visiting scientist at Sophia Antipolis from Jan until Feb 2016. He worked on visuo-haptic environment perception.</p>
          </li>
          <li id="uid191">
            <p noindent="true">Alejandro Perez Yus, Ph.D. student at Universidad de Zaragoza, spent a 3-month visit in Sophia Antipolis from Sep until Nov 2016. He worked on the calibration of multi-camera RGB-D systems.</p>
          </li>
          <li id="uid192">
            <p noindent="true">Prof. Denis Wolf, associate professor at Univ. Sao Paulo, Brasil, spends a sabbatical year in Sophia Antipolis from Jul 2016 to Aug 2017. He works on semantic learning applied to intelligent vehicles.</p>
          </li>
          <li id="uid193">
            <p noindent="true">Nicola Battilani, Ph.D. student at University of Modena and Reggio Emilia, spent a 6-month visit in Rennes from May until Oct 2016. He worked on shared control algorithms for optimal 3D reconstruction from vision.</p>
          </li>
          <li id="uid194">
            <p noindent="true">Prof. Volkan Isler from University of Minnesota, Phillip Schmidt, Ph.D. student from DLR, Prof. Ivan Petrovic from Univ. of Zagreb, Prof. Purang Abolmaesumi from Univ. of British Columbia, Prof. Nassir Navab from Technical University of Munich, and Prof. Russ Taylor from John Hopkins University spent a short visit in the group in 2016.</p>
          </li>
        </simplelist>
      </subsection>
    </subsection>
  </partenariat>
  <diffusion id="uid195">
    <bodyTitle>Dissemination</bodyTitle>
    <subsection id="uid196" level="1">
      <bodyTitle>Promoting Scientific Activities</bodyTitle>
      <subsection id="uid197" level="2">
        <bodyTitle>Scientific Events Organization</bodyTitle>
        <subsection id="uid198" level="3">
          <bodyTitle>General Chair, Scientific Chair</bodyTitle>
          <simplelist>
            <li id="uid199">
              <p noindent="true">Marie Babel was the Scientific Chair of the workshop "Innovation Robotique et Sant√© : Assistance √† la conduite de fauteuil roulant" organized in Inria Rennes on December 2016 (150 participants).</p>
            </li>
          </simplelist>
        </subsection>
        <subsection id="uid200" level="3">
          <bodyTitle>Member of the Organizing Committees</bodyTitle>
          <simplelist>
            <li id="uid201">
              <p noindent="true">Fran√ßois Chaumette was in charge of organizing a Tutorial on Vision for Robotics at ICRA'2016: <ref xlink:href="http://www.icra2016.org/conference/tutorials" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>www.<allowbreak/>icra2016.<allowbreak/>org/<allowbreak/>conference/<allowbreak/>tutorials</ref>.
The plenary speakers were Peter Corke (QUT, Australia), Jana
Kosecka (George Mason University, US), Eric Marchand¬†<ref xlink:href="#lagadic-2016-bid66" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> and Fran√ßois Chaumette¬†<ref xlink:href="#lagadic-2016-bid67" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. Around 200¬†participants attended this tutorial.</p>
            </li>
            <li id="uid202">
              <p noindent="true">Patrick Rives was in charge of co-organizing the "Journ√©e Transports Intelligents " on behalf of the RFIA'2016 conference in Clermont-Ferrand: <ref xlink:href="http://rfia2016.iut-auvergne.com/index.php/rfp-ia" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>rfia2016.<allowbreak/>iut-auvergne.<allowbreak/>com/<allowbreak/>index.<allowbreak/>php/<allowbreak/>rfp-ia</ref></p>
            </li>
            <li id="uid203">
              <p noindent="true">Paolo Robuffo Giordano has co-organized the Invited Session ‚ÄúRigidity Theory for Problems in Multi-Agent Coordination‚Äù at the 54<sup>th</sup> IEEE Conf. on Decision and Control (CDC 2015), together by D. Zelazo (Technion, Israel) and A. Franchi (LAAS, France).</p>
            </li>
          </simplelist>
        </subsection>
      </subsection>
      <subsection id="uid204" level="2">
        <bodyTitle>Scientific Events Selection</bodyTitle>
        <subsection id="uid205" level="3">
          <bodyTitle>Chair of Conference Program Committees</bodyTitle>
          <simplelist>
            <li id="uid206">
              <p noindent="true">Paolo Robuffo Giordano: Workshop/Tutorial Session Chair for IROS 2016, Daejeon, Korea</p>
            </li>
          </simplelist>
        </subsection>
        <subsection id="uid207" level="3">
          <bodyTitle>Member of the Conference Program Committees</bodyTitle>
          <simplelist>
            <li id="uid208">
              <p noindent="true">Fran√ßois Chaumette: ICRA'2016</p>
            </li>
            <li id="uid209">
              <p noindent="true">Eric Marchand: ICRA 2016</p>
            </li>
            <li id="uid210">
              <p noindent="true">Patrick Rives: ICRA'2016, CVPR'2016</p>
            </li>
            <li id="uid211">
              <p noindent="true">Paolo Robuffo Giordano: ICRA'2016</p>
            </li>
          </simplelist>
        </subsection>
        <subsection id="uid212" level="3">
          <bodyTitle>Reviewer</bodyTitle>
          <simplelist>
            <li id="uid213">
              <p noindent="true">Marie Babel: IROS'2016 (2), ICIP'2016 (4)</p>
            </li>
            <li id="uid214">
              <p noindent="true">Fran√ßois Chaumette: IROS'2016 (1), ICRA'2017 (1)</p>
            </li>
            <li id="uid215">
              <p noindent="true">Vincent Drevelle: IFAC 2017 World Congress (1).</p>
            </li>
            <li id="uid216">
              <p noindent="true">Alexandre Krupa: IROS'2016 (1), ICRA'2017 (1)</p>
            </li>
            <li id="uid217">
              <p noindent="true">Eric Marchand: IROS 2016, ICRA 2017 (1), RFIA 2016 (2)</p>
            </li>
            <li id="uid218">
              <p noindent="true">Julien Pettr√©: SIGGRAPH 2016 (3), Collective Dynamics (1), CASA 2016 (3), IROS 2016 (3), ALIFE 2016 (2), ACM SCA 2016 (1), ACM CIE (1), TVCG (1), SIGGRAPH ASIA 2016 (1), Elsevier CAG (1), Pacific Graphics 2016 (1), Plos ONE (1), ACM MIG 2016 (2), IEEE VR 2017 (1), IEEE TBME (1), Taylor and Francis JMB (1), Eurographics 2017 (4), Elsevier TRB (1).</p>
            </li>
            <li id="uid219">
              <p noindent="true">Patrick Rives: ICRA'2016, ITSC'2016, CVPR'2016, ECCV'2016, IV'2016, ICINCO'2016, RFIA'2016, ICRA'2017</p>
            </li>
            <li id="uid220">
              <p noindent="true">Paolo Robuffo Giordano: ACC'2017 (1), DARS'2016 (2), ICRA'2017(2), IROS'2016 (3), RSS'2016 (1)</p>
            </li>
          </simplelist>
        </subsection>
      </subsection>
      <subsection id="uid221" level="2">
        <bodyTitle>Journal</bodyTitle>
        <subsection id="uid222" level="3">
          <bodyTitle>Member of the Editorial Boards</bodyTitle>
          <simplelist>
            <li id="uid223">
              <p noindent="true">Fran√ßois Chaumette: Editorial Board of the Int. Journal of Robotics Research, Senior Editor of the IEEE Robotics and Automation Letters, Editorial Board of the Springer Tracts in Advanced Robotics, Board Member of the Springer Encyclopedia of Robotics.</p>
            </li>
            <li id="uid224">
              <p noindent="true">Alexandre Krupa: Associate Editor of the IEEE Robotics and Automation Letters</p>
            </li>
            <li id="uid225">
              <p noindent="true">Eric Marchand: Associate Editor of the IEEE Robotics and Automation Letters</p>
            </li>
            <li id="uid226">
              <p noindent="true">Julien Pettr√©: Associate Editor for Computer Animation and Virtual Worlds, Associate Editor for Collective Dynamics</p>
            </li>
            <li id="uid227">
              <p noindent="true">Paolo Robuffo Giordano: Associate Editor of the IEEE Transactions on Robotics</p>
            </li>
          </simplelist>
        </subsection>
        <subsection id="uid228" level="3">
          <bodyTitle>Reviewer - Reviewing Activities</bodyTitle>
          <simplelist>
            <li id="uid229">
              <p noindent="true">Marie Babel: IEEE Trans. on Human-Machine Systems (1)</p>
            </li>
            <li id="uid230">
              <p noindent="true">Fran√ßois Chaumette: IEEE Trans. on Robotics (2), IEEE/ASME Trans. on Mechatronics (1), Robotics and Autonomous Systems (1), Journal of Intelligent and Robotic Systems (1), Control Engineering Practice (1)</p>
            </li>
            <li id="uid231">
              <p noindent="true">Vincent Drevelle: Transportation Research Part C (1)</p>
            </li>
            <li id="uid232">
              <p noindent="true">Eric Marchand : Int. Journal of robotics Research (1), Software, Practice and Experience (1), IEEE Trans. on Visualization and Computer Graphics (1), Visual Computer (1)</p>
            </li>
            <li id="uid233">
              <p noindent="true">Patrick Rives: IEEE Robotics and Automation Letters (1), Robotics and Autonomous Systems (1)</p>
            </li>
            <li id="uid234">
              <p noindent="true">Paolo Robuffo Giordano: IEEE Robotics and Automation letters (2), IEEE Trans. on Control of Network Systems (1), IEEE Trans. on Haptics (1)</p>
            </li>
          </simplelist>
        </subsection>
      </subsection>
      <subsection id="uid235" level="2">
        <bodyTitle>Invited Talks</bodyTitle>
        <simplelist>
          <li id="uid236">
            <p noindent="true">Fran√ßois Chaumette: Plenary talk at RCAR'2016, Angkor Wat, Cambodia¬†<ref xlink:href="#lagadic-2016-bid68" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
          </li>
          <li id="uid237">
            <p noindent="true">Paolo Robuffo Giordano. Invited Talk: Collective Control, State Estimation and Human Interaction for Quadrotors in Unstructured Environments. 2016 GIS Micro-Drones Day, ENAC, Toulouse, France, October 2016</p>
          </li>
          <li id="uid238">
            <p noindent="true">Paolo Robuffo Giordano. Invited Seminar: Estimation and Control for Multi-Robot Systems. 2016 IEEE RAS Summer School on Multi-Robot Systems, Singapore, June 2016</p>
          </li>
        </simplelist>
      </subsection>
      <subsection id="uid239" level="2">
        <bodyTitle>Leadership within the Scientific Community</bodyTitle>
        <simplelist>
          <li id="uid240">
            <p noindent="true">Fran√ßois Chaumette is a 2016-2019 elected member of the Administrative Committee of the IEEE Robotics and Automation Society. He is also a member of the Scientific Council of the CNRS INS2I.</p>
          </li>
          <li id="uid241">
            <p noindent="true">Fran√ßois Chaumette and Patrick Rives are members of the scientific council of the ‚ÄúGdR Robotique‚Äù.</p>
          </li>
        </simplelist>
      </subsection>
      <subsection id="uid242" level="2">
        <bodyTitle>Scientific Expertise</bodyTitle>
        <simplelist>
          <li id="uid243">
            <p noindent="true">Fran√ßois Chaumette is vice-president of the ANR Tremplin-ERC program (in charge of providing grants to recipients of non-funded A and B Starting and Consolidator ERC proposals). He was also a member of the ‚ÄúInstitut Universitaire de France (IUF)‚Äù selection committee in charge of evaluating senior proposals, and served in the jury to select an Irstea senior researcher (DR2).</p>
          </li>
          <li id="uid244">
            <p noindent="true">Julien Pettr√© is project proposal reviewer for the Netherlands Organisation for Scientific Research.</p>
          </li>
          <li id="uid245">
            <p noindent="true">Paolo Robuffo Giordano is a reviewer for EU FP7 projects, for the ANR (French National Research Agency) ASTRID 2016 Program, for the ‚ÄúComit√© ECOS Nord‚Äù of the ‚ÄúMinist√®re des affaires √©trang√®res et du d√©veloppement international‚Äù and the ‚ÄúMinist√®re de l'Enseignement sup√©rieur et de la Recherche‚Äù,
and for the DGA as expert on algorithms and sensors for drones.</p>
          </li>
        </simplelist>
      </subsection>
      <subsection id="uid246" level="2">
        <bodyTitle>Research Administration</bodyTitle>
        <simplelist>
          <li id="uid247">
            <p noindent="true">Fran√ßois Chaumette serves as the president of the committee in charge of all the temporary recruitments (‚ÄúCommission Personnel‚Äù) at Inria Rennes-Bretagne Atlantique and Irisa. He is also a member of the Head team of Inria Rennes-Bretagne Atlantique.</p>
          </li>
          <li id="uid248">
            <p noindent="true">Alexandre Krupa and Julien Pettr√© are members of the CUMIR (‚ÄúCommission des Utilisateurs des Moyens Informatiques pour la Recherche‚Äù) of Inria Rennes-Bretagne Atlantique.</p>
          </li>
          <li id="uid249">
            <p noindent="true">Eric Marchand served as secretary in the board of the ‚ÄúAssociation Fran√ßaise pour la Reconnaissance et l'Interpr√©tation des Formes (AFRIF)‚Äù. He is also in charge of the Irisa Ph.D. students in the committee in charge of all the temporary recruitments (‚ÄúCommission Personnel‚Äù) at Inria Rennes-Bretagne Atlantique and Irisa. He is in the board of the ‚ÄúP√¥le Images et R√©seaux‚Äù and in the board of ‚ÄúEcole doctorale Matisse‚Äù.</p>
          </li>
          <li id="uid250">
            <p noindent="true">Julien Pettr√© is an elected member of the ‚ÄúComit√© de Centre‚Äù at Inria Rennes-Bretagne Atlantique.</p>
          </li>
          <li id="uid251">
            <p noindent="true">Patrick Rives is a member of the ‚ÄúComit√© des projets‚Äù and ‚ÄúComit√© de Centre‚Äù at Inria Sophia Antipolis-M√©diterran√©e.</p>
          </li>
        </simplelist>
      </subsection>
    </subsection>
    <subsection id="uid252" level="1">
      <bodyTitle>Teaching - Supervision - Juries</bodyTitle>
      <subsection id="uid253" level="2">
        <bodyTitle>Teaching</bodyTitle>
        <p>Marie Babel:</p>
        <sanspuceslist>
          <li id="uid254">
            <p noindent="true">Master INSA2: ‚ÄúRobotics‚Äù, 26 hours, M1, INSA Rennes</p>
          </li>
          <li id="uid255">
            <p noindent="true">Master INSA1: ‚ÄúArchitecture‚Äù, 30 hours, L3, INSA Rennes</p>
          </li>
          <li id="uid256">
            <p noindent="true">Master INSA2: ‚ÄúComputer science project‚Äù, 30 hours, M1, INSA Rennes</p>
          </li>
          <li id="uid257">
            <p noindent="true">Master INSA2: ‚ÄúImage analysis‚Äù, 18 hours, M1, INSA Rennes</p>
          </li>
          <li id="uid258">
            <p noindent="true">Master INSA1: ‚ÄúRemedial math courses‚Äù, 50 hours, L3, INSA Rennes</p>
          </li>
        </sanspuceslist>
        <p>Fran√ßois Chaumette:</p>
        <sanspuceslist>
          <li id="uid259">
            <p noindent="true">Master ESIR3: ‚ÄúVisual servoing‚Äù, 8 hours, M2, Ecole sup√©rieure d'ing√©nieurs de Rennes</p>
          </li>
        </sanspuceslist>
        <p>Vincent Drevelle:</p>
        <sanspuceslist>
          <li id="uid260">
            <p noindent="true">Master ESIR2: ‚ÄúReal-time systems and RTOS‚Äù, 24 hours, M1, Esir Rennes</p>
          </li>
          <li id="uid261">
            <p noindent="true">Master GLA: ‚ÄúTerrain information systems‚Äù, 14 hours, M2, Universit√© de Rennes 1</p>
          </li>
          <li id="uid262">
            <p noindent="true">Master Info: ‚ÄúArtificial intelligence‚Äù, 12 hours, M1, Universit√© de Rennes 1</p>
          </li>
          <li id="uid263">
            <p noindent="true">Licence Info: ‚ÄúComputer systems architecture‚Äù, 20 hours, L1, Universit√© de Rennes 1</p>
          </li>
          <li id="uid264">
            <p noindent="true">Licence Miage: ‚ÄúComputer programming‚Äù, 78 hours, M1, Universit√© de Rennes 1</p>
          </li>
          <li id="uid265">
            <p noindent="true">Master CTS: ‚ÄúInstrumentation, localization, GPS‚Äù, 4 hours, M2, Universit√© de Rennes 1</p>
          </li>
          <li id="uid266">
            <p noindent="true">Licence and Master ET: ‚ÄúElectronics project‚Äù, 23 hours, L3 and M1, Universit√© de Rennes 1</p>
          </li>
        </sanspuceslist>
        <p>Alexandre Krupa:</p>
        <sanspuceslist>
          <li id="uid267">
            <p noindent="true">Master SIBM (Signals and Images in Biology and Medicine): ‚ÄúMedical robotics guided from images‚Äù, 4.5 hours, M2, Universit√© de Rennes 1, Brest and Angers</p>
          </li>
          <li id="uid268">
            <p noindent="true">Master FIP TIC-Sant√©: ‚ÄúUltrasound visual servoing‚Äù, 6 hours, M2, T√©l√©com Physique Strasbourg</p>
          </li>
          <li id="uid269">
            <p noindent="true">Master INSA3: ‚ÄúModeling and engineering for Biology and Health applications‚Äù, 12 hours, M2, INSA Rennes</p>
          </li>
          <li id="uid270">
            <p noindent="true">Master ESIR3: ‚ÄúUltrasound visual servoing‚Äù, 9 hours, M2, Esir Rennes</p>
          </li>
        </sanspuceslist>
        <p>Eric Marchand:</p>
        <sanspuceslist>
          <li id="uid271">
            <p noindent="true">Master Esir2: ‚ÄúColorimetry‚Äù, 24 hours, M1, Esir Rennes</p>
          </li>
          <li id="uid272">
            <p noindent="true">Master Esir2: ‚ÄúComputer vision: geometry‚Äù, 24 hours, M1, Esir Rennes</p>
          </li>
          <li id="uid273">
            <p noindent="true">Master Esir3: ‚ÄúSpecial effects‚Äù, 24 hours, M2, Esir Rennes</p>
          </li>
          <li id="uid274">
            <p noindent="true">Master Esir3: ‚ÄúComputer vision: tracking and recognition‚Äù, 24 hours, M2, Esir Rennes</p>
          </li>
          <li id="uid275">
            <p noindent="true">Master MRI: ‚ÄúComputer vision‚Äù, 24 hours, M2, Universit√© de Rennes¬†1</p>
          </li>
          <li id="uid276">
            <p noindent="true">Master MIA: ‚ÄúAugmented reality‚Äù, 4 hours, M2, Universit√© de Rennes¬†1</p>
          </li>
        </sanspuceslist>
        <p>Julien Pettr√©:</p>
        <sanspuceslist>
          <li id="uid277">
            <p noindent="true">Licence Info : ‚ÄúProgrammation Informatique‚Äù, 40, LI1, Universit√© de Rennes 1, Rennes</p>
          </li>
          <li id="uid278">
            <p noindent="true">INSA1: ‚ÄúProgrammation Informatique‚Äù, 40 hours, INSA Rennes</p>
          </li>
        </sanspuceslist>
      </subsection>
      <subsection id="uid279" level="2">
        <bodyTitle>Supervision</bodyTitle>
        <simplelist>
          <li id="uid280">
            <p noindent="true">Ph.D.: Le Cui, ‚ÄúRobust micro/nano-positioning by visual servoing‚Äù, defended on January¬†2016, supervised by Eric Marchand¬†<ref xlink:href="#lagadic-2016-bid35" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
          </li>
          <li id="uid281">
            <p noindent="true">Ph.D.: Vishnu Karakkat Narayanan, ‚ÄúSemi-autonomous navigation of a wheelchair by visual servoing and user intention analysis‚Äù, defended in November¬†2016, supervised by Marie Babel and Anne Spalanzani (Chroma group at Inria Rh√¥ne-Alpes)¬†<ref xlink:href="#lagadic-2016-bid53" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
          </li>
          <li id="uid282">
            <p noindent="true">Ph.D.: Nicolas Cazy, ‚ÄúCommande pr√©dictive pour la r√©alisation de t√¢ches d'asservissement visuel successives‚Äù, defended in November¬†2016, supervised by Paolo Robuffo Giordano, Fran√ßois Chaumette and Pierre-Brice Wieber (Bipop group at Inria Rh√¥ne-Alpes)¬†<ref xlink:href="#lagadic-2016-bid46" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
          </li>
          <li id="uid283">
            <p noindent="true">Ph.D.: Julien Bruneau, ‚ÄúStudying and modeling complex interactions for crowd simulation‚Äù, defended in November¬†2016, supervised by Julien Pettr√© and Anne-H√©l√®ne Olivier (Mimetic group at Inria Rennes-Bretagne Atlantique and Irisa)¬†<ref xlink:href="#lagadic-2016-bid59" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
          </li>
          <li id="uid284">
            <p noindent="true">Ph.D.: Aly Magassouba, ‚ÄúAural servo: towards an alternative approach to sound localization for robot motion control‚Äù, defended in December 2016, supervised by Fran√ßois Chaumette and Nancy Bertin (Panama group at Inria Rennes-Bretagne Atlantique and Irisa)</p>
          </li>
          <li id="uid285">
            <p noindent="true">Ph.D.: Lucas Royer, ‚ÄúReal-time tracking of deformable targets in 3D ultrasound sequences‚Äù, defended in December 2016, supervised by Alexandre Krupa, Maud Marchal (Hybrid group at Inria Rennes-Bretagne Atlantique and Irisa) and Guillaume Dardenne (IRT b<formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mo>&lt;</mo><mo>&gt;</mo></mrow></math></formula>com)</p>
          </li>
          <li id="uid286">
            <p noindent="true">Ph.D.: Pierre Chatelain, ‚ÄúQuality-driven control of a robotized ultrasound probe‚Äù, defended in December 2016, supervised by Alexandre Krupa and Nassir Navab (Technische Universit√§t M√ºnchen)¬†<ref xlink:href="#lagadic-2016-bid47" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
          </li>
          <li id="uid287">
            <p noindent="true">Ph.D.: Suman Raj Bista, ‚ÄúIndoor Navigation of Mobile Robots based on Visual Memory and Image-Based Visual Servoing‚Äù, defended in December 2016, supervised by Paolo Robuffo Giordano and Fran√ßois Chaumette¬†<ref xlink:href="#lagadic-2016-bid50" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
          </li>
          <li id="uid288">
            <p noindent="true">Ph.D. in progress: Renato Jos√© Martins, ‚ÄúRobust navigation and control of an autonomous vehicle‚Äù, started in November 2013, supervised by Patrick Rives
and Samuel Siqueira Bueno (CTI)</p>
          </li>
          <li id="uid289">
            <p noindent="true">Ph.D. in progress: Jason Chevrie, ‚ÄúControl of a flexible needle by visual servoing using B-mode ultrasound images‚Äù, started in September 2014, supervised by Alexandre Krupa and Marie Babel</p>
          </li>
          <li id="uid290">
            <p noindent="true">Ph.D. in progress: Quentin Bateux, ‚ÄúVisual servoing from global descriptors‚Äù, started in October¬†2014, supervised by Eric Marchand</p>
          </li>
          <li id="uid291">
            <p noindent="true">Ph.D. in progress: Fabrizio Schiano, ‚ÄúCollective control with onboard sensors for multiple quadrotor UAVs‚Äù, started in October 2014, supervised by Paolo Robuffo Giordano</p>
          </li>
          <li id="uid292">
            <p noindent="true">Ph.D. in progress: Pedro Patlan-Rosales, ‚ÄúEnhancement of ultrasound elastography by visual servoing and force control‚Äù, started in October 2014, supervised by Alexandre Krupa</p>
          </li>
          <li id="uid293">
            <p noindent="true">Ph.D. in progress: No√´l M√©riaux, ‚ÄúLanding by visual servoing‚Äù, started in October¬†2014, supervised by Fran√ßois Chaumette, Eric Marchand and Patrick Rives</p>
          </li>
          <li id="uid294">
            <p noindent="true">Ph.D. in progress: Lesley-Ann Duflot, ‚ÄúVisual servoing using shearlet transform‚Äù, started in November 2014, supervised by Alexandre Krupa and Brahim Tamadazte (Minarob group at FEMTO-ST, Besan√ßon)</p>
          </li>
          <li id="uid295">
            <p noindent="true">Ph.D. in progress: Firas Abi Farraj, ‚ÄúShared Control Architectures for Visual Servoing Tasks‚Äù, started in October¬†2015, supervised by Paolo Robuffo Giordano</p>
          </li>
          <li id="uid296">
            <p noindent="true">Ph.D. in progress: Salma Jiddi, ‚ÄúAnalyses g√©om√©trique et photom√©trique pour des applications de r√©alit√© mixte‚Äù, started in October¬†2015, supervised by Eric Marchand and Philippe Robert (Technicolor)</p>
          </li>
          <li id="uid297">
            <p noindent="true">Ph.D. in progress: Ide Flore Kenmogne Fokam, ‚ÄúCooperative localization in multi-robot fleets using interval analysis‚Äù, started in October¬†2015, supervised Vincent Drevelle and Eric Marchand</p>
          </li>
          <li id="uid298">
            <p noindent="true">Ph.D. in progress: Bryan Penin ‚ÄúModel predictive visual servoing for UAVS‚Äù, started in October¬†2015, supervised by Paolo Robuffo Giordano and Fran√ßois Chaumette</p>
          </li>
          <li id="uid299">
            <p noindent="true">Ph.D. in progress: Guillaume Cortes, ‚ÄúMotion Capture‚Äù, started in October¬†2015, supervised Eric Marchand and Anatole Lecuyer.</p>
          </li>
          <li id="uid300">
            <p noindent="true">Ph.D. in progress: Muhammad Usman, ‚ÄúRobust Vision-Based Navigation for Quadrotor UAVs‚Äù, started in October¬†2015, supervised by Paolo Robuffo Giordano</p>
          </li>
          <li id="uid301">
            <p noindent="true">Ph.D. in progress: Louise Devigne, ‚ÄúContribution d'une aide technique robotique √† l'√©valuation de pathologies neurologiques : Application √† la navigation d'un fauteuil roulant‚Äù, started in November¬†2015, supervised by Marie Babel and Philippe Gallien (P√¥le Saint H√©lier)</p>
          </li>
          <li id="uid302">
            <p noindent="true">Ph.D. in progress: Quentin Delamare, ‚ÄúAlgorithmes d'estimation et de commande pour des quadrirotors en interaction physique avec l'environnement‚Äù, started in September 2016, supervised by Paolo Robuffo Giordano</p>
          </li>
          <li id="uid303">
            <p noindent="true">Ph.D. in progress: Axel Lopes, ‚ÄúData assimilation for synthetic vision-based crowd simulation algorithms‚Äù, started in October 2016, supervised by Julien Pettr√© and Fran√ßois Chaumette</p>
          </li>
          <li id="uid304">
            <p noindent="true">Ph.D. in progress: Aline Baudry, ‚ÄúContribution √† la mod√©lisation des fauteuils roulants pour l'am√©lioration de leur navigation en mode semi-autonome‚Äù, started in October 2016, supervised by Marie Babel and Sylvain Gu√©gan (Mechanical Engineering Dpt/LGCGM at Insa Rennes)</p>
          </li>
          <li id="uid305">
            <p noindent="true">Ph.D. in progress: Hadrien Gurnel, ‚ÄúShared control of a biopsie needle from haptic and ultrasound visual feedback‚Äù, started in October 2016, supervised by Alexandre Krupa and Maud Marchal (Hybrid group at Inria Rennes-Bretagne Atlantique and Irisa)</p>
          </li>
          <li id="uid306">
            <p noindent="true">Ph.D. in progress: Dayana Hassan, ‚ÄúPlate-forme robotis√©e d'assistance aux personnes √† mobilit√© r√©duite‚Äù, started in November 2016, supervised by Paolo Salaris, Patrick Rives and Frank Anjeaux (Axyn robotique)</p>
          </li>
          <li id="uid307">
            <p noindent="true">Internship: Valentin Bureau from May 2016 until Sep 2016 (2 months in mobility in University College of London within ISI4NAVE associated team), M1/Second
year of Computer Science department, INSA Rennes, supervised by Marie Babel</p>
          </li>
          <li id="uid308">
            <p noindent="true">Internship: Timoth√©e Collard ‚ÄúAdapting a localization application to ROS and MAV navigation‚Äù from Jun 2016 until Aug 2016, M1, Universit√© de Rennes 1, supervised by Vincent Drevelle</p>
          </li>
          <li id="uid309">
            <p noindent="true">Internship: Benjamin Fasquelle from May 2016 until Aug 2016, M1, ENS Rennes, supervised by Eric Marchand</p>
          </li>
          <li id="uid310">
            <p noindent="true">Benoit Heintz from Mar 2016 until Jul 2016, L2 level, ENSIL Limoges, supervised by Fabien Spindler and Giovanni Claudio</p>
          </li>
          <li id="uid311">
            <p noindent="true">Internship: Manutea Huang from Jun 2016 until Sep 2016, L3/First year of Computer Science department, INSA Rennes, supervised by Marie Babel</p>
          </li>
          <li id="uid312">
            <p noindent="true">Internship: Daniel Huc from May 2016 until Sep 2016, M2, IM2AG Grenoble, supervised by Julien Pettr√©</p>
          </li>
          <li id="uid313">
            <p noindent="true">Internship: Valentin Limantour from Sep 2016 until December 2016, M1, ENIB Brest, supervised by Paolo Robuffo Giordano</p>
          </li>
          <li id="uid314">
            <p noindent="true">Internship: Etienne Moisdon from May 2016 until Jun 2016, M, Univ. Rennes¬†1, supervised by Julien Pettr√©</p>
          </li>
        </simplelist>
      </subsection>
      <subsection id="uid315" level="2">
        <bodyTitle>External Ph.D. and HdR Juries</bodyTitle>
        <simplelist>
          <li id="uid316">
            <p noindent="true">Fran√ßois Chaumette: Julien Bruneau (Ph.D., president, Inria Rennes),
Andrea Cherubini (HdR, president, Lirmm, Montpellier)</p>
          </li>
          <li id="uid317">
            <p noindent="true">Alexandre Krupa: Laure-Anais Chanel (Ph.D., reviewer, ICube, Strasbourg),
Mouloud Ourak (Ph.D., reviewer, FEMTO-ST, Besan√ßon),
Paul Mignon (Ph.D., reviewer, TIMC-IMAG, Grenoble)</p>
          </li>
          <li id="uid318">
            <p noindent="true">Eric Marchand: Vishnu Karakkat Narayanan (Ph.D., president, Inria Rennes), Suman Raj Bista (Ph.D., president, Inria Rennes), Limming Yang (Ph.D., reviewer, EC Nantes), Tu-Hoa Pham (Ph.D., reviewer, Lirmm, Montpellier).</p>
          </li>
          <li id="uid319">
            <p noindent="true">Julien Pettr√©: Christian Vassallo (Ph.D., Laas, Toulouse),
Fabien Ciss√© (Ph.D., UPMC, Paris), Thomas Pitiot (PH.D., ICube, Strasbourg)</p>
          </li>
          <li id="uid320">
            <p noindent="true">Patrick Rives: Zui Tao (Ph.D., president, UTC, Compi√®gne), Bruno Ricaud (Ph.D., reviewer, Ecole des Mines, Paris ), Victor Gibert (Ph.D., member, IRCCyN, Nantes), Yue Kang (Ph.D., reviewer, UTC, Compi√®gne), Abdelhamid Dine (Ph.D., president, ENS-Cachan, Paris-Saclay), Pierre Merriaux (Ph.D., reviewer, Esigelec, Rouen), Fabrice Mayran de Chamiso (Ph.D., member, CEA-LIST, Paris-Saclay), Bruno Vallet (HdR, member, IGN, Paris).</p>
          </li>
          <li id="uid321">
            <p noindent="true">Paolo Robuffo Giordano: Osamah Saif (Ph.D., UTC, Compi√®gne, France), Marco Aggravi (Ph.D., University of Siena, Italy), Leonardo Meli (Ph.D., University of Siena, Italy).</p>
          </li>
        </simplelist>
      </subsection>
    </subsection>
    <subsection id="uid322" level="1">
      <bodyTitle>Popularization</bodyTitle>
      <simplelist>
        <li id="uid323">
          <p noindent="true">Due to the visibility of our experimental platforms, the team is often requested to present its research activities to students, researchers or industry. Our panel of demonstrations allows us to highlight recent results concerning the positioning of an ultrasound probe by visual servoing, grasping and dual arm manipulation by Romeo, vision-based shared control using our haptic device for object manipulation, the control of a fleet of quadrotors, vision-based detection and tracking for space navigation in a rendezvous context, the semi-autonomous navigation of a wheelchair, and augmented reality applications. Some of these demonstrations are available as videos on VispTeam YouTube channel (<ref xlink:href="https://www.youtube.com/user/VispTeam/videos" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>www.<allowbreak/>youtube.<allowbreak/>com/<allowbreak/>user/<allowbreak/>VispTeam/<allowbreak/>videos</ref>). This year there were among others, demonstrations organized for the HCERES expert committee that evaluated IRISA, about twenty people affiliated to the CNRS electronics network, students of the ‚ÄúInnovation et entreprenariat‚Äù Master, those of the L3 R&amp;I at ENS Rennes, about twenty students from the ‚ÄúEcole des Mines de Nancy‚Äù, several classes of high school students around Rennes, without forgetting the members of the Ph.D. thesis juries.</p>
        </li>
        <li id="uid324">
          <p noindent="true">Fabien Spindler and Giovanni Claudio were interviewed by TV Rennes about Pepper robot (<ref xlink:href="https://www.facebook.com/166100743494694/videos/vb.166100743494694/959489554155805/?type=2&amp;theater" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>www.<allowbreak/>facebook.<allowbreak/>com/<allowbreak/>166100743494694/<allowbreak/>videos/<allowbreak/>vb.<allowbreak/>166100743494694/<allowbreak/>959489554155805/<allowbreak/>?type=2&amp;theater</ref>).</p>
        </li>
        <li id="uid325">
          <p noindent="true">Marie Babel participated to ‚Äú15<sup>e</sup> Journ√©e nationale des p√¥les de comp√©titivit√©‚Äù in March 2016 (Paris): HandiViz project was selected by the French Ministry of Finance</p>
        </li>
        <li id="uid326">
          <p noindent="true">Marie Babel participated to the Science Festival in October 2016 with an interview at ‚ÄúVillage des Sciences‚Äù and a workshop for general public organized in Acign√© near Rennes.</p>
        </li>
        <li id="uid327">
          <p noindent="true">Marie Babel participated to the ‚ÄúConvention Nationale des SATT‚Äù (October 2016, Paris).</p>
        </li>
        <li id="uid328">
          <p noindent="true">Marie Babel gave a talk on "HandiViz: a new driving experience" in October 2016, during the ‚ÄúSemaine des Technologies - Robotique et sant√©‚Äù organized at Insa Rennes.</p>
        </li>
        <li id="uid329">
          <p noindent="true">Vincent Drevelle participated to the ‚ÄúJourn√©e science et musique‚Äù in Rennes, with an interactive demonstration of sound-based tracking of a micro aerial vehicle with a beam projector (in cooperation with the Panama team).</p>
        </li>
        <li id="uid330">
          <p noindent="true">An article related to the research activity of Alexandre Krupa on robotic needle steering entitled ‚ÄúUn robot qui apprend √† viser‚Äù has been published in March 2016 in the general-audience magazine ‚ÄúSciences Ouest‚Äù: <ref xlink:href=" http://www.espace-sciences.org/sciences-ouest/340/dossier/un-robot-qui-apprend-a-viser" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"> http://<allowbreak/>www.<allowbreak/>espace-sciences.<allowbreak/>org/<allowbreak/>sciences-ouest/<allowbreak/>340/<allowbreak/>dossier/<allowbreak/>un-robot-qui-apprend-a-viser</ref>.</p>
        </li>
        <li id="uid331">
          <p noindent="true">Paolo Robuffo Giordano has given press releases on the activities involving formation control for multiple quadrotor UAVs to √âmergences Inria, Sciences Ouest, and ‚ÄúIndustrie &amp; Technologies‚Äù.
</p>
        </li>
      </simplelist>
    </subsection>
  </diffusion>
  <biblio id="bibliography" html="bibliography" numero="10" titre="Bibliography">
    
    <biblStruct id="lagadic-2016-bid0" type="incollection" rend="refer" n="refercite:Chaumette08Handbook">
      <identifiant type="hal" value="hal-00920414"/>
      <analytic>
        <title level="a">Visual servoing and visual tracking</title>
        <author>
          <persName key="lagadic-2014-idm27984">
            <foreName>Fran√ßois</foreName>
            <surname>Chaumette</surname>
            <initial>F.</initial>
          </persName>
          <persName>
            <foreName>Seth</foreName>
            <surname>Hutchinson</surname>
            <initial>S.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <editor role="editor">
          <persName>
            <foreName>B.</foreName>
            <surname>Siciliano</surname>
            <initial>B.</initial>
          </persName>
          <persName>
            <foreName>O.</foreName>
            <surname>Khatib</surname>
            <initial>O.</initial>
          </persName>
        </editor>
        <title level="m">Handbook of Robotics</title>
        <imprint>
          <biblScope type="chapter">24</biblScope>
          <publisher>
            <orgName>Springer</orgName>
          </publisher>
          <dateStruct>
            <year>2008</year>
          </dateStruct>
          <biblScope type="pages">563-583</biblScope>
          <ref xlink:href="http://hal.inria.fr/hal-00920414/en" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-00920414/<allowbreak/>en</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid38" type="article" rend="refer" n="refercite:Comport06b">
      <identifiant type="hal" value="inria-00161250"/>
      <analytic>
        <title level="a">Real-time markerless tracking for augmented reality: the virtual visual servoing framework</title>
        <author>
          <persName>
            <foreName>Andrew</foreName>
            <surname>Comport</surname>
            <initial>A.</initial>
          </persName>
          <persName key="lagadic-2014-idp71088">
            <foreName>Eric</foreName>
            <surname>Marchand</surname>
            <initial>E.</initial>
          </persName>
          <persName>
            <foreName>Muriel</foreName>
            <surname>Pressigout</surname>
            <initial>M.</initial>
          </persName>
          <persName key="lagadic-2014-idm27984">
            <foreName>Fran√ßois</foreName>
            <surname>Chaumette</surname>
            <initial>F.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">IEEE Trans. on Visualization and Computer Graphics</title>
        <imprint>
          <biblScope type="volume">12</biblScope>
          <biblScope type="number">4</biblScope>
          <dateStruct>
            <month>July</month>
            <year>2006</year>
          </dateStruct>
          <biblScope type="pages">615‚Äì628</biblScope>
          <ref xlink:href="https://hal.inria.fr/inria-00161250" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>inria-00161250</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid49" type="article" rend="refer" n="refercite:dame:2012:hal-00750528:1">
      <identifiant type="hal" value="hal-00750528"/>
      <analytic>
        <title level="a">Second order optimization of mutual information for real-time image registration</title>
        <author>
          <persName>
            <foreName>Amaury</foreName>
            <surname>Dame</surname>
            <initial>A.</initial>
          </persName>
          <persName key="lagadic-2014-idp71088">
            <foreName>Eric</foreName>
            <surname>Marchand</surname>
            <initial>E.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-editorial-board="yes" x-international-audience="yes">
        <title level="j">IEEE Trans. on Image Processing</title>
        <imprint>
          <biblScope type="volume">21</biblScope>
          <biblScope type="number">9</biblScope>
          <dateStruct>
            <year>2012</year>
          </dateStruct>
          <biblScope type="pages">4190-4203</biblScope>
          <ref xlink:href="http://hal.inria.fr/hal-00750528/en" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-00750528/<allowbreak/>en</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid48" type="article" rend="refer" n="refercite:Diosi11a">
      <identifiant type="hal" value="hal-00639680"/>
      <analytic>
        <title level="a">Experimental Evaluation of Autonomous Driving Based on Visual Memory and Image Based Visual Servoing</title>
        <author>
          <persName>
            <foreName>A.</foreName>
            <surname>Diosi</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>S.</foreName>
            <surname>Segvic</surname>
            <initial>S.</initial>
          </persName>
          <persName>
            <foreName>A.</foreName>
            <surname>Remazeilles</surname>
            <initial>A.</initial>
          </persName>
          <persName key="lagadic-2014-idm27984">
            <foreName>Fran√ßois</foreName>
            <surname>Chaumette</surname>
            <initial>F.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">IEEE Trans. on Intelligent Transportation Systems</title>
        <imprint>
          <biblScope type="volume">12</biblScope>
          <biblScope type="number">3</biblScope>
          <dateStruct>
            <month>September</month>
            <year>2011</year>
          </dateStruct>
          <biblScope type="pages">870‚Äì883</biblScope>
          <ref xlink:href="http://hal.inria.fr/hal-00639680/en" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-00639680/<allowbreak/>en</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid5" type="article" rend="refer" n="refercite:Marchand05b">
      <identifiant type="hal" value="inria-00351899"/>
      <analytic>
        <title level="a">ViSP for visual servoing: a generic software platform with a wide class of robot control skills</title>
        <author>
          <persName key="lagadic-2014-idp71088">
            <foreName>Eric</foreName>
            <surname>Marchand</surname>
            <initial>E.</initial>
          </persName>
          <persName key="lagadic-2014-idp72528">
            <foreName>Fabien</foreName>
            <surname>Spindler</surname>
            <initial>F.</initial>
          </persName>
          <persName key="lagadic-2014-idm27984">
            <foreName>Fran√ßois</foreName>
            <surname>Chaumette</surname>
            <initial>F.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">IEEE Robotics and Automation Magazine</title>
        <imprint>
          <biblScope type="volume">12</biblScope>
          <biblScope type="number">4</biblScope>
          <dateStruct>
            <month>December</month>
            <year>2005</year>
          </dateStruct>
          <biblScope type="pages">40-52</biblScope>
          <ref xlink:href="https://hal.inria.fr/inria-00351899" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>inria-00351899</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid79" type="article" rend="refer" n="refercite:Mebarki10a">
      <identifiant type="hal" value="inria-00544791"/>
      <analytic>
        <title level="a">2D ultrasound probe complete guidance by visual servoing using image moments</title>
        <author>
          <persName>
            <foreName>Rafik</foreName>
            <surname>Mebarki</surname>
            <initial>R.</initial>
          </persName>
          <persName key="lagadic-2014-idm26496">
            <foreName>Alexandre</foreName>
            <surname>Krupa</surname>
            <initial>A.</initial>
          </persName>
          <persName key="lagadic-2014-idm27984">
            <foreName>Fran√ßois</foreName>
            <surname>Chaumette</surname>
            <initial>F.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-editorial-board="yes" x-international-audience="yes">
        <title level="j">IEEE Trans. on Robotics</title>
        <imprint>
          <biblScope type="volume">26</biblScope>
          <biblScope type="number">2</biblScope>
          <dateStruct>
            <month>April</month>
            <year>2010</year>
          </dateStruct>
          <biblScope type="pages">296-306</biblScope>
          <ref xlink:href="https://hal.inria.fr/inria-00544791" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>inria-00544791</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid1" type="article" rend="refer" n="refercite:Meilland15">
      <identifiant type="hal" value="hal-01010429"/>
      <analytic>
        <title level="a">Dense omnidirectional RGB-D mapping of large scale outdoor environments for real-time localisation and autonomous navigation</title>
        <author>
          <persName>
            <foreName>Maxime</foreName>
            <surname>Meilland</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>Andrew</foreName>
            <surname>Comport</surname>
            <initial>A.</initial>
          </persName>
          <persName key="lagadic-2014-idp65680">
            <foreName>Patrick</foreName>
            <surname>Rives</surname>
            <initial>P.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">Journal of Field Robotics, Special Issue on Ground Robots Operating in dynamic, unstructured and large-scale outdoor environments</title>
        <imprint>
          <biblScope type="volume">32</biblScope>
          <biblScope type="number">4</biblScope>
          <dateStruct>
            <month>June</month>
            <year>2015</year>
          </dateStruct>
          <biblScope type="pages">474-503</biblScope>
          <ref xlink:href="http://hal.inria.fr/hal-01010429" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01010429</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid80" type="article" rend="refer" n="refercite:nadeau:hal-00854100">
      <identifiant type="doi" value="10.1109/TRO.2013.2256690"/>
      <identifiant type="hal" value="hal-00854100"/>
      <analytic>
        <title level="a">Intensity-based ultrasound visual servoing: modeling and validation with 2D and 3D probes</title>
        <author>
          <persName>
            <foreName>Caroline</foreName>
            <surname>Nadeau</surname>
            <initial>C.</initial>
          </persName>
          <persName key="lagadic-2014-idm26496">
            <foreName>Alexandre</foreName>
            <surname>Krupa</surname>
            <initial>A.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-editorial-board="yes" x-international-audience="yes">
        <title level="j">IEEE. Trans. on Robotics</title>
        <imprint>
          <biblScope type="volume">29</biblScope>
          <biblScope type="number">4</biblScope>
          <dateStruct>
            <month>August</month>
            <year>2013</year>
          </dateStruct>
          <biblScope type="pages">1003-1015</biblScope>
          <ref xlink:href="http://hal.inria.fr/hal-00854100" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-00854100</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid41" type="article" rend="refer" n="refercite:Spica15">
      <identifiant type="hal" value="hal-01010429"/>
      <analytic>
        <title level="a">Active Structure from Motion: Application to Point, Sphere and Cylinde</title>
        <author>
          <persName key="lagadic-2014-idp111520">
            <foreName>Riccardo</foreName>
            <surname>Spica</surname>
            <initial>R.</initial>
          </persName>
          <persName key="lagadic-2014-idp67120">
            <foreName>Paolo</foreName>
            <surname>Robuffo Giordano</surname>
            <initial>P.</initial>
          </persName>
          <persName key="lagadic-2014-idm27984">
            <foreName>Fran√ßois</foreName>
            <surname>Chaumette</surname>
            <initial>F.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">IEEE Trans. on Robotics</title>
        <imprint>
          <biblScope type="volume">30</biblScope>
          <biblScope type="number">6</biblScope>
          <dateStruct>
            <month>December</month>
            <year>2015</year>
          </dateStruct>
          <biblScope type="pages">1499-1513</biblScope>
          <ref xlink:href="http://hal.inria.fr/hal-01010429" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01010429</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid56" type="article" rend="refer" n="refercite:Zelazo15">
      <identifiant type="hal" value="hal-01076423"/>
      <analytic>
        <title level="a">Decentralized rigidity maintenance control with range measurements for multi-robot systems</title>
        <author>
          <persName>
            <foreName>Daniel</foreName>
            <surname>Zelazo</surname>
            <initial>D.</initial>
          </persName>
          <persName>
            <foreName>Antonio</foreName>
            <surname>Franchi</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Heinrich H.</foreName>
            <surname>B√ºlthoff</surname>
            <initial>H. H.</initial>
          </persName>
          <persName key="lagadic-2014-idp67120">
            <foreName>Paolo</foreName>
            <surname>Robuffo Giordano</surname>
            <initial>P.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">The Int. Journal of Robotics Research</title>
        <imprint>
          <biblScope type="volume">34</biblScope>
          <biblScope type="number">1</biblScope>
          <dateStruct>
            <month>January</month>
            <year>2015</year>
          </dateStruct>
          <biblScope type="pages">105-128</biblScope>
          <ref xlink:href="http://hal.inria.fr/hal-01076423" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01076423</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid50" type="phdthesis" rend="year" n="cite:bista:tel-01426763">
      <identifiant type="hal" value="tel-01426763"/>
      <monogr>
        <title level="m">Indoor Navigation of Mobile Robots based on Visual Memory and Image-Based Visual Servoing</title>
        <author>
          <persName key="lagadic-2014-idp91504">
            <foreName>Suman Raj</foreName>
            <surname>Bista</surname>
            <initial>S. R.</initial>
          </persName>
        </author>
        <imprint>
          <publisher>
            <orgName type="school">Universite de Rennes 1 ; Inria Rennes Bretagne Atlantique</orgName>
          </publisher>
          <dateStruct>
            <month>December</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://tel.archives-ouvertes.fr/tel-01426763" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>tel.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>tel-01426763</ref>
        </imprint>
      </monogr>
      <note type="typdoc">Theses</note>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid59" type="phdthesis" rend="year" n="cite:bruneau:tel-01425268">
      <identifiant type="hal" value="tel-01425268"/>
      <monogr>
        <title level="m">Studying and modeling complex interactions for crowd simulation</title>
        <author>
          <persName key="phoenix-2014-idp112224">
            <foreName>Julien</foreName>
            <surname>Bruneau</surname>
            <initial>J.</initial>
          </persName>
        </author>
        <imprint>
          <publisher>
            <orgName type="school">Universit√© de Rennes 1 [UR1]</orgName>
          </publisher>
          <dateStruct>
            <month>November</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/tel-01425268" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>tel-01425268</ref>
        </imprint>
      </monogr>
      <note type="typdoc">Theses</note>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid46" type="phdthesis" rend="year" n="cite:cazy:tel-01421363">
      <identifiant type="hal" value="tel-01421363"/>
      <monogr>
        <title level="m">Predictive control for the achievement of successive visual servoing tasks</title>
        <author>
          <persName key="lagadic-2014-idp92760">
            <foreName>Nicolas</foreName>
            <surname>Cazy</surname>
            <initial>N.</initial>
          </persName>
        </author>
        <imprint>
          <publisher>
            <orgName type="school">Universit√© Rennes 1</orgName>
          </publisher>
          <dateStruct>
            <month>November</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://tel.archives-ouvertes.fr/tel-01421363" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>tel.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>tel-01421363</ref>
        </imprint>
      </monogr>
      <note type="typdoc">Theses</note>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid47" type="phdthesis" rend="year" n="cite:chatelain:tel-01426511">
      <identifiant type="hal" value="tel-01426511"/>
      <monogr>
        <title level="m">Quality-Driven Control of a Robotized Ultrasound Probe</title>
        <author>
          <persName key="lagadic-2014-idp93992">
            <foreName>Pierre</foreName>
            <surname>Chatelain</surname>
            <initial>P.</initial>
          </persName>
        </author>
        <imprint>
          <publisher>
            <orgName type="school">Universit√© de Rennes 1 ; Technische Universit√§t M√ºnchen</orgName>
          </publisher>
          <dateStruct>
            <month>December</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://tel.archives-ouvertes.fr/tel-01426511" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>tel.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>tel-01426511</ref>
        </imprint>
      </monogr>
      <note type="typdoc">Theses</note>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid35" type="phdthesis" rend="year" n="cite:cui:tel-01267585">
      <identifiant type="hal" value="tel-01267585"/>
      <monogr>
        <title level="m">Robust micro/nano-positioning by visual servoing</title>
        <author>
          <persName key="lagadic-2014-idp96504">
            <foreName>Le</foreName>
            <surname>Cui</surname>
            <initial>L.</initial>
          </persName>
        </author>
        <imprint>
          <publisher>
            <orgName type="school">Universit√© Rennes 1</orgName>
          </publisher>
          <dateStruct>
            <month>January</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://tel.archives-ouvertes.fr/tel-01267585" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>tel.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>tel-01267585</ref>
        </imprint>
      </monogr>
      <note type="typdoc">Theses</note>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid53" type="phdthesis" rend="year" n="cite:karakkatnarayanan:tel-01426748">
      <identifiant type="hal" value="tel-01426748"/>
      <monogr>
        <title level="m">Characterizing assistive shared control through vision-based and human-aware designs for wheelchair mobility assistance </title>
        <author>
          <persName key="lagadic-2014-idp102752">
            <foreName>Vishnu</foreName>
            <surname>Karakkat Narayanan</surname>
            <initial>V.</initial>
          </persName>
        </author>
        <imprint>
          <publisher>
            <orgName type="school">Inria Rennes - Bretagne Atlantique ; INSA Rennes</orgName>
          </publisher>
          <dateStruct>
            <month>November</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://tel.archives-ouvertes.fr/tel-01426748" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>tel.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>tel-01426748</ref>
        </imprint>
      </monogr>
      <note type="typdoc">Theses</note>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid70" type="phdthesis" rend="year" n="cite:magassouba:tel-01426710">
      <identifiant type="hal" value="tel-01426710"/>
      <monogr>
        <title level="m">Aural servo: towards an alternative approach to sound localization for robot motion control</title>
        <author>
          <persName key="lagadic-2014-idp103992">
            <foreName>Aly</foreName>
            <surname>Magassouba</surname>
            <initial>A.</initial>
          </persName>
        </author>
        <imprint>
          <publisher>
            <orgName type="school">Universit√© Rennes 1</orgName>
          </publisher>
          <dateStruct>
            <month>December</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://tel.archives-ouvertes.fr/tel-01426710" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>tel.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>tel-01426710</ref>
        </imprint>
      </monogr>
      <note type="typdoc">Theses</note>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid76" type="hdrthesis" rend="year" n="cite:robuffogiordano:tel-01301644">
      <identifiant type="hal" value="tel-01301644"/>
      <monogr>
        <title level="m">Contributions to shared control and coordination of single and multiple robots</title>
        <author>
          <persName key="lagadic-2014-idp67120">
            <foreName>Paolo</foreName>
            <surname>Robuffo Giordano</surname>
            <initial>P.</initial>
          </persName>
        </author>
        <imprint>
          <publisher>
            <orgName type="school">Universit√© de Rennes 1</orgName>
          </publisher>
          <dateStruct>
            <month>January</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://tel.archives-ouvertes.fr/tel-01301644" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>tel.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>tel-01301644</ref>
        </imprint>
      </monogr>
      <note type="typdoc">Habilitation √† diriger des recherches</note>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid69" type="phdthesis" rend="year" n="cite:royer:tel-01426711">
      <identifiant type="hal" value="tel-01426711"/>
      <monogr>
        <title level="m">Real-time Tracking of Deformable Targets in 3D Ultrasound Sequences</title>
        <author>
          <persName key="lagadic-2014-idp109024">
            <foreName>Lucas</foreName>
            <surname>Royer</surname>
            <initial>L.</initial>
          </persName>
        </author>
        <imprint>
          <publisher>
            <orgName type="school">INSA de Rennes</orgName>
          </publisher>
          <dateStruct>
            <month>December</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://tel.archives-ouvertes.fr/tel-01426711" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>tel.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>tel-01426711</ref>
        </imprint>
      </monogr>
      <note type="typdoc">Theses</note>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid33" type="article" rend="year" n="cite:agravante:hal-01421734">
      <identifiant type="hal" value="hal-01421734"/>
      <analytic>
        <title level="a">Visual servoing in an optimization framework for the whole-body control of humanoid robots</title>
        <author>
          <persName key="lagadic-2016-idp184672">
            <foreName>Don Joven</foreName>
            <surname>Agravante</surname>
            <initial>D. J.</initial>
          </persName>
          <persName key="lagadic-2014-idp75040">
            <foreName>Giovanni</foreName>
            <surname>Claudio</surname>
            <initial>G.</initial>
          </persName>
          <persName key="lagadic-2014-idp72528">
            <foreName>Fabien</foreName>
            <surname>Spindler</surname>
            <initial>F.</initial>
          </persName>
          <persName key="lagadic-2014-idm27984">
            <foreName>Fran√ßois</foreName>
            <surname>Chaumette</surname>
            <initial>F.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid02878">
        <idno type="issn">I-notfound</idno>
        <title level="j">IEEE Robotics and Automation Letters</title>
        <imprint>
          <dateStruct>
            <year>2017</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01421734" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01421734</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid6" type="article" rend="year" n="cite:bateux:hal-01265560">
      <identifiant type="hal" value="hal-01265560"/>
      <analytic>
        <title level="a">Histograms-based Visual Servoing</title>
        <author>
          <persName key="lagadic-2014-idp90248">
            <foreName>Quentin</foreName>
            <surname>Bateux</surname>
            <initial>Q.</initial>
          </persName>
          <persName key="lagadic-2014-idp71088">
            <foreName>√âric</foreName>
            <surname>Marchand</surname>
            <initial>√â.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid02878">
        <idno type="issn">I-notfound</idno>
        <title level="j">IEEE Robotics and Automation Letters</title>
        <imprint>
          <biblScope type="volume">2</biblScope>
          <biblScope type="number">1</biblScope>
          <dateStruct>
            <month>January</month>
            <year>2017</year>
          </dateStruct>
          <biblScope type="pages">80-87</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01265560" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01265560</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid42" type="article" rend="year" n="cite:briot:hal-01399774">
      <identifiant type="hal" value="hal-01399774"/>
      <analytic>
        <title level="a">Revisiting the determination of the singularity cases in the visual servoing of image points through the concept of hidden robot</title>
        <author>
          <persName>
            <foreName>S√©bastien</foreName>
            <surname>Briot</surname>
            <initial>S.</initial>
          </persName>
          <persName key="lagadic-2014-idm27984">
            <foreName>Francois</foreName>
            <surname>Chaumette</surname>
            <initial>F.</initial>
          </persName>
          <persName>
            <foreName>Philippe</foreName>
            <surname>Martinet</surname>
            <initial>P.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid00749">
        <idno type="issn">1552-3098</idno>
        <title level="j">IEEE Transactions on Robotics</title>
        <imprint>
          <biblScope type="volume">33</biblScope>
          <biblScope type="number">2</biblScope>
          <dateStruct>
            <month>January</month>
            <year>2017</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01399774" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01399774</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid43" type="article" rend="year" n="cite:briot:hal-01398925">
      <identifiant type="hal" value="hal-01398925"/>
      <analytic>
        <title level="a">Determining the Singularities for the Observation of Three Image Lines</title>
        <author>
          <persName>
            <foreName>S√©bastien</foreName>
            <surname>Briot</surname>
            <initial>S.</initial>
          </persName>
          <persName>
            <foreName>Philippe</foreName>
            <surname>Martinet</surname>
            <initial>P.</initial>
          </persName>
          <persName key="lagadic-2014-idm27984">
            <foreName>Francois</foreName>
            <surname>Chaumette</surname>
            <initial>F.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid02878">
        <idno type="issn">I-notfound</idno>
        <title level="j">IEEE Robotics and Automation Letters</title>
        <imprint>
          <biblScope type="volume">2</biblScope>
          <biblScope type="number">2</biblScope>
          <dateStruct>
            <month>April</month>
            <year>2017</year>
          </dateStruct>
          <biblScope type="pages">412-419</biblScope>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01398925" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01398925</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid61" type="article" rend="year" n="cite:bruneau:hal-01392248">
      <identifiant type="doi" value="10.1111/cgf.13066"/>
      <identifiant type="hal" value="hal-01392248"/>
      <analytic>
        <title level="a">EACS: Effective Avoidance Combination Strategy</title>
        <author>
          <persName key="phoenix-2014-idp112224">
            <foreName>Julien</foreName>
            <surname>Bruneau</surname>
            <initial>J.</initial>
          </persName>
          <persName key="mimetic-2014-idm25552">
            <foreName>Julien</foreName>
            <surname>Pettr√©</surname>
            <initial>J.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid00391">
        <idno type="issn">0167-7055</idno>
        <title level="j">Computer Graphics Forum</title>
        <imprint>
          <dateStruct>
            <month>September</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01392248" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01392248</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid3" type="incollection" rend="year" n="cite:chaumette:hal-01355384">
      <identifiant type="hal" value="hal-01355384"/>
      <analytic>
        <title level="a">Visual Servoing</title>
        <author>
          <persName key="lagadic-2014-idm27984">
            <foreName>Francois</foreName>
            <surname>Chaumette</surname>
            <initial>F.</initial>
          </persName>
          <persName>
            <foreName>Seth</foreName>
            <surname>Hutchinson</surname>
            <initial>S.</initial>
          </persName>
          <persName>
            <foreName>Peter</foreName>
            <surname>Corke</surname>
            <initial>P.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no">
        <editor role="editor">
          <persName>
            <foreName>B.</foreName>
            <surname>Siciliano</surname>
            <initial>B.</initial>
          </persName>
          <persName>
            <foreName>O.</foreName>
            <surname>Khatib</surname>
            <initial>O.</initial>
          </persName>
        </editor>
        <title level="m">Handbook of Robotics, 2nd edition</title>
        <imprint>
          <publisher>
            <orgName>Springer</orgName>
          </publisher>
          <dateStruct>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">841-866</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01355384" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01355384</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid77" type="article" rend="year" n="cite:drevelle:hal-01258946">
      <identifiant type="doi" value="10.1002/acs.2535"/>
      <identifiant type="hal" value="hal-01258946"/>
      <analytic>
        <title level="a">Interval-based fast fault detection and identification applied to radio-navigation multipath</title>
        <author>
          <persName key="lagadic-2014-idp69816">
            <foreName>Vincent</foreName>
            <surname>Drevelle</surname>
            <initial>V.</initial>
          </persName>
          <persName>
            <foreName>Philippe</foreName>
            <surname>Bonnifait</surname>
            <initial>P.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid00846">
        <idno type="issn">0890-6327</idno>
        <title level="j">International Journal of Adaptive Control and Signal Processing</title>
        <imprint>
          <biblScope type="volume">30</biblScope>
          <dateStruct>
            <month>January</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">154‚Äì172</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01258946" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01258946</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid10" type="article" rend="year" n="cite:fernandezmoral:hal-01237845">
      <identifiant type="doi" value="10.1016/j.robot.2015.09.009"/>
      <identifiant type="hal" value="hal-01237845"/>
      <analytic>
        <title level="a">Scene structure registration for localization and mapping</title>
        <author>
          <persName>
            <foreName>Eduardo</foreName>
            <surname>Fern√°ndez-Moral</surname>
            <initial>E.</initial>
          </persName>
          <persName key="lagadic-2014-idp65680">
            <foreName>Patrick</foreName>
            <surname>Rives</surname>
            <initial>P.</initial>
          </persName>
          <persName>
            <foreName>Vicente</foreName>
            <surname>Ar√©valo</surname>
            <initial>V.</initial>
          </persName>
          <persName>
            <foreName>Javier</foreName>
            <surname>Gonz√°lez-Jim√©nez</surname>
            <initial>J.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid01721">
        <idno type="issn">0921-8890</idno>
        <title level="j">Robotics and Autonomous Systems</title>
        <imprint>
          <biblScope type="volume">75</biblScope>
          <biblScope type="number">B</biblScope>
          <dateStruct>
            <month>January</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">649-660</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01237845" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01237845</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid37" type="article" rend="year" n="cite:forshaw:hal-01342268">
      <identifiant type="doi" value="10.1016/j.actaastro.2016.06.018"/>
      <identifiant type="hal" value="hal-01342268"/>
      <analytic>
        <title level="a">REMOVEDEBRIS: An In-Orbit Active Debris Removal Demonstration Mission</title>
        <author>
          <persName>
            <foreName>Jason</foreName>
            <surname>Forshaw</surname>
            <initial>J.</initial>
          </persName>
          <persName>
            <foreName>Guglielmo</foreName>
            <surname>Aglietti</surname>
            <initial>G.</initial>
          </persName>
          <persName>
            <foreName>Nimal</foreName>
            <surname>Navarathinam</surname>
            <initial>N.</initial>
          </persName>
          <persName>
            <foreName>Haval</foreName>
            <surname>Kadhem</surname>
            <initial>H.</initial>
          </persName>
          <persName>
            <foreName>Thierry</foreName>
            <surname>Salmon</surname>
            <initial>T.</initial>
          </persName>
          <persName>
            <foreName>Aur√©lien</foreName>
            <surname>Pisseloup</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Eric</foreName>
            <surname>Joffre</surname>
            <initial>E.</initial>
          </persName>
          <persName>
            <foreName>Thomas</foreName>
            <surname>Chabot</surname>
            <initial>T.</initial>
          </persName>
          <persName>
            <foreName>Ingo</foreName>
            <surname>Retat</surname>
            <initial>I.</initial>
          </persName>
          <persName>
            <foreName>Robert</foreName>
            <surname>Axthelm</surname>
            <initial>R.</initial>
          </persName>
          <persName>
            <foreName>Simon</foreName>
            <surname>Barraclough</surname>
            <initial>S.</initial>
          </persName>
          <persName>
            <foreName>Andrew</foreName>
            <surname>Ratcliffe</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Cesar</foreName>
            <surname>Bernal</surname>
            <initial>C.</initial>
          </persName>
          <persName key="lagadic-2014-idm27984">
            <foreName>Fran√ßois</foreName>
            <surname>Chaumette</surname>
            <initial>F.</initial>
          </persName>
          <persName>
            <foreName>Alexandre</foreName>
            <surname>Pollini</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Willem</foreName>
            <surname>Steyn</surname>
            <initial>W.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid03129">
        <idno type="issn">0094-5765</idno>
        <title level="j">Acta Astronautica</title>
        <imprint>
          <biblScope type="volume">127</biblScope>
          <dateStruct>
            <month>October</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">448-463</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01342268" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01342268</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid55" type="article" rend="year" n="cite:franchi:hal-01315463">
      <identifiant type="doi" value="10.1109/TCNS.2016.2567222"/>
      <identifiant type="hal" value="hal-01315463"/>
      <analytic>
        <title level="a">Online Leader Selection for Improved Collective Tracking and Formation Maintenance</title>
        <author>
          <persName>
            <foreName>Antonio</foreName>
            <surname>Franchi</surname>
            <initial>A.</initial>
          </persName>
          <persName key="lagadic-2014-idp67120">
            <foreName>Paolo</foreName>
            <surname>Robuffo Giordano</surname>
            <initial>P.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid02721">
        <idno type="issn">2325-5870</idno>
        <title level="j">IEEE Transactions on Control of Network Systems</title>
        <imprint>
          <dateStruct>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01315463" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01315463</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid63" type="article" rend="year" n="cite:hoyet:hal-01357713">
      <identifiant type="doi" value="10.1145/2897824.2925931"/>
      <identifiant type="hal" value="hal-01357713"/>
      <analytic>
        <title level="a">Perceptual Effect of Shoulder Motions on Crowd Animations</title>
        <author>
          <persName key="mimetic-2015-idm26392">
            <foreName>Ludovic</foreName>
            <surname>Hoyet</surname>
            <initial>L.</initial>
          </persName>
          <persName key="mimetic-2014-idp77968">
            <foreName>Anne-H√©l√®ne</foreName>
            <surname>Olivier</surname>
            <initial>A.-H.</initial>
          </persName>
          <persName key="mimetic-2014-idp73992">
            <foreName>Richard</foreName>
            <surname>Kulpa</surname>
            <initial>R.</initial>
          </persName>
          <persName key="mimetic-2014-idm25552">
            <foreName>Julien</foreName>
            <surname>Pettr√©</surname>
            <initial>J.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid00024">
        <idno type="issn">0730-0301</idno>
        <title level="j">ACM Transactions on Graphics</title>
        <imprint>
          <dateStruct>
            <month>July</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01357713" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01357713</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid14" type="article" rend="year" n="cite:karakkatnarayanan:hal-01277585">
      <identifiant type="hal" value="hal-01277585"/>
      <analytic>
        <title level="a">Vision-based adaptive assistance and haptic guidance for safe wheelchair corridor following</title>
        <author>
          <persName key="lagadic-2014-idp102752">
            <foreName>Vishnu</foreName>
            <surname>Karakkat Narayanan</surname>
            <initial>V.</initial>
          </persName>
          <persName key="lagadic-2014-idp76296">
            <foreName>Fran√ßois</foreName>
            <surname>Pasteau</surname>
            <initial>F.</initial>
          </persName>
          <persName key="hybrid-2014-idp67480">
            <foreName>Maud</foreName>
            <surname>Marchal</surname>
            <initial>M.</initial>
          </persName>
          <persName key="lagadic-2014-idm26496">
            <foreName>Alexandre</foreName>
            <surname>Krupa</surname>
            <initial>A.</initial>
          </persName>
          <persName key="lagadic-2014-idp68368">
            <foreName>Marie</foreName>
            <surname>Babel</surname>
            <initial>M.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid00409">
        <idno type="issn">1077-3142</idno>
        <title level="j">Computer Vision and Image Understanding</title>
        <imprint>
          <biblScope type="volume">179</biblScope>
          <dateStruct>
            <month>August</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">171-185</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01277585" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01277585</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid21" type="article" rend="year" n="cite:krupa:hal-00986875">
      <identifiant type="doi" value="10.1109/JSYST.2014.2314773"/>
      <identifiant type="hal" value="hal-00986875"/>
      <analytic>
        <title level="a">Robotized Tele-Echography: an Assisting Visibility Tool to Support Expert Diagnostic</title>
        <author>
          <persName key="lagadic-2014-idm26496">
            <foreName>Alexandre</foreName>
            <surname>Krupa</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>David</foreName>
            <surname>Folio</surname>
            <initial>D.</initial>
          </persName>
          <persName>
            <foreName>Cyril</foreName>
            <surname>Novales</surname>
            <initial>C.</initial>
          </persName>
          <persName>
            <foreName>Pierre</foreName>
            <surname>Vieyres</surname>
            <initial>P.</initial>
          </persName>
          <persName>
            <foreName>Tao</foreName>
            <surname>Li</surname>
            <initial>T.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid00702">
        <idno type="issn">1932-8184</idno>
        <title level="j">IEEE Systems Journal</title>
        <imprint>
          <biblScope type="volume">10</biblScope>
          <biblScope type="number">3</biblScope>
          <dateStruct>
            <month>September</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">974-983</biblScope>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-00986875" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-00986875</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid73" type="article" rend="year" n="cite:lima:hal-01233046">
      <identifiant type="hal" value="hal-01233046"/>
      <analytic>
        <title level="a">Depth-Assisted Rectification for Real-Time Object Detection and Pose Estimation</title>
        <author>
          <persName>
            <foreName>J</foreName>
            <surname>Lima</surname>
            <initial>J.</initial>
          </persName>
          <persName>
            <foreName>F</foreName>
            <surname>Sim√µes</surname>
            <initial>F.</initial>
          </persName>
          <persName key="lagadic-2014-idp83968">
            <foreName>Hideaki</foreName>
            <surname>Uchiyama</surname>
            <initial>H.</initial>
          </persName>
          <persName>
            <foreName>Veronica</foreName>
            <surname>Teichrieb</surname>
            <initial>V.</initial>
          </persName>
          <persName key="lagadic-2014-idp71088">
            <foreName>Eric</foreName>
            <surname>Marchand</surname>
            <initial>E.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid01367">
        <idno type="issn">0932-8092</idno>
        <title level="j">Machine Vision and Applications</title>
        <imprint>
          <biblScope type="volume">27</biblScope>
          <biblScope type="number">2</biblScope>
          <dateStruct>
            <month>February</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">193-219</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01233046" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01233046</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid2" type="article" rend="year" n="cite:marchand:hal-01246370">
      <identifiant type="doi" value="10.1109/TVCG.2015.2513408"/>
      <identifiant type="hal" value="hal-01246370"/>
      <analytic>
        <title level="a">Pose Estimation for Augmented Reality: A Hands-On Survey</title>
        <author>
          <persName key="lagadic-2014-idp71088">
            <foreName>√âric</foreName>
            <surname>Marchand</surname>
            <initial>√â.</initial>
          </persName>
          <persName key="lagadic-2014-idp83968">
            <foreName>Hideaki</foreName>
            <surname>Uchiyama</surname>
            <initial>H.</initial>
          </persName>
          <persName key="lagadic-2014-idp72528">
            <foreName>Fabien</foreName>
            <surname>Spindler</surname>
            <initial>F.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid00761">
        <idno type="issn">1077-2626</idno>
        <title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
        <imprint>
          <biblScope type="volume">22</biblScope>
          <biblScope type="number">12</biblScope>
          <dateStruct>
            <month>December</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">2633 - 2651</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01246370" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01246370</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid22" type="article" rend="year" n="cite:nadeau:hal-01385661">
      <identifiant type="doi" value="10.1109/TRO.2016.2604482"/>
      <identifiant type="hal" value="hal-01385661"/>
      <analytic>
        <title level="a">Moments-Based Ultrasound Visual Servoing: From Mono to Multi-plane Approach</title>
        <author>
          <persName>
            <foreName>Caroline</foreName>
            <surname>Nadeau</surname>
            <initial>C.</initial>
          </persName>
          <persName key="lagadic-2014-idm26496">
            <foreName>Alexandre</foreName>
            <surname>Krupa</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Jan</foreName>
            <surname>Petr</surname>
            <initial>J.</initial>
          </persName>
          <persName key="visages-2014-idp101432">
            <foreName>Christian</foreName>
            <surname>Barillot</surname>
            <initial>C.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid00749">
        <idno type="issn">1552-3098</idno>
        <title level="j">IEEE Transactions on Robotics</title>
        <imprint>
          <dateStruct>
            <year>2017</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01385661" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01385661</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid54" type="article" rend="year" n="cite:nestmeyer:hal-01332937">
      <identifiant type="doi" value="10.1007/s10514-016-9578-9"/>
      <identifiant type="hal" value="hal-01332937"/>
      <analytic>
        <title level="a">Decentralized Simultaneous Multi-target Exploration using a Connected Network of Multiple Robots</title>
        <author>
          <persName>
            <foreName>Thomas</foreName>
            <surname>Nestmeyer</surname>
            <initial>T.</initial>
          </persName>
          <persName key="lagadic-2014-idp67120">
            <foreName>Paolo</foreName>
            <surname>Robuffo Giordano</surname>
            <initial>P.</initial>
          </persName>
          <persName>
            <foreName>Heinrich H.</foreName>
            <surname>B√ºlthoff</surname>
            <initial>H. H.</initial>
          </persName>
          <persName>
            <foreName>Antonio</foreName>
            <surname>Franchi</surname>
            <initial>A.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid00209">
        <idno type="issn">0929-5593</idno>
        <title level="j">Autonomous Robots</title>
        <imprint>
          <dateStruct>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01332937" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01332937</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid75" type="incollection" rend="year" n="cite:ourak:hal-01433996">
      <identifiant type="doi" value="10.1007/978-3-319-31898-1_9"/>
      <identifiant type="hal" value="hal-01433996"/>
      <analytic>
        <title level="a">Multimodal Image Registration and Visual Servoing</title>
        <author>
          <persName>
            <foreName>Mouloud</foreName>
            <surname>OURAK</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>Brahim</foreName>
            <surname>Tamadazte</surname>
            <initial>B.</initial>
          </persName>
          <persName>
            <foreName>Nicolas</foreName>
            <surname>Andreff</surname>
            <initial>N.</initial>
          </persName>
          <persName key="lagadic-2014-idp71088">
            <foreName>√âric</foreName>
            <surname>Marchand</surname>
            <initial>√â.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no">
        <title level="m">Lecture Notes in Electrical Engineering</title>
        <imprint>
          <biblScope type="volume">383</biblScope>
          <publisher>
            <orgName>Springer</orgName>
          </publisher>
          <dateStruct>
            <month>July</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">157 - 175</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01433996" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01433996</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid52" type="article" rend="year" n="cite:papadakis:hal-01342255">
      <identifiant type="doi" value="10.1007/s10514-016-9581-1"/>
      <identifiant type="hal" value="hal-01342255"/>
      <analytic>
        <title level="a">Binding human spatial interactions with mapping for enhanced mobility in dynamic environments</title>
        <author>
          <persName key="flowers-2014-idp86512">
            <foreName>Panagiotis</foreName>
            <surname>Papadakis</surname>
            <initial>P.</initial>
          </persName>
          <persName key="lagadic-2014-idp65680">
            <foreName>Patrick</foreName>
            <surname>Rives</surname>
            <initial>P.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid00209">
        <idno type="issn">0929-5593</idno>
        <title level="j">Autonomous Robots</title>
        <imprint>
          <dateStruct>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01342255" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01342255</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid12" type="article" rend="year" n="cite:pasteau:hal-01068163">
      <identifiant type="doi" value="10.1016/j.robot.2014.10.017"/>
      <identifiant type="hal" value="hal-01068163"/>
      <analytic>
        <title level="a">A visual servoing approach for autonomous corridor following and doorway passing in a wheelchair</title>
        <author>
          <persName key="lagadic-2014-idp76296">
            <foreName>Fran√ßois</foreName>
            <surname>Pasteau</surname>
            <initial>F.</initial>
          </persName>
          <persName key="lagadic-2014-idp102752">
            <foreName>Vishnu</foreName>
            <surname>Karakkat Narayanan</surname>
            <initial>V.</initial>
          </persName>
          <persName key="lagadic-2014-idp68368">
            <foreName>Marie</foreName>
            <surname>Babel</surname>
            <initial>M.</initial>
          </persName>
          <persName key="lagadic-2014-idm27984">
            <foreName>Fran√ßois</foreName>
            <surname>Chaumette</surname>
            <initial>F.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid01721">
        <idno type="issn">0921-8890</idno>
        <title level="j">Robotics and Autonomous Systems</title>
        <imprint>
          <biblScope type="volume">75, part A</biblScope>
          <dateStruct>
            <month>January</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">28-40</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01068163" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01068163</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid16" type="article" rend="year" n="cite:rajbista:hal-01259750">
      <identifiant type="hal" value="hal-01259750"/>
      <analytic>
        <title level="a">Appearance-based Indoor Navigation by IBVS using Line Segments</title>
        <author>
          <persName>
            <foreName>Suman</foreName>
            <surname>Raj Bista</surname>
            <initial>S.</initial>
          </persName>
          <persName key="lagadic-2014-idp67120">
            <foreName>Paolo</foreName>
            <surname>Robuffo Giordano</surname>
            <initial>P.</initial>
          </persName>
          <persName key="lagadic-2014-idm27984">
            <foreName>Fran√ßois</foreName>
            <surname>Chaumette</surname>
            <initial>F.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid02878">
        <idno type="issn">I-notfound</idno>
        <title level="j">IEEE Robotics and Automation Letters</title>
        <imprint>
          <biblScope type="volume">1</biblScope>
          <biblScope type="number">1</biblScope>
          <dateStruct>
            <month>January</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">423-430</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01259750" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01259750</ref>
        </imprint>
      </monogr>
      <note type="bnote">Also presented in IEEE Int. Conf. on Robotics and Automation, Stockolm, Sweden</note>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid60" type="article" rend="year" n="cite:ren:hal-01372766">
      <identifiant type="doi" value="10.1111/cgf.12993"/>
      <identifiant type="hal" value="hal-01372766"/>
      <analytic>
        <title level="a">Group Modeling: A Unified Velocity-Based Approach</title>
        <author>
          <persName key="mimetic-2014-idp93248">
            <foreName>Z</foreName>
            <surname>Ren</surname>
            <initial>Z.</initial>
          </persName>
          <persName key="mimetic-2014-idp86848">
            <foreName>Panayiotis</foreName>
            <surname>Charalambous</surname>
            <initial>P.</initial>
          </persName>
          <persName key="phoenix-2014-idp112224">
            <foreName>Julien</foreName>
            <surname>Bruneau</surname>
            <initial>J.</initial>
          </persName>
          <persName>
            <foreName>Qunsheng</foreName>
            <surname>Peng</surname>
            <initial>Q.</initial>
          </persName>
          <persName key="mimetic-2014-idm25552">
            <foreName>Julien</foreName>
            <surname>Pettr√©</surname>
            <initial>J.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid00391">
        <idno type="issn">0167-7055</idno>
        <title level="j">Computer Graphics Forum</title>
        <imprint>
          <dateStruct>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01372766" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01372766</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid20" type="article" rend="year" n="cite:royer:hal-01374589">
      <identifiant type="doi" value="10.1016/j.media.2016.09.004"/>
      <identifiant type="hal" value="hal-01374589"/>
      <analytic>
        <title level="a">Real-time Target Tracking of Soft Tissues in 3D Ultrasound Images Based on Robust Visual Information and Mechanical Simulation</title>
        <author>
          <persName key="lagadic-2014-idp109024">
            <foreName>Lucas</foreName>
            <surname>Royer</surname>
            <initial>L.</initial>
          </persName>
          <persName key="lagadic-2014-idm26496">
            <foreName>Alexandre</foreName>
            <surname>Krupa</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Guillaume</foreName>
            <surname>DARDENNE</surname>
            <initial>G.</initial>
          </persName>
          <persName>
            <foreName>Anthony</foreName>
            <surname>Le Bras</surname>
            <initial>A.</initial>
          </persName>
          <persName key="lagadic-2014-idp71088">
            <foreName>√âric</foreName>
            <surname>Marchand</surname>
            <initial>√â.</initial>
          </persName>
          <persName key="hybrid-2014-idp67480">
            <foreName>Maud</foreName>
            <surname>Marchal</surname>
            <initial>M.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid01414">
        <idno type="issn">1361-8415</idno>
        <title level="j">Medical Image Analysis</title>
        <imprint>
          <biblScope type="volume">35</biblScope>
          <dateStruct>
            <month>January</month>
            <year>2017</year>
          </dateStruct>
          <biblScope type="pages">582 - 598</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01374589" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01374589</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid51" type="article" rend="year" n="cite:vassallo:hal-01371202">
      <identifiant type="hal" value="hal-01371202"/>
      <analytic>
        <title level="a">How do walkers avoid a mobile robot crossing their way?</title>
        <author>
          <persName>
            <foreName>Christian</foreName>
            <surname>Vassallo</surname>
            <initial>C.</initial>
          </persName>
          <persName key="mimetic-2014-idp77968">
            <foreName>Anne-H√©l√®ne</foreName>
            <surname>Olivier</surname>
            <initial>A.-H.</initial>
          </persName>
          <persName>
            <foreName>Philippe</foreName>
            <surname>Sou√®res</surname>
            <initial>P.</initial>
          </persName>
          <persName key="mimetic-2014-idp69792">
            <foreName>Armel</foreName>
            <surname>Cr√©tual</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Olivier</foreName>
            <surname>Stasse</surname>
            <initial>O.</initial>
          </persName>
          <persName key="mimetic-2014-idm25552">
            <foreName>Julien</foreName>
            <surname>Pettr√©</surname>
            <initial>J.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid00624">
        <idno type="issn">0966-6362</idno>
        <title level="j">Gait and Posture</title>
        <imprint>
          <biblScope type="volume">51</biblScope>
          <dateStruct>
            <month>January</month>
            <year>2017</year>
          </dateStruct>
          <biblScope type="pages">97-103</biblScope>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01371202" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01371202</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid62" type="article" rend="year" n="cite:wolinski:hal-01411087">
      <identifiant type="doi" value="10.1145/2980179.2982442"/>
      <identifiant type="hal" value="hal-01411087"/>
      <analytic>
        <title level="a">WarpDriver: Context-Aware Probabilistic Motion Prediction for Crowd Simulation</title>
        <author>
          <persName key="mimetic-2014-idp109304">
            <foreName>David</foreName>
            <surname>Wolinski</surname>
            <initial>D.</initial>
          </persName>
          <persName>
            <foreName>Ming C.</foreName>
            <surname>Lin</surname>
            <initial>M. C.</initial>
          </persName>
          <persName key="mimetic-2014-idm25552">
            <foreName>Julien</foreName>
            <surname>Pettr√©</surname>
            <initial>J.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid00024">
        <idno type="issn">0730-0301</idno>
        <title level="j">ACM Transactions on Graphics</title>
        <imprint>
          <biblScope type="volume">35</biblScope>
          <biblScope type="number">6</biblScope>
          <dateStruct>
            <month>November</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01411087" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01411087</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid8" type="inproceedings" rend="year" n="cite:abifarraj:hal-01355785">
      <identifiant type="hal" value="hal-01355785"/>
      <analytic>
        <title level="a">A Visual-Based Shared Control Architecture for Remote Telemanipulation</title>
        <author>
          <persName>
            <foreName>Firas</foreName>
            <surname>Abi-Farraj</surname>
            <initial>F.</initial>
          </persName>
          <persName key="lagadic-2015-idp80232">
            <foreName>Nicol√≤</foreName>
            <surname>Pedemonte</surname>
            <initial>N.</initial>
          </persName>
          <persName key="lagadic-2014-idp67120">
            <foreName>Paolo</foreName>
            <surname>Robuffo Giordano</surname>
            <initial>P.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">IEEE/RSJ Int. Conf. on Intelligent Robots and Systems, IROS'16</title>
        <loc>Daejeon, South Korea</loc>
        <imprint>
          <dateStruct>
            <month>October</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">4266-4273</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01355785" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01355785</ref>
        </imprint>
        <meeting id="cid93437">
          <title>IEEE RSJ International Conference on Intelligent Robots and Systems</title>
          <num>2016</num>
          <abbr type="sigle">IROS</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid74" type="inproceedings" rend="year" n="cite:andrade:hal-01387573">
      <identifiant type="doi" value="10.1145/2929464.2929471"/>
      <identifiant type="hal" value="hal-01387573"/>
      <analytic>
        <title level="a">Enjoy 360¬∞ Vision with the FlyVIZ</title>
        <author>
          <persName>
            <foreName>Guillermo</foreName>
            <surname>Andrade</surname>
            <initial>G.</initial>
          </persName>
          <persName key="hybrid-2014-idp68928">
            <foreName>Florian</foreName>
            <surname>Nouviale</surname>
            <initial>F.</initial>
          </persName>
          <persName key="hybrid-2014-idp86328">
            <foreName>Jerome</foreName>
            <surname>Ardouin</surname>
            <initial>J.</initial>
          </persName>
          <persName key="lagadic-2014-idp71088">
            <foreName>√âric</foreName>
            <surname>Marchand</surname>
            <initial>√â.</initial>
          </persName>
          <persName key="hybrid-2014-idp67480">
            <foreName>Maud</foreName>
            <surname>Marchal</surname>
            <initial>M.</initial>
          </persName>
          <persName key="hybrid-2014-idm28656">
            <foreName>Anatole</foreName>
            <surname>L√©cuyer</surname>
            <initial>A.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">SIGGRAPH 2016 Emerging Technologies</title>
        <loc>Anaheim, United States</loc>
        <imprint>
          <dateStruct>
            <month>July</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01387573" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01387573</ref>
        </imprint>
        <meeting id="cid623993">
          <title>ACM SIGGRAPH Emerging Technologies</title>
          <num>2016</num>
          <abbr type="sigle"/>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid7" type="inproceedings" rend="year" n="cite:bateux:hal-01355396">
      <identifiant type="hal" value="hal-01355396"/>
      <analytic>
        <title level="a">Particle Filter-based Direct Visual Servoing</title>
        <author>
          <persName key="lagadic-2014-idp90248">
            <foreName>Quentin</foreName>
            <surname>Bateux</surname>
            <initial>Q.</initial>
          </persName>
          <persName key="lagadic-2014-idp71088">
            <foreName>Eric</foreName>
            <surname>Marchand</surname>
            <initial>E.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">IEEE/RSJ Int. Conf. on Intelligent Robots and Systems, IROS'16</title>
        <loc>Daejeon, South Korea</loc>
        <imprint>
          <dateStruct>
            <month>October</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">4180-4186</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01355396" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01355396</ref>
        </imprint>
        <meeting id="cid93437">
          <title>IEEE RSJ International Conference on Intelligent Robots and Systems</title>
          <num>2016</num>
          <abbr type="sigle">IROS</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid72" type="inproceedings" rend="year" n="cite:briot:hal-01435810">
      <identifiant type="hal" value="hal-01435810"/>
      <analytic>
        <title level="a">Revisiting the determination of the singularity cases in the visual servoing of image points through the concept of ‚Äúhidden robot‚Äù</title>
        <author>
          <persName>
            <foreName>S√©bastien</foreName>
            <surname>Briot</surname>
            <initial>S.</initial>
          </persName>
          <persName key="lagadic-2014-idm27984">
            <foreName>Francois</foreName>
            <surname>Chaumette</surname>
            <initial>F.</initial>
          </persName>
          <persName>
            <foreName>Philippe</foreName>
            <surname>Martinet</surname>
            <initial>P.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="no" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">2017 IEEE International Conference on Robotics and Automation (ICRA 2017)</title>
        <loc>Singapour, Singapore</loc>
        <imprint>
          <dateStruct>
            <month>May</month>
            <year>2017</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01435810" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01435810</ref>
        </imprint>
        <meeting id="cid85011">
          <title>IEEE International Conference on Robotics and Automation</title>
          <num>2017</num>
          <abbr type="sigle">ICRA</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid71" type="inproceedings" rend="year" n="cite:briot:hal-01435811">
      <identifiant type="hal" value="hal-01435811"/>
      <analytic>
        <title level="a">Singularity Cases in the Visual Servoing of Three Image Lines</title>
        <author>
          <persName>
            <foreName>S√©bastien</foreName>
            <surname>Briot</surname>
            <initial>S.</initial>
          </persName>
          <persName>
            <foreName>Philippe</foreName>
            <surname>Martinet</surname>
            <initial>P.</initial>
          </persName>
          <persName key="lagadic-2014-idm27984">
            <foreName>Francois</foreName>
            <surname>Chaumette</surname>
            <initial>F.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">2017 IEEE International Conference on Robotics and Automation (ICRA 2017)</title>
        <loc>Singapour, Singapore</loc>
        <title level="s">Proceedings of 2017 IEEE International Conference on Robotics and Automation (ICRA 2017)</title>
        <imprint>
          <dateStruct>
            <month>May</month>
            <year>2017</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01435811" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01435811</ref>
        </imprint>
        <meeting id="cid85011">
          <title>IEEE International Conference on Robotics and Automation</title>
          <num>2017</num>
          <abbr type="sigle">ICRA</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid28" type="inproceedings" rend="year" n="cite:chatelain:hal-01274748">
      <identifiant type="hal" value="hal-01274748"/>
      <analytic>
        <title level="a">Confidence-Driven Control of an Ultrasound Probe: Target-Specific Acoustic Window Optimization</title>
        <author>
          <persName key="lagadic-2014-idp93992">
            <foreName>Pierre</foreName>
            <surname>Chatelain</surname>
            <initial>P.</initial>
          </persName>
          <persName key="lagadic-2014-idm26496">
            <foreName>Alexandre</foreName>
            <surname>Krupa</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Nassir</foreName>
            <surname>Navab</surname>
            <initial>N.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">IEEE Int. Conf. on Robotics and Automation, ICRA'16</title>
        <loc>Stockholm, Sweden</loc>
        <imprint>
          <dateStruct>
            <month>May</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01274748" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01274748</ref>
        </imprint>
        <meeting id="cid85011">
          <title>IEEE International Conference on Robotics and Automation</title>
          <num>2016</num>
          <abbr type="sigle">ICRA</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid68" type="inproceedings" rend="year" n="cite:chaumette:hal-01385400">
      <identifiant type="hal" value="hal-01385400"/>
      <analytic>
        <title level="a">Visual servoing with and without image processing</title>
        <author>
          <persName key="lagadic-2014-idm27984">
            <foreName>Francois</foreName>
            <surname>Chaumette</surname>
            <initial>F.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="yes" x-editorial-board="yes">
        <title level="m">Plenary talk, IEEE International Conference on Real-time Computing and Robotics, RCAR‚Äô2016</title>
        <loc>Angkor Wat, Cambodia</loc>
        <imprint>
          <dateStruct>
            <month>June</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01385400" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01385400</ref>
        </imprint>
        <meeting id="cid625465">
          <title>IEEE International Conference on Real-time Computing and Robotics</title>
          <num>2016</num>
          <abbr type="sigle">RCAR</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid67" type="inproceedings" rend="year" n="cite:chaumette:hal-01385401">
      <identifiant type="hal" value="hal-01385401"/>
      <analytic>
        <title level="a">Visual servoing</title>
        <author>
          <persName key="lagadic-2014-idm27984">
            <foreName>Francois</foreName>
            <surname>Chaumette</surname>
            <initial>F.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="yes" x-editorial-board="yes">
        <title level="m">Tutorial on Vision for Robotics, IEEE Int. Conf. on Robotics and Automation</title>
        <loc>Stockholm, Sweden</loc>
        <imprint>
          <dateStruct>
            <month>May</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01385401" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01385401</ref>
        </imprint>
        <meeting id="cid85011">
          <title>IEEE International Conference on Robotics and Automation</title>
          <num>2016</num>
          <abbr type="sigle">ICRA</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid29" type="inproceedings" rend="year" n="cite:chevrie:hal-01304860">
      <identifiant type="hal" value="hal-01304860"/>
      <analytic>
        <title level="a">Needle Steering Fusing Direct Base Manipulation and Tip-based Control</title>
        <author>
          <persName key="lagadic-2014-idp95240">
            <foreName>Jason</foreName>
            <surname>Chevrie</surname>
            <initial>J.</initial>
          </persName>
          <persName key="lagadic-2014-idm26496">
            <foreName>Alexandre</foreName>
            <surname>Krupa</surname>
            <initial>A.</initial>
          </persName>
          <persName key="lagadic-2014-idp68368">
            <foreName>Marie</foreName>
            <surname>Babel</surname>
            <initial>M.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">IEEE Int. Conf. on Robotics and Automation, ICRA'16</title>
        <loc>Stockholm, Sweden</loc>
        <imprint>
          <dateStruct>
            <month>May</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01304860" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01304860</ref>
        </imprint>
        <meeting id="cid85011">
          <title>IEEE International Conference on Robotics and Automation</title>
          <num>2016</num>
          <abbr type="sigle">ICRA</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid26" type="inproceedings" rend="year" n="cite:chevrie:hal-01355484">
      <identifiant type="hal" value="hal-01355484"/>
      <analytic>
        <title level="a">Online prediction of needle shape deformation in moving soft tissues from visual feedback</title>
        <author>
          <persName key="lagadic-2014-idp95240">
            <foreName>Jason</foreName>
            <surname>Chevrie</surname>
            <initial>J.</initial>
          </persName>
          <persName key="lagadic-2014-idm26496">
            <foreName>Alexandre</foreName>
            <surname>Krupa</surname>
            <initial>A.</initial>
          </persName>
          <persName key="lagadic-2014-idp68368">
            <foreName>Marie</foreName>
            <surname>Babel</surname>
            <initial>M.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">IEEE/RSJ Int. Conf. on Intelligent Robots and Systems, IROS'16</title>
        <loc>Daejeon, South Korea</loc>
        <imprint>
          <dateStruct>
            <month>October</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">2357-2362</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01355484" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01355484</ref>
        </imprint>
        <meeting id="cid93437">
          <title>IEEE RSJ International Conference on Intelligent Robots and Systems</title>
          <num>2016</num>
          <abbr type="sigle">IROS</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid30" type="inproceedings" rend="year" n="cite:claudio:hal-01385408">
      <identifiant type="hal" value="hal-01385408"/>
      <analytic>
        <title level="a">Vision-based manipulation with the humanoid robot Romeo</title>
        <author>
          <persName key="lagadic-2014-idp75040">
            <foreName>Giovanni</foreName>
            <surname>Claudio</surname>
            <initial>G.</initial>
          </persName>
          <persName key="lagadic-2014-idp72528">
            <foreName>Fabien</foreName>
            <surname>Spindler</surname>
            <initial>F.</initial>
          </persName>
          <persName key="lagadic-2014-idm27984">
            <foreName>Fran√ßois</foreName>
            <surname>Chaumette</surname>
            <initial>F.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">IEEE-RAS International Conference on Humanoid Robotics, Humanoids 2016</title>
        <loc>Cancun, Mexico</loc>
        <imprint>
          <dateStruct>
            <month>November</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01385408" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01385408</ref>
        </imprint>
        <meeting id="cid623883">
          <title>IEEE International Conference on Humanoid Robotics</title>
          <num>2016</num>
          <abbr type="sigle">Humanoids</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid36" type="inproceedings" rend="year" n="cite:cui:hal-01355393">
      <identifiant type="hal" value="hal-01355393"/>
      <analytic>
        <title level="a">Three-Dimensional Visual Tracking and Pose Estimation in Scanning Electron Microscopes</title>
        <author>
          <persName key="lagadic-2014-idp96504">
            <foreName>Le</foreName>
            <surname>Cui</surname>
            <initial>L.</initial>
          </persName>
          <persName key="lagadic-2014-idp71088">
            <foreName>Eric</foreName>
            <surname>Marchand</surname>
            <initial>E.</initial>
          </persName>
          <persName>
            <foreName>Sinan</foreName>
            <surname>Haliyo</surname>
            <initial>S.</initial>
          </persName>
          <persName>
            <foreName>St√©phane</foreName>
            <surname>R√©gnier</surname>
            <initial>S.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">IEEE/RSJ Int. Conf. on Intelligent Robots and Systems, IROS'16</title>
        <loc>Daejeon, South Korea</loc>
        <imprint>
          <dateStruct>
            <month>October</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">5210-5215</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01355393" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01355393</ref>
        </imprint>
        <meeting id="cid93437">
          <title>IEEE RSJ International Conference on Intelligent Robots and Systems</title>
          <num>2016</num>
          <abbr type="sigle">IROS</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid15" type="inproceedings" rend="year" n="cite:devigne:hal-01355410">
      <identifiant type="hal" value="hal-01355410"/>
      <analytic>
        <title level="a">Low complex sensor-based shared control for power wheelchair navigation</title>
        <author>
          <persName key="lagadic-2015-idp94120">
            <foreName>Louise</foreName>
            <surname>Devigne</surname>
            <initial>L.</initial>
          </persName>
          <persName key="lagadic-2014-idp102752">
            <foreName>Vishnu</foreName>
            <surname>Karakkat Narayanan</surname>
            <initial>V.</initial>
          </persName>
          <persName key="lagadic-2014-idp76296">
            <foreName>Fran√ßois</foreName>
            <surname>Pasteau</surname>
            <initial>F.</initial>
          </persName>
          <persName key="lagadic-2014-idp68368">
            <foreName>Marie</foreName>
            <surname>Babel</surname>
            <initial>M.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">IROS2016 - IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
        <loc>Daejeon, South Korea</loc>
        <imprint>
          <dateStruct>
            <month>October</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">5434-5439</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01355410" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01355410</ref>
        </imprint>
        <meeting id="cid93437">
          <title>IEEE RSJ International Conference on Intelligent Robots and Systems</title>
          <num>2016</num>
          <abbr type="sigle">IROS</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid44" type="inproceedings" rend="year" n="cite:drevelle:hal-01415435">
      <identifiant type="hal" value="hal-01415435"/>
      <analytic>
        <title level="a">Convergence domain of image-based visual servoing with a line-scan camera</title>
        <author>
          <persName key="lagadic-2014-idp69816">
            <foreName>Vincent</foreName>
            <surname>Drevelle</surname>
            <initial>V.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">Summer Workshop on Interval Methods, SWIM 2016</title>
        <loc>Lyon, France</loc>
        <imprint>
          <dateStruct>
            <month>June</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01415435" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01415435</ref>
        </imprint>
        <meeting id="cid363358">
          <title>Small Workshop on Interval Methods</title>
          <num>2016</num>
          <abbr type="sigle">SWIM</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid24" type="inproceedings" rend="year" n="cite:duflot:hal-01355488">
      <identifiant type="hal" value="hal-01355488"/>
      <analytic>
        <title level="a">Shearlet Transform: a Good Candidate for Compressed Sensing in Optical Coherence Tomography</title>
        <author>
          <persName key="lagadic-2014-idp100248">
            <foreName>Lesley-Ann</foreName>
            <surname>Duflot</surname>
            <initial>L.-A.</initial>
          </persName>
          <persName key="lagadic-2014-idm26496">
            <foreName>Alexandre</foreName>
            <surname>Krupa</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Brahim</foreName>
            <surname>Tamadazte</surname>
            <initial>B.</initial>
          </persName>
          <persName>
            <foreName>Nicolas</foreName>
            <surname>Andreff</surname>
            <initial>N.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">IEEE Int. Conf. on Engineering in Medicine and Biology Society, EMBC'16</title>
        <loc>Orlando, United States</loc>
        <imprint>
          <dateStruct>
            <month>August</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">435-438</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01355488" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01355488</ref>
        </imprint>
        <meeting id="cid31825">
          <title>Annual International Conference of the IEEE Engineering in Medicine and Biology Society</title>
          <num>38</num>
          <abbr type="sigle">EMBC</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid23" type="inproceedings" rend="year" n="cite:duflot:hal-01355414">
      <identifiant type="hal" value="hal-01355414"/>
      <analytic>
        <title level="a">Shearlet-based vs. Photometric-based Visual Servoing for Robot-assisted Medical Applications</title>
        <author>
          <persName key="lagadic-2014-idp100248">
            <foreName>Lesley-Ann</foreName>
            <surname>Duflot</surname>
            <initial>L.-A.</initial>
          </persName>
          <persName key="lagadic-2014-idm26496">
            <foreName>Alexandre</foreName>
            <surname>Krupa</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Brahim</foreName>
            <surname>Tamadazte</surname>
            <initial>B.</initial>
          </persName>
          <persName>
            <foreName>Nicolas</foreName>
            <surname>Andreff</surname>
            <initial>N.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">IEEE/RSJ Int. Conf. on Intelligent Robots and Systems, IROS'16</title>
        <loc>Daejeon, South Korea</loc>
        <imprint>
          <dateStruct>
            <month>October</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">4099-4104</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01355414" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01355414</ref>
        </imprint>
        <meeting id="cid93437">
          <title>IEEE RSJ International Conference on Intelligent Robots and Systems</title>
          <num>2016</num>
          <abbr type="sigle">IROS</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid27" type="inproceedings" rend="year" n="cite:duflot:hal-01304753">
      <identifiant type="hal" value="hal-01304753"/>
      <analytic>
        <title level="a">Toward Ultrasound-based Visual Servoing using Shearlet Coefficients</title>
        <author>
          <persName key="lagadic-2014-idp100248">
            <foreName>Lesley-Ann</foreName>
            <surname>Duflot</surname>
            <initial>L.-A.</initial>
          </persName>
          <persName key="lagadic-2014-idm26496">
            <foreName>Alexandre</foreName>
            <surname>Krupa</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Brahim</foreName>
            <surname>Tamadazte</surname>
            <initial>B.</initial>
          </persName>
          <persName>
            <foreName>Nicolas</foreName>
            <surname>Andreff</surname>
            <initial>N.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">IEEE Int. Conf. on Robotics and Automation, ICRA'16</title>
        <loc>Stockholm, Sweden</loc>
        <imprint>
          <dateStruct>
            <month>May</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01304753" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01304753</ref>
        </imprint>
        <meeting id="cid85011">
          <title>IEEE International Conference on Robotics and Automation</title>
          <num>2016</num>
          <abbr type="sigle">ICRA</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid40" type="inproceedings" rend="year" n="cite:jiddi:hal-01355581">
      <identifiant type="hal" value="hal-01355581"/>
      <analytic>
        <title level="a">Reflectance and Illumination Estimation for Realistic Augmentations of Real Scenes</title>
        <author>
          <persName key="lagadic-2015-idp98008">
            <foreName>Salma</foreName>
            <surname>Jiddi</surname>
            <initial>S.</initial>
          </persName>
          <persName key="rap-2014-idm40528">
            <foreName>Philippe</foreName>
            <surname>Robert</surname>
            <initial>P.</initial>
          </persName>
          <persName key="lagadic-2014-idp71088">
            <foreName>Eric</foreName>
            <surname>Marchand</surname>
            <initial>E.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">IEEE Int. Symp. on Mixed and Augmented Reality, ISMAR'16 (poster session)</title>
        <loc>Merida, Mexico</loc>
        <imprint>
          <dateStruct>
            <month>September</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01355581" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01355581</ref>
        </imprint>
        <meeting id="cid399588">
          <title>IEEE International Symposium on Mixed and Augmented Reality</title>
          <num>2016</num>
          <abbr type="sigle">ISMAR</abbr>
        </meeting>
      </monogr>
      <note type="bnote">Poster</note>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid11" type="inproceedings" rend="year" n="cite:karakkatnarayanan:hal-01355481">
      <identifiant type="hal" value="hal-01355481"/>
      <analytic>
        <title level="a">A semi-autonomous framework for human-aware and user intention driven wheelchair mobility assistance</title>
        <author>
          <persName key="lagadic-2014-idp102752">
            <foreName>Vishnu</foreName>
            <surname>Karakkat Narayanan</surname>
            <initial>V.</initial>
          </persName>
          <persName key="e-motion-2014-idp69016">
            <foreName>Anne</foreName>
            <surname>Spalanzani</surname>
            <initial>A.</initial>
          </persName>
          <persName key="lagadic-2014-idp68368">
            <foreName>Marie</foreName>
            <surname>Babel</surname>
            <initial>M.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">IEEE/RSJ Int. Conf. on Intelligent Robots and Systems, IROS'16</title>
        <loc>Daejeon, South Korea</loc>
        <imprint>
          <dateStruct>
            <month>October</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">4700-4707</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01355481" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01355481</ref>
        </imprint>
        <meeting id="cid93437">
          <title>IEEE RSJ International Conference on Intelligent Robots and Systems</title>
          <num>2016</num>
          <abbr type="sigle">IROS</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid13" type="inproceedings" rend="year" n="cite:karakkatnarayanan:hal-01330889">
      <identifiant type="hal" value="hal-01330889"/>
      <analytic>
        <title level="a">Analysis of an adaptive strategy for equitably approaching and joining human interactions</title>
        <author>
          <persName key="lagadic-2014-idp102752">
            <foreName>Vishnu</foreName>
            <surname>Karakkat Narayanan</surname>
            <initial>V.</initial>
          </persName>
          <persName key="e-motion-2014-idp69016">
            <foreName>Anne</foreName>
            <surname>Spalanzani</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Ren C.</foreName>
            <surname>Luo</surname>
            <initial>R. C.</initial>
          </persName>
          <persName key="lagadic-2014-idp68368">
            <foreName>Marie</foreName>
            <surname>Babel</surname>
            <initial>M.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">IEEE Int. Symp. on Robot and Human Interactive Communication, RO-MAN</title>
        <loc>New-York, United States</loc>
        <title level="s">IEEE Int. Symp. on Robot and Human Interactive Communication, RO-MAN</title>
        <imprint>
          <dateStruct>
            <month>August</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01330889" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01330889</ref>
        </imprint>
        <meeting id="cid90473">
          <title>IEEE International Symposium on Robot and Human Interactive Communication</title>
          <num>23</num>
          <abbr type="sigle">RO-MAN</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid58" type="inproceedings" rend="year" n="cite:kenmogne:hal-01415432">
      <identifiant type="hal" value="hal-01415432"/>
      <analytic>
        <title level="a">Image-based Mobile Robot localization using Interval Methods</title>
        <author>
          <persName>
            <foreName>Ide-Flore</foreName>
            <surname>Kenmogne</surname>
            <initial>I.-F.</initial>
          </persName>
          <persName key="lagadic-2014-idp69816">
            <foreName>Vincent</foreName>
            <surname>Drevelle</surname>
            <initial>V.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">Summer Workshop on Interval Methods, SWIM 2016</title>
        <loc>Lyon, France</loc>
        <imprint>
          <dateStruct>
            <month>June</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01415432" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01415432</ref>
        </imprint>
        <meeting id="cid363358">
          <title>Small Workshop on Interval Methods</title>
          <num>2016</num>
          <abbr type="sigle">SWIM</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid17" type="inproceedings" rend="year" n="cite:magassouba:hal-01355394">
      <identifiant type="hal" value="hal-01355394"/>
      <analytic>
        <title level="a">Audio-based robot controlfrom interchannel level difference and absolute sound energy</title>
        <author>
          <persName key="lagadic-2014-idp103992">
            <foreName>Aly</foreName>
            <surname>Magassouba</surname>
            <initial>A.</initial>
          </persName>
          <persName key="panama-2014-idm11584">
            <foreName>Nancy</foreName>
            <surname>Bertin</surname>
            <initial>N.</initial>
          </persName>
          <persName key="lagadic-2014-idm27984">
            <foreName>Fran√ßois</foreName>
            <surname>Chaumette</surname>
            <initial>F.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">IEEE/RSJ Int. Conf. on Intelligent Robots and Systems, IROS'16</title>
        <loc>Daejeon, South Korea</loc>
        <imprint>
          <dateStruct>
            <month>October</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">1992-1999</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01355394" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01355394</ref>
        </imprint>
        <meeting id="cid93437">
          <title>IEEE RSJ International Conference on Intelligent Robots and Systems</title>
          <num>2016</num>
          <abbr type="sigle">IROS</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid32" type="inproceedings" rend="year" n="cite:magassouba:hal-01408422">
      <identifiant type="hal" value="hal-01408422"/>
      <analytic>
        <title level="a">Binaural auditory interaction without HRTF for humanoid robots: A sensor-based control approach</title>
        <author>
          <persName key="lagadic-2014-idp103992">
            <foreName>Aly</foreName>
            <surname>Magassouba</surname>
            <initial>A.</initial>
          </persName>
          <persName key="panama-2014-idm11584">
            <foreName>Nancy</foreName>
            <surname>Bertin</surname>
            <initial>N.</initial>
          </persName>
          <persName key="lagadic-2014-idm27984">
            <foreName>Fran√ßois</foreName>
            <surname>Chaumette</surname>
            <initial>F.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">Workshop on Multimodal Sensor-based Control for HRI and soft manipulation, IROS'2016</title>
        <loc>Daejeon, South Korea</loc>
        <imprint>
          <dateStruct>
            <month>October</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01408422" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01408422</ref>
        </imprint>
        <meeting id="cid625466">
          <title>IROS Workshop on Multimodal Sensor-based Control for HRI and soft manipulation</title>
          <num>2016</num>
          <abbr type="sigle"/>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid18" type="inproceedings" rend="year" n="cite:magassouba:hal-01277589">
      <identifiant type="hal" value="hal-01277589"/>
      <analytic>
        <title level="a">First applications of sound-based control on a mobile robot equipped with two microphones</title>
        <author>
          <persName key="lagadic-2014-idp103992">
            <foreName>Aly</foreName>
            <surname>Magassouba</surname>
            <initial>A.</initial>
          </persName>
          <persName key="panama-2014-idm11584">
            <foreName>Nancy</foreName>
            <surname>Bertin</surname>
            <initial>N.</initial>
          </persName>
          <persName key="lagadic-2014-idm27984">
            <foreName>Fran√ßois</foreName>
            <surname>Chaumette</surname>
            <initial>F.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">IEEE Int. Conf. on Robotics and Automation, ICRA'16</title>
        <loc>Stockholm, Sweden</loc>
        <imprint>
          <dateStruct>
            <month>May</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01277589" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01277589</ref>
        </imprint>
        <meeting id="cid85011">
          <title>IEEE International Conference on Robotics and Automation</title>
          <num>2016</num>
          <abbr type="sigle">ICRA</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid66" type="inproceedings" rend="year" n="cite:marchand:hal-01385405">
      <identifiant type="hal" value="hal-01385405"/>
      <analytic>
        <title level="a">Visual tracking</title>
        <author>
          <persName key="lagadic-2014-idp71088">
            <foreName>Eric</foreName>
            <surname>Marchand</surname>
            <initial>E.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="yes" x-editorial-board="yes">
        <title level="m">Tutorial on Vision for Robotics, IEEE Int. Conf. on Robotics and Automation</title>
        <loc>Stockholm, Sweden</loc>
        <imprint>
          <dateStruct>
            <month>May</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01385405" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01385405</ref>
        </imprint>
        <meeting id="cid85011">
          <title>IEEE International Conference on Robotics and Automation</title>
          <num>2016</num>
          <abbr type="sigle">ICRA</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid9" type="inproceedings" rend="year" n="cite:martins:hal-01403953">
      <identifiant type="hal" value="hal-01403953"/>
      <analytic>
        <title level="a">Adaptive Direct RGB-D Registration and Mapping for Large Motions</title>
        <author>
          <persName key="lagadic-2014-idp105232">
            <foreName>Renato</foreName>
            <surname>Martins</surname>
            <initial>R.</initial>
          </persName>
          <persName>
            <foreName>Eduardo</foreName>
            <surname>Fernandez-Moral</surname>
            <initial>E.</initial>
          </persName>
          <persName key="lagadic-2014-idp65680">
            <foreName>Patrick</foreName>
            <surname>Rives</surname>
            <initial>P.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">Asian Conference on Computer Vision, ACCV 2016</title>
        <loc>Taipei, Taiwan</loc>
        <imprint>
          <dateStruct>
            <month>November</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01403953" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01403953</ref>
        </imprint>
        <meeting id="cid35974">
          <title>Asian Conference on Computer Vision</title>
          <num>2016</num>
          <abbr type="sigle">ACCV</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid19" type="inproceedings" rend="year" n="cite:martins:hal-01403961">
      <identifiant type="hal" value="hal-01403961"/>
      <analytic>
        <title level="a">Increasing the Convergence Domain of RGB-D Direct Registration Methods for Vision-based Localization in Large Scale Environments</title>
        <author>
          <persName key="lagadic-2014-idp105232">
            <foreName>Renato</foreName>
            <surname>Martins</surname>
            <initial>R.</initial>
          </persName>
          <persName key="lagadic-2014-idp65680">
            <foreName>Patrick</foreName>
            <surname>Rives</surname>
            <initial>P.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">Workshop on Planning, Perception and Navigation for Intelligent Vehicles ‚Äì IEEE Intelligent Transportation Systems Conference, ITSC PPNIV</title>
        <loc>Rio de Janeiro, Brazil</loc>
        <imprint>
          <dateStruct>
            <month>November</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01403961" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01403961</ref>
        </imprint>
        <meeting id="cid103437">
          <title>IROS Workshop on Planning, Perception and Navigation for Intelligent Vehicles</title>
          <num>2016</num>
          <abbr type="sigle"/>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid65" type="inproceedings" rend="year" n="cite:park:hal-01290368">
      <identifiant type="doi" value="10.1145/2856400.2856405"/>
      <identifiant type="hal" value="hal-01290368"/>
      <analytic>
        <title level="a">Dynamically balanced and plausible trajectory planning for human-like characters</title>
        <author>
          <persName>
            <foreName>Chonhyon</foreName>
            <surname>Park</surname>
            <initial>C.</initial>
          </persName>
          <persName>
            <foreName>Jae Sung</foreName>
            <surname>Park</surname>
            <initial>J. S.</initial>
          </persName>
          <persName key="mimetic-2014-idp106864">
            <foreName>Steve</foreName>
            <surname>Tonneau</surname>
            <initial>S.</initial>
          </persName>
          <persName>
            <foreName>Nicolas</foreName>
            <surname>Mansard</surname>
            <initial>N.</initial>
          </persName>
          <persName key="mimetic-2014-idm27032">
            <foreName>Franck</foreName>
            <surname>Multon</surname>
            <initial>F.</initial>
          </persName>
          <persName key="mimetic-2014-idm25552">
            <foreName>Julien</foreName>
            <surname>Pettr√©</surname>
            <initial>J.</initial>
          </persName>
          <persName>
            <foreName>Dinesh</foreName>
            <surname>Manocha</surname>
            <initial>D.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">20th ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games</title>
        <loc>Redmond, United States</loc>
        <imprint>
          <publisher>
            <orgName type="organisation">ACM</orgName>
          </publisher>
          <dateStruct>
            <month>February</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">39-48</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01290368" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01290368</ref>
        </imprint>
        <meeting id="cid21691">
          <title>ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games</title>
          <num>20</num>
          <abbr type="sigle">I3D</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid25" type="inproceedings" rend="year" n="cite:patlanrosales:hal-01355406">
      <identifiant type="hal" value="hal-01355406"/>
      <analytic>
        <title level="a">Automatic palpation for quantitative ultrasound elastography by visual servoing and force control</title>
        <author>
          <persName>
            <foreName>Pedro</foreName>
            <surname>Patlan-Rosales</surname>
            <initial>P.</initial>
          </persName>
          <persName key="lagadic-2014-idm26496">
            <foreName>Alexandre</foreName>
            <surname>Krupa</surname>
            <initial>A.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">IEEE/RSJ Int. Conf. on Intelligent Robots and Systems, IROS'16</title>
        <loc>Daejeon, South Korea</loc>
        <imprint>
          <dateStruct>
            <month>October</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">2357-2362</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01355406" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01355406</ref>
        </imprint>
        <meeting id="cid93437">
          <title>IEEE RSJ International Conference on Intelligent Robots and Systems</title>
          <num>2016</num>
          <abbr type="sigle">IROS</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid4" type="inproceedings" rend="year" n="cite:rajbista:hal-01355382">
      <identifiant type="hal" value="hal-01355382"/>
      <analytic>
        <title level="a">Appearance-based Indoor Navigation by IBVS using Mutual Information</title>
        <author>
          <persName>
            <foreName>Suman</foreName>
            <surname>Raj Bista</surname>
            <initial>S.</initial>
          </persName>
          <persName key="lagadic-2014-idp67120">
            <foreName>Paolo</foreName>
            <surname>Robuffo Giordano</surname>
            <initial>P.</initial>
          </persName>
          <persName key="lagadic-2014-idm27984">
            <foreName>Francois</foreName>
            <surname>Chaumette</surname>
            <initial>F.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">IEEE Int. Conf. on Control, Automation, Robotics and Vision, ICARCV 2016</title>
        <loc>Phuket, Thailand</loc>
        <imprint>
          <dateStruct>
            <month>November</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01355382" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01355382</ref>
        </imprint>
        <meeting id="cid82825">
          <title>IEEE International Conference on Control, Automation, Robotics and Vision</title>
          <num>2016</num>
          <abbr type="sigle">ICARCV</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid34" type="inproceedings" rend="year" n="cite:schiano:hal-01348543">
      <identifiant type="hal" value="hal-01348543"/>
      <analytic>
        <title level="a">A Rigidity-Based Decentralized Bearing Formation Controller for Groups of Quadrotor UAVs</title>
        <author>
          <persName key="lagadic-2014-idp110272">
            <foreName>Fabrizio</foreName>
            <surname>Schiano</surname>
            <initial>F.</initial>
          </persName>
          <persName>
            <foreName>Antonio</foreName>
            <surname>Franchi</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Daniel</foreName>
            <surname>Zelazo</surname>
            <initial>D.</initial>
          </persName>
          <persName key="lagadic-2014-idp67120">
            <foreName>Paolo</foreName>
            <surname>Robuffo Giordano</surname>
            <initial>P.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2016</title>
        <loc>Daejeon, South Korea</loc>
        <imprint>
          <dateStruct>
            <month>October</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">5099-5106</biblScope>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01348543" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01348543</ref>
        </imprint>
        <meeting id="cid93437">
          <title>IEEE RSJ International Conference on Intelligent Robots and Systems</title>
          <num>2016</num>
          <abbr type="sigle">IROS</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid57" type="inproceedings" rend="year" n="cite:spica:hal-01355789">
      <identifiant type="hal" value="hal-01355789"/>
      <analytic>
        <title level="a">Active Decentralized Scale Estimation for Bearing-Based Localization</title>
        <author>
          <persName key="lagadic-2014-idp111520">
            <foreName>Riccardo</foreName>
            <surname>Spica</surname>
            <initial>R.</initial>
          </persName>
          <persName key="lagadic-2014-idp67120">
            <foreName>Paolo</foreName>
            <surname>Robuffo Giordano</surname>
            <initial>P.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">IEEE/RSJ Int. Conf. on Intelligent Robots and Systems, IROS'16</title>
        <loc>Daejeon, South Korea</loc>
        <imprint>
          <dateStruct>
            <month>October</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01355789" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01355789</ref>
        </imprint>
        <meeting id="cid93437">
          <title>IEEE RSJ International Conference on Intelligent Robots and Systems</title>
          <num>2016</num>
          <abbr type="sigle">IROS</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid64" type="inproceedings" rend="year" n="cite:vantoll:hal-01392267">
      <identifiant type="doi" value="10.1145/2994258.2994262"/>
      <identifiant type="hal" value="hal-01392267"/>
      <analytic>
        <title level="a">A comparative study of navigation meshes</title>
        <author>
          <persName>
            <foreName>Wouter</foreName>
            <surname>Van Toll</surname>
            <initial>W.</initial>
          </persName>
          <persName>
            <foreName>Roy</foreName>
            <surname>Triesscheijn</surname>
            <initial>R.</initial>
          </persName>
          <persName>
            <foreName>Marcelo</foreName>
            <surname>Kallmann</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>Ramon</foreName>
            <surname>Oliva</surname>
            <initial>R.</initial>
          </persName>
          <persName>
            <foreName>Nuria</foreName>
            <surname>Pelechano</surname>
            <initial>N.</initial>
          </persName>
          <persName key="mimetic-2014-idm25552">
            <foreName>Julien</foreName>
            <surname>Pettr√©</surname>
            <initial>J.</initial>
          </persName>
          <persName>
            <foreName>Roland</foreName>
            <surname>Geraerts</surname>
            <initial>R.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">MIG '16 - 9th International Conference on Motion in Games</title>
        <loc>San Francisco, United States</loc>
        <imprint>
          <publisher>
            <orgName>ACM</orgName>
          </publisher>
          <dateStruct>
            <month>October</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">91 - 100</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01392267" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01392267</ref>
        </imprint>
        <meeting id="cid292760">
          <title>International Conference on Motion in Games</title>
          <num>9</num>
          <abbr type="sigle">MIG</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid39" type="inproceedings" rend="year" n="cite:yol:hal-01304728">
      <identifiant type="hal" value="hal-01304728"/>
      <analytic>
        <title level="a">Vision-based navigation in low earth orbit</title>
        <author>
          <persName key="lagadic-2014-idp78864">
            <foreName>Aurelien</foreName>
            <surname>Yol</surname>
            <initial>A.</initial>
          </persName>
          <persName key="lagadic-2014-idp71088">
            <foreName>Eric</foreName>
            <surname>Marchand</surname>
            <initial>E.</initial>
          </persName>
          <persName key="lagadic-2014-idm27984">
            <foreName>Francois</foreName>
            <surname>Chaumette</surname>
            <initial>F.</initial>
          </persName>
          <persName>
            <foreName>Keyvan</foreName>
            <surname>Kanani</surname>
            <initial>K.</initial>
          </persName>
          <persName>
            <foreName>Thomas</foreName>
            <surname>Chabot</surname>
            <initial>T.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">Int. Symp. on Artificial Intelligence, Robotics and Automation in Space, i-SAIRAS'16</title>
        <loc>Beijing, China</loc>
        <imprint>
          <dateStruct>
            <month>June</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01304728" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01304728</ref>
        </imprint>
        <meeting id="cid311983">
          <title>International Symposium on Artificial Intelligence, Robotics and Automation in Space</title>
          <num>2016</num>
          <abbr type="sigle">i-SAIRAS</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid78" type="techreport" rend="year" n="cite:briot:hal-01400575">
      <identifiant type="hal" value="hal-01400575"/>
      <monogr>
        <title level="m">Technical Report associated with the Paper: "Determining the Singularities for the Observation of Three Image Lines"</title>
        <author>
          <persName>
            <foreName>S√©bastien</foreName>
            <surname>Briot</surname>
            <initial>S.</initial>
          </persName>
          <persName>
            <foreName>Philippe</foreName>
            <surname>Martinet</surname>
            <initial>P.</initial>
          </persName>
          <persName key="lagadic-2014-idm27984">
            <foreName>Fran√ßois</foreName>
            <surname>Chaumette</surname>
            <initial>F.</initial>
          </persName>
        </author>
        <imprint>
          <publisher>
            <orgName type="institution">IRCCyN ; IRISA</orgName>
          </publisher>
          <dateStruct>
            <month>November</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01400575" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01400575</ref>
        </imprint>
      </monogr>
      <note type="typdoc">Research Report</note>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid45" type="misc" rend="year" n="cite:agravante:hal-01358639">
      <identifiant type="hal" value="hal-01358639"/>
      <monogr x-scientific-popularization="no">
        <title level="m">Combining visual servoing and walking in an acceleration resolved whole-body control framework</title>
        <author>
          <persName key="lagadic-2016-idp184672">
            <foreName>Don Joven</foreName>
            <surname>Agravante</surname>
            <initial>D. J.</initial>
          </persName>
          <persName key="lagadic-2014-idm27984">
            <foreName>Francois</foreName>
            <surname>Chaumette</surname>
            <initial>F.</initial>
          </persName>
        </author>
        <imprint>
          <dateStruct>
            <month>June</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01358639" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01358639</ref>
        </imprint>
      </monogr>
      <note type="bnote">Journ√©es Nationales de la Recherche Humano√Øde, Toulouse, France</note>
    </biblStruct>
    
    <biblStruct id="lagadic-2016-bid31" type="misc" rend="year" n="cite:claudio:hal-01358124">
      <identifiant type="hal" value="hal-01358124"/>
      <monogr x-scientific-popularization="no">
        <title level="m">Dual arm manipulation and whole body control with the humanoid robot Romeo by visual servoing</title>
        <author>
          <persName key="lagadic-2014-idp75040">
            <foreName>Giovanni</foreName>
            <surname>Claudio</surname>
            <initial>G.</initial>
          </persName>
          <persName key="lagadic-2016-idp184672">
            <foreName>Don Joven</foreName>
            <surname>Agravante</surname>
            <initial>D. J.</initial>
          </persName>
          <persName key="lagadic-2014-idp72528">
            <foreName>Fabien</foreName>
            <surname>Spindler</surname>
            <initial>F.</initial>
          </persName>
          <persName key="lagadic-2014-idm27984">
            <foreName>Fran√ßois</foreName>
            <surname>Chaumette</surname>
            <initial>F.</initial>
          </persName>
        </author>
        <imprint>
          <dateStruct>
            <month>June</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01358124" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01358124</ref>
        </imprint>
      </monogr>
      <note type="bnote">Journ√©es Nationales de la Recherche Humano√Øde, Toulouse, France</note>
    </biblStruct>
  </biblio>
</raweb>
