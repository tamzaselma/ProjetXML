<?xml version="1.0" encoding="utf-8"?>
<raweb xmlns:xlink="http://www.w3.org/1999/xlink" xml:lang="en" year="2016">
  <identification id="pervasive_interaction" isproject="true">
    <shortname>PERVASIVE INTERACTION</shortname>
    <projectName>Pervasive Interaction</projectName>
    <theme-de-recherche>Robotics and Smart environments</theme-de-recherche>
    <domaine-de-recherche>Perception, Cognition and Interaction</domaine-de-recherche>
    <header_dates_team>Creation of the Team: 2016 April 01</header_dates_team>
    <LeTypeProjet>Team</LeTypeProjet>
    <keywordsSdN>
      <term>1.2.5. - Internet of things</term>
      <term>1.4. - Ubiquitous Systems</term>
      <term>2.6.2. - Middleware</term>
      <term>3.2.3. - Inference</term>
      <term>3.2.5. - Ontologies</term>
      <term>3.4.1. - Supervised learning</term>
      <term>3.4.2. - Unsupervised learning</term>
      <term>3.4.3. - Reinforcement learning</term>
      <term>3.4.5. - Bayesian methods</term>
      <term>3.4.6. - Neural networks</term>
      <term>5. - Interaction, multimedia and robotics</term>
      <term>5.1. - Human-Computer Interaction</term>
      <term>5.1.3. - Haptic interfaces</term>
      <term>5.1.5. - Body-based interfaces</term>
      <term>5.1.6. - Tangible interfaces</term>
      <term>5.1.7. - Multimodal interfaces</term>
      <term>5.1.8. - 3D User Interfaces</term>
      <term>5.3.3. - Pattern recognition</term>
      <term>5.4. - Computer vision</term>
      <term>5.4.1. - Object recognition</term>
      <term>5.4.2. - Activity recognition</term>
      <term>5.4.4. - 3D and spatio-temporal reconstruction</term>
      <term>5.4.5. - Object tracking and motion analysis</term>
      <term>5.4.6. - Object localization</term>
      <term>5.6. - Virtual reality, augmented reality</term>
      <term>5.7.1. - Sound</term>
      <term>5.7.3. - Speech</term>
      <term>5.7.4. - Analysis</term>
      <term>5.10. - Robotics</term>
      <term>5.10.1. - Design</term>
      <term>5.10.2. - Perception</term>
      <term>5.10.3. - Planning</term>
      <term>5.10.4. - Robot control</term>
      <term>5.10.5. - Robot interaction (with the environment, humans, other robots)</term>
      <term>5.10.7. - Learning</term>
      <term>5.10.8. - Cognitive robotics and systems</term>
      <term>5.11. - Smart spaces</term>
      <term>5.11.1. - Human activity analysis and recognition</term>
      <term>5.11.2. - Home/building control and interaction</term>
      <term>8. - Artificial intelligence</term>
      <term>8.1. - Knowledge</term>
      <term>8.2. - Machine learning</term>
      <term>8.3. - Signal analysis</term>
      <term>8.5. - Robotics</term>
      <term>8.7. - AI algorithmics</term>
    </keywordsSdN>
    <keywordsSecteurs>
      <term>1.3.2. - Cognitive science</term>
      <term>2.1. - Well being</term>
      <term>6.4. - Internet of things</term>
      <term>8.1. - Smart building/home</term>
      <term>8.2. - Connected city</term>
    </keywordsSecteurs>
    <DescriptionTeam>Inria teams are typically groups of researchers working on the definition of a common project, and objectives, with the goal to arrive at the creation of a project-team. Such project-teams may include other partners (universities or research institutions).</DescriptionTeam>
    <UR name="Grenoble"/>
  </identification>
  <team id="uid1">
    <person key="prima-2014-idm28656">
      <firstname>James</firstname>
      <lastname>Crowley</lastname>
      <categoryPro>Enseignant</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Team leader, INP Grenoble, Professor</moreinfo>
      <hdr>oui</hdr>
    </person>
    <person key="prima-2014-idm27392">
      <firstname>Sabine</firstname>
      <lastname>Coquillart</lastname>
      <categoryPro>Chercheur</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Inria, Senior Researcher</moreinfo>
      <hdr>oui</hdr>
    </person>
    <person key="prima-2014-idp66112">
      <firstname>Thierry</firstname>
      <lastname>Fraichard</lastname>
      <categoryPro>Chercheur</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Inria, Senior Researcher</moreinfo>
      <hdr>oui</hdr>
    </person>
    <person key="e-motion-2014-idp66304">
      <firstname>Emmanuel</firstname>
      <lastname>Mazer</lastname>
      <categoryPro>Chercheur</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>CNRS, Senior Researcher</moreinfo>
      <hdr>oui</hdr>
    </person>
    <person key="prima-2014-idp70256">
      <firstname>Patrick</firstname>
      <lastname>Reignier</lastname>
      <categoryPro>Enseignant</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>INP Grenoble, Professor</moreinfo>
      <hdr>oui</hdr>
    </person>
    <person key="prima-2014-idp71704">
      <firstname>Dominique</firstname>
      <lastname>Vaufreydaz</lastname>
      <categoryPro>Enseignant</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Univ Grenoble Alpes, Associate Professor</moreinfo>
    </person>
    <person key="prima-2014-idp74216">
      <firstname>Raffaella</firstname>
      <lastname>Balzarini</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="prima-2015-idp78304">
      <firstname>Pierre</firstname>
      <lastname>Baret</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Inria, until Nov 2016</moreinfo>
    </person>
    <person key="pervasive_interaction-2016-idp32112">
      <firstname>Maxime</firstname>
      <lastname>Belgodere</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Inria, from Oct 2016</moreinfo>
    </person>
    <person key="prima-2014-idp76720">
      <firstname>Nicolas</firstname>
      <lastname>Bonnefond</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="prima-2015-idp82088">
      <firstname>Stanislaw</firstname>
      <lastname>Borkowski</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="prima-2015-idp115456">
      <firstname>Remi</firstname>
      <lastname>Canillas</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>CNRS, until Aug 2016</moreinfo>
    </person>
    <person key="prima-2015-idp84592">
      <firstname>Thibaud</firstname>
      <lastname>Flury</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Inria, until Sep 2016</moreinfo>
    </person>
    <person key="prima-2014-idp90512">
      <firstname>Remi</firstname>
      <lastname>Pincent</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="pervasive_interaction-2016-idp46896">
      <firstname>Nachwa</firstname>
      <lastname>Abou Bakr</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Univ Grenoble Alpes, from Feb 2016</moreinfo>
    </person>
    <person key="prima-2015-idp92168">
      <firstname>Amr</firstname>
      <lastname>Al-Zouhri Al-Yafi</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Univ Grenoble Alpes, from Oct 2015</moreinfo>
    </person>
    <person key="prima-2014-idp101904">
      <firstname>Etienne</firstname>
      <lastname>Balit</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Univ Grenoble Alpes, from Oct 2014</moreinfo>
    </person>
    <person key="prima-2014-idp104384">
      <firstname>Jingtao</firstname>
      <lastname>Chen</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Univ Grenoble Alpes, from Sep 2015</moreinfo>
    </person>
    <person key="prima-2015-idp96864">
      <firstname>Julien</firstname>
      <lastname>Cumin</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Orange Labs, from Oct 2015</moreinfo>
    </person>
    <person key="prima-2015-idp98096">
      <firstname>Jose</firstname>
      <lastname>Da Silva Filho</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Univ Grenoble Alpes, from Sep 2015</moreinfo>
    </person>
    <person key="e-motion-2014-idp85304">
      <firstname>Marvin</firstname>
      <lastname>Faix</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>CNRS</moreinfo>
    </person>
    <person key="pervasive_interaction-2016-idp63984">
      <firstname>Raphael Lambert</firstname>
      <lastname>Frisch</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Univ Grenoble Alpes, from Oct 2016</moreinfo>
    </person>
    <person key="pervasive_interaction-2016-idp66432">
      <firstname>Thomas</firstname>
      <lastname>Guntz</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Inria, from Oct 2016</moreinfo>
    </person>
    <person key="prima-2014-idp110640">
      <firstname>Remi</firstname>
      <lastname>Paulin</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Univ Grenoble Alpes, from Feb 2013</moreinfo>
    </person>
    <person key="prima-2014-idp114328">
      <firstname>Viet Cuong</firstname>
      <lastname>Ta</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Hanoi Polytechnique Institut, from Oct 2015</moreinfo>
    </person>
    <person key="imagine-2014-idp152680">
      <firstname>Catherine</firstname>
      <lastname>Bessiere</lastname>
      <categoryPro>Assistant</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="exmo-2016-idp152656">
      <firstname>Lydie</firstname>
      <lastname>Leon</lastname>
      <categoryPro>Assistant</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Inria, from Sep 2016</moreinfo>
    </person>
    <person key="pervasive_interaction-2016-idp78672">
      <firstname>Inass</firstname>
      <lastname>Ayoub Salloum</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Inria, from Feb 2016 until Jun 2016</moreinfo>
    </person>
    <person key="pervasive_interaction-2016-idp81152">
      <firstname>Clarisse</firstname>
      <lastname>Bayol</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Inria, from Mar 2016 until Jul 2016</moreinfo>
    </person>
    <person key="pervasive_interaction-2016-idp83632">
      <firstname>Thibaut</firstname>
      <lastname>Bouchet</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Inria, until Jul 2016</moreinfo>
    </person>
    <person key="pervasive_interaction-2016-idp86096">
      <firstname>Arthur</firstname>
      <lastname>Canonne</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Inria, from Feb 2016 until Jun 2016</moreinfo>
    </person>
    <person key="pervasive_interaction-2016-idp88576">
      <firstname>Antonin</firstname>
      <lastname>Carlesso</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Inria, from Mar 2016</moreinfo>
    </person>
    <person key="prima-2014-idp105648">
      <firstname>Joelle</firstname>
      <lastname>Coutaz</lastname>
      <categoryPro>Enseignant</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Univ Grenoble Alpes, Professor</moreinfo>
    </person>
    <person key="pervasive_interaction-2016-idp93520">
      <firstname>Sebastien</firstname>
      <lastname>Crouzy</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Inria, from Feb 2016 until Jun 2016</moreinfo>
    </person>
    <person key="pervasive_interaction-2016-idp96000">
      <firstname>Paul</firstname>
      <lastname>Faye</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Inria, from Feb 2016 until Jun 2016</moreinfo>
    </person>
    <person key="pervasive_interaction-2016-idp98432">
      <firstname>Jeremie</firstname>
      <lastname>Finiel</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Inria, from Feb 2016 until Jun 2016</moreinfo>
    </person>
    <person key="ctrl-a-2015-idp79576">
      <firstname>Alia</firstname>
      <lastname>Hajjar</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Inria, until Jul 2016</moreinfo>
    </person>
    <person key="pervasive_interaction-2016-idp103344">
      <firstname>Joanne</firstname>
      <lastname>Poulenard</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Inria, Nov 2016</moreinfo>
    </person>
    <person key="pervasive_interaction-2016-idp105808">
      <firstname>Mauricio</firstname>
      <lastname>Vazquez</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Inria, from Feb 2016 until Jun 2016</moreinfo>
    </person>
    <person key="willow-2014-idp128352">
      <firstname>Tuan Hung</firstname>
      <lastname>Vu</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Univ. Grenoble Alpes, from Feb 2016 until Jun 2016</moreinfo>
    </person>
    <person key="pervasive_interaction-2016-idp110800">
      <firstname>Hang</firstname>
      <lastname>Yu</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Univ. Grenoble Alpes, until Jun 2016</moreinfo>
    </person>
  </team>
  <presentation id="uid2">
    <bodyTitle>Overall Objectives</bodyTitle>
    <subsection id="uid3" level="1">
      <bodyTitle>Overall Objectives</bodyTitle>
      <p>Pervasive Interaction will develop theories and models for context aware, sociable interaction with systems and services that are dynamically composed from collections of interconnected smart objects.
The project is structured a focused 4-year project to explore the use of situation modelling as a technological foundation for situated behavior for smart objects.</p>
      <p>The research program Pervasive Interaction is designed to respond to the following four research questions:</p>
      <simplelist>
        <li id="uid4">
          <p noindent="true">Q1: What are the most appropriate computational techniques for acquiring and using situation models for situated behavior by smart objects?</p>
        </li>
        <li id="uid5">
          <p noindent="true">Q2: What perception and action techniques are most appropriate for situated smart objects?</p>
        </li>
        <li id="uid6">
          <p noindent="true">Q3: Can we use situation modelling as a foundation for sociable interaction with smart objects?</p>
        </li>
        <li id="uid7">
          <p noindent="true">Q4: Can we use situated smart objects as a form of immersive media?</p>
        </li>
      </simplelist>
    </subsection>
  </presentation>
  <fondements id="uid8">
    <bodyTitle>Research Program</bodyTitle>
    <subsection id="uid9" level="1">
      <bodyTitle>Situation Models</bodyTitle>
      <p>Situation Modelling, Situation Awareness, Probabilistic Description Logistics
</p>
      <p>The objectives of this research area are to develop and refine new computational techniques that improve the reliability and performance of situation models, extend the range of possible application domains, and reduce the cost of developing and maintaining situation models. Important research challenges include developing machine-learning techniques to automatically acquire and adapt situation models through interaction, development of techniques to reason and learn about appropriate behaviors, and the development of new algorithms and data structures for representing situation models.</p>
      <p>Over the next four years we will address the following research challenges:</p>
      <p>Techniques for learning and adapting situation models:
Hand crafting of situation models is currently an expensive process requiring extensive trial and error. We will investigate combination of interactive design tools coupled with supervised and semi-supervised learning techniques for constructing initial, simplified prototype situation models in the laboratory. One possible approach is to explore developmental learning to enrich and adapt the range of situations and behaviors through interaction with users.</p>
      <p>Reasoning about actions and behaviors:
Constructing systems for reasoning about actions and their consequences is an important open challenge. We will explore integration of planning techniques for operationalizing actions sequences within behaviors, and for constructing new action sequences when faced with unexpected difficulties. We will also investigate reasoning techniques within the situation modeling process for anticipating the consequences of actions, events and phenomena.</p>
      <p>Algorithms and data structures for situation models:
In recent years, we have experimented with an architecture for situated interaction inspired by work in human factors. This model organises perception and interaction as a cyclic process in which directed perception is used to detect and track entities, verify relations between entities, detect trends, anticipate consequences and plan actions. Each phase of this process raises interesting challenges questions algorithms and programming techniques. We will experiment alternative programming techniques representing and reasoning about situation models both in terms of difficulty of specification and development and in terms of efficiency of the resulting implementation. We will also investigate the use of probabilistic graph models as a means to better accommodate uncertain and unreliable information. In particular, we will experiment with using probabilistic predicates for defining situations, and maintaining likelihood scores over multiple situations within a context. Finally, we will investigate the use of simulation as technique for reasoning about consequences of actions and phenomena.</p>
      <p>Probabilistic Description Logics:
In our work, we will explore the use of probabilistic predicates for representing relations within situation models. As with our earlier work, entities and roles will be recognized using multi-modal perceptual processes constructed with supervised and semi-supervised learning [Brdiczka 07], [Barraquand 12]. However, relations will be expressed with probabilistic predicates. We will explore learning based techniques to probabilistic values for elementary predicates, and propagate these through probabilistic representation for axioms using Probabilistic Graphical Models and/or Bayesian Networks.</p>
      <p>The challenges in this research area will be addressed through three specific research actions covering situation modelling in homes, learning on mobile devices, and reasoning in critical situations.</p>
      <subsection id="uid10" level="2">
        <bodyTitle>Learning Routine patterns of activity in the home. </bodyTitle>
        <p>The objective of this research action is to develop a scalable approach to learning routine patterns of activity in a home using situation models. Information about user actions is used to construct situation models in which key elements are semantic time, place, social role and actions. Activities are encoded as sequences of situations. Recurrent activities are detected as sequences of activities that occur at a specific time and place each day. Recurrent activities provide routines what can be used to predict future actions and anticipate needs and services. An early demonstration has been to construct an intelligent assistant that can respond to and filter communications.</p>
        <p>This research action is carried out as part of the doctoral research of Julian Cumin in cooperation with researchers at Orange labs, Meylan. Results are to be published at Ubicomp, Ambient intelligence, Intelligent Environments and IEEE Transactions on System Man and Cybernetics. Julien Cumin will complete and defend his doctoral thesis in 2018.</p>
      </subsection>
      <subsection id="uid11" level="2">
        <bodyTitle>Learning Patterns of Activity with Mobile Devices</bodyTitle>
        <p>The objective of this research action is to develop techniques to observe and learn recurrent patterns of activity using the full suite of sensors available on mobile devices such as tablets and smart phones. Most mobile devices include seven or more sensors organized in 4 groups: Positioning Sensors, Environmental Sensors, Communications Subsystems, and Sensors for Human-Computer Interaction. Taken together, these sensors can provide a very rich source of information about individual activity.</p>
        <p>In this area we explore techniques to observe activity with mobiles devices in order to learn daily patterns of activity. We will explore supervised and semi-supervised learning to construct systems to recognize places and relevant activities. Location and place information, semantic time of day, communication activities, inter-personal interactions, and travel activities (walking, driving, riding public transportation, etc.) are recognized as probabilistic predicates and used to construct situation models. Recurrent sequences of situations will be detected and recorded to provide an ability to predict upcoming situations and anticipate needs for information and services.</p>
        <p>Our goal is to develop a theory for building context aware services that can be deployed as part of the mobile applications that companies such as SNCF and RATP use to interact with clients. For example, a current project concerns systems that observe daily travel routines for the Paris region RATP metro and SNCF commuter trains. This system learns individual travel routines on the mobile device without the need to divulge information about personal travel to a cloud based system. The resulting service will consult train and metro schedules to assure that planned travel is feasible and to suggest alternatives in the case of travel disruptions. Similar applications are under discussion for the SNCF inter-city travel and Air France for air travel.</p>
        <p>This research action is conducted in collaboration with the Inria Startup Situ8ed. The current objective is to deploy and evaluate a first prototype App during 2017. Techniques will be used commercially by Situ8ed for products to be deployed as early as 2019.</p>
      </subsection>
      <subsection id="uid12" level="2">
        <bodyTitle>Observing and Modelling Competence and Awareness in Critical Situations</bodyTitle>
        <p>The aim of this research action is to experimentally evaluate and compare current theories for mental modelling for problem solving and attention in stressful situations, as well as to refine theories and techniques for observing visual fixation, attention and emotion. We are currently investigating differences in visual attention, emotional response and mental states of chess experts and chess novices solving chess problems and participating in chess matches. We observe physiological responses, mental states and visual attention using eye-tracking, long term and instantaneous face-expressions (micro-expressions), skin conductivity, blood flow (BVP), posture and other information extracted from audio-visual recordings of players.</p>
        <p>We expect that a high degree of expertise in chess should be reflected in patterns of eye movement and emotional reaction in accordance with the game situation. Information from visual attention will be used to determine and model the degree to which a player understands the game situation in terms of abstract configurations of chess pieces rather than the positions of individual pieces. Information about the emotional reactions of players will be expressed as trajectories in the physiological space of pleasure, arousal and dominance to determine if a players understanding of the game situation can be observed from emotional reaction to game play.</p>
        <p>This work is supported by the ANR project CEEGE in cooperation with the department of NeuroCognition of Univ. Bielefeld, as well as the LIG internal project AirBorne in cooperation with the French Air Force training center at ISTRE. Work in this area includes the Doctoral research of Thomas Guntz to be defended in 2019.</p>
      </subsection>
      <subsection id="uid13" level="2">
        <bodyTitle>Bibliography</bodyTitle>
        <p>[Brdiczka 07] O. Brdiczka, "Learning Situation Models for Context-Aware Services", Doctoral Thesis of the INPG, 25 may 2007.</p>
        <p>[Barraquand 12] R. Barraquand, "Design of Sociable Technologies", Doctoral Thesis of the University Grenoble Alps, 2 Feb 2012.</p>
      </subsection>
    </subsection>
    <subsection id="uid14" level="1">
      <bodyTitle>Perception of People, Activities and Emotions</bodyTitle>
      <p>Machine perception is fundamental for situated behavior. Work in this area will concern construction of perceptual components using computer vision, acoustic perception, accelerometers and other embedded sensors. These include low-cost accelerometers [Bao 04], gyroscopic sensors and magnetometers, vibration sensors, electromagnetic spectrum and signal strength (wifi, bluetooth, GSM), infrared presence detectors, and bolometric imagers, as well as microphones and cameras. With electrical usage monitoring, every power switch can be used as a sensor [Fogarty 06], [Coutaz 16]. We will develop perceptual components for integrated vision systems that combine a low-cost imaging sensors with on-board image processing and wireless communications in a small, low-cost package. Such devices are increasingly available, with the enabling manufacturing technologies driven by the market for integrated imaging sensors on mobile devices. Such technology enables the use of embedded computer vision as a practical sensor for smart objects.</p>
      <p>Research challenges to be addressed in this area include development of practical techniques that can be deployed on smart objects for perception of people and their activities in real world environments, integration and fusion of information from a variety of sensor modalities with different response times and levels of abstraction, and perception of human attention, engagement, and emotion using visual and acoustic sensors.</p>
      <p>Work in this research area will focus on three specific Research Actions</p>
      <subsection id="uid15" level="2">
        <bodyTitle>Multi-modal perception and modeling of activities</bodyTitle>
        <p>The objective of this research action is to develop techniques for observing and scripting activities for common household tasks such as cooking and cleaning. An important part of this project involves acquiring annotated multi-modal datasets of activity using an extensive suite of visual, acoustic and other sensors. We are interested in real-time on-line techniques that capture and model full body movements, head motion and manipulation actions as 3D articulated motion sequences decorated with semantic labels for individual actions and activities with multiple RGB and RGB-D cameras.</p>
        <p>We will explore the integration of 3D articulated models with appearance based recognition approaches and statistical learning for modeling behaviors. Such techniques provide an important enabling technology for context aware services in smart environments [Coutaz 05], [Crowley 15], investigated by Pervasive Interaction team, as well as research on automatic cinematography and film editing investigated by the Imagine team [Gandhi 13] [Gandhi 14] [Ronfard 14] [Galvane 15]. An important challenge is to determine which techniques are most appropriate for detecting, modeling and recognizing a large vocabulary of actions and activities under different observational conditions.</p>
        <p>We will explore representations of behavior that encodes both temporal-spatial structure and motion at multiple levels of abstraction. We will further propose parameters to encode temporal constraints between actions in the activity classification model using a combination of higher-level action grammars [Pirsiavash 14] and episodic reasoning [Santofimia 14] [Edwards 14].</p>
        <p>Our method will be evaluated using long-term recorded dataset that contains recordings of activities in home environments. This work will be reported in the IEEE Conference on Face and Gesture Recognition, IEEE transactions on Pattern Analysis and Machine Intelligence, (PAMI) et IEEE Transactions on Systems man and Cybernetics. This work is carried out in the doctoral research of Nachwa Abubakr in cooperation with Remi Ronfard of the Imagine Team of Inria.</p>
      </subsection>
      <subsection id="uid16" level="2">
        <bodyTitle>Perception with low-cost integrated sensors </bodyTitle>
        <p>In this research action, we will continue work on low-cost integrated sensors using visible light, infrared, and acoustic perception. We will continue development of integrated visual sensors that combine micro-cameras and embedded image processing for detecting and recognizing objects in storage areas. We will combine visual and acoustic sensors to monitor activity at work-surfaces. Low cost real-time image analysis procedures will be designed that acquire and process images directly as they are acquired by the sensor.</p>
        <p>Bolometric image sensors measure the Far Infrared emissions of surfaces in order to provide an image in which each pixel is an estimate of surface temperature. Within the European MIRTIC project, Grenoble startup, ULIS has created a relatively low-cost Bolometric image sensor (Retina) that provides small images of 80 by 80 pixels taken from the Far-infrared spectrum. Each pixel provides an estimate of surface temperature. Working with Schneider Electric, engineers in the Pervasive Interaction team had developed a small, integrated sensor that combines the MIRTIC Bolometric imager with a microprocessor for on-board image processing. The package has been equipped with a fish-eye lens so that an overhead sensor mounted at a height of 3 meters has a field of view of approximately 5 by 5 meters. Real-time algorithms have been demonstrated for detecting, tracking and counting people, estimating their trajectories and work areas, and estimating posture.</p>
        <p>Many of the applications scenarios for Bolometric sensors proposed by Schneider Electric assume a scene model that assigns pixels to surfaces of the floor, walls, windows, desks or other items of furniture. The high cost of providing such models for each installation of the sensor would prohibit most practical applications. We have recently developed a novel automatic calibration algorithm that determines the nature of the surface under each pixel of the sensor.</p>
        <p>Work in this area will continue to develop low-cost real time infrared image sensing, as well as explore combinations of far-infrared images with RGB and RGBD images.</p>
      </subsection>
      <subsection id="uid17" level="2">
        <bodyTitle>Observation of emotion from physiological responses in critical situations</bodyTitle>
        <p>Recent research in Cognitive Science indicates that the human emotions result in physiological manifestations in the heart rate, skin conductance, skin color, body movements and facial expressions. It has been proposed that these manifestations can be measured by observation of skin color, body motions, and facial expressions and modeled as activation levels in three dimensions known as Valence, Arousal and Dominance. The goal if this project is to evaluate the effectiveness of visual and acoustic perception technique for measuring these physiological manifestations.</p>
        <p>Experimental data will be collected by observing subjects engaged in playing chess. A special apparatus has been constructed that allows synchronized recording from a color camera, Kinect2 3D camera, and Tobi Eye Tracker of a player seated before a computer generated display of a chess board. The masters student will participate in the definition and recording of scenarios for recording test data, apply recently proposed techniques from the scientific literature for measuring emotions, and provide a comparative performance evaluation of various techniques. The project is expected to reveal the relative effectiveness of computer vision and other techniques for observing human emotions.</p>
      </subsection>
      <subsection id="uid18" level="2">
        <bodyTitle>Bibliography</bodyTitle>
        <p>[Bao 04] L. Bao, and S. S. Intille. "Activity recognition from user-annotated acceleration data.", IEEE Pervasive computing. Springer Berlin Heidelberg, pp1-17, 2004.</p>
        <p>[Fogarty 06] J. Fogarty, C. Au and S. E. Hudson. "Sensing from the basement: a feasibility study of unobtrusive and low-cost home activity recognition." In Proceedings of the 19th annual ACM symposium on User interface software and technology, UIST 2006, pp. 91-100. ACM, 2006.</p>
        <p>[Coutaz 16] J. Coutaz and J.L. Crowley, A First-Person Experience with End-User Development for Smart Homes. IEEE Pervasive Computing, 15(2), pp.26-39, 2016.</p>
        <p>[Coutaz 05] J. Coutaz, J.L. Crowley, S. Dobson, D. Garlan, "Context is key", Communications of the ACM, 48 (3), 49-53, 2005.</p>
        <p>[Crowley 15] J. L. Crowley and J. Coutaz, "An Ecological View of Smart Home Technologies", 2015 European Conference on Ambient Intelligence, Athens, Greece, Nov. 2015.</p>
        <p>[Gandhi 13] Vineet Gandhi, Remi Ronfard. "Detecting and Naming Actors in Movies using Generative Appearance Models", Computer Vision and Pattern Recognition, 2013.</p>
        <p>[Gandhi 14] Vineet Gandhi, R√©mi Ronfard, Michael Gleicher. "Multi-Clip Video Editing from a Single Viewpoint", European Conference on Visual Media Production, 2014</p>
        <p>[Ronfard 14] R. Ronfard, N. Szilas. "Where story and media meet: computer generation of narrative discourse". Computational Models of Narrative, 2014.</p>
        <p>[Galvane 15] Quentin Galvane, R√©mi Ronfard, Christophe Lino, Marc Christie. "Continuity Editing for 3D Animation". AAAI Conference on Artificial Intelligence, Jan 2015.</p>
        <p>[Pirsiavash 14] Hamed Pirsiavash , Deva Ramanan, "Parsing Videos of Actions with Segmental Grammars", Computer Vision and Pattern Recognition, p.612-619, 2014.</p>
        <p>[Edwards 14] C. Edwards. 2014, "Decoding the language of human movement". Commun. ACM 57, 12, 12-14, November 2014.</p>
      </subsection>
    </subsection>
    <subsection id="uid19" level="1">
      <bodyTitle>Sociable Interaction with Smart Objects</bodyTitle>
      <p>Reeves and Nass argue that a social interface may be the truly universal interface [Reeves 98]. Current systems lack ability for social interaction because they are unable to perceive and understand humans or to learn from interaction with humans. One of the goals of the research to be performed in Pervasive Interaction is to provide such abilities.</p>
      <p>Work in research area RA3 will demonstrate the use of situation models for sociable interaction with smart objects and companion robots. We will explore the use of situation models as a representation for sociable interaction. Our goal in this research is to develop methods to endow an artificial agent with the ability to acquire social common sense using the implicit feedback obtained from interaction with people. We believe that such methods can provide a foundation for socially polite man-machine interaction, and ultimately for other forms of cognitive abilities. We propose to capture social common sense by training the appropriateness of behaviors in social situations. A key challenge is to employ an adequate representation for social situations.</p>
      <p>Knowledge for sociable interaction will be encoded as a network of situations that capture both linguistic and non-verbal interaction cues and proper behavioral responses. Stereotypical social interactions will be represented as trajectories through the situation graph. We will explore methods that start from simple stereotypical situation models and extending a situation graph through the addition of new situations and the splitting of existing situations.
An important aspect of social common sense is the ability to act appropriately in social situations. We propose to learn the association between behaviors and social situation using reinforcement learning. Situation models will be used as a structure for learning appropriateness of actions and behaviors that may be chosen in each situation, using reinforcement learning to determine a score for appropriateness based on feedback obtained by observing partners during interaction.</p>
      <p>Work in this research area will focus on four specific Research Actions</p>
      <subsection id="uid20" level="2">
        <bodyTitle>Moving with people</bodyTitle>
        <p>Our objective in this area is to establish the foundations for robot motions that are aware of human social situation that move in a manner that complies with the social context, social expectations, social conventions and cognitive abilities of humans. Appropriate and socially compliant interactions require the ability for real time perception of the identity, social role, actions, activities and intents of humans. Such perception can be used to dynamically model the current situation in order to understand the situation and to compute the appropriate course of action for the robot depending on the task at hand.</p>
        <p>To reach this objective, we propose to investigate three interacting research areas:</p>
        <simplelist>
          <li id="uid21">
            <p noindent="true">Modeling the context and situation of human activities for motion planning</p>
          </li>
          <li id="uid22">
            <p noindent="true">Planning and acting in a social context.</p>
          </li>
          <li id="uid23">
            <p noindent="true">Identifying and modeling interaction behaviors.</p>
          </li>
        </simplelist>
        <p>In particular, we will investigate techniques that allow a tele-presence robot, such as the BEAM system, to autonomously navigate in crowds of people as may be found at the entry to a conference room, or in the hallway of a scientific meeting. We will also continue experiments on autonomous motion for personal assistance robots (project PRAMAD). Work in this area includes the doctoral work of Jos‚aö¬© da Silva, to be defended in 2019.</p>
      </subsection>
      <subsection id="uid24" level="2">
        <bodyTitle>Understanding and communicating intentions from motion</bodyTitle>
        <p>This research area concerns the communication through motion. When two or more people move as a group, their motion is regulated by implicit rules that signal a shared sense of social conventions and social roles. For example, moving towards someone while looking directly at them signals an intention for engagement. In certain cultures, subtle rules dictate who passes through a door first or last. When humans move in groups, they implicitly communicate intentions with motion. In this research area, we will explore the scientific literature on proxemics and the social sciences on such movements, in order to encode and evaluate techniques for socially appropriate motion by robots.</p>
      </subsection>
      <subsection id="uid25" level="2">
        <bodyTitle>Socially aware interaction</bodyTitle>
        <p>This research area concerns socially aware man-machine interaction. Appropriate and socially compliant interaction requires the ability for real time perception of the identity, social role, actions, activities and intents of humans. Such perception can be used to dynamically model the current situation in order to understand the context and to compute the appropriate course of action for the task at hand. Performing such interactions in manner that respects and complies with human social norms and conventions requires models for social roles and norms of behavior as well as the ability to adapt to local social conventions and individual user preferences.
In this research area, we will complement research area 3.2 with other forms of communication and interaction, including expression with stylistic face expressions rendered on a tablet, facial gestures, body motions and speech synthesis. We will experiment with use of commercially available tool for spoken language interaction in conjunction with expressive gestures.</p>
      </subsection>
      <subsection id="uid26" level="2">
        <bodyTitle>Stimulating affection and persuasion with affective devices. </bodyTitle>
        <p>This research area concerns technologies that can stimulate affection and engagement, as well as induce changes in behavior. When acting as a coach or cooking advisor, smart objects must be credible and persuasive. One way to achieve this goal is to express affective feedbacks while interacting. This can be done using sound, light and/or complex moves when the system is composed of actuators.</p>
        <p>Research in this area will address 3 questions:</p>
        <orderedlist>
          <li id="uid27">
            <p noindent="true">How do human perceive affective signals expressed by smart objects (including robots)?</p>
          </li>
          <li id="uid28">
            <p noindent="true">How does physical embodiment effect perception of affect by humans?</p>
          </li>
          <li id="uid29">
            <p noindent="true">What are the most effective models and tools for animation of affective expression?</p>
          </li>
        </orderedlist>
        <p>Both the physical form and the range of motion have important impact on the ability of a system to inspire affection. We will create new models to propose a generic animation model, and explore the effectiveness of different forms of motion in stimulating affect.</p>
      </subsection>
      <subsection id="uid30" level="2">
        <bodyTitle>Bibliography</bodyTitle>
        <p>[Reeves 98] B. Reeves and C. Nass, The Media Equation: how People Treat Computers, Television, and New Media Like Real People and Places. Cambridge University Press, 1998.</p>
      </subsection>
    </subsection>
    <subsection id="uid31" level="1">
      <bodyTitle>Interaction with Pervasive Smart Objects and Displays</bodyTitle>
      <p>Currently, the most effective technologies for new media for sensing, perception and experience are provided by virtual and augmented realities [Van Krevelen 2010]. At the same time, the most effective means to augment human cognitive abilities are provided by access to information spaces such as the world-wide-web using graphical user interfaces. A current challenge is to bring these two media together.</p>
      <p>Display technologies continue to decrease exponentially, driven largely by investment in consumer electronics as well as the overall decrease in cost of microelectronics. A consequence has been an increasing deployment of digital displays in both public and private spaces. This trend is likely to accelerate, as new technologies and growth in available communications bandwidth enable ubiquitous low-cost access to information and communications.</p>
      <p>The arrival of pervasive displays raises a number of interesting challenges for situated multi-modal interaction. For example:</p>
      <orderedlist>
        <li id="uid32">
          <p noindent="true">Can we use perception to detect user engagement and identify users in public spaces?</p>
        </li>
        <li id="uid33">
          <p noindent="true">Can we replace traditional pointing hardware with gaze and gesture based interaction?</p>
        </li>
        <li id="uid34">
          <p noindent="true">Can we tailor information and interaction for truly situated interaction, providing the right information at the right time using the right interaction modality?</p>
        </li>
        <li id="uid35">
          <p noindent="true">How can we avoid information overload and unnecessary distraction with pervasive displays?</p>
        </li>
      </orderedlist>
      <p>It is increasingly possible to embed sensors and displays in clothing and ordinary devices, leading to new forms of tangible and wearable interaction with information. This raises challenges such as</p>
      <orderedlist>
        <li id="uid36">
          <p noindent="true">What are the tradeoffs between large-scale environmental displays and wearable displays using technologies such as e-textiles and pico-projector?</p>
        </li>
        <li id="uid37">
          <p noindent="true">How can we manage the tradeoffs between implicit and explicit interaction with both tangible and wearable interaction?</p>
        </li>
        <li id="uid38">
          <p noindent="true">How can we determine the appropriate modalities for interaction?</p>
        </li>
        <li id="uid39">
          <p noindent="true">How can we make users aware of interaction possibilities without creating distraction?</p>
        </li>
      </orderedlist>
      <p>In addition to display and communications, the continued decrease in microelectronics has also driven an exponential decrease in cost of sensors, actuators, and computing resulting in an exponential growth in the number of smart objects in human environments. Current models for systems organization are based on centralized control, in which a controller or local hub, orchestrates smart objects, generally in connection with cloud computing. This model creates problems with privacy and ownership of information. An alternative is to organize local collections of smart objects to provide distributed services without the use of a centralized controller. The science of ecology can provide an architectural model for such organization.</p>
      <p>This approach raises a number of interesting research challenges for pervasive interaction:</p>
      <orderedlist>
        <li id="uid40">
          <p noindent="true">Can we devise distributed models for multi-modal fusion and interaction with information on heterogeneous devices?</p>
        </li>
        <li id="uid41">
          <p noindent="true">Can we devise models for distributed interaction that migrates over available devices as the user changes location and task?</p>
        </li>
        <li id="uid42">
          <p noindent="true">Can we manage migration of interaction over devices in a manner that provides seamless immersive interaction with information, services and media?</p>
        </li>
        <li id="uid43">
          <p noindent="true">Can we provide models of distributed interaction that conserve the interaction context as services migrate?</p>
        </li>
      </orderedlist>
      <p>Research Actions for Interaction with Pervasive Smart Objects for the period 2017 - 2020 include</p>
      <subsection id="uid44" level="2">
        <bodyTitle>Situated interaction with pervasive displays </bodyTitle>
        <p>The emergence of low-cost interactive displays will enable a confluence of virtual and physical environments. Our goal in this area is to go beyond simple graphical user interfaces in such environments to provide immersive multi-sensorial interaction and communication. A primary concern will be interaction technologies that blend visual with haptic/tactile feedback and 3D interaction and computer vision. We will investigate the use of visual-tactile feedback as well as vibratory signals to augment multi-sensorial interaction and communication. The focus will be on the phenomena of immersive interaction in real worlds that can be made possible by the blending of physical and virtual in ordinary environments.</p>
      </subsection>
      <subsection id="uid45" level="2">
        <bodyTitle>Wearable and tangible interaction with smart textiles and wearable projectors </bodyTitle>
        <p>Opportunities in this area result from the emergence of new forms of interactive media using smart objects. We will explore the use of smart objects as tangible interfaces that make it possible to experience and interact with information and services by grasping and manipulating objects. We will explore the use of sensors and actuators in clothing and wearable devices such as gloves, hats and wrist bands both as a means of unobtrusively sensing human intentions and emotional states and as a means of stimulating human senses through vibration and sound. We will explore the new forms of interaction and immersion made possible by deploying interactive displays over large areas of an environment.</p>
      </subsection>
      <subsection id="uid46" level="2">
        <bodyTitle>Pervasive interaction with ecologies of smart objects in the home</bodyTitle>
        <p>In this research area, we will explore and evaluate interaction with ecologies of smart objects in home environments. We will explore development of a range of smart objects that provide information services, such as devices for Episodic Memory for work surfaces and storage areas, devices to provide energy efficient control of environmental conditions, and interactive media that collect and display information. We propose to develop a new class of socially aware managers that coordinate smart objects and manage logistics in functional areas such as the kitchen, living rooms, closets, bedrooms, bathroom or office.</p>
      </subsection>
      <subsection id="uid47" level="2">
        <bodyTitle>Bibliography</bodyTitle>
        <p>[Van Krevelen 10] D. W. F. Van Krevelen and R. Poelman, A survey of augmented reality technologies, applications and limitations. International Journal of Virtual Reality, 9(2), 1, 2010</p>
      </subsection>
    </subsection>
  </fondements>
  <domaine id="uid48">
    <bodyTitle>Application Domains</bodyTitle>
    <subsection id="uid49" level="1">
      <bodyTitle>Smart Energy Systems</bodyTitle>
      <p>Participants: Amr Alyafi, Patrick Reignier
Partners: UMR G-SCOP, UMR LIG (Persuasive Interaction, IIHM), CEA Liten, PACTE, Vesta Systems and Elithis.</p>
      <p>Work in this area explores techniques for a user centric energy management system, where user needs and tacit knowledge drive the search of solutions. These are calculated using a flexible energy model of the living areas. The system is personified by energy consultants with which building actors such as building owners, building managers, technical operators but also occupants, can interact with in order to co-define energy strategies, benefiting of both assets: tacit knowledge of human actors, and measurement with computation capabilities of calculators. Putting actors in the loop, i.e. making energy not only visible but also controllable is the needed step before large deployment of energy management solutions. It is proposed to develop interactive energy consultants for all the actors, which are energy management aided systems embedding models in order to support the decision making processes. MIRROR (interactive monitoring), WHAT-IF (interactive quantitative simulation), EXPLAIN (interactive qualitative simulation), SUGGEST-AND-ADJUST (interactive management) and RECOMMEND (interactive diagnosis) functionalities will be developed.</p>
    </subsection>
    <subsection id="uid50" level="1">
      <bodyTitle> E-Textile</bodyTitle>
      <p>Participant: Sabine Coquillart</p>
      <p>Partner: LIMSI</p>
      <p>Collaboration with the HAPCO team from LIMSI on e-textiles.</p>
    </subsection>
    <subsection id="uid51" level="1">
      <bodyTitle>Interaction with Pervasive Media</bodyTitle>
      <p>Participants: Sabine Coquillart, Jingtao Chen</p>
      <p>Partners: Inria GRA, GIPSA, G-SCOP</p>
      <p>Pseudo-haptic feedback is a technique aiming to simulate haptic sensations without active haptic feedback devices.
Peudo-haptic techniques have been used to simulate various haptic feedbacks such as stiffness, torques, and mass.
In the framework of Jingtao Chen PhD thesis, a novel pseudo-haptic experiment has been set up. The aim of this experiment is to study the EMG signals during a pseudo-haptic task.
A stiffness discrimination task similar to the one published in Lecuyer's PhD thesis has been chosen. The experimental set-up has been developed, as well as the software controlling the experiment.
Pre-tests are under way. They will be followed by the tests with subjects.</p>
    </subsection>
    <subsection id="uid52" level="1">
      <bodyTitle>Bayesian Reasoning</bodyTitle>
      <p>Participants: Emmanuel Mazer, Marvin Faix</p>
      <p>The development of modern computers is mainly based on increase of performances and decrease of size and energy consumption, with no notable modification of the basic principles of computation. In particular, all the components perform deterministic and exact operations on sets of binary signals. These constraints obviously impede further sizable progresses in terms of speed, miniaturization and power consumption. The main goal of the project MicroBayes is to investigate a radically different approach, using stochastic bit streams to perform computations. The aim of this project is to show that stochastic architectures can outperform standard computers to solve complex inference problems both in terms of execution speed and of power consumption. We will demonstrate the feasibility on two applications involving low level information processing from sensor signals, namely sound source localization and separation.</p>
    </subsection>
  </domaine>
  <logiciels id="uid53">
    <bodyTitle>New Software and Platforms</bodyTitle>
    <subsection id="uid54" level="1">
      <bodyTitle>DomiCube</bodyTitle>
      <simplelist>
        <li id="uid55">
          <p noindent="true">Participant: Rémi Pincent</p>
        </li>
        <li id="uid56">
          <p noindent="true">Contact: Rémi Pincent</p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid57" level="1">
      <bodyTitle>EmoPRAMAD</bodyTitle>
      <p><span class="smallcap" align="left">Keywords:</span> Health - Home care</p>
      <p noindent="true">
        <span class="smallcap" align="left">Functional Description</span>
      </p>
      <p>Within the Pramad project, we want to offer a full affective loop between the companion robot and the elderly people at home. This affective loop is necessary within the context of everyday interaction of elderly and the companion robot. A part of this loop is to make the robot express emotions in response to the emotional state of the user. To do that, we need to test our working hypothesis about the visual representation of emotions with the 3D face of robot. EmoPRAMAD is an evaluation tool designed to conduct comparative studies between human faces and the 3D faces expressing a defined set of emotions.</p>
      <p>The evaluation conducted though EmoPRAMAD concerns both unimodal (facial only) and bimodal conditions (facial/sound). The emotions set is composed of 4 basic emotions (joy, fear, anger, sadness) and a neutral state. While experimenting, the software collects several parameters in order to evaluate more than correctness of the answers: time to respond, length of mouse moves, etc.</p>
      <simplelist>
        <li id="uid58">
          <p noindent="true">Contact: Dominique Vaufreydaz</p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid59" level="1">
      <bodyTitle>Online Movie Director</bodyTitle>
      <simplelist>
        <li id="uid60">
          <p noindent="true">Participants: Patrick Reignier, Dominique Vaufreydaz and James Crowley</p>
        </li>
        <li id="uid61">
          <p noindent="true">Contact: Dominique Vaufreydaz</p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid62" level="1">
      <bodyTitle>SmartEnergy</bodyTitle>
      <p>
        <span class="smallcap" align="left">Functional Description</span>
      </p>
      <p>Inhabitants play a key role in buildings global energy consumption but it is difficult to involve them in energy management. Our objective is to make energy consumption visible by simulating inside a serious game the energy impact of inhabitants behaviours. A serious game is curently under development, coupling a 3D virtual environment and a building energy simulator. The 3D virtual environment is based on the JMonkey 3D engine. New houses can be easily imported using SweetHome 3D and Blender. The building energy simulator is EnergyPlus. The 3D engine and the energy engine are coupled using the Functional Mock-up Interface (FMI) standard. Using this standard will allow to easily switch between existing building energy simulators.</p>
      <simplelist>
        <li id="uid63">
          <p noindent="true">Participant: Patrick Reignier</p>
        </li>
        <li id="uid64">
          <p noindent="true">Contact: Patrick Reignier</p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid65" level="1">
      <bodyTitle>SmartServoFramework</bodyTitle>
      <simplelist>
        <li id="uid66">
          <p noindent="true">Participants: Dominique Vaufreydaz and Emeric Grange</p>
        </li>
        <li id="uid67">
          <p noindent="true">Contact: James Crowley</p>
        </li>
      </simplelist>
      <p>SmartServoFramework is a C++ multiplatform framework used to drive "smart servo" devices such as Dynamixel or HerkuleX actuators. The Framework, developed by members of the PRIMA team runs under Linux (and most Unix systems), Mac OS X and Windows operating systems. SmartServoFramework can run on Raspberry Pi or other similar boards.
This framework can be used with any Dynamixel or HerkuleX devices. Dynamixel devices from Robotis and HerkuleX devices from Dongbu Robot are high-performance networked actuators for robots available in wide range of sizes and strengths. They have adjustable torque, speed, angle limits, and provide various feedback like position, load, voltage and temperature.</p>
    </subsection>
    <subsection id="uid68" level="1">
      <bodyTitle>MobileRGBD</bodyTitle>
      <p><span class="smallcap" align="left">Keywords:</span> Benchmark corpus - Health - Home Care</p>
      <simplelist>
        <li id="uid69">
          <p noindent="true">Contact: Dominique Vaufreydaz</p>
        </li>
        <li id="uid70">
          <p noindent="true">
            <ref xlink:href="http://mobilergbd.inrialpes.fr" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>mobilergbd.<allowbreak/>inrialpes.<allowbreak/>fr</ref>
          </p>
        </li>
      </simplelist>
      <p><span class="smallcap" align="left">Functional Description</span>
MobileRGBD is corpus dedicated to low level RGB-D algorithms benchmarking on mobile platform. We reversed the usual corpus recording paradigm. Our goal is to facilitate ground truth annotation and reproducibility of records among speed, trajectory and environmental variations. As we want to get rid of unpredictable human moves, we used dummies in order to play static users in the environment (see figure). Interest of dummies resides in the fact that they do not move between two recordings. It is possible to record the same robot move in order to evaluate performance of detection algorithms varying speed.
This benchmark corpus is intended for ¬´low level¬ª RGB-D algorithm family like 3D-SLAM, body/skeleton tracking or face tracking using a mobile robot. Using this open corpus, researchers can find a way to answer several questions:
System performance under variations in operating conditions?
on a mobile robot, what is the maximum linear/angular speed supported by the algorithm?
which variables impact the algorithm?
evaluate suitable height/angle of the mounted RGB-D sensor to reach goals: monitoring everyday live is different from searching fallen persons on the floor;
finally, what is the performance on an algorithm with regards to others?</p>
    </subsection>
    <subsection id="uid71" level="1">
      <bodyTitle>Unix Interface for InfraRed Sensor</bodyTitle>
      <simplelist>
        <li id="uid72">
          <p noindent="true">Author: Pierre Baret</p>
        </li>
        <li id="uid73">
          <p noindent="true">Contact: James Crowley</p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid74" level="1">
      <bodyTitle>Virtual Reality rehabilitation platform for Complex Regional Pain Syndrome</bodyTitle>
      <p>Participants: Charles-Henry Dufetel, Jing Tao Chen, Sabine Coquillart
Design and development of a Virtual Reality rehabilitation platform for CRPS (Complex Regional Pain Syndrome). This application is aimed at studying the impact of visual feedback on the rehabilitation process of a patient. It focuses on the hand by allowing the physical therapist to perturb (amplify or decrease) the feedback that the patient gets from his/her hand movment. First pilot tests have been conducted.
</p>
    </subsection>
    <subsection id="uid75" level="1">
      <bodyTitle>EquipEx AmiQual4Home - Ambient Intelligence for Quality of Life</bodyTitle>
      <p>The AmiQual4Home Innovation Factory is an open research facility for innovation and experimentation with human-centered services based on the use of large-scale deployment of interconnected digital devices capable of perception, action, interaction and communication.
The Innovation Factory is composed of a collection of workshops for rapid creation of prototypes, surrounded by a collection of living labs and supported by an industrial innovation and transfer service. Creation of the Innovation Factory has been made possible by a grant from French National programme Investissement d'avenir, together with substantial contributions of resources by Grenoble INP, Univ Joseph Fourier, UPMF, CNRS, Schneider Electric and the Communaute de Communes of Montbonnot. The objective is to provide the academic and industrial communities with an open platform to enable research on design, integration and evaluation of systems and services for smart habitats.</p>
      <p>The AmiQual4Home Innovation Factory is a unique combination of three different innovation instruments:</p>
      <orderedlist>
        <li id="uid76">
          <p noindent="true">Workshops for rapid prototyping of devices that embed perception, action, interaction and communication in ordinary objects based on the MIT FabLab model,</p>
        </li>
        <li id="uid77">
          <p noindent="true">Facilities for real-world test and evaluation of devices and services organized as open Living Labs,</p>
        </li>
        <li id="uid78">
          <p noindent="true">Resources for assisting students, researchers, entrepreneurs and industrial partners in creating new economic activities.</p>
        </li>
      </orderedlist>
      <p>The AmiQual4Home Innovation Factory works with the Inovallee TARMAC technology incubator as well as the SAT Linksium to proved innovation and transfer services to enable students, researchers and local entrepreneurs to create and grow new commercial activities based on smart objects and services.
</p>
    </subsection>
  </logiciels>
  <resultats id="uid79">
    <bodyTitle>New Results</bodyTitle>
    <subsection id="uid80" level="1">
      <bodyTitle>Simulating Haptic Sensations</bodyTitle>
      <p>Participants: Jingtao Chen, Sabine Coquillart,
Partners: Inria GRA, LIG, GIPSA, G-SCOP</p>
      <p>Pseudo-haptic feedback is a technique aiming to simulate haptic sensations without active haptic feedback devices.
Peudo-haptic techniques have been used to simulate various haptic feedbacks such as stiffness, torques, and mass.
In the framework of the Persyval project, a novel pseudo-haptic experiment has been set up. The aim of this
experiment is to study the force and EMG signals during a pseudo-haptic task. A stiffness discrimination task
similar to the one published in Lecuyer's PhD thesis has been chosen. The experimental set-up has been developed,
as well as the software controlling the experiment. Pre-tests have been conducted. They have been followed by formal tests with subjects.
</p>
    </subsection>
  </resultats>
  <contrats id="uid81">
    <bodyTitle>Bilateral Contracts and Grants with Industry</bodyTitle>
    <subsection id="uid82" level="1">
      <bodyTitle>Bilateral Contracts with Industry</bodyTitle>
      <subsection id="uid83" level="2">
        <bodyTitle>Learning daily routines by observing activity in a smart home. </bodyTitle>
        <p>Members of the Pervasive interaction team are working with Orange Labs on techniques for observing activity and learning routines in a smart home. Activity is observed by monitoring use of electrical appliances and Communications media (Telephone, Television, Internet). Activities are described using Bayesian Situation Modeling techniques demonstrated in earlier projects. A log of daily activities is used to discover daily routines expressed as temporal sequences of contexts, where each context is expressed as a network of situations.
Experiments will be performed using the Smart home living lab that has been constructed as part of the EquipEx Amiqual4home.</p>
      </subsection>
      <subsection id="uid84" level="2">
        <bodyTitle>IRT Silver Economy</bodyTitle>
        <p>Participants: James Crowley, Pierre Baret, Maxime Belgodere
Partners: CEA, Schneider Electric.</p>
        <p>Members of the Pervasive Interaction team are working with the CEA and Schneider Electric to develop environmental sensors that can detect when a hospital patient or elderly person has fallen and is unable to get up. The project uses an infrared Bolometric image sensor to observe human activity. Image processing and fall detection logic are to be performed by an embedded image processor on board.</p>
      </subsection>
    </subsection>
  </contrats>
  <partenariat id="uid85">
    <bodyTitle>Partnerships and Cooperations</bodyTitle>
    <subsection id="uid86" level="1">
      <bodyTitle>National Initiatives</bodyTitle>
      <subsection id="uid87" level="2">
        <bodyTitle>ANR Project Involved</bodyTitle>
        <p>Participants: Amr Alyafi, Patrick Reignier.</p>
        <p>Other Partners: UMR G-SCOP, UMR LIG (Persuasive Interaction, IIHM), CEA Liten, PACTE, Vesta Systems and Elithis.</p>
        <p>Dates: Jan 2015 to Dec 2018</p>
        <p>The ANR project Involved focuses on bringing solutions to building actors for upcoming challenges in energy management in residential buildings. The project explores a user centric energy management system, where user needs and tacit knowledge drive the search of solutions. These are calculated using a flexible energy model of the living areas. The system is personified by energy consultants with which building actors such as building owners, building managers, technical operators but also occupants, can interact with in order to co-define energy strategies, benefiting of both assets: tacit knowledge of human actors, and measurement with computation capabilities of calculators. Putting actors in the loop, i.e. making energy not only visible but also controllable is the needed step before large deployment of energy management solutions. It is proposed to develop interactive energy consultants for all the actors, which are energy management aided systems embedding models in order to support the decision making processes. MIRROR (interactive monitoring), WHAT-IF (interactive quantitative simulation), EXPLAIN (interactive qualitative simulation), SUGGEST-AND-ADJUST (interactive management) and RECOMMEND (interactive diagnosis) functionalities will be developed.</p>
      </subsection>
      <subsection id="uid88" level="2">
        <bodyTitle>ANR Project CEEGE: Chess Expertise from Eye Gaze and Emotion</bodyTitle>
        <p>Participants: James Crowley, Dominique Vaufreydaz, Rafaellea Balzarini</p>
        <p>Other Partners: Dept of NeuroCognition, CITEN, Bielefeld University</p>
        <p>Dates: Jan 2016 to Dec 2019</p>
        <p>CEEGE is a multidisciplinary scientific research project conducted by the Inria PRIMA team in cooperation with the Dept of Cognitive Neuroscience at the University of Bielefeld. The primary impacts will be improved scientific understanding in the disciplines of Computer Science and Cognitive NeuroScience. The aim of this project is to experimentally evaluate and compare current theories for mental modelling for problem solving and attention, as well as to refine and evaluate techniques for observing the physiological reactions of humans to situation that inspire pleasure, displeasure, arousal, dominance and fear.</p>
        <p>In this project, we will observe the visual attention, physiological responses and mental states of subject with different levels of expertise solving classic chess problems, and participating in chess matches. We will observe chess players using eye-tracking, sustained and instantaneous face-expressions (micro-expressions), skin conductivity, blood flow (BVP), respiration, posture and other information extracted from audio-visual recordings and sensor readings of players. We will use the recorded information to estimate the mental constructs with which the players understand the game situation. Information from visual attention as well as physiological reactions will be used to determine and model the degree to which a player understands the game situation in terms of abstract configurations of chess pieces. This will provide a structured environment that we will use for experimental evaluation of current theories of mental modeling and emotional response during problem solving and social interaction.</p>
        <p>The project is organized in three phases. During the first phase, we will observe individual players of different levels of chess expertise solving known chess problems. We will correlate scan-path from eye tracking and other information about visual attention to established configurations of pieces and known solutions to chess problems. This will allow us to construct a labeled corpus of chess play that can be used to evaluate competing techniques for estimating mental models and physiological responses. In a second phase, we will observe the attention and face expressions of pairs of players of different levels of chess ability during game play. In particular, we will seek to annotate and segment recordings with respect to the difficulty of the game situation as well as situations that elicit particularly strong physiological reactions. In the final phase, we will use these recordings to evaluate the effectiveness of competing techniques for mental modeling and observation of emotions in terms of their abilities to predict the chess abilities of players, game outcomes and individual moves and player self reports. Results of our work will be published in scientific conferences and journals concerned with cognitive science and cognitive neuroscience as well as computer vision, multimodal interaction, affective computing and pervasive computing. Possible applications include construction of systems that can monitor the cognitive abilities and emotional reactions of users of interactive systems to provide assistance that is appropriate but not excessive, companion systems that can aid with active healthy ageing, and tutoring systems that can assist users in developing skills in a variety of domains including chess.</p>
      </subsection>
    </subsection>
    <subsection id="uid89" level="1">
      <bodyTitle>European Initiatives</bodyTitle>
      <subsection id="uid90" level="2">
        <bodyTitle>ICT FET Bambi (FET Open FP7-ICT-2013-C) </bodyTitle>
        <p>Participants: Emmanuel Mazer, Marvin Faix</p>
        <p>Partners: Hebrew University of Jerusalem, Probayes, Universit√© de Liege, Instituto de Sistemas e Robotica (Portugal),CNRS (LIG,ISIR,IEF,UMIPhi)</p>
        <p>Dates January 2014 to December 2016</p>
        <p>FET Open BAMBI explores a theory and a hardware implementation of probabilistic computation inspired by biochemical cell signalling. The project studies probabilistic computation following three axes: algebra, biology, and hardware. In each case, we will develop a bottom-up hierarchical approach starting from the elementary components, and study how to combine them to build more complex systems. It proposes a Bayesian Gate operating on probability distributions on binary variables as the building blocks of our probabilistic algebra. These Bayesian gates can be seen as a generalization of logical operators in Boolean algebra. The consortium interprets elementary cell signalling pathways as biological implementation of these probabilistic gates. In turn, the key features of biochemical processes give new insights for new probabilistic hardware implementation. They associate conventional electronics and novel stochastic nano-devices to build the required hardware elements. Combining these will lead to new artificial information processing systems, which could, in the future, outperform classical computers in tasks involving a direct interaction with the physical world. For this purpose, this project associates research in Bayesian probability theory, molecular biology, nanophysics, computer science and electronics.</p>
      </subsection>
    </subsection>
  </partenariat>
  <diffusion id="uid91">
    <bodyTitle>Dissemination</bodyTitle>
    <subsection id="uid92" level="1">
      <bodyTitle>Promoting Scientific Activities</bodyTitle>
      <subsection id="uid93" level="2">
        <bodyTitle>General Chair, Scientific Chair</bodyTitle>
        <simplelist>
          <li id="uid94">
            <p noindent="true">Sabine Coquillart has served as general co-chair for IEEE VR 2016, Greenville South Carolina.</p>
          </li>
        </simplelist>
      </subsection>
      <subsection id="uid95" level="2">
        <bodyTitle>Member of the Conference Program Committees</bodyTitle>
        <simplelist>
          <li id="uid96">
            <p noindent="true">Patrick Reignier served as a member program committee for the National Conference on Artificial Intelligence</p>
          </li>
          <li id="uid97">
            <p noindent="true">James L. Crowley served as a member program committee for Ubicomp 2016</p>
          </li>
          <li id="uid98">
            <p noindent="true">Dominique Vaufreydaz served on the program committee of the 11th IEEE International Workshop on Multimedia Technologies for E-Learning (MTEL2016).</p>
          </li>
          <li id="uid99">
            <p noindent="true">Dominique Vaufreydaz served on the program committee for the 8th international on Knowledge
and Systems Engineering (KSE 2016)</p>
          </li>
          <li id="uid100">
            <p noindent="true">Thierry Fraichasd served as a member of the programme committee of the 1st Int. Workshop on Robot Learning and Planning (RLP) in conjunction with the "Robotics: Science and Systems" conference.</p>
          </li>
        </simplelist>
      </subsection>
      <subsection id="uid101" level="2">
        <bodyTitle>Reviewer for conferences</bodyTitle>
        <simplelist>
          <li id="uid102">
            <p noindent="true">Patrick Reignier served as a review for the International Journal of Datascience and Analytics</p>
          </li>
          <li id="uid103">
            <p noindent="true">James L. Crowley served as review for IEEE CVPR 2016</p>
          </li>
          <li id="uid104">
            <p noindent="true">James L. Crowley served as review for ICPR 2016</p>
          </li>
          <li id="uid105">
            <p noindent="true">James L. Crowley served as review for CNIA 2016</p>
          </li>
          <li id="uid106">
            <p noindent="true">James L. Crowley served as review for ECCV 2016</p>
          </li>
          <li id="uid107">
            <p noindent="true">James L. Crowley served as review for RFIA 2016</p>
          </li>
          <li id="uid108">
            <p noindent="true">Dominique Vaufreydaz served as review for Robotics and Autonomous Systems Journal</p>
          </li>
          <li id="uid109">
            <p noindent="true">Dominique Vaufreydaz served as review for UbiComp 2016</p>
          </li>
          <li id="uid110">
            <p noindent="true">Dominique Vaufreydaz served as review for ICSR 2016</p>
          </li>
          <li id="uid111">
            <p noindent="true">Thierry Fraichard served as a reviewer for the following conferences: RLP, IEEE Int. Conf. on Robotics and Automation (ICRA), and IEEE Int. Conf. on Advanced Intelligent Mechatronics (AIM).</p>
          </li>
          <li id="uid112">
            <p noindent="true">Thierry Fraichard reviewed articles for the following journals: Int. Journal of Aerospace Engineering, IEEE Trans. Intelligent Vehicles, and IEEE Trans. on Industrial Electronics.</p>
          </li>
        </simplelist>
      </subsection>
      <subsection id="uid113" level="2">
        <bodyTitle>Journals</bodyTitle>
        <simplelist>
          <li id="uid114">
            <p noindent="true">Thierry Fraichard reviewed articles for the Int. Journal of Aerospace Engineering, IEEE Trans. Intelligent Vehicles, and IEEE Trans. on Industrial Electronics.</p>
          </li>
        </simplelist>
        <subsection id="uid115" level="3">
          <bodyTitle>Member of the Editorial Boards</bodyTitle>
          <simplelist>
            <li id="uid116">
              <p noindent="true">Sabine Coquillart is a member of the Editorial Board of the Advances in Human-Computer Interaction</p>
            </li>
            <li id="uid117">
              <p noindent="true">Sabine Coquillart is a member of the Scientific Committee of the Journal of Virtual Reality and Broadcasting.</p>
            </li>
            <li id="uid118">
              <p noindent="true">Sabine Coquillart is a member of the Advisory/Editorial Board for the International Journal of Computer Graphics SERSC.</p>
            </li>
            <li id="uid119">
              <p noindent="true">Sabine Coquillart is a Member of the Editorial Board of Peer Computer Science open access Journal in Computer Science.</p>
            </li>
            <li id="uid120">
              <p noindent="true">Sabine Coquillart is a member of the Editorial board (computer Sciences) of the Scientific World Journal.</p>
            </li>
            <li id="uid121">
              <p noindent="true">Sabine Coquillart is Review Editor for the Frontiers in Virtual Environments journal.</p>
            </li>
            <li id="uid122">
              <p noindent="true">Patrick Reignier is a member of the "comite de redaction" of the Modeling and Using Context journal (ISTE OpenScience)</p>
            </li>
            <li id="uid123">
              <p noindent="true">Thierry Fraichard is serving as an Associate Editor for IEEE Robotics and Automation Letters. He also served as an Associate Editor for IEEE/RSJ Int. Conf. on Robots and Systems (IROS).</p>
            </li>
          </simplelist>
        </subsection>
      </subsection>
      <subsection id="cid1" level="2">
        <bodyTitle>Leadership within the scientific community</bodyTitle>
        <simplelist>
          <li id="uid124">
            <p noindent="true">Sabine Coquillart is elected member of the EUROGRAPHICS Executive Committee.</p>
          </li>
          <li id="uid125">
            <p noindent="true">Sabine Coquillart is member of the EUROGRAPHICS Working Group and Workshop
board.</p>
          </li>
          <li id="uid126">
            <p noindent="true">Patrick Reignier is an elected member of the Association Française pour l'Intelligence Artificielle Executive Committee</p>
          </li>
        </simplelist>
      </subsection>
      <subsection id="cid2" level="2">
        <bodyTitle>Scientific expertise</bodyTitle>
        <simplelist>
          <li id="uid127">
            <p noindent="true">James L. Crowley served on the selection committee for Institut Universitaire de France (IUF)</p>
          </li>
          <li id="uid128">
            <p noindent="true">James L. Crowley served on the selection panel for the EC H2020 ICT Robotics program</p>
          </li>
          <li id="uid129">
            <p noindent="true">Dominique Vaufreydaz served as evaluator pour l'ANR</p>
          </li>
          <li id="uid130">
            <p noindent="true">Patrick Reignier served on the selection committee for an associate professor position at the University Grenoble Alps</p>
          </li>
          <li id="uid131">
            <p noindent="true">Thierry Fraichard served as an expert reviewer for the European Commission (H2020 FET and ICT calls).</p>
          </li>
        </simplelist>
      </subsection>
      <subsection id="cid3" level="2">
        <bodyTitle>Research administration</bodyTitle>
        <simplelist>
          <li id="uid132">
            <p noindent="true">James L. Crowley has been elected member of the Conseil d'Administration of the COMUE University Grenoble Alpes</p>
          </li>
          <li id="uid133">
            <p noindent="true">James L. Crowley has been elected member of the Conseil du Laboratoire of the Laboratoire Informatique de Grenoble.</p>
          </li>
          <li id="uid134">
            <p noindent="true">James L. Crowley serves on the Administrative Office (Bureau) for the Laboratoire Informatique de Grenoble.</p>
          </li>
          <li id="uid135">
            <p noindent="true">James L. Crowley Serves on the Commission d'Habilitation de Diriger la Recherche (HDR) for the Pole Informatics and Mathematics of the University Grenoble Alpes.</p>
          </li>
          <li id="uid136">
            <p noindent="true">James L. Crowley Serves on the Comittee Scientific (CoS) d'Inria Grenoble Rhone-Alpes Research Center</p>
          </li>
          <li id="uid137">
            <p noindent="true">James L. Crowley Serves on Steering Committee of the Inovallee TARMAC technology Incubator.</p>
          </li>
          <li id="uid138">
            <p noindent="true">James L. Crowley is director of the Amiqual4Home Equipment of Excellence (EquipEx).</p>
          </li>
          <li id="uid139">
            <p noindent="true">Patrick Reignier is head of the engineering support group of the Laboratoire d'Informatique de Grenoble</p>
          </li>
          <li id="uid140">
            <p noindent="true">Patrick Reignier serves on the Administrative Office (Bureau) for the Laboratoire Informatique de Grenoble.</p>
          </li>
        </simplelist>
      </subsection>
    </subsection>
    <subsection id="uid141" level="1">
      <bodyTitle>Teaching - Supervision - Juries</bodyTitle>
      <subsection id="uid142" level="2">
        <bodyTitle>Teaching</bodyTitle>
        <subsection id="uid143" level="3">
          <bodyTitle>James Crowley</bodyTitle>
          <sanspuceslist>
            <li id="uid144">
              <p noindent="true">James Crowley is Director of the Master of Science in Informatics at Grenoble (MoSIG).</p>
            </li>
            <li id="uid145">
              <p noindent="true">Master : Computer Vision, Course 27h EqTD, M2 year, Master of Science in Informatics at Grenoble</p>
            </li>
            <li id="uid146">
              <p noindent="true">Master 1: Intelligent Systems, Cours 54h EqTD, ENSIMAG and UFRIM2AG</p>
            </li>
            <li id="uid147">
              <p noindent="true">ENSIMAG 3 : Pattern Recognition and Machine Learning, Cours 27h EqTD, ENSIMAG</p>
            </li>
          </sanspuceslist>
        </subsection>
        <subsection id="uid148" level="3">
          <bodyTitle>Sabine Coquillart</bodyTitle>
          <simplelist>
            <li id="uid149">
              <p noindent="true">Master : Sabine Coquillart Taught a course on Virtual Reality and 3D User Interfaces for the GVR Master 2R</p>
            </li>
            <li id="uid150">
              <p noindent="true">Master : Sabine Coquillart teaches a one day course on 3D User Interfaces and Augmented Reality for the Numerical Modeling and Virtual Reality Master 2 in Laval.</p>
            </li>
          </simplelist>
        </subsection>
        <subsection id="uid151" level="3">
          <bodyTitle>Thierry Fraichard</bodyTitle>
          <sanspuceslist>
            <li id="uid152">
              <p noindent="true">Master MOSIG 1st year: Introduction to Perception and Robotics, 22.5 hEqTD.</p>
            </li>
            <li id="uid153">
              <p noindent="true">Master MOSIG 2nd year: Autonomous Robotics, 22.5 heqTD</p>
            </li>
          </sanspuceslist>
        </subsection>
        <subsection id="uid154" level="3">
          <bodyTitle>Dominique Vaufreydaz</bodyTitle>
          <sanspuceslist>
            <li id="uid155">
              <p noindent="true">Co-responsibility of the Graphic, Vision and Robotics track of the MOSIG Master programme.</p>
            </li>
            <li id="uid156">
              <p noindent="true">Licence: Competences Numerique, 80h eqTD,L1,
Universite Grenoble Alpes, France.</p>
            </li>
            <li id="uid157">
              <p noindent="true">Licence: Informatique appliquee a l’economie et a la gestion, enseignement
a distance, Licence, Universite Grenoble Alpes, France.</p>
            </li>
            <li id="uid158">
              <p noindent="true">Licence: Pratique avancee du Tableur, 72 h eqTD, L3, Universite Grenoble Alpes, France.</p>
            </li>
            <li id="uid159">
              <p noindent="true">Licence Professionnelle: Enquêtes et traitement d’enquêtes avec le logiciel Sphinx, 12h eqTD, Licence pro Metiers de l’Emploi et de la Formation, Universite Grenoble Alpes, France.</p>
            </li>
            <li id="uid160">
              <p noindent="true">Licence Professionnelle: Administration en environnement heterogene, 20h eqTD, Licence
pro Administration et Securite des Systemes et des Reseaux, Universite Grenoble Alpes, France.</p>
            </li>
            <li id="uid161">
              <p noindent="true">IUT annee speciale: Programmation C++, 18h eqTD, Annee Speciale IUT Informatique, Universite Grenoble Alpes,
France.</p>
            </li>
            <li id="uid162">
              <p noindent="true">Master: Pratique avancee du Tableur, 22 h eqTD, M1 economie internationale
et strategies d’acteurs, Universite Grenoble Alpes, France.</p>
            </li>
            <li id="uid163">
              <p noindent="true">Master: Mise a niveau Informatique pour l’economie, 22h eqTD, M1 Economie des Organisations,
Universite Grenoble Alpes, France</p>
            </li>
            <li id="uid164">
              <p noindent="true">Master: Traitement des donnees du Web, 15h eqTD, M2 Mathematiques et Informatique Appliquees aux Sciences Humaines et Sociales, Universite Grenoble Alpes, France</p>
            </li>
            <li id="uid165">
              <p noindent="true">Master: Developpement Web Mobile, 15h eqTD, M2 Mathematiques et Informatique Appliquees aux Sciences Humaines et Sociales, Universite Grenoble Alpes, France</p>
            </li>
            <li id="uid166">
              <p noindent="true">Master: co-responsable de l'option Graphic Vision Robotic (GVR) du Master 2 MOSIG.</p>
            </li>
          </sanspuceslist>
        </subsection>
        <subsection id="uid167" level="3">
          <bodyTitle>Thierry Fraichard</bodyTitle>
          <simplelist>
            <li id="uid168">
              <p noindent="true">Master: Thierry Fraichard, Advanced Robotics, 54h eqTD, M2 MOSIG, Univ. of Grenoble, France.</p>
            </li>
          </simplelist>
        </subsection>
        <subsection id="uid169" level="3">
          <bodyTitle>Patrick Reignier</bodyTitle>
          <simplelist>
            <li id="uid170">
              <p noindent="true">Patrick Reignier has been elected member of the Conseil des Etudes et de la Vie Universitaire of Grenoble INP</p>
            </li>
            <li id="uid171">
              <p noindent="true">Patrick Reignier has been nominated as a member of the Conseil de la Formation Continue de Grenoble INP</p>
            </li>
            <li id="uid172">
              <p noindent="true">Patrick Reignier Supervises the industrial part of the “formation en apprentissage” of the ENSIMAG engineering school.</p>
            </li>
            <li id="uid173">
              <p noindent="true">Master: Patrick Reignier, Projet Genie Logiciel, 55h eqTD, M1, ENSIMAG/Grenoble INP, France.</p>
            </li>
            <li id="uid174">
              <p noindent="true">Master : Patrick Reignier, Developpement d'applications communicantes, 18h eqTD, M2, ENSIMAG/Grenoble - INP, France</p>
            </li>
            <li id="uid175">
              <p noindent="true">Master : Patrick Reignier, Introduction aux applications reparties, 18h eqTD, M2, ENSIMAG/Grenoble - INP, France</p>
            </li>
            <li id="uid176">
              <p noindent="true">Master : Patrick Reignier, Applications Web et Mobiles , 27h eqTD, M1, ENSIMAG/Grenoble - INP, France</p>
            </li>
            <li id="uid177">
              <p noindent="true">Master : Patrick Reignier, Projet Systeme, 12h eq TD, M1,  ENSIMAG/Grenoble - INP, France</p>
            </li>
            <li id="uid178">
              <p noindent="true">Master : Patrick Reignier, Algorithmique, 50h eq TD, M1, ENSIMAG/Grenoble INP, France</p>
            </li>
            <li id="uid179">
              <p noindent="true">Licence: Patrick Reignier, Projet C, 20h eqTD, L3, ENSIMAG/Grenoble INP, France.</p>
            </li>
          </simplelist>
        </subsection>
      </subsection>
      <subsection id="uid180" level="2">
        <bodyTitle>Juries</bodyTitle>
        <sanspuceslist>
          <li id="uid181">
            <p noindent="true">Thierry Fraichard served as a rapporteur for a doctoral jury at Universitat Politecnica de Valencia</p>
          </li>
          <li id="uid182">
            <p noindent="true">Thierry Fraichard served as a rapporteur for a doctoral jury at INP Toulouse.</p>
          </li>
          <li id="uid183">
            <p noindent="true">James Crowley served as a reporter for the doctoral jury of Ninghang Hu at the University of Amsterdam</p>
          </li>
        </sanspuceslist>
      </subsection>
    </subsection>
  </diffusion>
  <biblio id="bibliography" html="bibliography" numero="10" titre="Bibliography">
    
    <biblStruct id="pervasive_interaction-2016-bid10" type="article" rend="year" n="cite:coutaz:hal-01422364">
      <identifiant type="doi" value="10.1109/MPRV.2016.24"/>
      <identifiant type="hal" value="hal-01422364"/>
      <analytic>
        <title level="a">A First-Person Experience with End-User Development for Smart Homes</title>
        <author>
          <persName key="prima-2014-idp105648">
            <foreName>Joëlle</foreName>
            <surname>Coutaz</surname>
            <initial>J.</initial>
          </persName>
          <persName key="prima-2014-idm28656">
            <foreName>James L</foreName>
            <surname>Crowley</surname>
            <initial>J. L.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid03079">
        <idno type="issn">1536-1268</idno>
        <title level="j">IEEE Pervasive Computing</title>
        <imprint>
          <biblScope type="volume">15</biblScope>
          <dateStruct>
            <month>May</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">26 - 39</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01422364" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01422364</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="pervasive_interaction-2016-bid4" type="article" rend="year" n="cite:faix:hal-01374906">
      <identifiant type="doi" value="10.1109/TETC.2016.2609926"/>
      <identifiant type="hal" value="hal-01374906"/>
      <analytic>
        <title level="a">Design of Stochastic Machines Dedicated to Approximate Bayesian inferences</title>
        <author>
          <persName key="e-motion-2014-idp85304">
            <foreName>Marvin</foreName>
            <surname>Faix</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>Raphä El</foreName>
            <surname>Laurent</surname>
            <initial>R. E.</initial>
          </persName>
          <persName key="e-motion-2014-idp97864">
            <foreName>Pierre</foreName>
            <surname>Bessière</surname>
            <initial>P.</initial>
          </persName>
          <persName key="e-motion-2014-idp66304">
            <foreName>Emmanuel</foreName>
            <surname>Mazer</surname>
            <initial>E.</initial>
          </persName>
          <persName>
            <foreName>Jacques</foreName>
            <surname>Droulez</surname>
            <initial>J.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid03080">
        <idno type="issn">2168-6750</idno>
        <title level="j">IEEE Transactions on Emerging Topics in Computing</title>
        <imprint>
          <dateStruct>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01374906" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01374906</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="pervasive_interaction-2016-bid6" type="inproceedings" rend="year" n="cite:amayri:hal-01407401">
      <identifiant type="hal" value="hal-01407401"/>
      <analytic>
        <title level="a">Towards Interactive Learning for Occupancy Estimation</title>
        <author>
          <persName>
            <foreName>Manar</foreName>
            <surname>Amayri</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>Stéphane</foreName>
            <surname>Ploix</surname>
            <initial>S.</initial>
          </persName>
          <persName key="prima-2014-idp70256">
            <foreName>Patrick</foreName>
            <surname>Reignier</surname>
            <initial>P.</initial>
          </persName>
          <persName>
            <foreName>Sanghamitra</foreName>
            <surname>Bandyopadhyay</surname>
            <initial>S.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">ICAI'16 - International Conference on Artificial Intelligence (as part of WORLDCOMP'16 - World Congress in Computer Science, Computer Engineering and Applied Computing)</title>
        <loc>Las Vegas, United States</loc>
        <imprint>
          <dateStruct>
            <month>July</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01407401" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01407401</ref>
        </imprint>
        <meeting id="cid625364">
          <title>WORLDCOMP International Conference on Artificial Intelligence</title>
          <num>2016</num>
          <abbr type="sigle">ICAI</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="pervasive_interaction-2016-bid0" type="inproceedings" rend="year" n="cite:badeig:hal-01344312">
      <identifiant type="hal" value="hal-01344312"/>
      <analytic>
        <title level="a">Impact of Head Motion on the Assistive Robot Expressiveness - Evaluation with Elderly Persons</title>
        <author>
          <persName key="prima-2014-idp72968">
            <foreName>Fabien</foreName>
            <surname>Badeig</surname>
            <initial>F.</initial>
          </persName>
          <persName>
            <foreName>Pierre</foreName>
            <surname>Wargnier</surname>
            <initial>P.</initial>
          </persName>
          <persName>
            <foreName>Maribel</foreName>
            <surname>Pino</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>Philippe</foreName>
            <surname>De Oliveira Lopes</surname>
            <initial>P.</initial>
          </persName>
          <persName key="prima-2014-idp82960">
            <foreName>Emeric</foreName>
            <surname>Grange</surname>
            <initial>E.</initial>
          </persName>
          <persName key="prima-2014-idm28656">
            <foreName>James L.</foreName>
            <surname>Crowley</surname>
            <initial>J. L.</initial>
          </persName>
          <persName>
            <foreName>Anne-Sophie</foreName>
            <surname>Rigaud</surname>
            <initial>A.-S.</initial>
          </persName>
          <persName key="prima-2014-idp71704">
            <foreName>Dominique</foreName>
            <surname>Vaufreydaz</surname>
            <initial>D.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">1st International Workshop on Affective Computing for Social Robotics Workshop at the IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)</title>
        <loc>New York, United States</loc>
        <imprint>
          <dateStruct>
            <month>August</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01344312" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01344312</ref>
        </imprint>
        <meeting id="cid625362">
          <title>at theIEEE RO-MAN International Workshop on Affective Computing for Social Robotics Workshop</title>
          <num>1</num>
          <abbr type="sigle"/>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="pervasive_interaction-2016-bid2" type="inproceedings" rend="year" n="cite:balit:hal-01262251">
      <identifiant type="hal" value="hal-01262251"/>
      <analytic>
        <title level="a">Integrating Animation Artists into the Animation Design of Social Robots: An Open-Source Robot Animation Software</title>
        <author>
          <persName key="prima-2014-idp101904">
            <foreName>Etienne</foreName>
            <surname>Balit</surname>
            <initial>E.</initial>
          </persName>
          <persName key="prima-2014-idp71704">
            <foreName>Dominique</foreName>
            <surname>Vaufreydaz</surname>
            <initial>D.</initial>
          </persName>
          <persName key="prima-2014-idp70256">
            <foreName>Patrick</foreName>
            <surname>Reignier</surname>
            <initial>P.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">ACM/IEEE Human-Robot Interaction 2016</title>
        <loc>Christchurch, New Zealand</loc>
        <imprint>
          <dateStruct>
            <month>March</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01262251" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01262251</ref>
        </imprint>
        <meeting id="cid395964">
          <title>ACM/IEEE International Conference on Human-Robot Interaction</title>
          <num>2016</num>
          <abbr type="sigle">HRI</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="pervasive_interaction-2016-bid7" type="inproceedings" rend="year" n="cite:bouraine:hal-01400075">
      <identifiant type="hal" value="hal-01400075"/>
      <analytic>
        <title level="a">Real-time Safe Path Planning for Robot Navigation in Unknown Dynamic Environments</title>
        <author>
          <persName>
            <foreName>Sara</foreName>
            <surname>Bouraine</surname>
            <initial>S.</initial>
          </persName>
          <persName key="prima-2014-idp66112">
            <foreName>Thierry</foreName>
            <surname>Fraichard</surname>
            <initial>T.</initial>
          </persName>
          <persName>
            <foreName>Ouahiba</foreName>
            <surname>Azouaoui</surname>
            <initial>O.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">CSA 2016 - 2nd Conference on Computing Systems and Applications</title>
        <loc>Algiers, Algeria</loc>
        <imprint>
          <dateStruct>
            <month>December</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01400075" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01400075</ref>
        </imprint>
        <meeting id="cid625363">
          <title>Conference on Computing Systems and Applications</title>
          <num>2</num>
          <abbr type="sigle">CSA</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="pervasive_interaction-2016-bid3" type="inproceedings" rend="year" n="cite:canillas:hal-01316568">
      <identifiant type="hal" value="hal-01316568"/>
      <analytic>
        <title level="a">Autonomous Robot Controller Using Bitwise GIBBS Sampling</title>
        <author>
          <persName key="prima-2015-idp115456">
            <foreName>Rémi</foreName>
            <surname>Canillas</surname>
            <initial>R.</initial>
          </persName>
          <persName>
            <foreName>Raphael</foreName>
            <surname>Laurent</surname>
            <initial>R.</initial>
          </persName>
          <persName key="e-motion-2014-idp85304">
            <foreName>Marvin</foreName>
            <surname>Faix</surname>
            <initial>M.</initial>
          </persName>
          <persName key="prima-2014-idp71704">
            <foreName>Dominique</foreName>
            <surname>Vaufreydaz</surname>
            <initial>D.</initial>
          </persName>
          <persName key="e-motion-2014-idp66304">
            <foreName>Emmanuel</foreName>
            <surname>Mazer</surname>
            <initial>E.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="no" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">15th IEEE International Conference on Cognitive Informatics and Cognitive Computing</title>
        <loc>Calgary, Canada</loc>
        <imprint>
          <dateStruct>
            <month>August</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01316568" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01316568</ref>
        </imprint>
        <meeting id="cid625361">
          <title>IEEE International Conference on Cognitive Informatics and Cognitive Computing</title>
          <num>15</num>
          <abbr type="sigle"/>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="pervasive_interaction-2016-bid8" type="inproceedings" rend="year" n="cite:nieto:hal-01393942">
      <identifiant type="hal" value="hal-01393942"/>
      <analytic>
        <title level="a">Rendu basé image avec contraintes sur les gradients</title>
        <author>
          <persName key="prima-2014-idp109400">
            <foreName>Grégoire</foreName>
            <surname>Nieto</surname>
            <initial>G.</initial>
          </persName>
          <persName key="prima-2014-idp64856">
            <foreName>Frédéric</foreName>
            <surname>Devernay</surname>
            <initial>F.</initial>
          </persName>
          <persName key="prima-2014-idm28656">
            <foreName>James</foreName>
            <surname>Crowley</surname>
            <initial>J.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="no" x-proceedings="no" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">Reconnaissance des Formes et l'Intelligence Artificielle, RFIA 2016</title>
        <loc>Clermont-Ferrand, France</loc>
        <imprint>
          <dateStruct>
            <month>June</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01393942" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01393942</ref>
        </imprint>
        <meeting id="cid54223">
          <title>Congrès Francophone de Reconnaissance des Formes et Intelligence Artificielle</title>
          <num>2016</num>
          <abbr type="sigle">RFIA</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="pervasive_interaction-2016-bid9" type="inproceedings" rend="year" n="cite:nieto:hal-01402528">
      <identifiant type="hal" value="hal-01402528"/>
      <analytic>
        <title level="a">Variational Image-Based Rendering with Gradient Constraints</title>
        <author>
          <persName key="prima-2014-idp109400">
            <foreName>Grégoire</foreName>
            <surname>Nieto</surname>
            <initial>G.</initial>
          </persName>
          <persName key="prima-2014-idp64856">
            <foreName>Frédéric</foreName>
            <surname>Devernay</surname>
            <initial>F.</initial>
          </persName>
          <persName key="prima-2014-idm28656">
            <foreName>James</foreName>
            <surname>Crowley</surname>
            <initial>J.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="no" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">IC3D - 2016 International Conference on 3D Imaging</title>
        <loc>Liège, France</loc>
        <imprint>
          <dateStruct>
            <month>December</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01402528" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01402528</ref>
        </imprint>
        <meeting id="cid623490">
          <title>International Conference on 3D Imaging, Modeling, Processing, Visualization and Transmission</title>
          <num>2016</num>
          <abbr type="sigle">3DIMPVT</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="pervasive_interaction-2016-bid1" type="inproceedings" rend="year" n="cite:ta:hal-01370252">
      <identifiant type="hal" value="hal-01370252"/>
      <analytic>
        <title level="a">Smartphone-based User Location Tracking in Indoor Environment</title>
        <author>
          <persName key="prima-2014-idp114328">
            <foreName>Viet Cuong</foreName>
            <surname>Ta</surname>
            <initial>V. C.</initial>
          </persName>
          <persName key="prima-2014-idp71704">
            <foreName>Dominique</foreName>
            <surname>Vaufreydaz</surname>
            <initial>D.</initial>
          </persName>
          <persName>
            <foreName>Trung-Kien</foreName>
            <surname>Dao</surname>
            <initial>T.-K.</initial>
          </persName>
          <persName>
            <foreName>Eric</foreName>
            <surname>Castelli</surname>
            <initial>E.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">International Conference on Indoor Positioning and Indoor Navigation (IPIN)</title>
        <loc>Madrid, Spain</loc>
        <imprint>
          <dateStruct>
            <month>October</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01370252" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01370252</ref>
        </imprint>
        <meeting id="cid624875">
          <title>Indoor Positioning and Indoor Navigation</title>
          <num>2016</num>
          <abbr type="sigle"/>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="pervasive_interaction-2016-bid5" type="patent" rend="year" n="cite:bessiere:hal-01399353">
      <identifiant type="hal" value="hal-01399353"/>
      <monogr>
        <title level="m">Machine stochastique modulaire et procédé associé</title>
        <author>
          <persName key="e-motion-2014-idp97864">
            <foreName>Pierre</foreName>
            <surname>Bessière</surname>
            <initial>P.</initial>
          </persName>
          <persName>
            <foreName>Jacques</foreName>
            <surname>Droulez</surname>
            <initial>J.</initial>
          </persName>
          <persName key="e-motion-2014-idp66304">
            <foreName>Emmanuel</foreName>
            <surname>Mazer</surname>
            <initial>E.</initial>
          </persName>
          <persName>
            <foreName>Raphael</foreName>
            <surname>Laurent</surname>
            <initial>R.</initial>
          </persName>
          <persName>
            <foreName>Damien</foreName>
            <surname>Querlioz</surname>
            <initial>D.</initial>
          </persName>
          <persName>
            <foreName>Julie</foreName>
            <surname>Grollier</surname>
            <initial>J.</initial>
          </persName>
          <persName key="e-motion-2014-idp85304">
            <foreName>Marvin</foreName>
            <surname>Faix</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>Alexandre</foreName>
            <surname>Coninx</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>David</foreName>
            <surname>Colliaux</surname>
            <initial>D.</initial>
          </persName>
        </author>
        <imprint>
          <biblScope type="number">FR 16 01463</biblScope>
          <dateStruct>
            <month>October</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01399353" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01399353</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="pervasive_interaction-2016-bid11" type="unpublished" rend="year" n="cite:bregier:hal-01415027">
      <identifiant type="hal" value="hal-01415027"/>
      <monogr>
        <title level="m">Defining the Pose of any 3D Rigid Object and an Associated Metric</title>
        <author>
          <persName key="prima-2014-idp103120">
            <foreName>Romain</foreName>
            <surname>Brégier</surname>
            <initial>R.</initial>
          </persName>
          <persName key="prima-2014-idp64856">
            <foreName>Frédéric</foreName>
            <surname>Devernay</surname>
            <initial>F.</initial>
          </persName>
          <persName>
            <foreName>Laetitia</foreName>
            <surname>Leyrit</surname>
            <initial>L.</initial>
          </persName>
          <persName key="prima-2014-idm28656">
            <foreName>James</foreName>
            <surname>Crowley</surname>
            <initial>J.</initial>
          </persName>
        </author>
        <imprint>
          <dateStruct>
            <month>September</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01415027" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01415027</ref>
        </imprint>
      </monogr>
      <note type="bnote">working paper or preprint</note>
    </biblStruct>
  </biblio>
</raweb>
