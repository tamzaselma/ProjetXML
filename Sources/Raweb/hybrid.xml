<?xml version="1.0" encoding="utf-8"?>
<raweb xmlns:xlink="http://www.w3.org/1999/xlink" xml:lang="en" year="2016">
  <identification id="hybrid" isproject="true">
    <shortname>HYBRID</shortname>
    <projectName>3D interaction with virtual environments using body and mind</projectName>
    <theme-de-recherche>Interaction and visualization</theme-de-recherche>
    <domaine-de-recherche>Perception, Cognition and Interaction</domaine-de-recherche>
    <urlTeam>http://team.inria.fr/hybrid/</urlTeam>
    <structure_exterieure type="Organism">
      <libelle>Institut national des sciences appliquées de Rennes</libelle>
    </structure_exterieure>
    <structure_exterieure type="Organism">
      <libelle>Université Rennes 1</libelle>
    </structure_exterieure>
    <header_dates_team>Creation of the Team: 2013 January 01, updated into Project-Team: 2013 July 01</header_dates_team>
    <LeTypeProjet>Project-Team</LeTypeProjet>
    <keywordsSdN>
      <term>2.5. - Software engineering</term>
      <term>5. - Interaction, multimedia and robotics</term>
      <term>5.1. - Human-Computer Interaction</term>
      <term>5.1.2. - Evaluation of interactive systems</term>
      <term>5.1.3. - Haptic interfaces</term>
      <term>5.1.4. - Brain-computer interfaces, physiological computing</term>
      <term>5.1.5. - Body-based interfaces</term>
      <term>5.1.7. - Multimodal interfaces</term>
      <term>5.1.8. - 3D User Interfaces</term>
      <term>5.5.4. - Animation</term>
      <term>5.6. - Virtual reality, augmented reality</term>
      <term>6. - Modeling, simulation and control</term>
      <term>6.2. - Scientific Computing, Numerical Analysis &amp; Optimization</term>
      <term>6.3. - Computation-data interaction</term>
    </keywordsSdN>
    <keywordsSecteurs>
      <term>1.3. - Neuroscience and cognitive science</term>
      <term>2. - Health</term>
      <term>2.4. - Therapies</term>
      <term>2.5. - Handicap and personal assistances</term>
      <term>2.6. - Biological and medical imaging</term>
      <term>2.7. - Medical devices</term>
      <term>2.7.1. - Surgical devices</term>
      <term>2.8. - Sports, performance, motor skills</term>
      <term>5. - Industry of the future</term>
      <term>5.1. - Factory of the future</term>
      <term>5.2. - Design and manufacturing</term>
      <term>5.8. - Learning and training</term>
      <term>5.9. - Industrial maintenance</term>
      <term>8.1. - Smart building/home</term>
      <term>8.3. - Urbanism and urban planning</term>
      <term>9.1. - Education</term>
      <term>9.2. - Art</term>
      <term>9.2.2. - Cinema, Television</term>
      <term>9.2.3. - Video games</term>
      <term>9.3. - Sports</term>
      <term>9.5.6. - Archeology, History</term>
    </keywordsSecteurs>
    <UR name="Rennes"/>
  </identification>
  <team id="uid1">
    <person key="hybrid-2014-idm28656">
      <firstname>Anatole</firstname>
      <lastname>Lécuyer</lastname>
      <categoryPro>Chercheur</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Team leader, Inria, Senior Researcher</moreinfo>
      <hdr>oui</hdr>
    </person>
    <person key="hybrid-2014-idm27216">
      <firstname>Fernando</firstname>
      <lastname>Argelaguet Sanz</lastname>
      <categoryPro>Chercheur</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Inria, Researcher</moreinfo>
    </person>
    <person key="hybrid-2014-idm25952">
      <firstname>Bruno</firstname>
      <lastname>Arnaldi</lastname>
      <categoryPro>Enseignant</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>INSA Rennes, Professor</moreinfo>
      <hdr>oui</hdr>
    </person>
    <person key="hybrid-2014-idp66240">
      <firstname>Valérie</firstname>
      <lastname>Gouranton</lastname>
      <categoryPro>Enseignant</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>INSA Rennes, Associate Professor</moreinfo>
    </person>
    <person key="hybrid-2014-idp67480">
      <firstname>Maud</firstname>
      <lastname>Marchal</lastname>
      <categoryPro>Enseignant</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>INSA Rennes, Associate Professor</moreinfo>
      <hdr>oui</hdr>
    </person>
    <person key="hybrid-2016-idp152560">
      <firstname>Ronan</firstname>
      <lastname>Gaugne</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Univ. Rennes 1, SED Research Engineer 15%</moreinfo>
    </person>
    <person key="hybrid-2014-idp68928">
      <firstname>Florian</firstname>
      <lastname>Nouviale</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>INSA Rennes, SED Research Engineer 20%</moreinfo>
    </person>
    <person key="hybrid-2015-idp73800">
      <firstname>Jérôme</firstname>
      <lastname>Chabrol</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Inria, until Oct 2016</moreinfo>
    </person>
    <person key="hybrid-2014-idp88784">
      <firstname>Guillaume</firstname>
      <lastname>Claude</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>INSA Rennes, from Oct 2016</moreinfo>
    </person>
    <person key="hybrid-2015-idp76304">
      <firstname>Charles</firstname>
      <lastname>Garraud</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Inria, until Oct 2016</moreinfo>
    </person>
    <person key="hybrid-2014-idp96456">
      <firstname>Francois</firstname>
      <lastname>Lehericey</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>VINCI, from Nov 2016</moreinfo>
    </person>
    <person key="hybrid-2014-idp75112">
      <firstname>Jussi Tapio</firstname>
      <lastname>Lindgren</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="visages-2014-idp123184">
      <firstname>Marsel</firstname>
      <lastname>Mano</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="hybrid-2014-idp87552">
      <firstname>Jean-Baptiste</firstname>
      <lastname>Barreau</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>CNRS</moreinfo>
    </person>
    <person key="hybrid-2014-idp88784">
      <firstname>Guillaume</firstname>
      <lastname>Claude</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>INSA Rennes, until Sept 2016</moreinfo>
    </person>
    <person key="hybrid-2015-idp87392">
      <firstname>Guillaume</firstname>
      <lastname>Cortes</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Realyz</moreinfo>
    </person>
    <person key="hybrid-2015-idp88624">
      <firstname>Antoine</firstname>
      <lastname>Costes</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Technicolor</moreinfo>
    </person>
    <person key="hybrid-2015-idp89856">
      <firstname>Anne-Solène</firstname>
      <lastname>Dris-Kerdreux</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>VINCI</moreinfo>
    </person>
    <person key="hybrid-2014-idp91464">
      <firstname>Andéol</firstname>
      <lastname>Evain</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="hybrid-2014-idp92688">
      <firstname>Jérémy</firstname>
      <lastname>Lacoche</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>IRT B-COM, until Aug 2016</moreinfo>
    </person>
    <person key="hybrid-2014-idp93968">
      <firstname>Morgan</firstname>
      <lastname>Le Chénéchal</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>IRT B-COM, until Sep 2016</moreinfo>
    </person>
    <person key="hybrid-2014-idp95232">
      <firstname>Benoît</firstname>
      <lastname>Le Gouis</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>INSA Rennes</moreinfo>
    </person>
    <person key="diverse-2015-idp141624">
      <firstname>Gwendal</firstname>
      <lastname>Le Moulec</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>INSA Rennes</moreinfo>
    </person>
    <person key="hybrid-2014-idp96456">
      <firstname>Francois</firstname>
      <lastname>Lehericey</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>INSA Rennes, until Oct 2016</moreinfo>
    </person>
    <person key="visages-2014-idp143576">
      <firstname>Lorraine</firstname>
      <lastname>Perronnet</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="hybrid-2016-idp201744">
      <firstname>Gautier</firstname>
      <lastname>Picard</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>INSA Rennes, from Oct 2016</moreinfo>
    </person>
    <person key="hybrid-2016-idp204192">
      <firstname>Hakim</firstname>
      <lastname>Si Mohammed</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Inria, from Oct 2016</moreinfo>
    </person>
    <person key="hybrid-2016-idp206640">
      <firstname>Kevin - Yoren</firstname>
      <lastname>Gaffary</lastname>
      <categoryPro>PostDoc</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="hybrid-2014-idp80088">
      <firstname>Adrien</firstname>
      <lastname>Girard</lastname>
      <categoryPro>PostDoc</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Inria, until May 2016</moreinfo>
    </person>
    <person key="hybrid-2015-idp101104">
      <firstname>Nataliya</firstname>
      <lastname>Kos'Myna</lastname>
      <categoryPro>PostDoc</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="hybrid-2014-idp83880">
      <firstname>Nathalie</firstname>
      <lastname>Denis</lastname>
      <categoryPro>Assistant</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
  </team>
  <presentation id="uid2">
    <bodyTitle>Overall Objectives</bodyTitle>
    <subsection id="uid3" level="1">
      <bodyTitle>Overall Objectives</bodyTitle>
      <p>Our research project belongs to the scientific field of Virtual Reality (VR) and 3D interaction with virtual environments. VR systems can be used in numerous applications such as for industry (virtual prototyping, assembly or maintenance operations, data visualization), entertainment (video games, theme parks), arts and design (interactive sketching or sculpture, CAD, architectural mock-ups), education and science (physical simulations, virtual classrooms), or medicine (surgical training, rehabilitation systems). A major change that we foresee in the next decade concerning the field of Virtual Reality relates to the emergence of new paradigms of interaction (input/output) with Virtual Environments (VE).</p>
      <p>As for today, the most common way to interact with 3D content still remains by measuring user's motor activity, i.e., his/her gestures and physical motions when manipulating different kinds of input device. However, a recent trend consists in soliciting more movements and more physical engagement of the body of the user. We can notably stress the emergence of bimanual interaction, natural walking interfaces, and whole-body involvement. These new interaction schemes bring a new level of complexity in terms of generic physical simulation of potential interactions between the virtual body and the virtual surrounding, and a challenging "trade-off" between performance and realism. Moreover, research is also needed to characterize the influence of these new sensory cues on the resulting feelings of "presence" and immersion of the user.</p>
      <p>Besides, a novel kind of user input has recently appeared in the field of virtual reality: the user's mental activity, which can be measured by means of a "Brain-Computer Interface" (BCI). Brain-Computer Interfaces are communication systems which measure user's electrical cerebral activity and translate it, in real-time, into an exploitable command. BCIs introduce a new way of interacting "by thought" with virtual environments. However, current BCI can only extract a small amount of mental states and hence a small number of mental commands. Thus, research is still needed here to extend the capacities of BCI, and to better exploit the few available mental states in virtual environments.</p>
      <p>
        <i>Our first motivation consists thus in designing novel “body-based” and “mind-based” controls of virtual environments and reaching, in both cases, more immersive and more efficient 3D interaction.</i>
      </p>
      <p>Furthermore, in current VR systems, motor activities and mental activities are always considered separately and exclusively. This reminds the well-known “body-mind dualism” which is at the heart of historical philosophical debates. In this context, our objective is to introduce novel “hybrid” interaction schemes in virtual reality, by considering motor and mental activities jointly, i.e., in a harmonious, complementary, and optimized way. Thus, we intend to explore novel paradigms of 3D interaction mixing body and mind inputs. Moreover, our approach becomes even more challenging when considering and connecting multiple users which implies multiple bodies and multiple brains collaborating and interacting in virtual reality.</p>
      <p>
        <i>Our second motivation consists thus in introducing a “hybrid approach” which will mix mental and motor activities of one or multiple users in virtual reality.</i>
      </p>
    </subsection>
  </presentation>
  <fondements id="uid4">
    <bodyTitle>Research Program</bodyTitle>
    <subsection id="uid5" level="1">
      <bodyTitle>Research Program</bodyTitle>
      <p>The scientific objective of Hybrid team is to improve 3D interaction of one or multiple users with virtual environments, by making full use of physical engagement of the body, and by incorporating the mental states by means of brain-computer interfaces. We intend to improve each component of this framework individually, but we also want to improve the subsequent combinations of these components.</p>
      <p>The "hybrid" 3D interaction loop between one or multiple users and a virtual environment is depicted in Figure <ref xlink:href="#uid6" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. Different kinds of 3D interaction situations are distinguished (red arrows, bottom): 1) body-based interaction, 2) mind-based interaction, 3) hybrid and/or 4) collaborative interaction (with at least two users). In each case, three scientific challenges arise which correspond to the three successive steps of the 3D interaction loop (blue squares, top): 1) the 3D interaction technique, 2) the modeling and simulation of the 3D scenario, and 3) the design of appropriate sensory feedback.</p>
      <object id="uid6">
        <table>
          <tr>
            <td>
              <ressource xlink:href="IMG/3DLoop.png" type="float" width="227.62204pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
            </td>
          </tr>
        </table>
        <caption>3D hybrid interaction loop between one or multiple users and a virtual reality system. Top (in blue) three steps of 3D interaction with a virtual environment: (1-blue) interaction technique, (2-blue) simulation of the virtual environment, (3-blue) sensory feedbacks. Bottom (in red) different cases of interaction: (1-red) body-based, (2-red) mind-based, (3-red) hybrid, and (4-red) collaborative 3D interaction.</caption>
      </object>
      <p>The 3D interaction loop involves various possible inputs from the user(s) and different kinds of output (or sensory feedback) from the simulated environment. Each user can involve his/her body and mind by means of corporal and/or brain-computer interfaces. A hybrid 3D interaction technique (1) mixes mental and motor inputs and translates them into a command for the virtual environment. The real-time simulation (2) of the virtual environment is taking into account these commands to change and update the state of the virtual world and virtual objects. The state changes are sent back to the user and perceived by means of different sensory feedbacks (e.g., visual, haptic and/or auditory) (3). The sensory feedbacks are closing the 3D interaction loop. Other users can also interact with the virtual environment using the same procedure, and can eventually “collaborate” by means of “collaborative interactive techniques” (4).</p>
      <p>This description is stressing three major challenges which correspond to three mandatory steps when designing 3D interaction with virtual environments:</p>
      <simplelist>
        <li id="uid7">
          <p noindent="true"><b>3D interaction techniques:</b> This first step consists in translating the actions or intentions of the user (inputs) into an explicit command for the virtual environment. In virtual reality, the classical tasks that require such kinds of user command were early categorized in four <ref xlink:href="#hybrid-2016-bid0" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>: navigating the virtual world, selecting a virtual object, manipulating it, or controlling the application (entering text, activating options, etc). The addition of a third dimension, the use of stereoscopic rendering and the use of advanced VR interfaces make however inappropriate many techniques that proved efficient in 2D, and make it necessary to design specific interaction techniques and adapted tools. This challenge is here renewed by the various kinds of 3D interaction which are targeted. In our case, we consider various cases, with motor and/or cerebral inputs, and potentially multiple users.</p>
        </li>
        <li id="uid8">
          <p noindent="true"><b>Modeling and simulation of complex 3D scenarios:</b> This second step corresponds to the update of the state of the virtual environment, in real-time, in response to all the potential commands or actions sent by the user. The complexity of the data and phenomena involved in 3D scenarios is constantly increasing. It corresponds for instance to the multiple states of the entities present in the simulation (rigid, articulated, deformable, fluids, which can constitute both the user’s virtual body and the different manipulated objects), and the multiple physical phenomena implied by natural human interactions (squeezing, breaking, melting, etc). The challenge consists here in modeling and simulating these complex 3D scenarios and meeting, at the same time, two strong constraints of virtual reality systems: performance (real-time and interactivity) and genericity (e.g., multi-resolution, multi-modal, multi-platform, etc).</p>
        </li>
        <li id="uid9">
          <p noindent="true"><b>Immersive sensory feedbacks:</b> This third step corresponds to the display of the multiple sensory feedbacks (output) coming from the various VR interfaces. These feedbacks enable the user to perceive the changes occurring in the virtual environment. They are closing the 3D interaction loop, making the user immersed, and potentially generating a subsequent feeling of presence. Among the various VR interfaces which have been developed so far we can stress two kinds of sensory feedback: visual feedback (3D stereoscopic images using projection-based systems such as CAVE systems or Head Mounted Displays); and haptic feedback (related to the sense of touch and to tactile or force-feedback devices). The Hybrid team has a strong expertize in haptic feedback, and in the design of haptic and “pseudo-haptic” rendering <ref xlink:href="#hybrid-2016-bid1" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. Note that a major trend in the community, which is strongly supported by the Hybrid team, relates to a “perception-based” approach, which aims at designing sensory feedbacks which are well in line with human perceptual capacities.</p>
        </li>
      </simplelist>
      <p noindent="true">These three scientific challenges are addressed differently according to the context and the user inputs involved. We propose to consider three different contexts, which correspond to the three different research axes of the Hybrid research team, namely : 1) body-based interaction (motor input only), 2) mind-based interaction (cerebral input only), and then 3) hybrid and collaborative interaction (i.e., the mixing of body and brain inputs from one or multiple users).</p>
    </subsection>
    <subsection id="uid10" level="1">
      <bodyTitle>Research Axes</bodyTitle>
      <p>The scientific activity of Hybrid team follows three main axes of research:</p>
      <simplelist>
        <li id="uid11">
          <p noindent="true"><b>Body-based interaction in virtual reality.</b> Our first research axis concerns the design of immersive and effective "body-based" 3D interactions, i.e., relying on a physical engagement of the user’s body. This trend is probably the most popular one in VR research at the moment. Most VR setups make use of tracking systems which measure specific positions or actions of the user in order to interact with a virtual environment. However, in recent years, novel options have emerged for measuring “full-body” movements or other, even less conventional, inputs (e.g. body equilibrium).
In this first research axis we are thus concerned by the emergence of new kinds of “body-based interaction” with virtual environments. This implies the design of novel 3D user interfaces and novel 3D interactive techniques, novel simulation models and techniques, and novel sensory feedbacks for body-based interaction with virtual worlds. It involves real-time physical simulation of complex interactive phenomena, and the design of corresponding haptic and pseudo-haptic feedback.</p>
        </li>
        <li id="uid12">
          <p noindent="true"><b>Mind-based interaction in virtual reality.</b> Our second research axis concerns the design of immersive and effective “mind-based” 3D interactions in Virtual Reality. Mind-based interaction with virtual environments is making use of Brain-Computer Interface technology. This technology corresponds to the direct use of brain signals to send “mental commands” to an automated system such as a robot, a prosthesis, or a virtual environment. BCI is a rapidly growing area of research and several impressive prototypes are already available. However, the emergence of such a novel user input is also calling for novel and dedicated 3D user interfaces. This implies to study the extension of the mental vocabulary available for 3D interaction with VE, then the design of specific 3D interaction techniques "driven by the mind" and, last, the design of immersive sensory feedbacks that could help improving the learning of brain control in VR.</p>
        </li>
        <li id="uid13">
          <p noindent="true"><b>Hybrid and collaborative 3D interaction.</b> Our third research axis intends to study the combination of motor and mental inputs in VR, for one or multiple users. This concerns the design of mixed systems, with potentially collaborative scenarios involving multiple users, and thus, multiple bodies and multiple brains sharing the same VE. This research axis therefore involves two interdependent topics: 1) collaborative virtual environments, and 2) hybrid interaction. It should end up with collaborative virtual environments with multiple users, and shared systems with body and mind inputs.</p>
        </li>
      </simplelist>
    </subsection>
  </fondements>
  <domaine id="uid14">
    <bodyTitle>Application Domains</bodyTitle>
    <subsection id="uid15" level="1">
      <bodyTitle>Overview</bodyTitle>
      <p>The research program of Hybrid team aims at next generations of virtual reality and 3D user interfaces which could possibly address both the “body” and “mind” of the user. Novel interaction schemes are designed, for one or multiple users. We target better integrated systems and more compelling user experiences.</p>
      <p>The applications of our research program correspond to the applications of virtual reality technologies which could benefit from the addition of novel body-based or mind-based interaction capabilities:</p>
      <simplelist>
        <li id="uid16">
          <p noindent="true"><b>Industry</b>: with training systems, virtual prototyping, or scientific visualization;</p>
        </li>
        <li id="uid17">
          <p noindent="true"><b>Medicine</b>: with rehabilitation and reeducation systems, or surgical training simulators;</p>
        </li>
        <li id="uid18">
          <p noindent="true"><b>Entertainment</b>: with 3D web navigations, video games, or attractions in theme parks,</p>
        </li>
        <li id="uid19">
          <p noindent="true"><b>Construction</b>: with virtual mock-ups design and review, or historical/architectural visits.</p>
        </li>
      </simplelist>
    </subsection>
  </domaine>
  <highlights id="uid20">
    <bodyTitle>Highlights of the Year</bodyTitle>
    <subsection id="uid21" level="1">
      <bodyTitle>Highlights of the Year</bodyTitle>
      <simplelist>
        <li id="uid22">
          <p noindent="true">Two new permanent staff have joined our team this year: Ronan Gaugne (Research Engineer, Univ. Rennes 1), Ferran Argelaguet (CR2 Inria Research Scientist).</p>
        </li>
        <li id="uid23">
          <p noindent="true">There has been an outstanding total of six PhD Theses defended this year by members of Hybrid.</p>
        </li>
        <li id="uid24">
          <p noindent="true">Our team organized, together with MimeTIC team, a press conference in Paris on the "6-Finger Illusion" in May 2016, followed by a huge media coverage.</p>
        </li>
      </simplelist>
      <subsection id="uid25" level="2">
        <bodyTitle>Awards</bodyTitle>
        <simplelist>
          <li id="uid26">
            <p noindent="true">Paper and demo "When the Giant meets the Ant: An Asymmetric Approach for Collaborative and Concurrent Object Manipulation in a Multi-Scale Environment" <ref xlink:href="#hybrid-2016-bid2" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> obtained the Second Prize at the IEEE 3DUI Contest 2016.</p>
          </li>
          <li id="uid27">
            <p noindent="true">Project MANDARIN received the "Economical Impact Award 2016" from ANR (French National Research Agency).</p>
          </li>
          <li id="uid28">
            <p noindent="true">Project PREVIZ received a "Loading the Future' Trophy 2016" from Images et Réseaux French Competitivity Cluster.</p>
          </li>
        </simplelist>
      </subsection>
    </subsection>
  </highlights>
  <logiciels id="uid29">
    <bodyTitle>New Software and Platforms</bodyTitle>
    <subsection id="uid30" level="1">
      <bodyTitle>#FIVE</bodyTitle>
      <p><span class="smallcap" align="left">Keywords:</span> Virtual reality - Behaviour - 3D interaction</p>
      <p noindent="true">
        <span class="smallcap" align="left">Functional Description</span>
      </p>
      <p>#FIVE (Framework for Interactive Virtual Environments) is a framework for the development of interactive and collaborative virtual environments. #FIVE was developed to answer the need for an easier and a faster design and development of virtual reality applications. #FIVE provides a toolkit that simplifies the declaration of possible actions and behaviours of objects in a VE. It also provides a toolkit that facilitates the setting and the management of collaborative interactions in a VE. It is compliant with a distribution of the VE on different setups. It also proposes guidelines to efficiently create a collaborative and interactive VE. The current implementation is in C# and comes with a Unity3D engine integration, compatible with MiddleVR framework. #FIVE contains software modules that can be interconnected and helps in building interactive and collaborative virtual environments. The user can focus on domain-specific aspects for his/her application thanks to #FIVE's modules. These modules can be used in a vast range of domains using virtual reality applications and requiring interactive environments and collaboration, such as in training for example.</p>
      <simplelist>
        <li id="uid31">
          <p noindent="true">Participants: Bruno Arnaldi, Valerie Gouranton, Florian Nouviale, Guillaume Claude</p>
        </li>
        <li id="uid32">
          <p noindent="true">Contact: Valerie Gouranton and Florian Nouviale</p>
        </li>
        <li id="uid33">
          <p noindent="true">URL: <ref xlink:href="https://bil.inria.fr/fr/software/view/2527/tab" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>bil.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>fr/<allowbreak/>software/<allowbreak/>view/<allowbreak/>2527/<allowbreak/>tab</ref></p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid34" level="1">
      <bodyTitle>#SEVEN</bodyTitle>
      <p><span class="smallcap" align="left">Keywords:</span> Virtual reality - Scenario - Training - Petri Net - 3D interaction</p>
      <p noindent="true">
        <span class="smallcap" align="left">Functional Description</span>
      </p>
      <p>#SEVEN (Sensor Effector Based Scenarios Model for Driving Collaborative Virtual Environments) is a sensor effector based scenario engine that enables the execution of complex scenarios for
driving Virtual Reality applications. #SEVEN's scenarios are based on an enhanced Petri net model which is able to describe and solve intricate event sequences. #SEVEN comes with an editor for creating, editing and remotely controlling and running scenarios. #SEVEN is implemented in C# and can be used as a stand-alone application or as a library. An integration to the Unity3D engine, compatible with MiddleVR, also exists.</p>
      <simplelist>
        <li id="uid35">
          <p noindent="true">Participants: Bruno Arnaldi, Valerie Gouranton, Florian Nouviale, Guillaume Claude</p>
        </li>
        <li id="uid36">
          <p noindent="true">Contact: Valerie Gouranton and Florian Nouviale</p>
        </li>
        <li id="uid37">
          <p noindent="true">URL: <ref xlink:href="https://bil.inria.fr/fr/software/view/2528/tab" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>bil.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>fr/<allowbreak/>software/<allowbreak/>view/<allowbreak/>2528/<allowbreak/>tab</ref></p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid38" level="1">
      <bodyTitle>OpenViBE</bodyTitle>
      <p><span class="smallcap" align="left">Keywords:</span> Neurosciences - Interaction - Virtual reality - Health - Real time - Neurofeedback - Brain-Computer Interface - EEG - 3D interaction</p>
      <p noindent="true">
        <span class="smallcap" align="left">Functional Description</span>
      </p>
      <p>OpenViBE is a free and open-source software platform devoted to the design, test and use of Brain-Computer Interfaces (BCI). The platform consists of a set of software modules that can be integrated easily and efficiently to design BCI applications. The key features of OpenViBE software are its modularity, its high-performance, its portability, its multiple-users facilities and its connection with high-end/VR displays. The designer of the platform enables to build complete scenarios based on existing software modules using a dedicated graphical language and a simple Graphical User Interface (GUI). This software is available on the Inria Forge under the terms of the AGPL licence, and it was officially released in June 2009. Since then, the OpenViBE software has already been downloaded more than 40000 times, and it is used by numerous laboratories, projects, or individuals worldwide. More information, downloads, tutorials, videos, documentations are available on the OpenViBE website.</p>
      <simplelist>
        <li id="uid39">
          <p noindent="true">Participants: Anatole Lécuyer, Jussi Tapio Lindgren, Jerome Chabrol, Charles Garraud, and Marsel Mano</p>
        </li>
        <li id="uid40">
          <p noindent="true">Partners: Inria teams POTIOC, ATHENA and NEUROSYS</p>
        </li>
        <li id="uid41">
          <p noindent="true">Contact: Anatole Lécuyer</p>
        </li>
        <li id="uid42">
          <p noindent="true">URL: <ref xlink:href="http://openvibe.inria.fr" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>openvibe.<allowbreak/>inria.<allowbreak/>fr</ref></p>
        </li>
        <li id="uid43">
          <p noindent="true">URL: <ref xlink:href="https://bil.inria.fr/fr/software/view/1194/tab" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>bil.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>fr/<allowbreak/>software/<allowbreak/>view/<allowbreak/>1194/<allowbreak/>tab</ref></p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid44" level="1">
      <bodyTitle>Platform: Immerstar</bodyTitle>
      <simplelist>
        <li id="uid45">
          <p noindent="true">Participants : Florian Nouviale, Ronan Gaugne</p>
        </li>
      </simplelist>
      <p>With the two platforms of virtual reality, Immersia and Immermove, grouped under the name Immerstar, the team has access to high level scientific facilities. This equipment benefits the research teams of the center and has allowed them to extend their local, national and international collaborations. The Immerstar platform is granted by a Inria CPER funding for 2015-2019 that enables important evolutions of the equipment. In 2016, the first technical evolutions have been decided, with, for Immermove, the addition of a third face to the immersive space, and the extension of the Vicon tracking system, and for Immersia, the installation of WQXGA laser projectors and of a new tracking system.</p>
    </subsection>
  </logiciels>
  <resultats id="uid46">
    <bodyTitle>New Results</bodyTitle>
    <subsection id="uid47" level="1">
      <bodyTitle>Virtual Reality and 3D Interaction</bodyTitle>
      <subsection id="cid1" level="2">
        <bodyTitle>Perception in Virtual Environments</bodyTitle>
        <p>With the increasing demand in consumer VR applications, the need to understand how users perceive the virtual environment and their virtual self (avatar) is becoming more and more important. In particular, with the potential of virtual reality to alter and control avatars in different ways, the user representation in the virtual world does not always necessarily match the user body structure. Besides, the study of how the users perceive their surrounding environment (e.g. depth perception) is another active field of research in VR.</p>
        <p>
          <b>The role of interaction in virtual embodiment: Effects of the virtual hand representation</b>
        </p>
        <p><b>Participants:</b> Ferran Argelaguet and Anatole Lécuyer</p>
        <p>First, we have studied how people appropriate their virtual hand representation when interacting in virtual environments <ref xlink:href="#hybrid-2016-bid3" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. In order to answer this question, we conducted an experiment studying the sense of embodiment when interacting with three different virtual hand representations (see Figure <ref xlink:href="#uid48" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>), each one providing a different degree of visual realism but keeping the same control mechanism. The main experimental task was a Pick-and-Place task in which participants had to grasp a virtual cube and place it to an indicated position while avoiding an obstacle (brick, barbed wire or fire). Results show that the sense of agency is stronger for less realistic virtual hands which also provide less mismatch between the participant's actions and the animation of the virtual hand. In contrast, the sense of ownership is increased for the human virtual hand which provides a direct mapping between the degrees of freedom of the real and virtual hand.</p>
        <p>This work was done in collaboration with MimeTIC team.</p>
        <object id="uid48">
          <table>
            <tr>
              <td>
                <ressource xlink:href="IMG/hand-interaction.png" type="float" width="213.5pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
          </table>
          <caption>Evaluated virtual hand representations: abstract (left), iconic (center) and realistic virtual hands (right). Each virtual hand had its own visual
feedback when the grasping operation was triggered.</caption>
        </object>
        <p>
          <b>Wow! I Have Six Fingers!”: Would You Accept Structural Changes of Your Hand in VR?</b>
        </p>
        <p><b>Participants:</b> Ferran Argelaguet and Anatole Lécuyer</p>
        <p>In a different context, we have explored how users would accept as their own a six-digit realistic virtual hand <ref xlink:href="#hybrid-2016-bid4" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. By measuring participants’ senses of ownership (i.e., the impression that the virtual hand is actually our own hand) and agency (i.e., the impression to be able to control the actions of the virtual hand), we somehow evaluate the possibility of creating a Six-Finger Illusion in VR. We measured these two dimensions of virtual embodiment in a virtual reality experiment where participants performed two tasks successively: (1) a self-manipulation task inducing visuomotor feedback, where participants mimicked finger movements presented in the virtual scene and (2) a visuotactile task inspired by Rubber Hand Illusion protocols, where an experimenter stroked the hand of the user with a brush (see Figure <ref xlink:href="#uid49" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>). The real and virtual brushes were synchronously stroking the participants’ real and virtual hand, and in the case when the virtual brush was stroking the additional virtual digit, the real ring finger was also synchronously stroked to provide consistent tactile stimulation and elicit a sense of embodiment. Results of the experiment show that participants did experience high levels of ownership and agency of the six-digit virtual hand as a whole. These results bring preliminary insights about how avatar with structural differences can affect the senses of ownership and agency experienced by users in VR.</p>
        <p>This work was done in collaboration with MimeTIC team.</p>
        <object id="uid49">
          <table>
            <tr>
              <td>
                <ressource xlink:href="IMG/six-fingers.jpg" type="float" width="320.25pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
          </table>
          <caption>The virtual six-finger hand and the participant's hand are synchronously stimulated using a virtual and a real brush respectively.</caption>
        </object>
        <p>
          <b>CAVE Size Matters: Effects of Screen Distance and Parallax on Distance Estimation in Large Immersive Display Setups</b>
        </p>
        <p><b>Participants:</b> Ferran Argelaguet and Anatole Lécuyer</p>
        <p>When walking within a CAVE-like system, accommodation distance, parallax, and angular resolution vary according to the distance between the user and the projection walls, which can alter spatial perception. As these systems get bigger, there is a need to assess the main factors influencing spatial perception in order to better design immersive projection systems and virtual reality applications. In this work, we performed two experiments that analyze distance perception when considering the distance toward the projection screens and parallax as main factors. Both experiments were conducted in a large immersive projection system with up to 10-meter interaction space. The first experiment showed that both the screen distance and parallax have a strong asymmetric effect on distance judgments. We observed increased underestimation for positive parallax conditions and slight distance overestimation for negative and zero parallax conditions. The second experiment further analyzed the factors contributing to these effects and confirmed the observed effects of the first experiment with a high-resolution projection setup providing twice the angular resolution and improved accommodative stimuli. In conclusion, our results suggest that space is the most important characteristic for distance perception, optimally requiring about 6- to 7-meter distance around the user, and virtual objects with high demands on accurate spatial perception should be displayed at zero or negative parallax <ref xlink:href="#hybrid-2016-bid5" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
        <p>This work was done in collaboration with MimeTIC team and the University of Hamburg.</p>
      </subsection>
      <subsection id="cid2" level="2">
        <bodyTitle>3D User Interfaces</bodyTitle>
        <p>
          <b>GiAnt: stereoscopic-compliant multi-scale navigation in VEs</b>
        </p>
        <p><b>Participants:</b> Ferran Argelaguet</p>
        <p>Navigation in multi-scale virtual environments (MSVE) requires the adjustment of the navigation parameters to ensure optimal navigation experiences at each level of scale (see Figure <ref xlink:href="#uid50" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>). In particular, in immersive stereoscopic systems, e.g. when performing zoom-in and zoom-out operations, the navigation speed and the stereoscopic rendering parameters have to be adjusted accordingly. Although this adjustment can be done manually by the user, it can be complex, tedious and strongly depends on the virtual environment. We have proposed GiAnt (GIant/ANT) <ref xlink:href="#hybrid-2016-bid6" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, a new multi-scale navigation technique which automatically and seamlessly adjusts the navigation speed and the scale factor of the virtual environment based on the user's perceived navigation speed. The adjustment ensures an almost-constant perceived navigation speed while avoiding diplopia effects or diminished depth perception due to improper stereoscopic rendering configurations. The results from the conducted user evaluation shows that GiAnt is an efficient multi-scale navigation which minimizes the changes of the scale factor of the virtual environment compared to state-of-the-art multi-scale navigation techniques.</p>
        <object id="uid50">
          <table>
            <tr>
              <td>
                <ressource xlink:href="IMG/giant.png" type="float" width="384.2974pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
          </table>
          <caption>Multi-scale navigation sequence requiring the adaptation of the camera speed and the stereoscopic rendering parameters (e.g. parallax). GiAnt ensures that the navigation speed and the scale factor of the virtual environment are adjusted ensuring a comfortable navigation experience.</caption>
        </object>
        <p>
          <b>Enjoying 360<formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msup><mrow/><mo>∘</mo></msup></math></formula> Vision with the FlyVIZ</b>
        </p>
        <p><b>Participants:</b> Florian Nouviale, Maud Marchal and Anatole Lécuyer</p>
        <p>FlyVIZ is a novel concept of wearable display device which enables to extend the human field-of-view up to 360°. With the FlyVIZ users can enjoy an artificial omnidirectional vision and see "with eyes behind their back"! We propose a novel version of our approach called the FlyVIZ v2. It is based on affordable and on the shelf components. For image acquisition, the FlyVIZ v2 relies on an iPhone4S smart-phone combined with a GoPano lens that contains a curved mirror enabling the capture of video with 360<formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msup><mrow/><mo>∘</mo></msup></math></formula> horizontal field-of-view. For image transformation, we developed a dedicated software for iPhone that processes the video stream and transforms it into a real-time meaningful representation for the user.
The “FlyVIZ_v2” was demonstrated at the ACM SIGGRAPH Emerging Technologies (2016).</p>
        <object id="uid51">
          <table>
            <tr>
              <td>
                <ressource xlink:href="IMG/flyviz_v2.jpg" type="float" width="384.2974pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
          </table>
          <caption>(Left) Overview of the system. (Middle) 360<formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msup><mrow/><mo>∘</mo></msup></math></formula> panoramic image displayed in the HMD when walking in a corridor. (Right) User grabbing an object located outside his natural field-of-view.</caption>
        </object>
        <p>
          <b>D3PART: A new Model for Redistribution and Plasticity of 3D User Interfaces</b>
        </p>
        <p><b>Participants:</b> Jérémy Lacoche and Bruno Arnaldi</p>
        <p>D3PART (Dynamic 3D Plastic And Redistribuable Technology) is a new model that we introduced to handle redistribution for 3D user interfaces. Redistribution consists in changing the components distribution of an interactive system across different dimensions such as platform, display and user. We extended previous plasticity models with redistribution capabilities, which lets developers create applications where 3D content and interaction tasks can be automatically redistributed across the different dimensions at runtime <ref xlink:href="#hybrid-2016-bid7" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
        <p>This work was done in collaboration with b&lt;&gt;com, ENIB and Telecom Bretagne.</p>
        <p>
          <b>Integration concept and model of Industry Foundation Classes (IFC) for interactive virtual environments</b>
        </p>
        <p><b>Participants:</b> Anne-Solène Dris, Valérie Gouranton and Bruno Arnaldi</p>
        <p>We defined a concept of Building Information Modeling (BIM) in combination with an integration model in order to enable interaction in Virtual Environments (see Figure <ref xlink:href="#uid52" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>). Such model, rich of information could be used to increase the level of abstraction of the interaction process. We proposed to explore and define how to create a BIM to ensure interoperability with the Industry Foundation Classes (IFC) model. The IFC model provides a definition of building objects, geometry, relation between objects, and other attributes such as layers, systems, link to planning, construction method, materials, domain (HVAC, Electrical, Architectural, Structure...) and quantities. The interoperability will enrich the virtual environment with the aim of creating an informed and interactive virtual environments, thus reducing the costs of applications' development. We defined a BIM modeling methodology extending the IFC interoperability to the interactive virtual environment <ref xlink:href="#hybrid-2016-bid8" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
        <object id="uid52">
          <table>
            <tr>
              <td>
                <ressource xlink:href="IMG/IFCStructure.png" type="float" width="384.2974pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
          </table>
          <caption>Interaction in virtual environments related to construction area based on BIM and a model of Industry Foundation Classes (IFC).</caption>
        </object>
      </subsection>
      <subsection id="cid3" level="2">
        <bodyTitle>Virtual Archaeology</bodyTitle>
        <p>
          <b>Digital and handcrafting processes applied to sound-studies of archaeological bone flutes</b>
        </p>
        <p><b>Participants:</b> Jean-Baptiste Barreau, Ronan Gaugne, Bruno Arnaldi and Valérie Gouranton.</p>
        <p>Bone flutes make use of a naturally hollow raw-material. As nature does not produce duplicates, each bone has its own inner cavity, and thus its own sound-potential. This morphological variation implies acoustical specificities, thus making it impossible to handcraft a true and exact sound-replica in another bone. This phenomenon has been observed in a handcrafting context and has led us to conduct two series of experiments (the first one using a handcrafting process, the second one using a 3D process) in order to investigate its exact influence on acoustics as well as on sound-interpretation based on replicas. The comparison of the results has shed light upon epistemological and methodological issues that have yet to be fully understood. This work contributes to assessing the application of digitization, 3D printing and handcrafting to flute-like sound instruments studied in the field of archaeomusicology <ref xlink:href="#hybrid-2016-bid9" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
        <p>This work was done in collaboration with MimeTIC team, ARTeHis, LBBE and Atelier El Block.</p>
        <object id="uid53">
          <table>
            <tr>
              <td>
                <ressource xlink:href="IMG/flutes.jpg" type="float" width="320.25pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
          </table>
          <caption>Sound-studies of archaeological bone flutes: (a) control flute (left) and its replica (right) both made out of goat’s tibias, (b) 3D sculpted patch (transparent gray) on Blender (based on the geometry of the cloud), (c) Diagrams analysis , (d) The sound proximity of each replica comparing to the control flute, for each finger hole (numeric scale in semi-tones).</caption>
        </object>
        <p>
          <b>Internal 3D Printing of Intricate Structures</b>
        </p>
        <p><b>Participants:</b> Ronan Gaugne, Valérie Gouranton and Bruno Arnaldi.</p>
        <p>Additive technologies are increasingly used in Cultural Heritage process, for example in order to reproduce, complete, study or exhibit artefacts. 3D copies are based on digitization techniques such as laser scan or photogrammetry. In this case, the 3D copy remains limited to the external surface of objects. Medical images based digitization such as MRI or CT scan are also increasingly used in CH as they provide information on the internal structure of archaeological material. Different previous works illustrated the interest of combining 3D printing and CT scan in order to extract concealed artefacts from larger archaeological material. The method was based on 3D segmentation techniques within volume data obtained by CT scan to isolate nested objects. This approach was useful to perform a digital extraction, but in some case it is also interesting to observe the internal spatial organization of an intricate object in order to understand its production process. We propose a method for the representation of a complex internal structure based on a combination of CT scan and emerging 3D printing techniques mixing colored and transparent parts <ref xlink:href="#hybrid-2016-bid10" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, <ref xlink:href="#hybrid-2016-bid11" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. This method was successfully applied to visualize the interior of a funeral urn and is currently applied on a set of tools agglomerated in a gangue of corrosion (see Figure <ref xlink:href="#uid54" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>).</p>
        <p>This work was done in collaboration with Inrap and Image ET.</p>
        <object id="uid54">
          <table>
            <tr>
              <td>
                <ressource xlink:href="IMG/3DPrinting.png" type="float" width="320.25pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
          </table>
          <caption>Front and bottom views of our 3D printed urn.</caption>
        </object>
      </subsection>
    </subsection>
    <subsection id="uid55" level="1">
      <bodyTitle>Physically-Based Simulation and Multisensory Feedback</bodyTitle>
      <subsection id="cid4" level="2">
        <bodyTitle>Physically-based Simulation</bodyTitle>
        <p>
          <b>Real-time tracking of deformable targets in 3D ultrasound sequences</b>
        </p>
        <p><b>Participants:</b> Maud Marchal</p>
        <p>Soft-tissue motion tracking is an active research area that consists in providing accurate evaluation about the location of anatomical structures. To do so, ultrasound imaging is often used since it is non-invasive, real-time and portable.
Thus, several ultrasound tracking approaches have been developed in order to estimate soft tissue displacements that are caused by physiological motions and manipulations by medical tools. These methods have gained significant interest
for image-guided therapies such as radio-frequency ablation or high-intensity focused ultrasound.
In our work, we present a real-time approach that allows tracking deformable
structures in 3D ultrasound sequences <ref xlink:href="#hybrid-2016-bid12" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. Our method consists in obtaining the
target displacements by combining robust dense motion estimation and mechanical model simulation. We performed an evaluation of our method through simulated data, phantom data, and real-data. Results demonstrate that this novel approach has the advantage of providing correct motion estimation regarding different ultrasound shortcomings including speckle noise, large shadows and ultrasound gain variation. Furthermore, we show the good performance of our method with respect to state-of-the-art techniques by testing on the 3D databases provided by MICCAI CLUST'14 and CLUST'15 challenges.</p>
        <p>This work was done in collaboration with LAGADIC team and b&lt;&gt;com.</p>
      </subsection>
      <subsection id="cid5" level="2">
        <bodyTitle>3D Haptic Interaction</bodyTitle>
        <p>
          <b>DesktopGlove: a Multi-finger Force Feedback Interface Separating Degrees of Freedom Between Hands</b>
        </p>
        <p><b>Participants:</b> Merwan Achibet and Maud Marchal</p>
        <p>In virtual environments, interacting directly with our hands and fingers greatly contributes to immersion, especially when force feedback is provided for simulating the touch of virtual objects. Yet, common haptic interfaces are unfit for multi-finger manipulation and only costly and cumbersome grounded exoskeletons do provide all the efforts expected from object manipulation. To make multi-finger haptic interaction more accessible, we have proposed to combine two affordable haptic interfaces into a bimanual setup named DesktopGlove. With this approach, each hand is in charge of different components of object manipulation: one commands the global motion of a virtual hand while the other controls its fingers for grasping (see Figure <ref xlink:href="#uid56" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>). In addition, each hand is subjected to forces that relate to its own degrees of freedom so that users perceive a variety of haptic effects through both of them. Our results show that (1) users are able to integrate the separated degrees of freedom of DesktopGlove to efficiently control a virtual hand in a posing task, (2) DesktopGlove shows overall better performance than a traditional data glove and is preferred by users, and (3) users considered the separated haptic feedback realistic and accurate for manipulating objects in virtual environments <ref xlink:href="#hybrid-2016-bid13" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
        <p>This work was done in collaboration with MJOLNIR team.</p>
        <object id="uid56">
          <table>
            <tr>
              <td>
                <ressource xlink:href="IMG/desktopGlove.jpg" type="float" width="384.2974pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
          </table>
          <caption>DesktopGlove separates the control of one virtual hand between both user's hands: a common haptic arm handles the global motion and a custom
multi-finger interface controls the virtual fingers. The force feedback is split between both interfaces so that each hand is exposed to forces that relate to its own frame of reference.</caption>
        </object>
        <p>
          <b>ElasticArm: leveraging passive haptic feedback in virtual environments</b>
        </p>
        <p><b>Participants:</b> Merwan Achibet, Adrien Girard, Anatole Lécuyer and Maud Marchal</p>
        <p>Haptic feedback is known to improve 3D interaction in virtual environments but current haptic interfaces remain complex and tailored to desktop interaction. In <ref xlink:href="#hybrid-2016-bid14" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, we describe an alternative approach called “Elastic-Arm” for incorporating haptic feedback in immersive virtual environments in a simple and cost-effective way. The Elastic-Arm is based on a body-mounted elastic armature that links the user's hand to his body and generates a progressive egocentric force when extending the arm. A variety of designs can be proposed with multiple links attached to various locations on the body in order to simulate different haptic properties and sensations such as different levels of stiffness, weight lifting, bimanual interaction, etc. Our passive haptic approach can be combined with various 3D interaction techniques and we illustrate the possibilities offered by the Elastic-Arm through several use cases based on well-known techniques such as the Bubble technique, redirected touching, and pseudo-haptics. A user study was conducted which showed the effectiveness of our pseudo-haptic technique as well as the general appreciation of the Elastic-Arm. We believe that the Elastic-Arm could be used in various VR applications which call for mobile haptic feedback or human-scale haptic sensations.</p>
        <p>
          <b>Vision-based adaptive assistance and haptic guidance for safe wheelchair corridor following</b>
        </p>
        <p><b>Participants:</b> Maud Marchal</p>
        <p>In case of motor impairments, steering a wheelchair can become a hazardous
task. Joystick jerks induced by uncontrolled motions may lead to wall collisions when a user steers a wheelchair along a corridor. In <ref xlink:href="#hybrid-2016-bid15" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> we introduce a low-cost assistive and guidance system for indoor corridor navigation in a wheelchair, which uses purely visual information, and which is capable of providing automatic trajectory correction and haptic guidance in order to avoid wall collisions. A visual servoing approach to autonomous corridor following serves as the backbone to this system. The algorithm employs natural image features which can be robustly extracted in real time. This algorithm is then fused with manual joystick input from the user so that progressive assistance and trajectory correction can be activated as soon as the user is in danger of collision. A force feedback in conjunction with the assistance is provided on the joystick in order to guide the user out of his dangerous trajectory. This ensures intuitive guidance and minimal interference from the trajectory correction system. In addition to being a low-cost approach, it can be seen that the proposed solution does not require an a-priori environment model. Experiments on a robotised wheelchair equipped with a monocular camera prove the capability of the system to adaptively guide and assist a user navigating in a corridor.</p>
        <p>This work was done in collaboration with LAGADIC team.</p>
      </subsection>
      <subsection id="cid6" level="2">
        <bodyTitle>Tactile Interaction at Fingertips</bodyTitle>
        <p>The fingertips are one of the most important and sensitive parts of our body. They are the first stimulated areas of the hand when we interact with our environment. Providing haptic feedback to the fingertips in virtual reality could, thus, drastically improve perception and interaction with virtual environments. Within this context, we proposed two contributions for tactile feedback and haptic interaction at the fingertips.</p>
        <p>
          <b>The Haptip</b>
        </p>
        <p><b>Participants:</b> Adrien Girard, Yoren Gaffary, Anatole Lécuyer and Maud Marchal</p>
        <p>In <ref xlink:href="#hybrid-2016-bid16" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, we present a modular approach called HapTip to display such haptic sensations at the level of the fingertips. This approach relies on a wearable and compact haptic device able to simulate 2 Degree of Freedom (DoF) shear forces on the fingertip with a displacement range of 2 mm. Several modules can be added and used jointly in order to address multi-finger and/or bimanual scenarios in virtual environments. For that purpose, we introduce several haptic rendering techniques to cover different cases of 3D interaction, such as touching a rough virtual surface, or feeling the inertia or weight of a virtual object. In order to illustrate the possibilities offered by HapTip, we provide four use cases focused on touching or grasping virtual objects (see Figure <ref xlink:href="#uid57" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>). To validate the efficiency of our approach, we also conducted experiments to assess the tactile perception obtained with HapTip. Our results show that participants can successfully discriminate the directions of the 2 DoF stimulation of our haptic device. We found also that participants could well perceive different weights of virtual objects simulated using two HapTip devices. We believe that HapTip could be used in numerous applications in virtual reality for which 3D manipulation and tactile sensations are often crucial, such as in virtual prototyping or virtual training.</p>
        <object id="uid57">
          <table>
            <tr>
              <td>
                <ressource xlink:href="IMG/HapTip.png" type="float" width="384.2974pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
          </table>
          <caption>Illustrative use cases of our approach HapTip: the user can get in contact and tap a virtual bottle, touch a surface and feel its texture, and heft an object and feel its weight.</caption>
        </object>
        <p>This work was done in collaboration with CEA List.</p>
        <p>
          <b>Studying one and two-finger perception of tactile directional cues</b>
        </p>
        <p><b>Participants:</b> Yoren Gaffary, Anatole Lécuyer and Maud Marchal</p>
        <p>In <ref xlink:href="#hybrid-2016-bid17" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, we study the perception of tactile directional cues by one or two fingers, using either the index, middle, or ring finger, or any of their combination.
Therefore, we use tactile devices able to stretch the skin of the fingertips in 2 DOF along four directions: horizontal, vertical, and the two diagonals.
We measure the recognition rate in each direction, as well as the subjective preference, depending on the (couple of) finger(s) stimulated (see Figure <ref xlink:href="#uid58" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>).
Our results show first that using the index and/or middle finger performs significantly better than using the ring finger on both qualitative and quantitative measures.
The results when comparing one versus two-finger configurations are more contrasted.
The recognition rate of the diagonals is higher when using one finger than two, whereas two fingers enable a better perception of the horizontal direction. These results pave the way to other studies on one versus two-finger perception, and raise methodological considerations for the design of multi-finger tactile devices.</p>
        <object id="uid58">
          <table>
            <tr>
              <td>
                <ressource xlink:href="IMG/procedureYoren.jpg" type="float" width="213.5pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
          </table>
          <caption>Experimental setup of our study on the perception of tactile directional cues by one or two fingers: the participant reports the direction of the stimulus he just perceived on the fingertip of his middle finger.</caption>
        </object>
        <p>This work was done in collaboration with CEA List, IRMAR and Agrocampus Ouest.</p>
      </subsection>
    </subsection>
    <subsection id="uid59" level="1">
      <bodyTitle>Collaborative Virtual Environments</bodyTitle>
      <subsection id="cid7" level="2">
        <bodyTitle>Acting in Collaborative Virtual Environments</bodyTitle>
        <p>
          <b>VR Rehearsals for Acting with Visual Effects</b>
        </p>
        <p><b>Participants:</b> Rozenn Bouville, Valérie Gouranton and Bruno Arnaldi,</p>
        <p>We studied the use of Virtual Reality for movie actors rehearsals of VFX-enhanced scenes. The impediment behind VFX scenes is that actors must be filmed in front of monochromatic green or blue screens with hardly any cue to the digital scenery that is supposed to surround them. The problem is worsens when the scene includes interaction with digital partners. The actors must pretend they are sharing the set with imaginary creatures when they are, in fact, on their own on an empty set. To support actors in this complicated task, we introduced the use of VR for acting rehearsals not only to immerse actors in the digital scenery but to provide them with advanced features for rehearsing their play. Indeed, our approach combines a fully interactive environment with a dynamic scenario feature to allow actors to become familiar with the virtual elements while rehearsing dialogue and action at their own speed. The interactive and creative rehearsals enabled by the system can be either single-user or multiuser. Moreover, thanks to the wide range of supported platforms, VR rehearsals can take place either onset or offset. We conducted a preliminary study to assess whether VR training can replace classical training (see Figure <ref xlink:href="#uid60" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>). The results show that VR-trained actors deliver a performance just as good as ordinarily trained actors. Moreover, all the subjects in our experiment preferred VR training to classic training <ref xlink:href="#hybrid-2016-bid18" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
        <object id="uid60">
          <table>
            <tr>
              <td>
                <ressource xlink:href="IMG/gi.png" type="float" width="427.0pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
          </table>
          <caption>The use of VR for acting rehearsal enables actors to rehearse being immersed in the virtual scenery before being shot on a green and empty set.</caption>
        </object>
        <p>
          <b>Synthesis and Simulation of Collaborative Surgical Process Models</b>
        </p>
        <p><b>Participants:</b> Guillaume Claude, Valérie Gouranton and Bruno Arnaldi</p>
        <p>The use of Virtual Reality for surgical training has been mostly focused on technical surgical skills. We proposed a novel approach by focusing on the procedural aspects <ref xlink:href="#hybrid-2016-bid19" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. Our system relies on a specific work-flow, which enbables to generate a model of the procedure based on real case surgery observations made in the operating room (see Figure <ref xlink:href="#uid61" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>). In addition, in the context of the project S3PM we then proposed an innovative workflow to integrate the generic model of the procedure (generated from the real-case surgery observation) as a scenario model in the VR training system (see Figure <ref xlink:href="#uid62" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>). We described how the generic procedure model could be generated, as well as its integration in the virtual environment <ref xlink:href="#hybrid-2016-bid20" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
        <object id="uid61">
          <table>
            <tr>
              <td>
                <ressource xlink:href="IMG/mmvr.png" type="float" width="427.0pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
          </table>
          <caption>Collaborative Virtual Environments for Training in Surgical Procedures, based on observations during real surgeries. Observation data is integrated into a system providing a Generalised Surgical Process Model (gSPM) of the procedure. This Model is integrated as the scenario of the Virtual Environment.</caption>
        </object>
        <object id="uid62">
          <table>
            <tr>
              <td>
                <ressource xlink:href="IMG/s3pm.png" type="float" width="427.0pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
          </table>
          <caption>Virtual replica of a real operating room of Rennes hospital (CHU Rennes) in the Immersia CAVE-like setup (IRISA/Inria Rennes).</caption>
        </object>
        <p>This work was done in collaboration with HYCOMES team and LTSI Inserm Medicis.</p>
      </subsection>
      <subsection id="cid8" level="2">
        <bodyTitle>Awareness for Collaboration in Virtual Environments</bodyTitle>
        <p>
          <b>Take-Over Control Paradigms in Collaborative Virtual Environments for Training</b>
        </p>
        <p><b>Participants:</b> Gwendal Le Moulec, Ferran Argelaguet, Anatole Lécuyer and Valérie Gouranton</p>
        <p>We studied the notion of Take-Over Control in Collaborative Virtual Environments for Training (CVET). The Take-Over Control represents the transfer (the take over) of the interaction control of an object between two or more users. This paradigm is particularly useful for training scenarios, in which the interaction control could be continuously exchanged between the trainee and the trainer, e.g. the latter guiding and correcting the trainee’s actions. We proposed a formalization of the Take-Over Control followed by an illustration focusing in a use-case of collaborative maritime navigation. In the presented use-case, the trainee has to avoid an under-water obstacle with the help of a trainer who has additional information about the obstacle. The use-case allows to highlight the different elements a Take-Over Control situation should enforce, such as user’s awareness. Different Take-Over Control techniques were provided and evaluated focusing on the transfer exchange mechanism and the visual feedback (see Figure <ref xlink:href="#uid63" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>). The results show that participants preferred the Take-Over Control technique which maximized the user awareness <ref xlink:href="#hybrid-2016-bid21" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
        <object id="uid63">
          <table>
            <tr>
              <td>
                <ressource xlink:href="IMG/TakeOverControl.png" type="float" width="427.0pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
          </table>
          <caption>Our illustrative use case inspired by maritime navigation for Take-Over Control during training in a collaborative virtual environment. The user was instructed to steer a boat towards a semi-transparent red column (target destination) by controlling the heading of the boat. A white handle indicated the rotation angle of the boat(a). The sequence (b,c,d) shows the evolution of the contribution of the trainer on the steering angle, from no control to full control.</caption>
        </object>
        <p>
          <b>Vishnu: Virtual Immersive Support for HelpiNg Users: An Interaction Paradigm for Collaborative Remote Guiding in Mixed Reality</b>
        </p>
        <p><b>Participants:</b> Morgan Le Chénéchal, Valérie Gouranton and Bruno Arnaldi</p>
        <p>Increasing networking performances as well as the emergence of Mixed Reality (MR) technologies make possible providing advanced interfaces to improve remote collaboration. We presented a novel interaction paradigm called Vishnu that aims to ease collaborative remote guiding. We focus on collaborative remote maintenance as an illustrative use case. It relies on an expert immersed in Virtual Reality (VR) in the remote workspace of a local agent helped through an Augmented Reality (AR) interface. The main idea of the Vishnu paradigm is to provide the local agent with two additional virtual arms controlled by the remote expert who can use them as interactive guidance tools. Many challenges come with this: collocation, inverse kinematics (IK), the perception of the remote collaborator and gestures coordination. Vishnu aims to enhance the maintenance procedure thanks to a remote expert who can show to the local agent the exact gestures and actions to perform (see Figure <ref xlink:href="#uid64" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>). Our pilot user study shows that it may decrease the cognitive load compared to a usual approach based on the mapping of 2D and de-localized informations, and it could be used by agents in order to perform specific procedures without needing to have an available local expert <ref xlink:href="#hybrid-2016-bid22" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
        <p>This work was done in collaboration with b&lt;&gt;com and Telecom Bretagne.</p>
        <object id="uid64">
          <table>
            <tr>
              <td>
                <ressource xlink:href="IMG/vishnu.png" type="float" width="427.0pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
          </table>
          <caption>Illustration of the Vishnu approach: system and viewpoints of the agent (left) and the expert (right) in a motherboard assembly scenario.</caption>
        </object>
        <p>
          <b>When the Giant meets the Ant: An Asymmetric Approach for Collaborative and Concurrent Object Manipulation in a Multi-Scale Environment</b>
        </p>
        <p><b>Participants:</b> Morgan Le Chénéchal, Jérémy Lacoche, Valérie Gouranton and Bruno Arnaldi</p>
        <p>We proposed a novel approach to enable two or more users to manipulate an object collaboratively. Our goal is to benefits from the wide variety of todays VR devices. Our solution is based on an asymmetric collaboration pattern at different scales in which users benefit from suited points of views and interaction techniques according to their device setups. Each user application is adapted thanks to plasticity mechanisms. Our system provides an efficient way to co-manipulate an object within irregular and narrow courses, taking advantages of asymmetric roles in synchronous collaboration (see Figure <ref xlink:href="#uid65" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>). Moreover, it aims to provide a way to maximize the filling of the courses while the object moves on its path <ref xlink:href="#hybrid-2016-bid23" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>,<ref xlink:href="#hybrid-2016-bid2" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
        <p>This work was done in collaboration with b&lt;&gt;com and Telecom Bretagne.</p>
        <object id="uid65">
          <table>
            <tr>
              <td>
                <ressource xlink:href="IMG/giantant.png" type="float" width="427.0pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
          </table>
          <caption>When the Giant meets the Ant: Collaborative manipulation of a virtual object (here, a cube) based on an asymmetric setting between two users who can be helped by two additional users. (a) The first participant has a global view of the scene and moves the object with a 3D bent ray. (b) The second user is placed inside the object and precisely rotates and scales it. (c) Two additional roles can be added. The first one helps to scale the object using a third person view of it. The other one is a spectator who switches between the other participants’ viewpoints and helps them with oral communication.</caption>
        </object>
      </subsection>
    </subsection>
    <subsection id="uid66" level="1">
      <bodyTitle>Brain-Computer Interfaces</bodyTitle>
      <subsection id="cid9" level="2">
        <bodyTitle>Contribution to a Reference Book on BCI</bodyTitle>
        <p>We have largely contributed to a reference book on BCI released in 2016 in French and English, and co-edited by Fabien Lotte, Maureen Clerc and Laurent Bougrain for ISTE (French version <ref xlink:href="#hybrid-2016-bid24" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> <ref xlink:href="#hybrid-2016-bid25" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>) and Wiley (English version <ref xlink:href="#hybrid-2016-bid26" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> <ref xlink:href="#hybrid-2016-bid27" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>) publishers. This book provides keys for understanding and designing these multi-disciplinary interfaces, which require many fields of expertise such as neuroscience, statistics, informatics and psychology. This work corresponds to four different book chapters, all published in both French and English, which are presented hereafter.</p>
        <p>
          <b>Book chapter on BCI and videogames</b>
        </p>
        <p><b>Participants:</b> Anatole Lécuyer</p>
        <p>Videos games are often cited as a very promising field of applications for brain-computer interfaces. In a first chapter  <ref xlink:href="#hybrid-2016-bid28" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> <ref xlink:href="#hybrid-2016-bid29" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, we described state of the art in the field of video games played "with the mind". In particular, we considered the results of the OpenViBE2 project: one of the most important research projects in this area. We presented a selection of prototypes developed during this OpenViBE2 project which is illustrative of the state of the art in this field and of the use of BCIs in video games, such as based on imagining a motion of the left and right hands to score goals, or in another example using the P300 cerebral potential to destroy spaceships in a remake of well-known Japanese game.</p>
        <p>
          <b>Book chapter on BCI softwares</b>
        </p>
        <p><b>Participants:</b> Jussi Lindgren and Anatole Lécuyer</p>
        <p>In a second chapter <ref xlink:href="#hybrid-2016-bid30" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> <ref xlink:href="#hybrid-2016-bid31" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, we described OpenViBE and other software platforms used to study the subject. The chapter gave an overview of such platforms. We described how the software components of the platforms reflect typical signal acquisition and signal processing stages used in BCI. Finally, we presented a high-level account of differences between major BCI platforms and gave a few pieces of advice and recommendation regarding BCI platform selection.</p>
        <p>
          <b>Book chapter on BCI and HCI</b>
        </p>
        <p><b>Participants:</b> Andéol Evain, Ferran Argelaguet and Anatole Lécuyer</p>
        <p>In a third chapter <ref xlink:href="#hybrid-2016-bid32" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, we focused on the link between BCI and Human-Computer Interaction (HCI), and studied how HCI concepts can apply to BCIs. First, we presented an overview of the main concepts of HCI. We then studied the main characteristics of BCIs related to these concepts. This chapter also discussed the choice of cerebral patterns to use, depending on the interaction task and the use context. Finally, we presented the most promising new interaction paradigms for interaction with BCIs.</p>
        <p>This work was done in collaboration with MJOLNIR team.</p>
        <p>
          <b>Book chapter on Neurofeedback</b>
        </p>
        <p><b>Participants:</b> Lorraine Perronnet and Anatole Lécuyer</p>
        <p>We proposed a fourth chapter called Brain training with Neurofeedback <ref xlink:href="#hybrid-2016-bid33" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> <ref xlink:href="#hybrid-2016-bid34" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. We first defined the concept of neurofeedback (NF) and gave an overall view of the current status in this domain. Then we described the design of a NF training program and the typical course of a NF session, as well as the learning mechanisms underlying NF. We retraced the history of NF, explaining the origin of its questionable reputation and providing a foothold for understanding the diversity of existing approaches. We also discussed how the fields of NF and BCIs might potentially overlap in future with the development of "restorative" BCIs. Finally, we presented a few applications of NF and summarized the state of research of some of its major clinical applications.</p>
        <p>This work was done in collaboration with VISAGES team.</p>
      </subsection>
      <subsection id="cid10" level="2">
        <bodyTitle>BCI Methods and Techniques</bodyTitle>
        <p>
          <b>Do the Stimuli of a BCI Have to be the Same as the Ones Used for Training it?</b>
        </p>
        <p><b>Participants:</b> Andéol Evain, Ferran Argelaguet and Anatole Lécuyer</p>
        <p>Does the stimulation used during the training on an SSVEP-based BCI have to be similar to that of the end use? We conducted an experiment in which we recorded six-channel EEG data from 12 subjects in various conditions of distance between targets , and of difference in color between targets <ref xlink:href="#hybrid-2016-bid35" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. Our analysis revealed that the stimulation configuration used for training which leads to the best classification accuracy is not always the one which is closest to the end use configuration. We found that the distance between targets during training is of little influence if the end use targets are close to each other, but that training at far distance can lead to a better accuracy for far distance end use. Additionally, an interaction effect is observed between training and testing color: while training with monochrome targets leads to good performance only when the test context involves monochrome targets as well, a classifier trained on colored targets can be efficient for both colored and monochrome targets. In a nutshell, in the context of SSVEP-based BCI, training using distant targets of different colors seems to lead to the best and more robust performance in all end use contexts.</p>
        <p>This work was done in collaboration with MJOLNIR team.</p>
        <p>
          <b>A Novel Fusion Approach Combining Brain and Gaze Inputs for Target Selection</b>
        </p>
        <p><b>Participants:</b> Andéol Evain, Ferran Argelaguet and Anatole Lécuyer</p>
        <p>Gaze-based interfaces and Brain-Computer Interfaces (BCIs) allow for hands-free human-computer interaction. We investigated the combination of gaze and BCIs. We proposed a novel selection technique for 2D target acquisition based on input fusion <ref xlink:href="#hybrid-2016-bid36" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. This new approach combines the probabilistic models for each input, in order to better estimate the intent of the user. We evaluated its performance against the existing gaze and brain-computer interaction techniques. Twelve participants took part in our study, in which they had to search and select 2D targets with each of the evaluated techniques (see Figure <ref xlink:href="#uid67" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>). Our fusion-based hybrid interaction technique was found to be more reliable than the previous gaze and BCI hybrid interaction techniques for 10 participants over 12, while being 29% faster on average. However, similarly to what has been observed in hybrid gaze-and-speech interaction, gaze-only interaction technique still provides the best performance. Our results should encourage the use of input fusion, as opposed to sequential interaction, in order to design better hybrid interfaces.</p>
        <p>This work was done in collaboration with MJOLNIR team.</p>
        <object id="uid67">
          <table>
            <tr>
              <td>
                <ressource xlink:href="IMG/targets.png" type="float" width="298.8987pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
          </table>
          <caption>Experimental task combining gaze and brain inputs. The user has to look for the goal word displayed at the top of the screen, then, the user has to select the target with the exact same word. The detected gaze position is displayed under the form of a circle and a central point (visual feedback). For all trials the size of the targets remained constant, and only the length of the target word and the separation (d) between targets varied. The targets at the outer circle were distractors in which the target word was never placed.</caption>
        </object>
      </subsection>
      <subsection id="cid11" level="2">
        <bodyTitle>BCI User Experience and Neurofeedback</bodyTitle>
        <p>
          <b>Influence of Error Rate on Frustration of BCI Users</b>
        </p>
        <p><b>Participants:</b> Andéol Evain, Ferran Argelaguet and Anatole Lécuyer</p>
        <p>Brain-Computer Interfaces (BCIs) are still much less reliable than other input devices. The error rates of BCIs range from 5% up to 60%. We assessed the subjective frustration, motivation, and fatigue of BCI users, when confronted to different levels of error rate <ref xlink:href="#hybrid-2016-bid37" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. We conducted a BCI experiment in which the error rate was artificially controlled (see Figure <ref xlink:href="#uid68" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>). Our results first show that a prolonged use of BCI significantly increases the perceived fatigue, and induces a drop in motivation. We also found that user frustration increases with the error rate of the system but this increase does not seem critical for small differences of error rate. Thus, for future BCIs, we advise to favor user comfort over accuracy when the potential gain of accuracy remains small.</p>
        <p>This work was done in collaboration with MJOLNIR team.</p>
        <object id="uid68">
          <table>
            <tr>
              <td>
                <ressource xlink:href="IMG/photoV2.jpg" type="float" width="298.8987pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
          </table>
          <caption>Experimental setup: a participant is using an SSVEP-based BCI which error rate is artificially controlled.</caption>
        </object>
        <p>
          <b>Design of an Experimental Platform for Hybrid EEG-fMRI Neurofeedback Studies</b>
        </p>
        <p><b>Participants:</b> Marsel Mano, Lorraine Perronnet and Anatole Lécuyer</p>
        <p>During a neurofeedback (NF) experiment one or more brain activity measuring technologies are used to estimate the changes of the acquired neural signals that reflect the changes of the subject's brain activity in real-time. There exist a variety of NF research applications that use only one type of neural signals (i.e. uni-modal) like EEG or fMRI, but there are very few NF researches that use two or more neural signals (i.e. multi-modal). We have developed a hybrid EEG-fMRI platform for bi-modal NF experiments, as part of the project Hemisfer. Our system is based on the integration and the synchronization of an MR-compatible EEG and fMRI acquisition subsystems. The EEG signals are acquired with a 64 channel MR-compatible solution from Brain Products and the MR imaging is performed on a 3T Verio Siemens scanner (VB17) with a 12-ch head coil. We have developed two real-time pipelines for EEG and fMRI that handle all the necessary signal processing, the Joint NF module that calculates and fuses the NF and a visualize module that displays the NF to the subject. The control and the synchronization of both subsystems with each-other and with the experimental protocol is handled by the NF Control. Our platform showed very good real-time performance with various pre-processing, filtering, and NF estimation and visualization methods. The entire fMRI process from acquisition to NF takes always less than 200ms, well below the TR of regular EPI sequences (2s). The same process for EEG, with NF update cycles varying 2-5Hz, is done in virtually real time ( 50Hz).</p>
        <object id="uid69">
          <table>
            <tr>
              <td>
                <ressource xlink:href="IMG/NF_Diagram2.png" type="float" width="298.8987pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
          </table>
          <caption>Architecture of our hybrid EEG-fMRI neurofeedback platform.</caption>
        </object>
        <p>This work was done in collaboration with VISAGES team and presented as poster at OHBM 2016.</p>
        <p>
          <b>Unimodal versus Bimodal EEG-fMRI Neurofeedback</b>
        </p>
        <p><b>Participants:</b> Lorraine Perronnet, Anatole Lécuyer and Marsel Mano</p>
        <object id="uid70">
          <table>
            <tr>
              <td>
                <ressource xlink:href="IMG/august_protocol2.png" type="float" width="298.8987pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
          </table>
          <caption>Experimental procedure for comparing unimodal versus bimodal EEG-fMRI neurofeedback.</caption>
        </object>
        <p>In the context of the HEMISFER project, we proposed a simultaneous EEG-fMRI experimental protocol in which 10 healthy participants performed a motor-imagery task in unimodal and bimodal neurofeedback conditions. With this protocol we were able to compare for the first time the effects of unimodal EEG-neurofeedback and fMRI-neurofeedback versus bimodal EEG-fMRI-neurofeedback by looking both at EEG and fMRI activations. We also introduced a new feedback metaphor for bimodal EEG-fMRI-neurofeedback that integrates both EEG and fMRI signal in a single bi-dimensional feedback (a ball moving in 2D). Such a feedback is intended to relieve the cognitive load of the subject by presenting the bimodal neurofeedback task as a single regulation task instead of two. Additionally, this integrated feedback metaphor gives flexibility on defining a bimodal neurofeedback target. Participants were able to regulate activity in their motor regions in all neurofeedback conditions. Moreover, motor activations as revealed by offline fMRI analysis were stronger during EEG-fMRI-neurofeedback than during EEG-neurofeedback. This result suggests that EEG-fMRI-neurofeedback could be more specific or more engaging than EEG-neurofeedback. Our results also suggest that during EEG-fMRI-neurofeedback, participants tended to regulate more the modality that was harder to control. Taken together our results shed light on the specific mechanisms of bimodal EEG-fMRI-neurofeedback and on its added-value as compared to unimodal EEG-neurofeedback and fMRI-neurofeedback.</p>
        <p>This work was done in collaboration with VISAGES team and presented as poster at OHBM 2016. Experiments were conducted at NEURINFO platform from University of Rennes 1.</p>
      </subsection>
    </subsection>
  </resultats>
  <contrats id="uid71">
    <bodyTitle>Bilateral Contracts and Grants with Industry</bodyTitle>
    <subsection id="uid72" level="1">
      <bodyTitle>Bilateral Contracts with Industry</bodyTitle>
      <subsection id="cid12" level="2">
        <bodyTitle>Mensia Technologies</bodyTitle>
        <participants>
          <person key="hybrid-2014-idm28656">
            <firstname>Anatole</firstname>
            <lastname>Lécuyer</lastname>
          </person>
          <person key="hybrid-2014-idp75112">
            <firstname>Jussi Tapio</firstname>
            <lastname>Lindgren</lastname>
          </person>
        </participants>
        <p><ref xlink:href="http://www.mensiatech.com" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">Mensia Technologies</ref> is an Inria start-up company created in November 2012 as a spin-off of Hybrid team. Mensia is focused on wellness and healthcare applications emerging from the BCI and Neurofeedback technologies. The Mensia startup should benefit from the team’s expertise and of valuable and proprietary BCI research results. Mensia is based in Rennes and Paris. Anatole Lécuyer and Yann Renard (former Inria expert engineer who designed the OpenViBE software architecture and was involved in team projects for 5 years) are co-founders of Mensia Technologies together with CEO Jean-Yves Quentel.</p>
        <p>The on-going contract between Hybrid and Mensia started in November 2013 and supports the transfer of several softwares designed by Hybrid team ("OpenViBE", "StateFinder") related to our BCI activity to Mensia Technologies for multimedia or medical applications of Mensia.</p>
      </subsection>
    </subsection>
    <subsection id="uid73" level="1">
      <bodyTitle>Bilateral Grants with Industry</bodyTitle>
      <subsection id="cid13" level="2">
        <bodyTitle>Technicolor</bodyTitle>
        <participants>
          <person key="hybrid-2015-idp88624">
            <firstname>Antoine</firstname>
            <lastname>Costes</lastname>
          </person>
          <person key="hybrid-2014-idm28656">
            <firstname>Anatole</firstname>
            <lastname>Lécuyer</lastname>
          </person>
          <person key="PASUSERID">
            <firstname>Ferran</firstname>
            <lastname>Argelaguet</lastname>
          </person>
        </participants>
        <p>This grant started in December 2015. It supports Antoine Costes's CIFRE PhD program with Technicolor company on "Haptic Texturing".</p>
      </subsection>
      <subsection id="cid14" level="2">
        <bodyTitle>Realyz</bodyTitle>
        <participants>
          <person key="hybrid-2015-idp87392">
            <firstname>Guillaume</firstname>
            <lastname>Cortes</lastname>
          </person>
          <person key="hybrid-2014-idm28656">
            <firstname>Anatole</firstname>
            <lastname>Lécuyer</lastname>
          </person>
        </participants>
        <p>This grant started in December 2015. It supports Guillaume Cortes's CIFRE PhD program with Realyz company on "Improving tracking in VR".</p>
      </subsection>
      <subsection id="cid15" level="2">
        <bodyTitle>VINCI Construction</bodyTitle>
        <participants>
          <person key="hybrid-2015-idp89856">
            <firstname>Anne-Solène</firstname>
            <lastname>Dris-Kerdreux</lastname>
          </person>
          <person key="hybrid-2014-idm25952">
            <firstname>Bruno</firstname>
            <lastname>Arnaldi</lastname>
          </person>
          <person key="hybrid-2014-idp66240">
            <firstname>Valérie</firstname>
            <lastname>Gouranton</lastname>
          </person>
        </participants>
        <p>This grant started in November 2015. It supports Anne-Solene Dris-Kerdreux's CIFRE PhD program with Vinci company on "Training in VR for construction applications".</p>
      </subsection>
    </subsection>
  </contrats>
  <partenariat id="uid74">
    <bodyTitle>Partnerships and Cooperations</bodyTitle>
    <subsection id="uid75" level="1">
      <bodyTitle>Regional Initiatives</bodyTitle>
      <subsection id="cid16" level="2">
        <bodyTitle>Labex Cominlabs SUNSET</bodyTitle>
        <participants>
          <person key="hybrid-2014-idm25952">
            <firstname>Bruno</firstname>
            <lastname>Arnaldi</lastname>
          </person>
          <person key="hybrid-2014-idp88784">
            <firstname>Guillaume</firstname>
            <lastname>Claude</lastname>
          </person>
          <person key="hybrid-2016-idp201744">
            <firstname>Gautier</firstname>
            <lastname>Picard</lastname>
          </person>
          <person key="hybrid-2014-idp66240">
            <firstname>Valérie</firstname>
            <lastname>Gouranton</lastname>
            <moreinfo>contact</moreinfo>
          </person>
        </participants>
        <p>SUNSET is a 4-year Labex Cominlabs project (2016-2020). SUNSET partners are MediCIS-LTSI (coordinator), Hybrid, Hycomes (IRISA/Inria), and CHU Rennes. SUNSET aims at developing an innovative training software suite based on immersive and collaborative virtual reality technology for training and evaluating non-technical skills. This approach will be implemented and evaluated in the context of training neurosurgical scrub nurses. We will notably integrate methods and systems developed in the S3PM project (see bellow). By relying on Human Factors approaches, the project also addresses training and evaluation of interpersonal skills. Whereas the developed technologies and approaches will be generic and adaptable to any surgical specialty, the project will evaluate the developed system within training sessions performed with scrub nurses. We ambition to propose novel approaches for surgical non-technical skill learning and assessment, and to install the developed training factory at the University Hospital of Rennes, and evaluate it with real-scale user studies.</p>
      </subsection>
      <subsection id="cid17" level="2">
        <bodyTitle>Labex Cominlabs S3PM</bodyTitle>
        <participants>
          <person key="hybrid-2014-idm25952">
            <firstname>Bruno</firstname>
            <lastname>Arnaldi</lastname>
          </person>
          <person key="hybrid-2014-idp88784">
            <firstname>Guillaume</firstname>
            <lastname>Claude</lastname>
          </person>
          <person key="hybrid-2014-idp66240">
            <firstname>Valérie</firstname>
            <lastname>Gouranton</lastname>
            <moreinfo>contact</moreinfo>
          </person>
        </participants>
        <p><ref xlink:href="http://www.s3pm.cominlabs.ueb.eu/presentation" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">S3PM</ref> ("Synthesis and Simulation of Surgical Process Models") is a 4-year Labex Cominlabs project (2013-2017). S3PM partners are MediCIS-LTSI (coordinator), Hybrid, Hycomes (IRISA/Inria), and CHU Rennes. The objective of S3PM is to propose a solution for the computation of surgical procedural knowledge models from recordings of individual procedures, and their execution. The goal of the Hybrid team is to propose and use new models for collaborative and interactive virtual environments for procedural training. The Hybrid team also works on the creation of a surgical training application in virtual reality, exposing the different contributions.
Ar</p>
      </subsection>
      <subsection id="cid18" level="2">
        <bodyTitle>Labex Cominlabs HEMISFER</bodyTitle>
        <participants>
          <person key="hybrid-2014-idm28656">
            <firstname>Anatole</firstname>
            <lastname>Lécuyer</lastname>
            <moreinfo>contact</moreinfo>
          </person>
          <person key="visages-2014-idp123184">
            <firstname>Marsel</firstname>
            <lastname>Mano</lastname>
          </person>
          <person key="visages-2014-idp143576">
            <firstname>Lorraine</firstname>
            <lastname>Perronnet</lastname>
          </person>
        </participants>
        <p><ref xlink:href="http://www.hemisfer.cominlabs.ueb.eu/fr" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">HEMISFER</ref> is a 4-year project (2013-2017) funded by Labex CominLabs. It involves 4 Inria/IRISA teams (Hybrid, Visages (lead), Panama, Athena) and 2 medical centers: the Rennes Psychiatric Hospital (CHGR) and the Reeducation Department of Rennes Hospital (CHU Pontchaillou). The goal of HEMISFER is to make full use of neurofeedback paradigm in the context of rehabilitation and psychiatric disorders. The major breakthrough will come from the use of a coupling model associating functional and metabolic information from Magnetic Resonance Imaging (fMRI) to Electro-encephalography (EEG) to “enhance” the neurofeedback protocol. Clinical applications concern motor, neurological and psychiatric disorders (stroke, attention-deficit disorder, treatment-resistant mood disorders, etc).</p>
      </subsection>
      <subsection id="cid19" level="2">
        <bodyTitle>Labex Cominlabs SABRE</bodyTitle>
        <participants>
          <person key="hybrid-2014-idm28656">
            <firstname>Anatole</firstname>
            <lastname>Lécuyer</lastname>
            <moreinfo>contact</moreinfo>
          </person>
          <person key="hybrid-2014-idp75112">
            <firstname>Jussi Tapio</firstname>
            <lastname>Lindgren</lastname>
          </person>
          <person key="hybrid-2015-idp101104">
            <firstname>Nataliya</firstname>
            <lastname>Kos'Myna</lastname>
          </person>
        </participants>
        <p><ref xlink:href="http://www.sabre.cominlabs.ueb.eu" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">SABRE</ref> is a 3-year project (2014-2017) funded by Labex CominLabs. It involves 1 Inria/IRISA team (Hybrid) and 2 groups from TELECOM BREST engineering school. The goal of SABRE is to improve computational functionnalities and power of current real-time EEG processing pipelines. The project will investigate innovative EEG solution methods empowered and speeded-up by ad-hoc, transistor-level, implementations of their key algorithmic operations. A completely new family of fully-hardware-integrated, new computational EEG imaging methods will be developed that are expected to speed up the imaging process of an EEG device of several orders of magnitude in real case scenarios.</p>
      </subsection>
      <subsection id="cid20" level="2">
        <bodyTitle>IRT b&lt;&gt;com</bodyTitle>
        <participants>
          <person key="hybrid-2014-idm25952">
            <firstname>Bruno</firstname>
            <lastname>Arnaldi</lastname>
            <moreinfo>contact</moreinfo>
          </person>
          <person key="hybrid-2014-idp66240">
            <firstname>Valérie</firstname>
            <lastname>Gouranton</lastname>
          </person>
          <person key="hybrid-2014-idp67480">
            <firstname>Maud</firstname>
            <lastname>Marchal</lastname>
          </person>
        </participants>
        <p><ref xlink:href="https://b-com.com" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">b&lt;&gt;com</ref> is a French Institute of Research and Technology (IRT). The main goal of this IRT is to fasten the development and marketing of tools, products and services in the field of digital technologies. Our team has collaborated with b&lt;&gt;com within two 3-year projects: ImData (on "Immersive Interaction") and GestChir (on "Augmented Healthcare") which both ended in 2016. A new 3-year project "NeedleWare" (on "Augmented Healthcare") has been started on October 2016.</p>
      </subsection>
      <subsection id="cid21" level="2">
        <bodyTitle>CNPAO Project</bodyTitle>
        <participants>
          <person key="hybrid-2014-idp66240">
            <firstname>Valérie</firstname>
            <lastname>Gouranton</lastname>
            <moreinfo>contact</moreinfo>
          </person>
          <person key="hybrid-2014-idp87552">
            <firstname>Jean-Baptiste</firstname>
            <lastname>Barreau</lastname>
          </person>
          <person key="hybrid-2016-idp152560">
            <firstname>Ronan</firstname>
            <lastname>Gaugne</lastname>
          </person>
        </participants>
        <p><ref xlink:href="https://cnpao.univ-rennes1.fr/" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">CNPAO</ref> ("Conservatoire Numérique du Patrimoine Archéologique de l'Ouest") is an on-going research project partially funded by the Université Européenne de Bretagne (UEB) and Université de Rennes 1. It involves IRISA/Hybrid and CReAAH. The main objectives are: (i) a sustainable and centralized archiving of 2D/3D data produced by the archaeological community, (ii) a free access to metadata, (iii) a secure access to data for the different actors involved in scientific projects, and (iv) the support and advice for these actors in the 3D data production and exploration through the latest digital technologies, modeling tools and virtual reality systems.</p>
        <p noindent="true">This work was done in collaboration with Quentin Petit (SED Inria Rennes).</p>
      </subsection>
      <subsection id="cid22" level="2">
        <bodyTitle>Imag'In CNRS IRMA</bodyTitle>
        <participants>
          <person key="hybrid-2014-idm25952">
            <firstname>Bruno</firstname>
            <lastname>Arnaldi</lastname>
          </person>
          <person key="hybrid-2014-idp87552">
            <firstname>Jean-Baptiste</firstname>
            <lastname>Barreau</lastname>
          </person>
          <person key="hybrid-2016-idp152560">
            <firstname>Ronan</firstname>
            <lastname>Gaugne</lastname>
          </person>
          <person key="hybrid-2014-idp66240">
            <firstname>Valérie</firstname>
            <lastname>Gouranton</lastname>
            <moreinfo>contact</moreinfo>
          </person>
        </participants>
        <p>The IRMA project is an Imag'In project funded by CNRS which aims at developping innovative methodologies for research in the field of cultural heritage based on the combination of medical imaging technologies and interactive 3D technologies (virtual reality, augmented reality, haptics, additive manufacturing). It relies on close collaborations with the National Institute of Preventive Archaeological Research (Inrap), the Research Center Archaeology, and History Archéosciences (CReAAH UMR 6566) and the company Image ET. The developed tools are intended for cultural heritage professionals such as museums, curators, restorers, and archaeologists. We focus on a large number of archeological artefacts of different nature, and various time periods (Paleolithic, Mesolithic, and Iron Age Medieval) from all over France. We can notably mention the oldest human bones found in Brittany (clavicle Beg Er Vil), a funeral urn from Trebeurden (22), or a Bronze Cauldron from a burial of the Merovingian necropolis "Crassés Saint-Dizier" (51). This project involves a strong collaboration with Théophane Nicolas (Inrap/UMR Trajectoires), Quentin Petit (SED Inria Rennes), and Grégor Marchand (CNRS/UMR CReAAH).</p>
      </subsection>
    </subsection>
    <subsection id="uid76" level="1">
      <bodyTitle>National Initiatives</bodyTitle>
      <subsection id="cid23" level="2">
        <bodyTitle>ANR MANDARIN</bodyTitle>
        <participants>
          <person key="hybrid-2014-idp80088">
            <firstname>Adrien</firstname>
            <lastname>Girard</lastname>
          </person>
          <person key="hybrid-2014-idm28656">
            <firstname>Anatole</firstname>
            <lastname>Lécuyer</lastname>
          </person>
          <person key="hybrid-2014-idp67480">
            <firstname>Maud</firstname>
            <lastname>Marchal</lastname>
            <moreinfo>contact</moreinfo>
          </person>
        </participants>
        <p><ref xlink:href="http://www.anr-mandarin.fr/" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">MANDARIN</ref> ("MANipulation Dextre hAptique pour opéRations INdustrielles en RV") was a 4-year ANR project (2012-2016). MANDARIN partners were CEA-List (coordinator), Inria/Hybrid, UTC, Haption and Renault. It aimed at designing new hardware and software solutions to achieve natural and intuitive mono and bi-manual dextrous interactions, suitable for virtual environments. The objective of Hybrid in MANDARIN was to design novel multimodal 3D interaction techniques and metaphors allowing to deal with haptic gloves limitations (portability, under-actuation) and to assist the user in virtual reality applications requiring dexterous manipulation. The results were evaluated with a representative industrial application: the bi-manual manipulation of complex rigid objects and cables bundles.</p>
      </subsection>
      <subsection id="cid24" level="2">
        <bodyTitle>ANR HOMO-TEXTILUS</bodyTitle>
        <participants>
          <person key="hybrid-2014-idm28656">
            <firstname>Anatole</firstname>
            <lastname>Lécuyer</lastname>
            <moreinfo>contact</moreinfo>
          </person>
          <person key="hybrid-2014-idp67480">
            <firstname>Maud</firstname>
            <lastname>Marchal</lastname>
          </person>
        </participants>
        <p><ref xlink:href="http://tomorrowland.fr/hussein-chalayan" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">HOMO-TEXTILUS</ref> was a 4-year ANR project (2012-2016). Partners of the project were : Inria/Hybrid, CHART, LIP6, TOMORROW LAND, RCP and potential end-user is Hussein Chalayan fashion designer. The objective of HOMO TEXTILUS was to study what could be the next generation of smart and augmented clothes, and their influence and potential impact on behavior and habits of their users. The project was strongly oriented towards human science, with both user studies and sociological studies. The involvement of Hybrid team in the project consisted in studying the design of next-gen prototypes of clothes embedding novel kinds of sensors and actuators. These prototypes were used and tested in various experiments.</p>
      </subsection>
      <subsection id="cid25" level="2">
        <bodyTitle>FUI Previz</bodyTitle>
        <participants>
          <person key="hybrid-2014-idm25952">
            <firstname>Bruno</firstname>
            <lastname>Arnaldi</lastname>
            <moreinfo>contact</moreinfo>
          </person>
          <person key="hybrid-2014-idp66240">
            <firstname>Valérie</firstname>
            <lastname>Gouranton</lastname>
            <moreinfo>contact</moreinfo>
          </person>
        </participants>
        <p><ref xlink:href="http://previz.eu/" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">Previz</ref> was a 3-year project (2013-2016) funded by the competitive cluster "Images et Réseaux". Previz involved 4 Academic partners (Hybrid/INSA Rennes, ENS Louis-Lumière, LIRIS, Gipsa-Lab) and 9 Industrial partners (Technicolor, Ubisoft, SolidAnim, loumasystem, Polymorph). Previz aimed at proposing new previsualization tools for movie directors. The goal of Hybrid in Previz was to introduce new interactions between real and virtual actors so that the actor's actions, no matter his/her real or virtual nature, impact both the real and the virtual environment. The project ended up with a new production pipeline in order to automatically adapt and synchronize the visual effects (VFX), in space and time, to the real performance of an actor.</p>
      </subsection>
      <subsection id="cid26" level="2">
        <bodyTitle>Ilab CertiViBE</bodyTitle>
        <participants>
          <person key="hybrid-2014-idm28656">
            <firstname>Anatole</firstname>
            <lastname>Lécuyer</lastname>
            <moreinfo>contact</moreinfo>
          </person>
          <person key="hybrid-2014-idp75112">
            <firstname>Jussi Tapio</firstname>
            <lastname>Lindgren</lastname>
          </person>
          <person key="hybrid-2015-idp76304">
            <firstname>Charles</firstname>
            <lastname>Garraud</lastname>
          </person>
          <person key="hybrid-2015-idp73800">
            <firstname>Jérôme</firstname>
            <lastname>Chabrol</lastname>
          </person>
        </participants>
        <p>CertiViBE is a 2-year "Inria Innovation Lab" (2015-2017) funded by Inria for supporting the development of OpenViBE software, and notably its evolution in order to enable and fasten the medical transfer and the medical certification of products based on OpenViBE. This joint lab involves two partners: Hybrid and Mensia Technologies startup company. The project aims at setting up a quality environment, and developping a novel version of the software wich should comply with medical certification rules.</p>
      </subsection>
      <subsection id="cid27" level="2">
        <bodyTitle>IPL BCI-LIFT</bodyTitle>
        <participants>
          <person key="hybrid-2014-idm28656">
            <firstname>Anatole</firstname>
            <lastname>Lécuyer</lastname>
            <moreinfo>contact</moreinfo>
          </person>
          <person key="hybrid-2014-idp75112">
            <firstname>Jussi Tapio</firstname>
            <lastname>Lindgren</lastname>
            <moreinfo>contact</moreinfo>
          </person>
          <person key="hybrid-2014-idp91464">
            <firstname>Andéol</firstname>
            <lastname>Evain</lastname>
          </person>
          <person key="visages-2014-idp143576">
            <firstname>Lorraine</firstname>
            <lastname>Perronnet</lastname>
          </person>
          <person key="hybrid-2015-idp101104">
            <firstname>Nataliya</firstname>
            <lastname>Kos'Myna</lastname>
          </person>
        </participants>
        <p><ref xlink:href="https://bci-lift.inria.fr/fr/" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">BCI-LIFT</ref> is a 4-year "Inria Project Lab" initiative (2015-2019) funded by Inria for supporting a national research effort on Brain-Computer Interfaces. This joint lab involves sevearl Inria teams: Hybrid, Potioc, Athena, Neurosys, Mjolnir, Demar; as well as external partners: INSERM-Lyon, and INSA Rouen. This project aims at improving several aspects of Brain-Computer Interfaces : learning and adaptation of BCI systems, user interfaces and feedback, training protocols, etc.</p>
      </subsection>
    </subsection>
    <subsection id="uid77" level="1">
      <bodyTitle>European Initiatives</bodyTitle>
      <subsection id="uid78" level="2">
        <bodyTitle>FP7 &amp; H2020 Projects</bodyTitle>
        <subsection id="uid79" level="3">
          <bodyTitle>HAPPINESS</bodyTitle>
          <sanspuceslist>
            <li id="uid80">
              <p noindent="true">Title: HAptic Printed Patterned INtErfaces for Sensitive Surface</p>
            </li>
            <li id="uid81">
              <p noindent="true">Programm: H2020</p>
            </li>
            <li id="uid82">
              <p noindent="true">Duration: January 2015 - December 2017</p>
            </li>
            <li id="uid83">
              <p noindent="true">Coordinator: CEA</p>
            </li>
            <li id="uid84">
              <p noindent="true">Partners:</p>
              <sanspuceslist>
                <li id="uid85">
                  <p noindent="true">Arkema France (France)</p>
                </li>
                <li id="uid86">
                  <p noindent="true">Robert Bosch (Germany)</p>
                </li>
                <li id="uid87">
                  <p noindent="true">Commissariat A L'Energie Atomique et Aux Energies Alternatives (France)</p>
                </li>
                <li id="uid88">
                  <p noindent="true">Fundacion Gaiker (Spain)</p>
                </li>
                <li id="uid89">
                  <p noindent="true">Integrated Systems Development S.A. (Greece)</p>
                </li>
                <li id="uid90">
                  <p noindent="true">University of Glasgow (United Kingdom)</p>
                </li>
                <li id="uid91">
                  <p noindent="true">Walter Pak SL (Spain)</p>
                </li>
              </sanspuceslist>
            </li>
            <li id="uid92">
              <p noindent="true">Inria contact: Nicolas Roussel and Anatole Lécuyer</p>
            </li>
            <li id="uid93">
              <p noindent="true">The Automotive HMI (Human Machine Interface) will soon undergo dramatic changes, with large plastic dashboards moving from the ‘push-buttons’ era to the ‘tactile’ era. User demand for aesthetically pleasing and seamless interfaces is ever increasing, with touch sensitive interfaces now commonplace. However, these touch interfaces come at the cost of haptic feedback, which raises concerns regarding the safety of eyeless interaction during driving. The <ref xlink:href="http://www.happiness-project.eu/" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">HAPPINESS</ref> project intends to address these concerns through technological solutions, introducing new capabilities for haptic feedback on these interfaces. The main goal of the HAPPINESS project is to develop a smart conformable surface able to offer different tactile sensations via the development of a Haptic Thin and Organic Large Area Electronic technology (TOLAE), integrating sensing and feedback capabilities, focusing on user requirements and ergonomic designs. To this aim, by gathering all the value chain actors (materials, technology manufacturing, OEM integrator) for application within the automotive market, the HAPPINESS project will offer a new haptic Human-Machine Interface technology, integrating touch sensing and disruptive feedback capabilities directly into an automotive dashboard. Based on the consortium skills, the HAPPINESS project will demonstrate the integration of Electro-Active Polymers (EAP) in a matrix of mechanical actuators on plastic foils. The objectives are to fabricate these actuators with large area and cost effective printing technologies and to integrate them through plastic molding injection into a small-scale dashboard prototype. We will design, implement and evaluate new approaches to Human-Computer Interaction on a fully functional prototype that combines in packaging both sensors and actuator foils, driven by custom electronics, and accessible to end-users via software libraries, allowing for the reproduction of common and accepted sensations such as Roughness, Vibration and Relief. In this project, the role of Hybrid team is to design user studies on tactile perception, and study innovative usages of the technologies developed in HAPPINESS.</p>
            </li>
          </sanspuceslist>
        </subsection>
      </subsection>
    </subsection>
    <subsection id="uid94" level="1">
      <bodyTitle>International Research Visitors</bodyTitle>
      <subsection id="uid95" level="2">
        <bodyTitle>Visits of International Scientists</bodyTitle>
        <p>Michael Pereira (EPFL, Switzerland) visited Hybrid for a collaboration on Brain-Computer Interfaces and sports in January 2016.</p>
      </subsection>
      <subsection id="uid96" level="2">
        <bodyTitle>Visits to International Teams</bodyTitle>
        <p>Ferran Argelaguet visited the Virtual Reality Lab (Pr. Bernd Frohlich) at the Bauhaus University at Weimar (Germany) in October/November 2016.</p>
      </subsection>
    </subsection>
  </partenariat>
  <diffusion id="uid97">
    <bodyTitle>Dissemination</bodyTitle>
    <subsection id="uid98" level="1">
      <bodyTitle>Promoting Scientific Activities</bodyTitle>
      <subsection id="uid99" level="2">
        <bodyTitle>Scientific events organisation</bodyTitle>
        <subsection id="uid100" level="3">
          <bodyTitle>General chair, scientific chair</bodyTitle>
          <simplelist>
            <li id="uid101">
              <p noindent="true">Bruno Arnaldi was Scientific Chair of "Journées de l'AFRV" 2016, Brest, France.</p>
            </li>
            <li id="uid102">
              <p noindent="true">Anatole Lécuyer was Program Chair of IEEE Virtual Reality 2016.</p>
            </li>
            <li id="uid103">
              <p noindent="true">Maud Marchal was Program Chair of IEEE Symposium on 3D User Interfaces 2016.</p>
            </li>
          </simplelist>
        </subsection>
      </subsection>
      <subsection id="uid104" level="2">
        <bodyTitle>Scientific events selection</bodyTitle>
        <subsection id="uid105" level="3">
          <bodyTitle>Member of the conference program committees</bodyTitle>
          <simplelist>
            <li id="uid106">
              <p noindent="true">Anatole Lécuyer was Member of the conference program committee of Eurohaptics 2016.</p>
            </li>
            <li id="uid107">
              <p noindent="true">Ferran Argelaguet was Member of the conference program committee of IEEE Symposium on 3D User Interfaces 2016, ACM Virtual Reality Software and Technology 2016, and ACM Symposium on Spatial User Interfaces 2016.</p>
            </li>
            <li id="uid108">
              <p noindent="true">Maud Marchal was Member of the best paper committee of "Journées Françaises de l'Informatique Graphique" 2016.</p>
            </li>
            <li id="uid109">
              <p noindent="true">Valérie Gouranton was Member of the program committee of 3DCVE Workshop 2016.</p>
            </li>
          </simplelist>
        </subsection>
        <subsection id="uid110" level="3">
          <bodyTitle>Reviewer</bodyTitle>
          <simplelist>
            <li id="uid111">
              <p noindent="true">Ferran Argelaguet was Reviewer for ACM CHI 2016, Eurohaptics 2016.</p>
            </li>
            <li id="uid112">
              <p noindent="true">Maud Marchal was Reviewer for ACM Siggraph 2016, IEEE Virtual Reality 2016, IEEE Symposium on 3D User Interfaces 2016, Eurohaptics 2016.</p>
            </li>
            <li id="uid113">
              <p noindent="true">Valérie Gouranton was Reviewer for IEEE Virtual Reality 2016.</p>
            </li>
          </simplelist>
        </subsection>
      </subsection>
      <subsection id="uid114" level="2">
        <bodyTitle>Journal</bodyTitle>
        <subsection id="uid115" level="3">
          <bodyTitle>Member of the editorial boards</bodyTitle>
          <simplelist>
            <li id="uid116">
              <p noindent="true">Anatole Lécuyer is Associate Editor of Frontiers in Virtual Environments, and Presence journals.</p>
            </li>
            <li id="uid117">
              <p noindent="true">Ferran Argelaguet is Review Editor of Frontiers in Virtual Environments.</p>
            </li>
            <li id="uid118">
              <p noindent="true">Maud Marchal is Associate Editor of Computer Graphics Forum, Review Editor of Frontiers in Virtual Environments, and Member of the Editorial Board of Revue Francophone d'Informatique Graphique.</p>
            </li>
          </simplelist>
        </subsection>
        <subsection id="uid119" level="3">
          <bodyTitle>Reviewer - Reviewing activities</bodyTitle>
          <simplelist>
            <li id="uid120">
              <p noindent="true">Ferran Argelaguet was Reviewer for ACM Transactions on Graphics, IEEE Transactions on Visualization and Computer Graphics, and The Visual Computer.</p>
            </li>
            <li id="uid121">
              <p noindent="true">Maud Marchal was Reviewer for IEEE Transactions on Visualization and Computer Graphics, IEEE Transactions on Haptics, and The Visual Computer.</p>
            </li>
            <li id="uid122">
              <p noindent="true">Valérie Gouranton was Reviewer for Revue d'Intelligence Artificielle.</p>
            </li>
          </simplelist>
        </subsection>
      </subsection>
      <subsection id="uid123" level="2">
        <bodyTitle>Invited talks</bodyTitle>
        <simplelist>
          <li id="uid124">
            <p noindent="true">Ferran Argelaguet was invited at the Bauhaus University Weimar (November 16).</p>
          </li>
        </simplelist>
      </subsection>
      <subsection id="uid125" level="2">
        <bodyTitle>Leadership within the scientific community</bodyTitle>
        <simplelist>
          <li id="uid126">
            <p noindent="true">Bruno Arnaldi is Vice-President of AFRV (French Association for Virtual Reality).</p>
          </li>
          <li id="uid127">
            <p noindent="true">Valérie Gouranton is Member of Executive Committee of AFRV (French Association for Virtual Reality).</p>
          </li>
        </simplelist>
      </subsection>
      <subsection id="uid128" level="2">
        <bodyTitle>Scientific expertise</bodyTitle>
        <simplelist>
          <li id="uid129">
            <p noindent="true">Bruno Arnaldi was Expert for French ANR (Agence Nationale de la Recherche).</p>
          </li>
          <li id="uid130">
            <p noindent="true">Maud Marchal was Expert for the Natural Sciences and Engineering Research Council of Canada (NSERC).</p>
          </li>
        </simplelist>
      </subsection>
      <subsection id="uid131" level="2">
        <bodyTitle>Research administration</bodyTitle>
        <simplelist>
          <li id="uid132">
            <p noindent="true">Bruno Arnaldi is Deputy Director of IRISA.</p>
          </li>
          <li id="uid133">
            <p noindent="true">Maud Marchal is Co-Head of the MRI Master Research in Computer Science.</p>
          </li>
        </simplelist>
      </subsection>
    </subsection>
    <subsection id="uid134" level="1">
      <bodyTitle>Teaching - Supervision - Juries</bodyTitle>
      <p>Anatole Lécuyer:</p>
      <sanspuceslist>
        <li id="uid135">
          <p noindent="true">Master MNRV: "Haptic Interaction", 9h, M2, ENSAM, Laval, FR</p>
        </li>
        <li id="uid136">
          <p noindent="true">Ecole Centrale de Nantes : “Haptic Interaction and Brain-Computer Interfaces”, 4.5h, M1-M2, Ecole Centrale de Nantes, FR</p>
        </li>
        <li id="uid137">
          <p noindent="true">Master SIBM: "Haptic and Brain-Computer Interfaces", 4.5h, M2, University of Rennes 1, FR</p>
        </li>
      </sanspuceslist>
      <p>Bruno Arnaldi:</p>
      <sanspuceslist>
        <li id="uid138">
          <p noindent="true">Master INSA Rennes: "VAR: Virtual and Augmented Reality", 12h, M2, INSA Rennes, FR</p>
        </li>
        <li id="uid139">
          <p noindent="true">Master INSA Rennes: "Virtual Reality", courses 6h, projects 16h, M1 and M2, INSA Rennes, FR</p>
        </li>
        <li id="uid140">
          <p noindent="true">Master INSA Rennes: Projects on "Virtual Reality", 20h, M1, INSA Rennes, FR</p>
        </li>
      </sanspuceslist>
      <p>Ferran Argelaguet:</p>
      <sanspuceslist>
        <li id="uid141">
          <p noindent="true">Master STS Informatique MITIC: "Techniques d'Interaction Avancées", 26h, M2, ISTIC, University of Rennes 1, FR</p>
        </li>
        <li id="uid142">
          <p noindent="true">Master INSA Rennes: "Modeling and Engineering for Biology and Health Applications", 12h, M2, INSA Rennes, FR</p>
        </li>
      </sanspuceslist>
      <p>Maud Marchal:</p>
      <sanspuceslist>
        <li id="uid143">
          <p noindent="true">Master INSA Rennes: "Modeling and Engineering for Biology and Health Applications", 48h, M2 and responsible of this lecture, INSA Rennes, FR</p>
        </li>
        <li id="uid144">
          <p noindent="true">Master SIBM: "Biomedical simulation", 3h, M2, University of Rennes 1, FR</p>
        </li>
      </sanspuceslist>
      <p>Valérie Gouranton:</p>
      <sanspuceslist>
        <li id="uid145">
          <p noindent="true">Licence: "Introduction to Virtual Reality", 22h, L2 and responsible of this lecture, INSA Rennes, FR</p>
        </li>
        <li id="uid146">
          <p noindent="true">Licence: Project on "Virtual Reality", 16h, L3 and responsible of this lecture, INSA Rennes, FR</p>
        </li>
        <li id="uid147">
          <p noindent="true">Master INSA Rennes: "Virtual Reality", 16h, M2, INSA Rennes, FR</p>
        </li>
        <li id="uid148">
          <p noindent="true">Master INSA Rennes: Projects on "Virtual Reality", 20h, M1, INSA Rennes, FR</p>
        </li>
      </sanspuceslist>
      <p>Florian Nouviale:</p>
      <sanspuceslist>
        <li id="uid149">
          <p noindent="true">Ecole Centrale de Nantes: "Training on Unity3D", 6h, M1/M2, Ecole Centrale de Nantes, FR</p>
        </li>
      </sanspuceslist>
      <p>Ronan Gaugne:</p>
      <sanspuceslist>
        <li id="uid150">
          <p noindent="true">Insa Rennes: Projects on "Virtual Reality", 50h, L3/M1/M2, Insa Rennes, FR</p>
        </li>
      </sanspuceslist>
      <subsection id="uid151" level="2">
        <bodyTitle>Supervision</bodyTitle>
        <subsection id="uid152" level="3">
          <bodyTitle>PhD (defended)</bodyTitle>
          <simplelist>
            <li id="uid153">
              <p noindent="true">Guillaume Claude, "The sequencing of actions in collaborative virtual environments", INSA Rennes, July 2016, Supervised by Bruno Arnaldi, Valérie Gouranton</p>
            </li>
            <li id="uid154">
              <p noindent="true">François Lehericey, "Détection de collision par lancer de rayon : la quête de la performance", INSA Rennes, September 2016, Supervised by Bruno Arnaldi, Valérie Gouranton</p>
            </li>
            <li id="uid155">
              <p noindent="true">Jérémy Lacoche, "Plasticity for User Interfaces in Mixed Reality", Université Rennes 1, July 2016, Supervised by Thierry Duval, Bruno Arnaldi, Jérôme Royan, Eric Maisel</p>
            </li>
            <li id="uid156">
              <p noindent="true">Morgan Le Chénéchal, "Awareness Model for Asymmetric Remote Collaboration in Mixed Reality", INSA Rennes, July 2016, Supervised by Bruno Arnaldi, Thierry Duval, Valérie Gouranton, Jérôme Royan</p>
            </li>
            <li id="uid157">
              <p noindent="true">Lucas Royer, "Visualization tools for needle insertion in interventional radiology", INSA Rennes, December 6th, 2016, Supervised by Alexandre Krupa and Maud aud Marchal</p>
            </li>
            <li id="uid158">
              <p noindent="true">Andéol Evain, "Optimizing the Use of SSVEP-based Brain-Computer Interfaces for Human-Computer Interaction", University of Rennes 1, December 6th, 2016, Supervised by Anatole Lécuyer, Nicolas Roussel, Géry Casiez and Ferran Argelaguet</p>
            </li>
          </simplelist>
        </subsection>
        <subsection id="uid159" level="3">
          <bodyTitle>PhD (in progress)</bodyTitle>
          <simplelist>
            <li id="uid160">
              <p noindent="true">Jean-Baptiste Barreau, "Virtual Reality and Archaelogy", Started in February 2014, Supervised by Valérie Gouranton and Bruno Arnaldi</p>
            </li>
            <li id="uid161">
              <p noindent="true">Benoit Le Gouis, "Multi-scale physical simulation", Started in October 2014, Supervised by Bruno Arnaldi, Maud Marchal and Anatole Lécuyer</p>
            </li>
            <li id="uid162">
              <p noindent="true">Lorraine Perronet, "Neurofeedback applications based on EEG, fMRI and VR", Started in January 2014, Supervised by Christian Barillot and Anatole Lécuyer</p>
            </li>
            <li id="uid163">
              <p noindent="true">Gwendal Le Moulec, "Automatic generation of VR applications", Started in October 2015, Supervised by Valérie Gouranton, Bruno Arnaldi and Arnaud Blouin</p>
            </li>
            <li id="uid164">
              <p noindent="true">Anne-Solène Dris-Kerdreux, "Training in virtual reality for construction applications", Started in November 2015, Supervised by Valérie Gouranton and Bruno Arnaldi</p>
            </li>
            <li id="uid165">
              <p noindent="true">Antoine Costes, "Haptic texturing", Started in November 2015, Supervised by Anatole Lécuyer and Ferran Argelaguet</p>
            </li>
            <li id="uid166">
              <p noindent="true">Guillaume Cortes, "Improving tracking in VR", Started in November 2015, Supervised by Anatole Lécuyer</p>
            </li>
            <li id="uid167">
              <p noindent="true">Hakim Si-Mohammed, "BCI and HCI", Started in October 2016, Supervised by Anatole Lécuyer and Ferran Argelaguet</p>
            </li>
            <li id="uid168">
              <p noindent="true">Gautier Picard, "Collaborative VR", Started in October 2016, Supervised by Valérie Gouranton, Bernard Gibaud and Bruno Arnaldi</p>
            </li>
            <li id="uid169">
              <p noindent="true">Hadrien Gurnel, "Prise en compte de la déformation d’organe pour l’assistance robotisée d’insertion d’aiguille", Started in October 2016, Supervised by Alexander Krupa and Maud Marchal</p>
            </li>
          </simplelist>
        </subsection>
      </subsection>
      <subsection id="uid170" level="2">
        <bodyTitle>Juries</bodyTitle>
        <subsection id="uid171" level="3">
          <bodyTitle>Selection committees</bodyTitle>
          <simplelist>
            <li id="uid172">
              <p noindent="true">Bruno Arnaldi was Member of Selection committee of Assistant Professor Position at Ecole Centrale de Nantes.</p>
            </li>
          </simplelist>
        </subsection>
        <subsection id="uid173" level="3">
          <bodyTitle>PhD and HDR juries</bodyTitle>
          <simplelist>
            <li id="uid174">
              <p noindent="true">Anatole Lécuyer was Member of PhD committees of Henrique Debarba (EPFL, Switzerland), Axelle Pillain (Telecom Bretagne), Maxence Rangé (Univ. Rennes 1), Jérémie Plouzeau (ENSAM Chalon), Andéol Evain (Univ. Rennes 1), and Member of HDR committee of Reinhold Scherer (TU Graz, Austria).</p>
            </li>
            <li id="uid175">
              <p noindent="true">Bruno Arnaldi was President of PhD committees of Kévin Jordao (INSA de Rennes), Hui-Yin Wu (Univ. Rennes 1), Charlotte Hoareau (Enib Brest), Sabhi Ahmed (Paris 8), Guillaume Claude (INSA Rennes), Morgan Le Chénéchal (INSA Rennes) and François Lehericey (INSA Rennes), Jérémy Lacoche (Univ. Rennes 1)</p>
            </li>
            <li id="uid176">
              <p noindent="true">Valérie Gouranton was Member of PhD committees of Guillaume Claude (INSA Rennes), Morgan Le Chénéchal (INSA Rennes) and François Lehericey (INSA Rennes)</p>
            </li>
            <li id="uid177">
              <p noindent="true">Maud Marchal was Reviewer of PhD theses of Johan Sarrazin (Univ. Grenoble Alpes), Pierre-Luc Manteaux (Univ. Grenoble Alpes) and Camille Schreck (Univ. Grenoble Alpes), and Member of PhD committee of Lucas Royer (INSA Rennes).</p>
            </li>
            <li id="uid178">
              <p noindent="true">Ferran Arguelaguet was Member of PhD committee of Andéol Evain (Univ. Rennes 1).</p>
            </li>
          </simplelist>
        </subsection>
      </subsection>
    </subsection>
    <subsection id="uid179" level="1">
      <bodyTitle>Popularization</bodyTitle>
      <p>The team has organized, together with MimeTIC team, a press conference and a press release on the "6-Finger Illusion" in May 2016. This event has been followed by lots of articles (internet, press) and radio coverages, including: France Inter, RFI, Europe 1, Le Monde, Libération, Les Echos, etc.</p>
      <p>In addition, the results of the team have been disseminated in several other media coverages in 2016:</p>
      <simplelist>
        <li id="uid180">
          <p noindent="true">"Journal télévisé 20h" (National prime time news), France2 channel (02/16) : presentation of the BCI activity.</p>
        </li>
        <li id="uid181">
          <p noindent="true">"Journal télévisé du soir", TVRennes channel (04/16) : presentation of the Forum Eurocities visit in Immersia.</p>
        </li>
        <li id="uid182">
          <p noindent="true">"La tête au carré", France Inter radio (10/16) : participation of Anatole Lécuyer.</p>
        </li>
      </simplelist>
      <p>The team has also participated to numerous dissemination events in 2016 (chronological order):</p>
      <simplelist>
        <li id="uid183">
          <p noindent="true">"Open House ISTIC 2016" (Rennes, 01/16) : booth and demos of the team.</p>
        </li>
        <li id="uid184">
          <p noindent="true">"Forum EuroCities" (Rennes, 04/16) : presentation from Ronan Gaugne and demos in Immersia.</p>
        </li>
        <li id="uid185">
          <p noindent="true">"Les 10 ans du Quai Branly" (Paris, "Quai Branly" Museum, 06/16) : talk from Valérie Gouranton and Ronan Gaugne on virtual archaeology.</p>
        </li>
        <li id="uid186">
          <p noindent="true">"French-American Doctorat Exchange program 2016" (Rennes, 07/16): presentation from Ferran Argelaguet on Hybrid activities.</p>
        </li>
        <li id="uid187">
          <p noindent="true">"Journées du Patrimoine 2016" (Rennes, "Champs Libres" Museum, 09/16) : demos related to virtual archaelogy.</p>
        </li>
        <li id="uid188">
          <p noindent="true">"Journées Science et Musique 2016" (Rennes, 10/16) : co-organization of this event, and presentation of several demos.</p>
        </li>
        <li id="uid189">
          <p noindent="true">"Nuit Art et Science 2016" (Brest, 10/16) : demo on musical composition history in virtual reality and Immersia.</p>
        </li>
        <li id="uid190">
          <p noindent="true">"NEUROPLANETE 2016" (Nice, 10/16) : talk from Anatole Lécuyer and dissemination in "Le Point" journal.</p>
        </li>
        <li id="uid191">
          <p noindent="true">"Rencontres Inria Industrie: Interactions avec les objets et services numériques" (Lille, 11/16): demonstration from Ferran Argelaguet.</p>
        </li>
      </simplelist>
    </subsection>
  </diffusion>
  <biblio id="bibliography" html="bibliography" numero="10" titre="Bibliography">
    
    <biblStruct id="hybrid-2016-bid40" type="phdthesis" rend="year" n="cite:lechenechal:tel-01392708">
      <identifiant type="hal" value="tel-01392708"/>
      <monogr>
        <title level="m">Awareness Model for Asymmetric Remote Collaboration in Mixed Reality</title>
        <author>
          <persName key="hybrid-2014-idp93968">
            <foreName>Morgan</foreName>
            <surname>Le Chenechal</surname>
            <initial>M.</initial>
          </persName>
        </author>
        <imprint>
          <publisher>
            <orgName type="school">INSA de Rennes</orgName>
          </publisher>
          <dateStruct>
            <month>July</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://tel.archives-ouvertes.fr/tel-01392708" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>tel.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>tel-01392708</ref>
        </imprint>
      </monogr>
      <note type="typdoc">Theses</note>
    </biblStruct>
    
    <biblStruct id="hybrid-2016-bid14" type="article" rend="year" n="cite:achibet:hal-01414830">
      <identifiant type="doi" value="10.1162/PRES_a_00243"/>
      <identifiant type="hal" value="hal-01414830"/>
      <analytic>
        <title level="a">Leveraging Passive Haptic Feedback in Virtual Environments with the Elastic-Arm Approach</title>
        <author>
          <persName key="hybrid-2014-idp85112">
            <foreName>Merwan</foreName>
            <surname>Achibet</surname>
            <initial>M.</initial>
          </persName>
          <persName key="hybrid-2014-idp80088">
            <foreName>Adrien</foreName>
            <surname>Girard</surname>
            <initial>A.</initial>
          </persName>
          <persName key="hybrid-2014-idp67480">
            <foreName>Maud</foreName>
            <surname>Marchal</surname>
            <initial>M.</initial>
          </persName>
          <persName key="hybrid-2014-idm28656">
            <foreName>Anatole</foreName>
            <surname>Lécuyer</surname>
            <initial>A.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid01604">
        <idno type="issn">1054-7460</idno>
        <title level="j">Presence: Teleoperators and Virtual Environments</title>
        <imprint>
          <biblScope type="volume">25</biblScope>
          <biblScope type="number">1</biblScope>
          <dateStruct>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">17 - 32</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01414830" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01414830</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="hybrid-2016-bid5" type="article" rend="year" n="cite:bruder:hal-01388499">
      <identifiant type="doi" value="10.1162/PRES_a_00241"/>
      <identifiant type="hal" value="hal-01388499"/>
      <analytic>
        <title level="a">CAVE Size Matters: Effects of Screen Distance and Parallax on Distance Estimation in Large Immersive Display Setups</title>
        <author>
          <persName key="hybrid-2014-idp81336">
            <foreName>Gerd</foreName>
            <surname>Bruder</surname>
            <initial>G.</initial>
          </persName>
          <persName>
            <foreName>Ferran</foreName>
            <surname>Argelaguet</surname>
            <initial>F.</initial>
          </persName>
          <persName key="mimetic-2014-idp77968">
            <foreName>Anne-Héléne</foreName>
            <surname>Olivier</surname>
            <initial>A.-H.</initial>
          </persName>
          <persName key="hybrid-2014-idm28656">
            <foreName>Anatole</foreName>
            <surname>Lécuyer</surname>
            <initial>A.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid01604">
        <idno type="issn">1054-7460</idno>
        <title level="j">Presence: Teleoperators and Virtual Environments</title>
        <imprint>
          <biblScope type="volume">25</biblScope>
          <biblScope type="number">1</biblScope>
          <dateStruct>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">1 - 16</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01388499" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01388499</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="hybrid-2016-bid19" type="article" rend="year" n="cite:claude:hal-01300990">
      <identifiant type="doi" value="10.3233/978-1-61499-625-5-63"/>
      <identifiant type="hal" value="hal-01300990"/>
      <analytic>
        <title level="a">Synthesis and Simulation of Surgical Process Models</title>
        <author>
          <persName key="hybrid-2014-idp88784">
            <foreName>Guillaume</foreName>
            <surname>Claude</surname>
            <initial>G.</initial>
          </persName>
          <persName key="hybrid-2014-idp66240">
            <foreName>Valérie</foreName>
            <surname>Gouranton</surname>
            <initial>V.</initial>
          </persName>
          <persName key="hycomes-2014-idm10408">
            <foreName>Benoît</foreName>
            <surname>Caillaud</surname>
            <initial>B.</initial>
          </persName>
          <persName>
            <foreName>Bernard</foreName>
            <surname>Gibaud</surname>
            <initial>B.</initial>
          </persName>
          <persName key="hybrid-2014-idm25952">
            <foreName>Bruno</foreName>
            <surname>Arnaldi</surname>
            <initial>B.</initial>
          </persName>
          <persName>
            <foreName>Pierre</foreName>
            <surname>Jannin</surname>
            <initial>P.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid01816">
        <idno type="issn">0926-9630</idno>
        <title level="j">Studies in Health Technology and Informatics</title>
        <imprint>
          <biblScope type="volume">220</biblScope>
          <dateStruct>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">63–70</biblScope>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01300990" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01300990</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="hybrid-2016-bid16" type="article" rend="year" n="cite:girard:hal-01414862">
      <identifiant type="doi" value="10.3389/fict.2016.00006"/>
      <identifiant type="hal" value="hal-01414862"/>
      <analytic>
        <title level="a">HapTip: Displaying Haptic Shear Forces at the Fingertips for Multi-Finger Interaction in Virtual Environments</title>
        <author>
          <persName key="hybrid-2014-idp80088">
            <foreName>Adrien</foreName>
            <surname>Girard</surname>
            <initial>A.</initial>
          </persName>
          <persName key="hybrid-2014-idp67480">
            <foreName>Maud</foreName>
            <surname>Marchal</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>Florian</foreName>
            <surname>Gosselin</surname>
            <initial>F.</initial>
          </persName>
          <persName>
            <foreName>Anthony</foreName>
            <surname>Chabrier</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>François</foreName>
            <surname>Louveau</surname>
            <initial>F.</initial>
          </persName>
          <persName key="hybrid-2014-idm28656">
            <foreName>Anatole</foreName>
            <surname>Lécuyer</surname>
            <initial>A.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid02833">
        <idno type="issn">2296-9144</idno>
        <title level="j">Frontiers in Robotics and AI</title>
        <imprint>
          <dateStruct>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01414862" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01414862</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="hybrid-2016-bid4" type="article" rend="year" n="cite:hoyet:hal-01334359">
      <identifiant type="doi" value="10.3389/frobt.2016.00027"/>
      <identifiant type="hal" value="hal-01334359"/>
      <analytic>
        <title level="a">"Wow! I Have Six Fingers!": Would You Accept Structural Changes of Your Hand in VR?</title>
        <author>
          <persName key="mimetic-2015-idm26392">
            <foreName>Ludovic</foreName>
            <surname>Hoyet</surname>
            <initial>L.</initial>
          </persName>
          <persName>
            <foreName>Ferran</foreName>
            <surname>Argelaguet</surname>
            <initial>F.</initial>
          </persName>
          <persName>
            <foreName>Corentin</foreName>
            <surname>Nicole</surname>
            <initial>C.</initial>
          </persName>
          <persName key="hybrid-2014-idm28656">
            <foreName>Anatole</foreName>
            <surname>Lécuyer</surname>
            <initial>A.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid02833">
        <idno type="issn">2296-9144</idno>
        <title level="j">Frontiers in Robotics and AI</title>
        <imprint>
          <biblScope type="volume">3</biblScope>
          <biblScope type="number">27</biblScope>
          <dateStruct>
            <month>March</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01334359" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01334359</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="hybrid-2016-bid15" type="article" rend="year" n="cite:karakkatnarayanan:hal-01277585">
      <identifiant type="hal" value="hal-01277585"/>
      <analytic>
        <title level="a">Vision-based adaptive assistance and haptic guidance for safe wheelchair corridor following</title>
        <author>
          <persName key="lagadic-2014-idp102752">
            <foreName>Vishnu</foreName>
            <surname>Karakkat Narayanan</surname>
            <initial>V.</initial>
          </persName>
          <persName key="lagadic-2014-idp76296">
            <foreName>François</foreName>
            <surname>Pasteau</surname>
            <initial>F.</initial>
          </persName>
          <persName key="hybrid-2014-idp67480">
            <foreName>Maud</foreName>
            <surname>Marchal</surname>
            <initial>M.</initial>
          </persName>
          <persName key="lagadic-2014-idm26496">
            <foreName>Alexandre</foreName>
            <surname>Krupa</surname>
            <initial>A.</initial>
          </persName>
          <persName key="lagadic-2014-idp68368">
            <foreName>Marie</foreName>
            <surname>Babel</surname>
            <initial>M.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid00409">
        <idno type="issn">1077-3142</idno>
        <title level="j">Computer Vision and Image Understanding</title>
        <imprint>
          <biblScope type="volume">179</biblScope>
          <dateStruct>
            <month>August</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">171-185</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01277585" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01277585</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="hybrid-2016-bid30" type="incollection" rend="year" n="cite:lindgren:hal-01420642">
      <identifiant type="hal" value="hal-01420642"/>
      <analytic>
        <title level="a">Introduction to BCI Software Platforms</title>
        <author>
          <persName key="hybrid-2014-idp75112">
            <foreName>Jussi</foreName>
            <surname>Lindgren</surname>
            <initial>J.</initial>
          </persName>
          <persName key="hybrid-2014-idm28656">
            <foreName>Anatole</foreName>
            <surname>Lécuyer</surname>
            <initial>A.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no">
        <editor role="editor">
          <persName key="athena-2014-idm29272">
            <foreName>Maureen</foreName>
            <surname>Clerc</surname>
            <initial>M.</initial>
          </persName>
          <persName key="neurosys-2014-idm28696">
            <foreName>Laurent</foreName>
            <surname>Bougrain</surname>
            <initial>L.</initial>
          </persName>
          <persName key="potioc-2014-idp65160">
            <foreName>Fabien</foreName>
            <surname>Lotte</surname>
            <initial>F.</initial>
          </persName>
        </editor>
        <title level="m">Brain-Computer Interfaces 2: Technology and Applications</title>
        <imprint>
          <biblScope type="volume">2</biblScope>
          <publisher>
            <orgName>Wiley</orgName>
          </publisher>
          <dateStruct>
            <month>August</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01420642" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01420642</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="hybrid-2016-bid31" type="incollection" rend="year" n="cite:lindgren:hal-01420637">
      <identifiant type="hal" value="hal-01420637"/>
      <analytic>
        <title level="a">OpenViBE et les outils logiciels pour les BCI</title>
        <author>
          <persName key="hybrid-2014-idp75112">
            <foreName>Jussi</foreName>
            <surname>Lindgren</surname>
            <initial>J.</initial>
          </persName>
          <persName key="hybrid-2014-idm28656">
            <foreName>Anatole</foreName>
            <surname>Lécuyer</surname>
            <initial>A.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no">
        <editor role="editor">
          <persName key="athena-2014-idm29272">
            <foreName>Maureen</foreName>
            <surname>Clerc</surname>
            <initial>M.</initial>
          </persName>
          <persName key="neurosys-2014-idm28696">
            <foreName>Laurent</foreName>
            <surname>Bougrain</surname>
            <initial>L.</initial>
          </persName>
          <persName key="potioc-2014-idp65160">
            <foreName>Fabien</foreName>
            <surname>Lotte</surname>
            <initial>F.</initial>
          </persName>
        </editor>
        <title level="m">Les interfaces Cerveau-Ordinateur 2 : Technologies et Applications</title>
        <imprint>
          <biblScope type="volume">2</biblScope>
          <publisher>
            <orgName>ISTE</orgName>
          </publisher>
          <dateStruct>
            <month>July</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">336</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01420637" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01420637</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="hybrid-2016-bid28" type="incollection" rend="year" n="cite:lecuyer:hal-01420623">
      <identifiant type="hal" value="hal-01420623"/>
      <analytic>
        <title level="a">BCIs and Video Games: State of the Art with the OpenViBE2 Project</title>
        <author>
          <persName key="hybrid-2014-idm28656">
            <foreName>Anatole</foreName>
            <surname>Lécuyer</surname>
            <initial>A.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no">
        <editor role="editor">
          <persName key="athena-2014-idm29272">
            <foreName>Maureen</foreName>
            <surname>Clerc</surname>
            <initial>M.</initial>
          </persName>
          <persName key="neurosys-2014-idm28696">
            <foreName>Laurent</foreName>
            <surname>Bougrain</surname>
            <initial>L.</initial>
          </persName>
          <persName key="potioc-2014-idp65160">
            <foreName>Fabien</foreName>
            <surname>Lotte</surname>
            <initial>F.</initial>
          </persName>
        </editor>
        <title level="m">Brain-Computer Interfaces 2: Technology and Applications</title>
        <imprint>
          <biblScope type="volume">1</biblScope>
          <publisher>
            <orgName>Wiley</orgName>
          </publisher>
          <dateStruct>
            <month>August</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01420623" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01420623</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="hybrid-2016-bid29" type="incollection" rend="year" n="cite:lecuyer:hal-01420629">
      <identifiant type="hal" value="hal-01420629"/>
      <analytic>
        <title level="a">BCI et jeux vidéo : état de l’art à travers le projet OpenViBE2</title>
        <author>
          <persName key="hybrid-2014-idm28656">
            <foreName>Anatole</foreName>
            <surname>Lécuyer</surname>
            <initial>A.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no">
        <editor role="editor">
          <persName key="athena-2014-idm29272">
            <foreName>Maureen</foreName>
            <surname>Clerc</surname>
            <initial>M.</initial>
          </persName>
          <persName key="neurosys-2014-idm28696">
            <foreName>Laurent</foreName>
            <surname>Bougrain</surname>
            <initial>L.</initial>
          </persName>
          <persName key="potioc-2014-idp65160">
            <foreName>Fabien</foreName>
            <surname>Lotte</surname>
            <initial>F.</initial>
          </persName>
        </editor>
        <title level="m">Les interfaces cerveau-ordinateur 2: Technologie et applications</title>
        <imprint>
          <biblScope type="volume">2</biblScope>
          <publisher>
            <orgName>ISTE</orgName>
          </publisher>
          <dateStruct>
            <month>July</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">336</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01420629" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01420629</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="hybrid-2016-bid11" type="article" rend="year" n="cite:nicolas:hal-01417753">
      <identifiant type="hal" value="hal-01417753"/>
      <analytic>
        <title level="a">La tomographie, l'impression 3D et la réalité virtuelle au service de l'archéologie</title>
        <author>
          <persName>
            <foreName>Théophane</foreName>
            <surname>Nicolas</surname>
            <initial>T.</initial>
          </persName>
          <persName key="hybrid-2016-idp152560">
            <foreName>Ronan</foreName>
            <surname>Gaugne</surname>
            <initial>R.</initial>
          </persName>
          <persName>
            <foreName>Cédric</foreName>
            <surname>Tavernier</surname>
            <initial>C.</initial>
          </persName>
          <persName key="hybrid-2014-idp66240">
            <foreName>Valérie</foreName>
            <surname>Gouranton</surname>
            <initial>V.</initial>
          </persName>
          <persName key="hybrid-2014-idm25952">
            <foreName>Bruno</foreName>
            <surname>Arnaldi</surname>
            <initial>B.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="no" id="rid03126">
        <idno type="issn">2425-1941</idno>
        <title level="j">Les Nouvelles de l'archéologie</title>
        <imprint>
          <biblScope type="number">146</biblScope>
          <dateStruct>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">16-22</biblScope>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01417753" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01417753</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="hybrid-2016-bid34" type="incollection" rend="year" n="cite:perronnet:hal-01413424">
      <identifiant type="hal" value="hal-01413424"/>
      <analytic>
        <title level="a">Brain training with neurofeedback</title>
        <author>
          <persName key="visages-2014-idp143576">
            <foreName>Lorraine</foreName>
            <surname>Perronnet</surname>
            <initial>L.</initial>
          </persName>
          <persName key="hybrid-2014-idm28656">
            <foreName>Anatole</foreName>
            <surname>Lécuyer</surname>
            <initial>A.</initial>
          </persName>
          <persName key="potioc-2014-idp65160">
            <foreName>Fabien</foreName>
            <surname>Lotte</surname>
            <initial>F.</initial>
          </persName>
          <persName key="athena-2014-idm29272">
            <foreName>Maureen</foreName>
            <surname>Clerc</surname>
            <initial>M.</initial>
          </persName>
          <persName key="visages-2014-idp101432">
            <foreName>Christian</foreName>
            <surname>Barillot</surname>
            <initial>C.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no">
        <title level="m">Brain-Computer Interfaces 1</title>
        <imprint>
          <publisher>
            <orgName>Wiley-ISTE</orgName>
          </publisher>
          <dateStruct>
            <month>July</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01413424" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01413424</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="hybrid-2016-bid33" type="incollection" rend="year" n="cite:perronnet:hal-01413408">
      <identifiant type="hal" value="hal-01413408"/>
      <analytic>
        <title level="a">Entraîner son cerveau avec le neurofeedback</title>
        <author>
          <persName key="visages-2014-idp143576">
            <foreName>Lorraine</foreName>
            <surname>Perronnet</surname>
            <initial>L.</initial>
          </persName>
          <persName key="hybrid-2014-idm28656">
            <foreName>Anatole</foreName>
            <surname>Lécuyer</surname>
            <initial>A.</initial>
          </persName>
          <persName key="potioc-2014-idp65160">
            <foreName>Fabien</foreName>
            <surname>Lotte</surname>
            <initial>F.</initial>
          </persName>
          <persName key="athena-2014-idm29272">
            <foreName>Maureen</foreName>
            <surname>Clerc</surname>
            <initial>M.</initial>
          </persName>
          <persName key="visages-2014-idp101432">
            <foreName>Christian</foreName>
            <surname>Barillot</surname>
            <initial>C.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no">
        <editor role="editor">
          <persName key="athena-2014-idm29272">
            <foreName>Maureen</foreName>
            <surname>Clerc</surname>
            <initial>M.</initial>
          </persName>
          <persName key="neurosys-2014-idm28696">
            <foreName>Laurent</foreName>
            <surname>Bougrain</surname>
            <initial>L.</initial>
          </persName>
          <persName key="potioc-2014-idp65160">
            <foreName>Fabien</foreName>
            <surname>Lotte</surname>
            <initial>F.</initial>
          </persName>
        </editor>
        <title level="m">Les interfaces cerveau-ordinateur 1</title>
        <imprint>
          <dateStruct>
            <month>July</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01413408" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01413408</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="hybrid-2016-bid12" type="article" rend="year" n="cite:royer:hal-01374589">
      <identifiant type="doi" value="10.1016/j.media.2016.09.004"/>
      <identifiant type="hal" value="hal-01374589"/>
      <analytic>
        <title level="a">Real-time Target Tracking of Soft Tissues in 3D Ultrasound Images Based on Robust Visual Information and Mechanical Simulation</title>
        <author>
          <persName key="lagadic-2014-idp109024">
            <foreName>Lucas</foreName>
            <surname>Royer</surname>
            <initial>L.</initial>
          </persName>
          <persName key="lagadic-2014-idm26496">
            <foreName>Alexandre</foreName>
            <surname>Krupa</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Guillaume</foreName>
            <surname>DARDENNE</surname>
            <initial>G.</initial>
          </persName>
          <persName>
            <foreName>Anthony</foreName>
            <surname>Le Bras</surname>
            <initial>A.</initial>
          </persName>
          <persName key="lagadic-2014-idp71088">
            <foreName>Éric</foreName>
            <surname>Marchand</surname>
            <initial>É.</initial>
          </persName>
          <persName key="hybrid-2014-idp67480">
            <foreName>Maud</foreName>
            <surname>Marchal</surname>
            <initial>M.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid01414">
        <idno type="issn">1361-8415</idno>
        <title level="j">Medical Image Analysis</title>
        <imprint>
          <biblScope type="volume">35</biblScope>
          <dateStruct>
            <month>January</month>
            <year>2017</year>
          </dateStruct>
          <biblScope type="pages">582 - 598</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01374589" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01374589</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="hybrid-2016-bid36" type="article" rend="year" n="cite:evain:hal-01388528">
      <identifiant type="doi" value="10.3389/fnins.2016.00454"/>
      <identifiant type="hal" value="hal-01388528"/>
      <analytic>
        <title level="a">Design and Evaluation of Fusion Approach for Combining Brain and Gaze Inputs for Target Selection</title>
        <author>
          <persName key="hybrid-2014-idp91464">
            <foreName>Andéol</foreName>
            <surname>Évain</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Ferran</foreName>
            <surname>Argelaguet</surname>
            <initial>F.</initial>
          </persName>
          <persName key="mint-2014-idp90176">
            <foreName>Géry</foreName>
            <surname>Casiez</surname>
            <initial>G.</initial>
          </persName>
          <persName key="mint-2014-idp87488">
            <foreName>Nicolas</foreName>
            <surname>Roussel</surname>
            <initial>N.</initial>
          </persName>
          <persName key="hybrid-2014-idm28656">
            <foreName>Anatole</foreName>
            <surname>Lécuyer</surname>
            <initial>A.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid00613">
        <idno type="issn">1662-453X</idno>
        <title level="j">Frontiers in Neuroscience</title>
        <imprint>
          <biblScope type="volume">10</biblScope>
          <biblScope type="number">454</biblScope>
          <dateStruct>
            <month>October</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">14</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01388528" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01388528</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="hybrid-2016-bid35" type="article" rend="year" n="cite:evain:hal-01388534">
      <identifiant type="doi" value="10.1080/2326263X.2016.1193458"/>
      <identifiant type="hal" value="hal-01388534"/>
      <analytic>
        <title level="a">Do the stimuli of an SSVEP-based BCI really have to be the same as the stimuli used for training it?</title>
        <author>
          <persName key="hybrid-2014-idp91464">
            <foreName>Andéol</foreName>
            <surname>Évain</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Ferran</foreName>
            <surname>Argelaguet</surname>
            <initial>F.</initial>
          </persName>
          <persName key="mint-2014-idp90176">
            <foreName>Géry</foreName>
            <surname>Casiez</surname>
            <initial>G.</initial>
          </persName>
          <persName key="mint-2014-idp87488">
            <foreName>Nicolas</foreName>
            <surname>Roussel</surname>
            <initial>N.</initial>
          </persName>
          <persName key="hybrid-2014-idm28656">
            <foreName>Anatole</foreName>
            <surname>Lécuyer</surname>
            <initial>A.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid02647">
        <idno type="issn">2326-263X</idno>
        <title level="j">Brain-Computer Interfaces</title>
        <imprint>
          <biblScope type="volume">3</biblScope>
          <biblScope type="number">2</biblScope>
          <dateStruct>
            <month>July</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">103 - 111</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01388534" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01388534</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="hybrid-2016-bid32" type="incollection" rend="year" n="cite:evain:hal-01416382">
      <identifiant type="hal" value="hal-01416382"/>
      <analytic>
        <title level="a">Brain-Computer Interfaces for Human–Computer Interaction</title>
        <author>
          <persName key="hybrid-2014-idp91464">
            <foreName>Andéol</foreName>
            <surname>Évain</surname>
            <initial>A.</initial>
          </persName>
          <persName key="mint-2014-idp87488">
            <foreName>Nicolas</foreName>
            <surname>Roussel</surname>
            <initial>N.</initial>
          </persName>
          <persName key="mint-2014-idp90176">
            <foreName>Géry</foreName>
            <surname>Casiez</surname>
            <initial>G.</initial>
          </persName>
          <persName key="hybrid-2014-idm27216">
            <foreName>Ferran</foreName>
            <surname>Argelaguet Sanz</surname>
            <initial>F.</initial>
          </persName>
          <persName key="hybrid-2014-idm28656">
            <foreName>Anatole</foreName>
            <surname>Lécuyer</surname>
            <initial>A.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no">
        <editor role="editor">
          <persName key="athena-2014-idm29272">
            <foreName>Maureen</foreName>
            <surname>Clerc</surname>
            <initial>M.</initial>
          </persName>
          <persName key="neurosys-2014-idm28696">
            <foreName>Laurent</foreName>
            <surname>Bougrain</surname>
            <initial>L.</initial>
          </persName>
          <persName key="potioc-2014-idp65160">
            <foreName>Fabien</foreName>
            <surname>Lotte</surname>
            <initial>F.</initial>
          </persName>
        </editor>
        <title level="m">Brain-Computer Interfaces 1: Foundations and Methods</title>
        <imprint>
          <publisher>
            <orgName>Wiley-ISTE</orgName>
          </publisher>
          <dateStruct>
            <month>August</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01416382" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01416382</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="hybrid-2016-bid13" type="inproceedings" rend="year" n="cite:achibet:hal-01267645">
      <identifiant type="doi" value="10.1109/3DUI.2016.7460024"/>
      <identifiant type="hal" value="hal-01267645"/>
      <analytic>
        <title level="a">DesktopGlove: a Multi-finger Force Feedback Interface Separating Degrees of Freedom Between Hands</title>
        <author>
          <persName key="hybrid-2014-idp85112">
            <foreName>Merwan</foreName>
            <surname>Achibet</surname>
            <initial>M.</initial>
          </persName>
          <persName key="mint-2014-idp90176">
            <foreName>Géry</foreName>
            <surname>Casiez</surname>
            <initial>G.</initial>
          </persName>
          <persName key="hybrid-2014-idp67480">
            <foreName>Maud</foreName>
            <surname>Marchal</surname>
            <initial>M.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">3DUI'16, the 11th Symposium on 3D User Interfaces</title>
        <loc>Greenville, United States</loc>
        <title level="s">In proceedings of 3DUI'16, the 11th Symposium on 3D User Interfaces</title>
        <imprint>
          <publisher>
            <orgName>IEEE Computer Society</orgName>
          </publisher>
          <dateStruct>
            <month>March</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">3-12</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01267645" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01267645</ref>
        </imprint>
        <meeting id="cid93832">
          <title>IEEE Symposium on 3D User Interfaces</title>
          <num>11</num>
          <abbr type="sigle">3DUI</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="hybrid-2016-bid38" type="inproceedings" rend="year" n="cite:andrade:hal-01387573">
      <identifiant type="doi" value="10.1145/2929464.2929471"/>
      <identifiant type="hal" value="hal-01387573"/>
      <analytic>
        <title level="a">Enjoy 360° Vision with the FlyVIZ</title>
        <author>
          <persName>
            <foreName>Guillermo</foreName>
            <surname>Andrade</surname>
            <initial>G.</initial>
          </persName>
          <persName key="hybrid-2014-idp68928">
            <foreName>Florian</foreName>
            <surname>Nouviale</surname>
            <initial>F.</initial>
          </persName>
          <persName key="hybrid-2014-idp86328">
            <foreName>Jerome</foreName>
            <surname>Ardouin</surname>
            <initial>J.</initial>
          </persName>
          <persName key="lagadic-2014-idp71088">
            <foreName>Éric</foreName>
            <surname>Marchand</surname>
            <initial>É.</initial>
          </persName>
          <persName key="hybrid-2014-idp67480">
            <foreName>Maud</foreName>
            <surname>Marchal</surname>
            <initial>M.</initial>
          </persName>
          <persName key="hybrid-2014-idm28656">
            <foreName>Anatole</foreName>
            <surname>Lécuyer</surname>
            <initial>A.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">SIGGRAPH 2016 Emerging Technologies</title>
        <loc>Anaheim, United States</loc>
        <imprint>
          <dateStruct>
            <month>July</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01387573" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01387573</ref>
        </imprint>
        <meeting id="cid623993">
          <title>ACM SIGGRAPH Emerging Technologies</title>
          <num>2016</num>
          <abbr type="sigle"/>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="hybrid-2016-bid3" type="inproceedings" rend="year" n="cite:argelaguet:hal-01346229">
      <identifiant type="doi" value="10.1109/VR.2016.7504682"/>
      <identifiant type="hal" value="hal-01346229"/>
      <analytic>
        <title level="a">The role of interaction in virtual embodiment: Effects of the virtual hand representation</title>
        <author>
          <persName>
            <foreName>Ferran</foreName>
            <surname>Argelaguet</surname>
            <initial>F.</initial>
          </persName>
          <persName key="mimetic-2015-idm26392">
            <foreName>Ludovic</foreName>
            <surname>Hoyet</surname>
            <initial>L.</initial>
          </persName>
          <persName>
            <foreName>Michaël</foreName>
            <surname>Trico</surname>
            <initial>M.</initial>
          </persName>
          <persName key="hybrid-2014-idm28656">
            <foreName>Anatole</foreName>
            <surname>Lécuyer</surname>
            <initial>A.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">IEEE Virtual Reality</title>
        <loc>Greenville, United States</loc>
        <imprint>
          <dateStruct>
            <month>March</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">3-10</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01346229" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01346229</ref>
        </imprint>
        <meeting id="cid86390">
          <title>IEEE International Conference on Virtual Reality</title>
          <num>2007</num>
          <abbr type="sigle">IEEE VR</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="hybrid-2016-bid6" type="inproceedings" rend="year" n="cite:argelaguetsanz:hal-01393243">
      <identifiant type="doi" value="10.1145/2993369.2993391"/>
      <identifiant type="hal" value="hal-01393243"/>
      <analytic>
        <title level="a">GiAnt: stereoscopic-compliant multi-scale navigation in VEs</title>
        <author>
          <persName key="hybrid-2014-idm27216">
            <foreName>Ferran</foreName>
            <surname>Argelaguet Sanz</surname>
            <initial>F.</initial>
          </persName>
          <persName>
            <foreName>Maignant</foreName>
            <surname>Morgan</surname>
            <initial>M.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">ACM Conference on Virtual Reality Software and Technology</title>
        <loc>Munich, Germany</loc>
        <imprint>
          <dateStruct>
            <month>November</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">269-277</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01393243" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01393243</ref>
        </imprint>
        <meeting id="cid25338">
          <title>ACM Symposium on Virtual Reality Software and Technology</title>
          <num>22</num>
          <abbr type="sigle">VRST</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="hybrid-2016-bid39" type="inproceedings" rend="year" n="cite:bernard:insu-01399040">
      <identifiant type="hal" value="insu-01399040"/>
      <analytic>
        <title level="a">3d digitisation and reconstruction of a capital in northwestern gaul: interim results on the city of alet </title>
        <author>
          <persName key="neurosys-2016-idp176144">
            <foreName>Yann</foreName>
            <surname>Bernard</surname>
            <initial>Y.</initial>
          </persName>
          <persName key="hybrid-2014-idp87552">
            <foreName>Jean-Baptiste</foreName>
            <surname>Barreau</surname>
            <initial>J.-B.</initial>
          </persName>
          <persName>
            <foreName>Catherine</foreName>
            <surname>Bizien-Jaglin</surname>
            <initial>C.</initial>
          </persName>
          <persName>
            <foreName>Laurent</foreName>
            <surname>Quesnel</surname>
            <initial>L.</initial>
          </persName>
          <persName>
            <foreName>Loïc</foreName>
            <surname>Langouët</surname>
            <initial>L.</initial>
          </persName>
          <persName>
            <foreName>Marie-Yvane</foreName>
            <surname>Daire</surname>
            <initial>M.-Y.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">8th International Congress on Archaeology, Computer Graphics, Cultural Heritage and Innovation 'ARQUEOLÓGICA 2.0'</title>
        <loc>Valencia, Spain</loc>
        <imprint>
          <publisher>
            <orgName>Universitat Politècnica De Valencia</orgName>
          </publisher>
          <dateStruct>
            <month>September</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">438-440</biblScope>
          <ref xlink:href="https://hal-insu.archives-ouvertes.fr/insu-01399040" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal-insu.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>insu-01399040</ref>
        </imprint>
        <meeting id="cid625460">
          <title>International Congress on Archaeology, Computer Graphics, Cultural Heritage and Innovation</title>
          <num>8</num>
          <abbr type="sigle">ARQUEOLÓGICA</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="hybrid-2016-bid18" type="inproceedings" rend="year" n="cite:bouville:hal-01314839">
      <identifiant type="hal" value="hal-01314839"/>
      <analytic>
        <title level="a">Virtual Reality Rehearsals for Acting with Visual Effects</title>
        <author>
          <persName>
            <foreName>Rozenn</foreName>
            <surname>Bouville</surname>
            <initial>R.</initial>
          </persName>
          <persName key="hybrid-2014-idp66240">
            <foreName>Valérie</foreName>
            <surname>Gouranton</surname>
            <initial>V.</initial>
          </persName>
          <persName key="hybrid-2014-idm25952">
            <foreName>Bruno</foreName>
            <surname>Arnaldi</surname>
            <initial>B.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">International Conference on Computer Graphics &amp; Interactive Techniques</title>
        <loc>Victoria-BC, Canada</loc>
        <title level="s">GI</title>
        <imprint>
          <dateStruct>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">1-8</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01314839" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01314839</ref>
        </imprint>
        <meeting id="cid21387">
          <title>ACM SIGGRAPH International Conference on Computer Graphics and Interactive Techniques</title>
          <num>43</num>
          <abbr type="sigle">SIGGRAPH</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="hybrid-2016-bid20" type="inproceedings" rend="year" n="cite:claude:hal-01391776">
      <identifiant type="hal" value="hal-01391776"/>
      <analytic>
        <title level="a">From Observations to Collaborative Simulation: Application to Surgical Training</title>
        <author>
          <persName key="hybrid-2014-idp88784">
            <foreName>Guillaume</foreName>
            <surname>Claude</surname>
            <initial>G.</initial>
          </persName>
          <persName key="hybrid-2014-idp66240">
            <foreName>Valérie</foreName>
            <surname>Gouranton</surname>
            <initial>V.</initial>
          </persName>
          <persName key="hycomes-2014-idm10408">
            <foreName>Benoît</foreName>
            <surname>Caillaud</surname>
            <initial>B.</initial>
          </persName>
          <persName>
            <foreName>Bernard</foreName>
            <surname>Gibaud</surname>
            <initial>B.</initial>
          </persName>
          <persName>
            <foreName>Pierre</foreName>
            <surname>Jannin</surname>
            <initial>P.</initial>
          </persName>
          <persName key="hybrid-2014-idm25952">
            <foreName>Bruno</foreName>
            <surname>Arnaldi</surname>
            <initial>B.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">ICAT-EGVE 2016 - International Conference on Artificial Reality and Telexistence, Eurographics Symposium on Virtual Environments</title>
        <loc>Little Rock, Arkansas, United States</loc>
        <imprint>
          <dateStruct>
            <month>December</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01391776" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01391776</ref>
        </imprint>
        <meeting id="cid393136">
          <title>International Conference on Artificial Reality and Telexistence</title>
          <num>26</num>
          <abbr type="sigle">ICAT</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="hybrid-2016-bid8" type="inproceedings" rend="year" n="cite:dris:hal-01391786">
      <identifiant type="hal" value="hal-01391786"/>
      <analytic>
        <title level="a">Integration concept and model of Industry Foundation Classes (IFC) for interactive virtual environments</title>
        <author>
          <persName>
            <foreName>Anne-Solène</foreName>
            <surname>Dris</surname>
            <initial>A.-S.</initial>
          </persName>
          <persName key="hybrid-2014-idp66240">
            <foreName>Valérie</foreName>
            <surname>Gouranton</surname>
            <initial>V.</initial>
          </persName>
          <persName key="hybrid-2014-idm25952">
            <foreName>Bruno</foreName>
            <surname>Arnaldi</surname>
            <initial>B.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">33rd CIB W78 Conference</title>
        <loc>Brisbane, Australia</loc>
        <imprint>
          <dateStruct>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01391786" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01391786</ref>
        </imprint>
        <meeting id="cid625459">
          <title>CIB W78 Conference</title>
          <num>33</num>
          <abbr type="sigle"/>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="hybrid-2016-bid17" type="inproceedings" rend="year" n="cite:gaffary:hal-01406434">
      <identifiant type="doi" value="10.1007/978-3-319-42324-1_39"/>
      <identifiant type="hal" value="hal-01406434"/>
      <analytic>
        <title level="a">Studying One and Two-Finger Perception of Tactile Directional Cues</title>
        <author>
          <persName>
            <foreName>Yoren</foreName>
            <surname>Gaffary</surname>
            <initial>Y.</initial>
          </persName>
          <persName key="hybrid-2014-idp67480">
            <foreName>Maud</foreName>
            <surname>Marchal</surname>
            <initial>M.</initial>
          </persName>
          <persName key="hybrid-2014-idp80088">
            <foreName>Adrien</foreName>
            <surname>Girard</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Marine</foreName>
            <surname>Pellan</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>Anouk</foreName>
            <surname>Asselin</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Benoît</foreName>
            <surname>Peigné</surname>
            <initial>B.</initial>
          </persName>
          <persName>
            <foreName>Mathieu</foreName>
            <surname>Emily</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>Florian</foreName>
            <surname>Gosselin</surname>
            <initial>F.</initial>
          </persName>
          <persName>
            <foreName>Anthony</foreName>
            <surname>Chabrier</surname>
            <initial>A.</initial>
          </persName>
          <persName key="hybrid-2014-idm28656">
            <foreName>Anatole</foreName>
            <surname>Lécuyer</surname>
            <initial>A.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <editor role="editor">
          <persName>
            <foreName>Fernando</foreName>
            <surname>Bello</surname>
            <initial>F.</initial>
          </persName>
          <persName>
            <foreName>Hiroyuki</foreName>
            <surname>Kajimoto</surname>
            <initial>H.</initial>
          </persName>
          <persName>
            <foreName>Yon</foreName>
            <surname>Visell</surname>
            <initial>Y.</initial>
          </persName>
        </editor>
        <title level="m">10th International Conference on Haptics - Perception, Devices, Control, and Applications (EuroHaptics)</title>
        <loc>Londres, United Kingdom</loc>
        <title level="s">Haptics: Perception, Devices, Control, and Applications. 10th International Conference, EuroHaptics 2016, London, UK, July 4-7, 2016, Proceedings, Part II</title>
        <imprint>
          <biblScope type="volume">9775</biblScope>
          <publisher>
            <orgName>Springer</orgName>
          </publisher>
          <publisher>
            <orgName type="organisation">Imperial College London</orgName>
          </publisher>
          <dateStruct>
            <month>July</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">396-405</biblScope>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01406434" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01406434</ref>
        </imprint>
        <meeting id="cid63428">
          <title>EuroHaptics International Conference</title>
          <num>2016</num>
          <abbr type="sigle">EUROHAPTICS</abbr>
        </meeting>
      </monogr>
      <note type="bnote">ISBN: 978-3-319-42324-1 ; 978-3-319-42323-4</note>
    </biblStruct>
    
    <biblStruct id="hybrid-2016-bid7" type="inproceedings" rend="year" n="cite:lacoche:hal-01293037">
      <identifiant type="doi" value="10.1109/3DUI.2016.7460026"/>
      <identifiant type="hal" value="hal-01293037"/>
      <analytic>
        <title level="a">D3PART: A new Model for Redistribution and Plasticity of 3D User Interfaces</title>
        <author>
          <persName>
            <foreName>Jérémy</foreName>
            <surname>LACOCHE</surname>
            <initial>J.</initial>
          </persName>
          <persName key="hybrid-2014-idp90000">
            <foreName>Thierry</foreName>
            <surname>Duval</surname>
            <initial>T.</initial>
          </persName>
          <persName key="hybrid-2014-idm25952">
            <foreName>Bruno</foreName>
            <surname>Arnaldi</surname>
            <initial>B.</initial>
          </persName>
          <persName>
            <foreName>Éric</foreName>
            <surname>MAISEL</surname>
            <initial>É.</initial>
          </persName>
          <persName>
            <foreName>Jérôme</foreName>
            <surname>Royan</surname>
            <initial>J.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">3DUI 2016 : IEEE symposium on 3D User Interfaces Summit</title>
        <loc>Greenville, SC, United States</loc>
        <imprint>
          <publisher>
            <orgName>IEEE</orgName>
          </publisher>
          <dateStruct>
            <month>March</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">23-36</biblScope>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01293037" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01293037</ref>
        </imprint>
        <meeting id="cid93832">
          <title>IEEE Symposium on 3D User Interfaces</title>
          <num>11</num>
          <abbr type="sigle">3DUI</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="hybrid-2016-bid22" type="inproceedings" rend="year" n="cite:lechenechal:hal-01293435">
      <identifiant type="doi" value="10.1109/3DCVE.2016.7563559"/>
      <identifiant type="hal" value="hal-01293435"/>
      <analytic>
        <title level="a">Vishnu: Virtual Immersive Support for HelpiNg Users - An Interaction Paradigm for Collaborative Remote Guiding in Mixed Reality</title>
        <author>
          <persName key="hybrid-2014-idp93968">
            <foreName>Morgan</foreName>
            <surname>Le Chénéchal</surname>
            <initial>M.</initial>
          </persName>
          <persName key="hybrid-2014-idp90000">
            <foreName>Thierry</foreName>
            <surname>Duval</surname>
            <initial>T.</initial>
          </persName>
          <persName key="hybrid-2014-idp66240">
            <foreName>Valérie</foreName>
            <surname>Gouranton</surname>
            <initial>V.</initial>
          </persName>
          <persName>
            <foreName>Jérôme</foreName>
            <surname>Royan</surname>
            <initial>J.</initial>
          </persName>
          <persName key="hybrid-2014-idm25952">
            <foreName>Bruno</foreName>
            <surname>Arnaldi</surname>
            <initial>B.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">3DCVE 2016 : International Workshop on Collaborative Virtual Environments</title>
        <loc>Greenville, United States</loc>
        <imprint>
          <publisher>
            <orgName>IEEE</orgName>
          </publisher>
          <dateStruct>
            <month>March</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">1 - 5</biblScope>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01293435" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01293435</ref>
        </imprint>
        <meeting id="cid624463">
          <title>International Workshop on Collaborative Virtual Environments</title>
          <num>2016</num>
          <abbr type="sigle">3DCVE</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="hybrid-2016-bid23" type="inproceedings" rend="year" n="cite:lechenechal:hal-01293041">
      <identifiant type="doi" value="10.1109/3DCVE.2016.7563562"/>
      <identifiant type="hal" value="hal-01293041"/>
      <analytic>
        <title level="a">When the Giant meets the Ant An Asymmetric Approach for Collaborative and Concurrent Object Manipulation in a Multi-Scale Environment</title>
        <author>
          <persName key="hybrid-2014-idp93968">
            <foreName>Morgan</foreName>
            <surname>Le Chénéchal</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>Jérémy</foreName>
            <surname>LACOCHE</surname>
            <initial>J.</initial>
          </persName>
          <persName>
            <foreName>Jérôme</foreName>
            <surname>Royan</surname>
            <initial>J.</initial>
          </persName>
          <persName key="hybrid-2014-idp90000">
            <foreName>Thierry</foreName>
            <surname>Duval</surname>
            <initial>T.</initial>
          </persName>
          <persName key="hybrid-2014-idp66240">
            <foreName>Valérie</foreName>
            <surname>Gouranton</surname>
            <initial>V.</initial>
          </persName>
          <persName key="hybrid-2014-idm25952">
            <foreName>Bruno</foreName>
            <surname>Arnaldi</surname>
            <initial>B.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">3DCVE 2016 : International Workshop on Collaborative Virtual Environments</title>
        <loc>Greenville, United States</loc>
        <imprint>
          <publisher>
            <orgName>IEEE</orgName>
          </publisher>
          <dateStruct>
            <month>March</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">1 - 4</biblScope>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01293041" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01293041</ref>
        </imprint>
        <meeting id="cid624463">
          <title>International Workshop on Collaborative Virtual Environments</title>
          <num>2016</num>
          <abbr type="sigle">3DCVE</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="hybrid-2016-bid21" type="inproceedings" rend="year" n="cite:lemoulec:hal-01393003">
      <identifiant type="hal" value="hal-01393003"/>
      <analytic>
        <title level="a">Take-over control paradigms in collaborative virtual environments for training</title>
        <author>
          <persName key="diverse-2015-idp141624">
            <foreName>Gwendal</foreName>
            <surname>Le Moulec</surname>
            <initial>G.</initial>
          </persName>
          <persName key="hybrid-2014-idm27216">
            <foreName>Ferran</foreName>
            <surname>Argelaguet Sanz</surname>
            <initial>F.</initial>
          </persName>
          <persName key="hybrid-2014-idm28656">
            <foreName>Anatole</foreName>
            <surname>Lécuyer</surname>
            <initial>A.</initial>
          </persName>
          <persName key="hybrid-2014-idp66240">
            <foreName>Valérie</foreName>
            <surname>Gouranton</surname>
            <initial>V.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">ACM Symposium on Virtual Reality Software and Technology (VRST)</title>
        <loc>Munich, Germany</loc>
        <imprint>
          <dateStruct>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01393003" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01393003</ref>
        </imprint>
        <meeting id="cid25338">
          <title>ACM Symposium on Virtual Reality Software and Technology</title>
          <num>17</num>
          <abbr type="sigle">VRST</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="hybrid-2016-bid10" type="inproceedings" rend="year" n="cite:nicolas:hal-01391762">
      <identifiant type="hal" value="hal-01391762"/>
      <analytic>
        <title level="a">Internal 3D Printing of Intricate Structures</title>
        <author>
          <persName>
            <foreName>Théophane</foreName>
            <surname>Nicolas</surname>
            <initial>T.</initial>
          </persName>
          <persName key="hybrid-2016-idp152560">
            <foreName>Ronan</foreName>
            <surname>Gaugne</surname>
            <initial>R.</initial>
          </persName>
          <persName>
            <foreName>Cédric</foreName>
            <surname>Tavernier</surname>
            <initial>C.</initial>
          </persName>
          <persName key="hybrid-2014-idp66240">
            <foreName>Valérie</foreName>
            <surname>Gouranton</surname>
            <initial>V.</initial>
          </persName>
          <persName key="hybrid-2014-idm25952">
            <foreName>Bruno</foreName>
            <surname>Arnaldi</surname>
            <initial>B.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <editor role="editor">
          <persName>
            <foreName>Marinos</foreName>
            <surname>Ioannides</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>Eleanor</foreName>
            <surname>Fink</surname>
            <initial>E.</initial>
          </persName>
          <persName>
            <foreName>Antonia</foreName>
            <surname>Moropoulou</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Monika</foreName>
            <surname>Hagedorn-Saupe</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>Antonella</foreName>
            <surname>Fresa</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Gunnar</foreName>
            <surname>Liestøl</surname>
            <initial>G.</initial>
          </persName>
          <persName>
            <foreName>Vlatka</foreName>
            <surname>Rajcic</surname>
            <initial>V.</initial>
          </persName>
          <persName>
            <foreName>Pierre</foreName>
            <surname>Grussenmeyer</surname>
            <initial>P.</initial>
          </persName>
        </editor>
        <title level="m">6th International Conference on Culturage Heritage - EuroMed 2016</title>
        <loc>Nicosia, Cyprus</loc>
        <title level="s">Lecture Notes in Computer Science</title>
        <imprint>
          <biblScope type="volume">10058</biblScope>
          <biblScope type="number">Part I</biblScope>
          <dateStruct>
            <month>October</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">432-441</biblScope>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01391762" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01391762</ref>
        </imprint>
        <meeting id="cid624584">
          <title>International Conference on Cultural Heritage</title>
          <num>6</num>
          <abbr type="sigle">EuroMed</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="hybrid-2016-bid9" type="inproceedings" rend="year" n="cite:safa:hal-01391755">
      <identifiant type="hal" value="hal-01391755"/>
      <analytic>
        <title level="a">Digital and handcrafting processes applied to sound-studies of archaeological bone flutes</title>
        <author>
          <persName>
            <foreName>Etienne</foreName>
            <surname>Safa</surname>
            <initial>E.</initial>
          </persName>
          <persName key="hybrid-2014-idp87552">
            <foreName>Jean-Baptiste</foreName>
            <surname>Barreau</surname>
            <initial>J.-B.</initial>
          </persName>
          <persName key="hybrid-2016-idp152560">
            <foreName>Ronan</foreName>
            <surname>Gaugne</surname>
            <initial>R.</initial>
          </persName>
          <persName key="beagle-2014-idp97688">
            <foreName>Wandrille</foreName>
            <surname>Duchemin</surname>
            <initial>W.</initial>
          </persName>
          <persName>
            <foreName>Jean-Daniel</foreName>
            <surname>Talma</surname>
            <initial>J.-D.</initial>
          </persName>
          <persName key="hybrid-2014-idm25952">
            <foreName>Bruno</foreName>
            <surname>Arnaldi</surname>
            <initial>B.</initial>
          </persName>
          <persName key="mimetic-2014-idp71256">
            <foreName>Georges</foreName>
            <surname>Dumont</surname>
            <initial>G.</initial>
          </persName>
          <persName key="hybrid-2014-idp66240">
            <foreName>Valérie</foreName>
            <surname>Gouranton</surname>
            <initial>V.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">International Conference on Culturage Heritage, EuroMed</title>
        <loc>Nicosia, Cyprus</loc>
        <imprint>
          <biblScope type="volume">1</biblScope>
          <biblScope type="number">10058</biblScope>
          <dateStruct>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">184-195</biblScope>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01391755" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01391755</ref>
        </imprint>
        <meeting id="cid624584">
          <title>International Conference on Cultural Heritage</title>
          <num>6</num>
          <abbr type="sigle">EuroMed</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="hybrid-2016-bid37" type="inproceedings" rend="year" n="cite:evain:hal-01388552">
      <identifiant type="doi" value="10.1145/2909132.2909278"/>
      <identifiant type="hal" value="hal-01388552"/>
      <analytic>
        <title level="a">Influence of Error Rate on Frustration of BCI Users</title>
        <author>
          <persName key="hybrid-2014-idp91464">
            <foreName>Andéol</foreName>
            <surname>Évain</surname>
            <initial>A.</initial>
          </persName>
          <persName key="hybrid-2014-idm27216">
            <foreName>Ferran</foreName>
            <surname>Argelaguet Sanz</surname>
            <initial>F.</initial>
          </persName>
          <persName>
            <foreName>Anthony</foreName>
            <surname>Strock</surname>
            <initial>A.</initial>
          </persName>
          <persName key="mint-2014-idp87488">
            <foreName>Nicolas</foreName>
            <surname>Roussel</surname>
            <initial>N.</initial>
          </persName>
          <persName key="mint-2014-idp90176">
            <foreName>Géry</foreName>
            <surname>Casiez</surname>
            <initial>G.</initial>
          </persName>
          <persName key="hybrid-2014-idm28656">
            <foreName>Anatole</foreName>
            <surname>Lécuyer</surname>
            <initial>A.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">AVI'16 - International Working Conference on Advanced Visual Interfaces</title>
        <loc>Bari, Italy</loc>
        <imprint>
          <publisher>
            <orgName>ACM</orgName>
          </publisher>
          <dateStruct>
            <month>June</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">248–251</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01388552" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01388552</ref>
        </imprint>
        <meeting id="cid320442">
          <title>International Working Conference on Advanced Visual Interfaces</title>
          <num>2016</num>
          <abbr type="sigle">AVI</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="hybrid-2016-bid2" type="misc" rend="year" n="cite:lechenechal:hal-01301770">
      <identifiant type="doi" value="10.1109/3DUI.2016.7460078"/>
      <identifiant type="hal" value="hal-01301770"/>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no">
        <title level="m">When the Giant meets the Ant An Asymmetric Approach for Collaborative Object Manipulation</title>
        <author>
          <persName key="hybrid-2014-idp93968">
            <foreName>Morgan</foreName>
            <surname>Le Chénéchal</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>Jérémy</foreName>
            <surname>LACOCHE</surname>
            <initial>J.</initial>
          </persName>
          <persName>
            <foreName>Jérôme</foreName>
            <surname>Royan</surname>
            <initial>J.</initial>
          </persName>
          <persName key="hybrid-2014-idp90000">
            <foreName>Thierry</foreName>
            <surname>Duval</surname>
            <initial>T.</initial>
          </persName>
          <persName key="hybrid-2014-idp66240">
            <foreName>Valérie</foreName>
            <surname>Gouranton</surname>
            <initial>V.</initial>
          </persName>
          <persName key="hybrid-2014-idm25952">
            <foreName>Bruno</foreName>
            <surname>Arnaldi</surname>
            <initial>B.</initial>
          </persName>
        </author>
        <imprint>
          <publisher>
            <orgName>IEEE</orgName>
          </publisher>
          <dateStruct>
            <month>March</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">273 - 274</biblScope>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01301770" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01301770</ref>
        </imprint>
      </monogr>
      <note type="howpublished">3DUI 2016 : 11th IEEE Symposium on 3D User Interfaces</note>
      <note type="bnote">Poster</note>
    </biblStruct>
    
    <biblStruct id="hybrid-2016-bid0" type="book" rend="foot" n="footcite:BK04">
      <monogr>
        <title level="m">3D User Interfaces: Theory and Practice</title>
        <author>
          <persName>
            <foreName>Doug A</foreName>
            <surname>Bowman</surname>
            <initial>D. A.</initial>
          </persName>
          <persName>
            <foreName>Ernest</foreName>
            <surname>Kruijff</surname>
            <initial>E.</initial>
          </persName>
          <persName>
            <foreName>Joseph J</foreName>
            <surname>LaViola</surname>
            <initial>J. J.</initial>
          </persName>
          <persName>
            <foreName>Ivan</foreName>
            <surname>Poupyrev</surname>
            <initial>I.</initial>
          </persName>
        </author>
        <imprint>
          <publisher>
            <orgName>Addison Wesley</orgName>
          </publisher>
          <dateStruct>
            <year>2004</year>
          </dateStruct>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="hybrid-2016-bid26" type="book" rend="foot" n="footcite:clerc:hal-01408991">
      <identifiant type="hal" value="hal-01408991"/>
      <monogr>
        <title level="m">Brain-Computer Interfaces 1</title>
        <author>
          <persName key="athena-2014-idm29272">
            <foreName>Maureen</foreName>
            <surname>Clerc</surname>
            <initial>M.</initial>
          </persName>
          <persName key="neurosys-2014-idm28696">
            <foreName>Laurent</foreName>
            <surname>Bougrain</surname>
            <initial>L.</initial>
          </persName>
          <persName key="potioc-2014-idp65160">
            <foreName>Fabien</foreName>
            <surname>Lotte</surname>
            <initial>F.</initial>
          </persName>
        </author>
        <editor role="editor">
          <persName>
            <foreName>Fabien Lotte</foreName>
            <surname>Maureen Clerc</surname>
            <initial>F. L.</initial>
          </persName>
        </editor>
        <imprint>
          <publisher>
            <orgName>Wiley-ISTE</orgName>
          </publisher>
          <dateStruct>
            <month>July</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01408991" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01408991</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="hybrid-2016-bid27" type="book" rend="foot" n="footcite:clerc:hal-01408998">
      <identifiant type="hal" value="hal-01408998"/>
      <monogr>
        <title level="m">Brain-Computer Interfaces 2</title>
        <author>
          <persName key="athena-2014-idm29272">
            <foreName>Maureen</foreName>
            <surname>Clerc</surname>
            <initial>M.</initial>
          </persName>
          <persName key="neurosys-2014-idm28696">
            <foreName>Laurent</foreName>
            <surname>Bougrain</surname>
            <initial>L.</initial>
          </persName>
          <persName key="potioc-2014-idp65160">
            <foreName>Fabien</foreName>
            <surname>Lotte</surname>
            <initial>F.</initial>
          </persName>
        </author>
        <editor role="editor">
          <persName>
            <foreName>Fabien Lotte</foreName>
            <surname>Maureen Clerc</surname>
            <initial>F. L.</initial>
          </persName>
        </editor>
        <imprint>
          <publisher>
            <orgName>Wiley-ISTE</orgName>
          </publisher>
          <dateStruct>
            <month>July</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01408998" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01408998</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="hybrid-2016-bid24" type="book" rend="foot" n="footcite:clerc:hal-01402539">
      <identifiant type="hal" value="hal-01402539"/>
      <monogr>
        <title level="m">Les interfaces Cerveau-Ordinateur 1</title>
        <editor role="editor">
          <persName key="athena-2014-idm29272">
            <foreName>Maureen</foreName>
            <surname>Clerc</surname>
            <initial>M.</initial>
          </persName>
          <persName key="neurosys-2014-idm28696">
            <foreName>Laurent</foreName>
            <surname>Bougrain</surname>
            <initial>L.</initial>
          </persName>
          <persName key="potioc-2014-idp65160">
            <foreName>Fabien</foreName>
            <surname>Lotte</surname>
            <initial>F.</initial>
          </persName>
        </editor>
        <imprint>
          <publisher>
            <orgName>ISTE</orgName>
          </publisher>
          <dateStruct>
            <month>July</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01402539" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01402539</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="hybrid-2016-bid25" type="book" rend="foot" n="footcite:clerc:hal-01402544">
      <identifiant type="hal" value="hal-01402544"/>
      <monogr>
        <title level="m">Les interfaces cerveau-ordinateur 2</title>
        <editor role="editor">
          <persName key="athena-2014-idm29272">
            <foreName>Maureen</foreName>
            <surname>Clerc</surname>
            <initial>M.</initial>
          </persName>
          <persName key="neurosys-2014-idm28696">
            <foreName>Laurent</foreName>
            <surname>Bougrain</surname>
            <initial>L.</initial>
          </persName>
          <persName key="potioc-2014-idp65160">
            <foreName>Fabien</foreName>
            <surname>Lotte</surname>
            <initial>F.</initial>
          </persName>
        </editor>
        <imprint>
          <publisher>
            <orgName>ISTE</orgName>
          </publisher>
          <dateStruct>
            <month>July</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01402544" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01402544</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="hybrid-2016-bid1" type="article" rend="foot" n="footcite:Lecuyer2009">
      <identifiant type="doi" value="doi/abs/10.1162/pres.18.1.39"/>
      <analytic>
        <title level="a">Simulating Haptic Feedback Using Vision: A Survey of Research and Applications of Pseudo-Haptic Feedback</title>
        <author>
          <persName key="hybrid-2014-idm28656">
            <foreName>Anatole</foreName>
            <surname>Lécuyer</surname>
            <initial>A.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">Presence: Teleoperators and Virtual Environments</title>
        <imprint>
          <biblScope type="volume">18</biblScope>
          <biblScope type="number">1</biblScope>
          <dateStruct>
            <month>January</month>
            <year>2009</year>
          </dateStruct>
          <biblScope type="pages">39–53</biblScope>
          <ref xlink:href="http://www.mitpressjournals.org/doi/abs/10.1162/pres.18.1.39" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>www.<allowbreak/>mitpressjournals.<allowbreak/>org/<allowbreak/>doi/<allowbreak/>abs/<allowbreak/>10.<allowbreak/>1162/<allowbreak/>pres.<allowbreak/>18.<allowbreak/>1.<allowbreak/>39</ref>
        </imprint>
      </monogr>
    </biblStruct>
  </biblio>
</raweb>
