<?xml version="1.0" encoding="utf-8"?>
<raweb xmlns:xlink="http://www.w3.org/1999/xlink" xml:lang="en" year="2016">
  <identification id="graphdeco" isproject="true">
    <shortname>GRAPHDECO</shortname>
    <projectName>GRAPHics and DEsign with hEterogeneous COntent</projectName>
    <theme-de-recherche>Interaction and visualization</theme-de-recherche>
    <domaine-de-recherche>Perception, Cognition and Interaction</domaine-de-recherche>
    <urlTeam>https://team.inria.fr/graphdeco/</urlTeam>
    <header_dates_team>Creation of the Team: 2015 January 01, updated into Project-Team: 2015 July 01</header_dates_team>
    <LeTypeProjet>Project-Team</LeTypeProjet>
    <keywordsSdN>
      <term>3.1.4. - Uncertain data</term>
      <term>5. - Interaction, multimedia and robotics</term>
      <term>5.1. - Human-Computer Interaction</term>
      <term>5.1.1. - Engineering of interactive systems</term>
      <term>5.1.2. - Evaluation of interactive systems</term>
      <term>5.3.5. - Computational photography</term>
      <term>5.4.4. - 3D and spatio-temporal reconstruction</term>
      <term>5.5. - Computer graphics</term>
      <term>5.5.1. - Geometrical modeling</term>
      <term>5.5.2. - Rendering</term>
      <term>5.5.3. - Computational photography</term>
      <term>5.6. - Virtual reality, augmented reality</term>
      <term>5.9.1. - Sampling, acquisition</term>
      <term>6.3.5. - Uncertainty Quantification</term>
      <term>7.5. - Geometry, Topology</term>
      <term>8.2. - Machine learning</term>
      <term>8.3. - Signal analysis</term>
    </keywordsSdN>
    <keywordsSecteurs>
      <term>5. - Industry of the future</term>
      <term>5.2. - Design and manufacturing</term>
      <term>5.7. - 3D printing</term>
      <term>8. - Smart Cities and Territories</term>
      <term>8.3. - Urbanism and urban planning</term>
      <term>9. - Society and Knowledge</term>
      <term>9.1.2. - Serious games</term>
      <term>9.2. - Art</term>
      <term>9.2.2. - Cinema, Television</term>
      <term>9.2.3. - Video games</term>
      <term>9.5. - Humanities</term>
      <term>9.5.6. - Archeology, History</term>
    </keywordsSecteurs>
    <UR name="Sophia"/>
  </identification>
  <team id="uid1">
    <person key="reves-2014-idm29368">
      <firstname>George</firstname>
      <lastname>Drettakis</lastname>
      <categoryPro>Chercheur</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Team leader, Inria, Research Scientist</moreinfo>
      <hdr>oui</hdr>
    </person>
    <person key="reves-2014-idm27888">
      <firstname>Adrien</firstname>
      <lastname>Bousseau</lastname>
      <categoryPro>Chercheur</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria, Research Scientist</moreinfo>
      <hdr>oui</hdr>
    </person>
    <person key="graphdeco-2016-idp111504">
      <firstname>Frederic</firstname>
      <lastname>Durand</lastname>
      <categoryPro>Visiteur</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>MIT, Visiting Scientist with an Inria Chair, from Sept 2016</moreinfo>
    </person>
    <person key="graphdeco-2016-idp114016">
      <firstname>Eugene</firstname>
      <lastname>Fiume</lastname>
      <categoryPro>Visiteur</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>University of Toronto, from Jul 2016</moreinfo>
    </person>
    <person key="graphdeco-2015-idp85992">
      <firstname>Georgios</firstname>
      <lastname>Kopanas</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria, until Aug 2016</moreinfo>
    </person>
    <person key="graphdeco-2015-idm27152">
      <firstname>Sebastien</firstname>
      <lastname>Bonopera</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria, from Sept 2015 to Sept 2016</moreinfo>
    </person>
    <person key="graphdeco-2015-idp66104">
      <firstname>Johanna</firstname>
      <lastname>Delanoy</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="graphdeco-2016-idp123936">
      <firstname>Valentin</firstname>
      <lastname>Deschaintre</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Optis, from Nov 2016, CIFRE funding</moreinfo>
    </person>
    <person key="reves-2014-idp75400">
      <firstname>Rodrigo</firstname>
      <lastname>Ortiz Cayon</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria, until Nov 2016</moreinfo>
    </person>
    <person key="graphdeco-2016-idp128848">
      <firstname>Julien</firstname>
      <lastname>Philip</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria, from Nov 2016, EU H2020 EMOTIVE funding</moreinfo>
    </person>
    <person key="graphdeco-2016-idp131328">
      <firstname>Simon</firstname>
      <lastname>Rodriguez</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>UCA (University), from Nov 2016</moreinfo>
    </person>
    <person key="graphdeco-2015-idp70872">
      <firstname>Theo</firstname>
      <lastname>Thonat</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="reves-2014-idp85432">
      <firstname>George</firstname>
      <lastname>Koulieris</lastname>
      <categoryPro>PostDoc</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="morpheo-2014-idp109816">
      <firstname>Abdelaziz</firstname>
      <lastname>Djelouah</lastname>
      <categoryPro>PostDoc</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="reves-2014-idp79168">
      <firstname>Kenneth</firstname>
      <lastname>Vanhoey</lastname>
      <categoryPro>PostDoc</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria, until Aug 2016</moreinfo>
    </person>
    <person key="graphdeco-2016-idp143680">
      <firstname>Miika</firstname>
      <lastname>Aittala</lastname>
      <categoryPro>Visiteur</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>MIT/Aaalto University, from Nov 2016</moreinfo>
    </person>
    <person key="galaad2-2014-idp73296">
      <firstname>Sophie</firstname>
      <lastname>Honnorat</lastname>
      <categoryPro>Assistant</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="graphdeco-2016-idp148656">
      <firstname>Sai Praveen</firstname>
      <lastname>Bangaru</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria, Intern, from May 2016 until Jul 2016</moreinfo>
    </person>
    <person key="graphdeco-2016-idp151152">
      <firstname>Srinivasa Rao</firstname>
      <lastname>Gadhamchetty</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria, Intern, from May 2016 until Jul 2016</moreinfo>
    </person>
    <person key="graphdeco-2016-idp153648">
      <firstname>Amelie</firstname>
      <lastname>Fondevilla</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria, Intern, from Feb 2016 to Jul 2016</moreinfo>
    </person>
  </team>
  <presentation id="uid2">
    <bodyTitle>Overall Objectives</bodyTitle>
    <subsection id="uid3" level="1">
      <bodyTitle>General Presentation</bodyTitle>
      <p>In traditional Computer Graphics (CG)
input is <i>accurately modeled</i> by hand by artists.
The artists first create the 3D geometry – i.e., the polygons
and surfaces used to represent the 3D scene.
They then need to assign colors, textures and more generally material properties
to each piece of geometry in the scene.
Finally they also define the position, type and intensity of the lights.
This modeling process is illustrated schematically in Fig. <ref xlink:href="#uid4" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>(left)).
Creating all this 3D content involves a high level of training and skills, and is reserved
to a small minority of expert modelers. This tedious process is a significant
distraction for creative exploration, during which artists and designers are primarily interested
in obtaining compelling imagery and prototypes rather than in
accurately specifying all the ingredients listed above.
Designers also often want to explore many variations of a concept, which requires them
to repeat the above steps multiple times.</p>
      <p>Once the 3D elements are in place, a <i>rendering</i> algorithm is employed to
generate a shaded, realistic image (Fig. <ref xlink:href="#uid4" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>(right)).
Costly rendering algorithms are then required to simulate light transport (or
<i>global illumination</i>)
from the light sources to the camera, accounting for the complex interactions between light and
materials and the visibility between objects.
Such rendering algorithms only provide meaningful results if
the input has been <i>accurately</i> modeled and is <i>complete</i>,
which is prohibitive as discussed above.</p>
      <object id="uid4">
        <table>
          <tr>
            <td>
              <ressource xlink:href="IMG/cg_simulation.png" type="float" width="341.6013pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
            </td>
          </tr>
        </table>
        <caption>Traditional computer graphics pipeline. Rendering from www.thegnomonworkshop.com</caption>
      </object>
      <p>A major recent development is that many alternative sources of 3D content
are becoming available. Cheap depth sensors allow anyone to capture real
objects but the resulting 3D models are often <i>uncertain</i>,
since the reconstruction can be inaccurate and is most often incomplete.
There have also been significant advances in casual content creation, e.g.,
sketch-based modeling tools. The resulting models are often approximate since people
rarely draw accurate perspective and proportions. These models also often lack details,
which can be seen as a form of uncertainty since a variety of refined models could
correspond to the rough one.
Finally, in recent years we have witnessed the emergence of new usage of 3D content
for rapid prototyping, which aims at accelerating the transition from rough ideas to
physical artifacts.</p>
      <p>The inability to handle <i>uncertainty</i> in the data is a major shortcoming of CG today
as it prevents the direct use of cheap and casual sources of 3D content for the
design and rendering of high-quality images.
The abundance and ease of access to <i>inaccurate</i>, <i>incomplete</i> and <i>heterogeneous</i> 3D content
imposes the need to <i>rethink the foundations of 3D computer graphics</i>
to allow <i>uncertainty</i> to be treated in inherent manner in Computer Graphics, from design all the way to rendering
and prototyping.</p>
      <p>The technological shifts we mention above, together with developments in computer vision, user-friendly sketch-based modeling,
online tutorials, but also image, video and 3D model repositories and 3D printing
represent a great opportunity for new imaging methods.
There are several significant challenges to overcome before such visual content can become widely accessible.</p>
      <p>In GraphDeco, we have identified two major scientific challenges of our field which we will address:</p>
      <simplelist>
        <li id="uid5">
          <p noindent="true">First, the design pipeline needs to be revisited to <b>explicitly account for
the variability and uncertainty of a concept and its representations</b>, from early sketches to 3D models
and prototypes. Professional practice also needs to be adapted and facilitated to be accessible to all.</p>
        </li>
        <li id="uid6">
          <p noindent="true">Second, a new approach is required to
<b>develop computer graphics models and algorithms capable of handling uncertain
and heterogeneous data</b> as well as traditional synthetic content.</p>
        </li>
      </simplelist>
      <p noindent="true">We next describe the context of our proposed research for these two challenges.
Both directions address hetereogeneous and uncertain input and (in some
cases) output, and build on a set of common methodological tools.</p>
    </subsection>
  </presentation>
  <fondements id="uid7">
    <bodyTitle>Research Program</bodyTitle>
    <subsection id="uid8" level="1">
      <bodyTitle>Introduction</bodyTitle>
      <p>Our research program is oriented around two main axes:
1) Computer-Assisted Design with Heterogeneous Representations and
2) Graphics with Uncertainty and Heterogeneous Content.
These two axes are governed by a set of common fundamental goals,
share many common methodological tools and are deeply intertwined in the development of applications.</p>
      <subsection id="cid1" level="2">
        <bodyTitle>Computer-Assisted Design with Heterogeneous Representations</bodyTitle>
        <p>Designers use a variety of visual representations to explore and communicate about a concept. Figure <ref xlink:href="#uid9" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> illustrates some typical representations, including sketches, hand-made prototypes, 3D models, 3D printed prototypes or instructions.</p>
        <object id="uid9">
          <table rend="inline">
            <tr style="">
              <td style="text-align:center;" halign="center">
                <ressource xlink:href="IMG/hair_dryer_ideation.png" type="inline" height="72.26999pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
              <td style="text-align:center;" halign="center">
                <ressource xlink:href="IMG/hair_dryer_pres.png" type="inline" height="72.26999pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
              <td style="text-align:center;" halign="center">
                <ressource xlink:href="IMG/hair_dryer_paper.png" type="inline" height="72.26999pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
              <td style="text-align:center;" halign="center">
                <ressource xlink:href="IMG/hair_dryer_3d.jpg" type="inline" height="72.26999pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
            <tr style="">
              <td style="text-align:center;" halign="center">
                <small>(a) Ideation sketch</small>
              </td>
              <td style="text-align:center;" halign="center">
                <small>(b) Presentation sketch</small>
              </td>
              <td style="text-align:center;" halign="center">
                <small>(c) Coarse prototype</small>
              </td>
              <td style="text-align:center;" halign="center">
                <small>(d) 3D model</small>
              </td>
            </tr>
            <tr style="">
              <td style="text-align:center;" halign="center">
                <ressource xlink:href="IMG/hair_dryer_simulation.jpg" type="inline" height="72.26999pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
              <td style="text-align:center;" halign="center">
                <ressource xlink:href="IMG/hair_dryer_3dprint.png" type="inline" height="72.26999pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
              <td style="text-align:center;" halign="center">
                <ressource xlink:href="IMG/hair_dryer_cutaway.jpg" type="inline" height="72.26999pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
              <td style="text-align:center;" halign="center">
                <ressource xlink:href="IMG/hair_dryer_instructions.png" type="inline" height="72.26999pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
            <tr style="">
              <td style="text-align:center;" halign="center">
                <small>(f) Simulation</small>
              </td>
              <td style="text-align:center;" halign="center">
                <small>(e) 3D Printing</small>
              </td>
              <td style="text-align:center;" halign="center">
                <small>(g) Technical diagram</small>
              </td>
              <td style="text-align:center;" halign="center">
                <small>(h) Instructions</small>
              </td>
            </tr>
            <caption/>
          </table>
          <caption>Various representations of a hair dryer at different stages of the design process.
Image source, in order: c-maeng on deviantart.com, shauntur on deviantart.com,
"Prototyping and Modelmaking for Product Design"
Hallgrimsson, B., Laurence King Publishers, 2012,
samsher511 on turbosquid.com,
my.solidworks.com, weilung tseng on cargocollective.com,
howstuffworks.com, u-manual.com</caption>
        </object>
        <p>The early representations of a concept, such as rough sketches and hand-made prototypes, help designers
formulate their ideas and test the form and function of multiple design alternatives. These low-fidelity representations are meant to be cheap and fast to produce, to allow quick exploration of the <i>design space</i> of the concept.
These representations are also often approximate to leave room for subjective interpretation and to stimulate imagination; in this sense, these representations can be considered <i>uncertain</i>.
As the concept gets more finalized, time and effort are invested in the production of more detailed and accurate
representations, such as high-fidelity 3D models suitable for simulation and fabrication. These
detailed models can also be used to create didactic instructions for assembly and usage.</p>
        <p>Producing these different representations of a concept requires specific skills in sketching, modeling, manufacturing and visual communication.
For these reasons, professional studios often employ different experts to produce the different representations of the same concept, at the cost of extensive discussions and numerous iterations between the actors of this process. The complexity of the multi-disciplinary skills involved in the design process also hinders their adoption by laymen.</p>
        <p>Existing solutions to facilitate design have focused on a subset of the representations used by designers.
However, no solution
considers all representations at once, for instance to directly convert a series of sketches
into a set of physical prototypes. In addition, all existing methods assume that the concept is unique
rather than ambiguous. As a result, rich information about the variability of the concept is
lost during each conversion step.</p>
        <p>We plan to facilitate design for professionals and laymen by adressing the following objectives:</p>
        <simplelist>
          <li id="uid10">
            <p noindent="true">We want to assist designers in the exploration of the <i>design space</i> that captures the possible variations of a concept. By considering a concept as a <i>distribution</i> of shapes and functionalities rather than a single object, our goal is to help designers consider multiple design alternatives more quickly and effectively. Such a representation should also allow designers to preserve multiple alternatives along all steps of the design process rather than committing to a single solution early on and pay the price of this decision for all subsequent steps. We expect that preserving alternatives will facilitate communication with engineers, managers and clients, accelerate design iterations and even allow mass personalization by the end consumers.</p>
          </li>
          <li id="uid11">
            <p noindent="true">We want to support the various representations used by designers during concept development. While drawings and 3D models have received significant attention in past Computer Graphics research, we will also account for the various forms of rough physical prototypes made to evaluate the shape and functionality of a concept. Depending on the task at hand, our algorithms will either analyse these prototypes to generate a virtual concept, or assist the creation of these prototypes from a virtual model. We also want to develop methods capable of adapting to the different drawing and manufacturing techniques used to create sketches and prototypes. We envision design tools that conform to the habits of users rather than impose specific techniques to them.</p>
          </li>
          <li id="uid12">
            <p noindent="true">We want to make professional design techniques available to novices. Affordable software, hardware and online instructions are democratizing technology and design, allowing small businesses and individuals to compete with large companies. New manufacturing processes and online interfaces also allow customers to participate in the design of an object via mass personalization. However, similarly to what happened for desktop publishing thirty years ago, desktop manufacturing tools need to be simplified to account for the needs and skills of novice designers. We hope to support this trend by adapting the techniques of professionals and by automating the tasks that require significant expertise.</p>
          </li>
        </simplelist>
      </subsection>
      <subsection id="cid2" level="2">
        <bodyTitle>Graphics with Uncertainty and Heterogeneous Content</bodyTitle>
        <p>Our research is motivated by the observation that traditional CG algorithms have not been designed to account for uncertain data. For example, global illumination rendering assumes accurate virtual models of geometry, light and materials to simulate light transport. While these algorithms produce images of high realism, capturing effects such as shadows, reflections and interreflections, they are not applicable to the growing mass of uncertain data available nowadays.</p>
        <p>The need to handle uncertainty in CG is timely and pressing, given the
large number of <i>heterogeneous sources of 3D content</i> that have become available
in recent years. These include data from cheap depth+image sensors (e.g., Kinect or the Tango),
3D reconstructions from image/video data, but also data from large 3D geometry
databases, or casual 3D models created using simplified sketch-based modeling tools.
Such alternate content has varying levels of <i>uncertainty</i> about the scene or
objects being modelled. This includes uncertainty in
geometry, but also in materials and/or lights – which are often not
even available with such content.
Since CG algorithms cannot be applied directly,
visual effects artists spend hundreds of hours correcting inaccuracies and completing the captured data to make them useable in film and advertising.</p>
        <object id="uid13">
          <table rend="inline">
            <tr style="">
              <td style="text-align:center;" halign="center">
                <ressource xlink:href="IMG/ibr.png" type="inline" width="209.23235pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
              <td style="text-align:center;" halign="center">
                <ressource xlink:href="IMG/ibr2.png" type="inline" width="209.23235pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
            <caption/>
          </table>
          <caption>Image-Based Rendering (IBR) techniques use input photographs and approximate 3D to produce new synthetic views.</caption>
        </object>
        <p>We identify a major scientific bottleneck which is the need to treat <i>heterogeneous</i>
content, i.e., containing both (mostly captured) uncertain and perfect, traditional content.
Our goal is to provide solutions to this bottleneck, by explicitly and formally modeling uncertainty
in CG, and to develop new algorithms that are capable of mixed rendering for this content.</p>
        <p>We strive to develop methods in which heterogeneous – and often uncertain – data can be handled automatically in CG with a principled methodology. Our main focus is on <i>rendering</i> in CG, including dynamic scenes (video/animations).</p>
        <p>Given the above, we need to address the following challenges:</p>
        <simplelist>
          <li id="uid14">
            <p noindent="true">Develop a theoretical model to handle uncertainty in computer graphics. We must define a new formalism that
inherently incorporates uncertainty, and must be able to express traditional CG rendering, both physically
accurate and approximate approaches. Most importantly, the new formulation must
elegantly handle mixed rendering of perfect synthetic data and captured uncertain content.
An important element of this goal is to incorporate <i>cost</i> in the choice of algorithm
and the optimizations used to obtain results, e.g., preferring solutions which may be slightly
less accurate, but cheaper in computation or memory.</p>
          </li>
          <li id="uid15">
            <p noindent="true">The development of rendering algorithms for heterogeneous content often requires preprocessing of image and video data, which sometimes also includes depth information. An example is the decomposition of images into intrinsic layers of reflectance and lighting, which is required to perform relighting. Such solutions are also useful as image-manipulation or computational photography techniques. The challenge will be to develop such “intermediate” algorithms for the uncertain and heterogeneous data we target.</p>
          </li>
          <li id="uid16">
            <p noindent="true">Develop efficient rendering algorithms for uncertain and heterogeneous content, reformulating rendering in a probabilistic setting where appropriate.
Such methods should allow us to develop approximate rendering algorithms using our formulation in a well-grounded manner.
The formalism should include probabilistic models of how the scene, the image and the data interact.
These models should be data-driven, e.g., building on the abundance of online geometry and image databases, domain-driven, e.g., based on requirements of the rendering algorithms or perceptually guided, leading to plausible solutions based on limitations of perception.</p>
          </li>
        </simplelist>
      </subsection>
    </subsection>
  </fondements>
  <highlights id="uid17">
    <bodyTitle>Highlights of the Year</bodyTitle>
    <subsection id="uid18" level="1">
      <bodyTitle>Highlights of the Year</bodyTitle>
      <p>In addition to publications in the leading conferences and journals in computer graphics (3 ACM Transactions on Graphics <ref xlink:href="#graphdeco-2016-bid0" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, <ref xlink:href="#graphdeco-2016-bid1" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, <ref xlink:href="#graphdeco-2016-bid2" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, 1 IEEE Virtual Reality), we made notable contributions to related fields such as human-computer interaction (1 ACM Conference on Human Factors in Computing Systems - CHI <ref xlink:href="#graphdeco-2016-bid3" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>) and computer vision (3 papers presented at the International Conference on 3DVision <ref xlink:href="#graphdeco-2016-bid4" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, <ref xlink:href="#graphdeco-2016-bid5" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, <ref xlink:href="#graphdeco-2016-bid6" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>). Several of these results were developed in the context of the CR-Play project, which was completed in November with excellent reviews.</p>
      <subsection id="uid19" level="2">
        <bodyTitle>Awards</bodyTitle>
        <p>Adrien Bousseau received a Young Researcher Award from the French National Research Agency (ANR) for the project ANR DRAO.</p>
        <p>Adrien Bousseau obtained an ERC Starting Grant funding, the project will start in February 2017.</p>
      </subsection>
    </subsection>
  </highlights>
  <logiciels id="uid20">
    <bodyTitle>New Software and Platforms</bodyTitle>
    <subsection id="uid21" level="1">
      <bodyTitle>SWARPI-Unity</bodyTitle>
      <p>SWARPI-Unity (for Superpixel Warp for Image-based rendering for Unity)</p>
      <p noindent="true">This is a software module developed in collaboration with Testaluna in the context of the CR-PLAY EU project. It involves an implementation of the Image-Based rendering algorithms of the group in the Unity3D framework. The software was improved this year to support mobile Android devices and was used in the evaluation step of the CR-PLAY project and for multiple demos.</p>
      <simplelist>
        <li id="uid22">
          <p noindent="true">Participants: Sebastien Bonopera, Jerome Esnault, George Drettakis and Gaurav Chaurasia</p>
        </li>
        <li id="uid23">
          <p noindent="true">Contact: George Drettakis</p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid24" level="1">
      <bodyTitle>SIBR</bodyTitle>
      <p>SIBR (for Simple Image-Based Rendering)</p>
      <p noindent="true">This is a framework containing libraries and tools used internally for research projects based on Image-Base Rendering. It includes both preprocessing tools (computing data used for rendering) and rendering utilities. This new framework replaces the previously used IBR-COMMON tools.</p>
      <simplelist>
        <li id="uid25">
          <p noindent="true">Participants: George Drettakis, Abdelaziz Djelouah, Rodrigo Ortiz Canyon, Theo Thonat, Sebastien Bonopera</p>
        </li>
        <li id="uid26">
          <p noindent="true">Contact: George Drettakis</p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid27" level="1">
      <bodyTitle>MVIIR</bodyTitle>
      <p>MVIIR (for Multi-View Image Intrinsic Images and Relighting)</p>
      <p noindent="true">This package is the software implementation of the intrinsic image algorithm of Duchêne et al.
It includes two libraries; one general-purpose that can be used to augment the functionalities of the previously mentioned SIBR framework, and another for specific logic concerning the relighting task. This package includes also programs to compute preprocess data required for the relighting of a dataset.</p>
      <simplelist>
        <li id="uid28">
          <p noindent="true">Participants: George Drettakis, Sebastien Bonopera, Adrien Bousseau</p>
        </li>
        <li id="uid29">
          <p noindent="true">Contact: George Drettakis</p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid30" level="1">
      <bodyTitle>SGTDGP</bodyTitle>
      <p>SGTDGP (for Synthetic Ground Truth Data Generation Platform)</p>
      <p noindent="true">We have started the development of a ground truth data generation platform
based on complex and realistically rendered scenes built in 3D modelling
packages such as 3DS Max. The platform includes an export module from
3DSMax with support for complex materials and shade trees such as those
developed for the physically based rendering VRay platform. This module
exports to the Mitsuba opensource renderer, and includes support for
various operations using Mitsuba, as well as rendering on the Inria cluster.
The platform is designed to generate ground truth data for learning as
well as data for ground truth comparisons for image-based rendering
projects in the group.</p>
      <simplelist>
        <li id="uid31">
          <p noindent="true">Participants: George Drettakis, George Kopanas, Sai Bangaru</p>
        </li>
        <li id="uid32">
          <p noindent="true">Contact: George Drettakis</p>
        </li>
      </simplelist>
    </subsection>
  </logiciels>
  <resultats id="uid33">
    <bodyTitle>New Results</bodyTitle>
    <subsection id="uid34" level="1">
      <bodyTitle>Computer-Assisted Design with Heterogeneous Representations</bodyTitle>
      <subsection id="uid35" level="2">
        <bodyTitle>How Novices Sketch and Prototype Hand-Fabricated Objects</bodyTitle>
        <participants>
          <person key="reves-2014-idm27888">
            <firstname>Adrien</firstname>
            <lastname>Bousseau</lastname>
          </person>
        </participants>
        <p>We are interested in how to create digital tools to support informal sketching and prototyping of physical objects by novices. Achieving this goal first requires a deeper understanding of how non-professional designers generate, explore, and communicate design ideas with traditional tools, i.e., sketches on paper and hands-on prototyping materials. We conducted a study framed around two all-day design charrettes where participants perform a complete design process: ideation sketching, concept development and presentation, fabrication planning documentation and collaborative fabrication of hand-crafted prototypes (Figure <ref xlink:href="#uid36" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>). This structure allows us to control key aspects of the design process while collecting rich data about creative tasks, including sketches on paper, physical models, and videos of collaboration discussions. Participants used a variety of drawing techniques to convey 3D concepts. They also extensively manipulated physical materials, such as paper, foam, and cardboard, both to support concept exploration and communication with design partners. Based on these observations, we propose design guidelines for CAD tools targeted at novice crafters.</p>
        <p>This work is a collaboration with Theophanis Tsandilas, Lora Oehlberg and Wendy Mackay from the ExSitu group, Inria Saclay. It has been published at ACM Conference on Human Factors in Computing Systems (CHI) 2016 <ref xlink:href="#graphdeco-2016-bid3" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
        <object id="uid36">
          <table>
            <tr>
              <td>
                <ressource xlink:href="IMG/fabsketch.png" type="float" width="427.0pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
          </table>
          <caption>We asked participants to design a costume, from an initial sketch to a physical prototype.</caption>
        </object>
      </subsection>
      <subsection id="uid37" level="2">
        <bodyTitle>Interactive Sketching of Urban Procedural Models</bodyTitle>
        <participants>
          <person key="reves-2014-idm27888">
            <firstname>Adrien</firstname>
            <lastname>Bousseau</lastname>
          </person>
        </participants>
        <p>3D modeling remains a notoriously difficult task for novices despite significant research effort to provide intuitive and automated systems. We tackle this problem by combining the strengths of two popular domains: sketch-based modeling and procedural modeling. On the one hand, sketch-based modeling exploits our ability to draw but requires detailed, unambiguous drawings to achieve complex models. On the other hand, procedural modeling automates the creation of precise and detailed geometry but requires the tedious definition and parameterization of procedural models. Our system uses a collection of simple procedural grammars, called snippets, as building blocks to turn sketches into realistic 3D models. We use a machine learning approach to solve the inverse problem of finding the procedural model that best explains a user sketch. We use non-photorealistic rendering to generate artificial data for training convolutional neural networks capable of quickly recognizing the procedural rule intended by a sketch and estimating its parameters. We integrate our algorithm in a coarse-to-fine urban modeling system that allows users to create rich buildings by successively sketching the building mass, roof, facades, windows, and ornaments (Figure <ref xlink:href="#uid38" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>). A user study shows that by using our approach non-expert users can generate complex buildings in just a few minutes.</p>
        <p>This work is a collaboration with Gen Nishida, Ignacio Garcia-Dorado, Daniel G. Aliaga and Bedrich Benes from Purdue University. It has been published at ACM Transactions on Graphics (proc. SIGGRAPH) 2016 <ref xlink:href="#graphdeco-2016-bid2" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
        <object id="uid38">
          <table>
            <tr>
              <td>
                <ressource xlink:href="IMG/procedural_sketch.jpg" type="float" width="256.2026pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
          </table>
          <caption>Our system allows novices to quickly create complex procedural 3D models of buildings by sketching.</caption>
        </object>
      </subsection>
      <subsection id="uid39" level="2">
        <bodyTitle>Fidelity vs. Simplicity: a Global Approach to Line Drawing Vectorization</bodyTitle>
        <participants>
          <person key="reves-2014-idm27888">
            <firstname>Adrien</firstname>
            <lastname>Bousseau</lastname>
          </person>
        </participants>
        <p>Vector drawing is a popular representation in graphic design because
of the precision, compactness and editability offered by parametric curves.
However, prior work on line drawing vectorization focused solely on faithfully
capturing input bitmaps, and largely overlooked the problem of producing a
compact and editable curve network. As a result, existing algorithms tend to
produce overly-complex drawings composed of many short curves and
control points, especially in the presence of thick or sketchy lines
that yield spurious curves at junctions. We propose the first vectorization
algorithm that explicitly balances fidelity to the input bitmap with simplicity
of the output, as measured by the number of curves and their degree.
By casting this trade-off as a global optimization, our algorithm generates
few yet accurate curves, and also disambiguates curve topology at junctions
by favoring the simplest interpretations overall. We demonstrate the robustness
of our algorithm on a variety of drawings, sketchy cartoons
and rough design sketches (Figure <ref xlink:href="#uid40" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>).</p>
        <p>The first author of this work, Jean-Dominique Favreau, is co-advised by Adrien Bousseau and Florent Lafarge (Titane team).
The work was published at ACM Transactions on Graphics (proc. SIGGRAPH) 2016 <ref xlink:href="#graphdeco-2016-bid0" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
        <object id="uid40">
          <table>
            <tr>
              <td>
                <ressource xlink:href="IMG/fidelity_teaser.png" type="float" width="427.0pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
          </table>
          <caption>Rough sketches often contain overlapping strokes (a), which existing
vectorization algorithms represent as
multiple curves (b).
Pre-filtering the drawing improves the vectorization,
but produces spurious curve segments at junctions (c).
Since existing algorithms analyze junctions locally,
they cannot recover the proper topology of these seemingly similar line
configurations. By adopting a global formulation that optimizes
for both fidelity to the input sketch and simplicity of the output
curve network, our algorithm recovers proper topology while
significantly reducing the overall number of curves and control points.
Design sketch after Sori Yanagi's ”Butterfly” stool.</caption>
        </object>
      </subsection>
      <subsection id="uid41" level="2">
        <bodyTitle>SketchSoup: Exploratory Ideation using Design Sketches</bodyTitle>
        <participants>
          <person key="reves-2014-idm27888">
            <firstname>Adrien</firstname>
            <lastname>Bousseau</lastname>
          </person>
        </participants>
        <p>A hallmark of early stage design is a number of quick-and-dirty sketches capturing design inspirations, model variations, and alternate viewpoints of a visual concept. We present SketchSoup, a workflow that allows designers to explore the design space induced by such sketches. We take an unstructured collection of drawings as input, register them using a multi-image matching algorithm, and present them as a 2D interpolation space (Figure <ref xlink:href="#uid42" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>).
By morphing sketches in this space, our approach produces plausible visualizations of shape and viewpoint variations despite the presence of sketch distortions that would prevent standard camera calibration and 3D reconstruction. In addition, our interpolated sketches can serve as inspiration for further drawings, which feed back into the design space as additional image inputs.
SketchSoup thus fills a significant gap in the early ideation stage of conceptual design by allowing designers to make better informed choices before proceeding to more expensive 3D modeling and prototyping. From a technical standpoint, we describe an end-to-end system that judiciously combines and adapts various image processing techniques to the drawing domain – where the images are dominated not by color, shading and texture, but by sketchy stroke contours.</p>
        <p>This work is a collaboration with Rahul Arora and Karan Singh from University of Toronto and Vinay P. Namboodiri from IIT Kampur. The project was initiated while Rahul Arora was an intern in our group. It will be published in Computer Graphics Forum in 2017.</p>
        <object id="uid42">
          <table>
            <tr>
              <td>
                <ressource xlink:href="IMG/sketch_soup.png" type="float" width="427.0pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
          </table>
          <caption>SketchSoup takes an unstructured set of sketches as input, along with a small number of correspondences (shown as red dots), registers the sketches and embeds them into a 2D interpolation space based on their shape differences. Users can explore the interpolation space to generate novel sketches.</caption>
        </object>
      </subsection>
      <subsection id="uid43" level="2">
        <bodyTitle>Modeling Symmetric Developable Surfaces from a Single Photo</bodyTitle>
        <participants>
          <person key="graphdeco-2016-idp153648">
            <firstname>Amelie</firstname>
            <lastname>Fondevilla</lastname>
          </person>
          <person key="reves-2014-idm27888">
            <firstname>Adrien</firstname>
            <lastname>Bousseau</lastname>
          </person>
        </participants>
        <p>We propose to reconstruct 3D developable surfaces from a single 2D drawing traced and annotated over a side-view photo of a partially symmetrical object (Figure <ref xlink:href="#uid44" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>). Our reconstruction algorithm combines symmetry and orthogonality shapes cues within a unified optimization framework that solves for the 3D position of the Bézier control points of the drawn curves while being tolerant to drawing inaccuracy and perspective distortions. We then rely on existing surface optimization methods to produce a developable surface that interpolates our 3D curves. Our method is particularly well suited for the modeling and fabrication of fashion items as it converts the input drawing into flattened developable patterns ready for sewing.</p>
        <p>This work is a collaboration with Damien Rohmer, Stefanie Hahmann and Marie-Paule Cani from the Imagine team (LJK/ Inria Grenoble Rhône Alpes). This work was presented at the AFIG French conference in November 2016, where it received the 3rd price for best student work.</p>
        <object id="uid44">
          <table>
            <tr>
              <td>
                <ressource xlink:href="IMG/developable_sketch.png" type="float" width="427.0pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
          </table>
          <caption>Our method reconstructs a 3D mesh and 2D pattern pieces of a sewed object from a single annotated drawing.</caption>
        </object>
      </subsection>
      <subsection id="uid45" level="2">
        <bodyTitle>DeepSketch: Sketch-Based Modeling using Deep Volumetric Prediction</bodyTitle>
        <participants>
          <person key="graphdeco-2015-idp66104">
            <firstname>Johanna</firstname>
            <lastname>Delanoy</lastname>
          </person>
          <person key="reves-2014-idm27888">
            <firstname>Adrien</firstname>
            <lastname>Bousseau</lastname>
          </person>
        </participants>
        <p>Drawing is the most direct way for people to express their visual thoughts. However, while humans are extremely good are perceiving 3D objects from line drawings, this task remains very challenging for computers as many 3D shapes can yield the same drawing.
Existing sketch-based 3D modeling systems rely on heuristics to reconstruct simple shapes, require extensive user interaction, or exploit specific drawing techniques and shape priors. Our goal is to lift these restrictions and offer a minimal interface to quickly model general 3D shapes with contour drawings. While our approach can produce approximate 3D shapes from a single drawing, it achieves its full potential once integrated into an interactive modeling system, which allows users to visualize the shape and refine it by drawing from several viewpoints.
At the core of our approach is a deep convolutional neural network (CNN) that processes a line drawing to predict occupancy in a voxel grid. The use of deep learning results in a flexible and robust 3D reconstruction engine that allows us to treat sketchy bitmap drawings without requiring complex, hand-crafted optimizations. While similar architectures have been proposed in the computer vision community, our originality is to extend this architecture to a multiview context by training an updater network that iteratively refines the prediction as novel drawings are provided</p>
        <p>This work is a collaboration with Mathieu Aubry from Ecole des Ponts ParisTech and Alexei Efros and Philip Isola from UC Berkeley. It is supported by the CRISP Inria associate team.</p>
      </subsection>
    </subsection>
    <subsection id="uid46" level="1">
      <bodyTitle>Graphics with Uncertainty and Heterogeneous Content</bodyTitle>
      <subsection id="uid47" level="2">
        <bodyTitle>Cotemporal Multi-View Video Segmentation</bodyTitle>
        <participants>
          <person key="morpheo-2014-idp109816">
            <firstname>Abdelaziz</firstname>
            <lastname>Djelouah</lastname>
          </person>
          <person key="reves-2014-idm29368">
            <firstname>George</firstname>
            <lastname>Drettakis</lastname>
          </person>
        </participants>
        <p>We address the problem of multi-view video segmentation
of dynamic scenes in general and outdoor environments
with possibly moving cameras. Multi-view methods for dynamic
scenes usually rely on geometric calibration to impose
spatial shape constraints between viewpoints. In this
work, we show that the calibration constraint can be relaxed
while still getting competitive segmentation results
using multi-view constraints. We introduce new multi-view
cotemporality constraints through motion correlation cues,
in addition to common appearance features used by cosegmentation
methods to identify co-instances of objects.
We also take advantage of learning based segmentation
strategies by casting the problem as the selection of monocular
proposals that satisfy multi-view constraints. This
yields a fully automated method that can segment subjects
of interest without any particular
pre-processing stage (see Fig. <ref xlink:href="#uid48" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>).
Results on several challenging outdoor datasets demonstrate
the feasibility and robustness of our approach.</p>
        <p>This work is a collaboration with Jean-Sébastien Franco and Edmond Boyer from
Morpheo team at Inria Grenoble, and Patrick Pérez from Technicolor.
The work has been published in the International Conference
on 3D Vision (3DV) - 2016 <ref xlink:href="#graphdeco-2016-bid6" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
        <object id="uid48">
          <table>
            <tr>
              <td>
                <ressource xlink:href="IMG/aziz_mvseg.png" type="float" width="427.0pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
          </table>
          <caption>Synchronized videos of the same scene are partitioned into short clips. At a small number of instants
where motion is sufficiently informative, cross-view correspondences between with similar appearance
and motion are obtained by graph matching. In each view, matched superpixels,
which are likely to lie on moving foreground objects, are used as sparse anchors to guide
the selection process among a large pool of moving objects proposals extracted from all clips.</caption>
        </object>
      </subsection>
      <subsection id="uid49" level="2">
        <bodyTitle>Automatic 3D Car Model Alignment for Mixed Image-Based Rendering</bodyTitle>
        <participants>
          <person key="reves-2014-idp75400">
            <firstname>Rodrigo</firstname>
            <lastname>Ortiz Cayon</lastname>
          </person>
          <person key="morpheo-2014-idp109816">
            <firstname>Abdelaziz</firstname>
            <lastname>Djelouah</lastname>
          </person>
          <person key="reves-2014-idm29368">
            <firstname>George</firstname>
            <lastname>Drettakis</lastname>
          </person>
        </participants>
        <p>We present a method that improves quality of Image-Based Rendering of poorly reconstructed objects. We focus on the case of reflective objects which are hard to reconstruct, such as cars. The key insight is to replace these poorly reconstructed objects with models from existing rich 3D CAD databases, and subsequently align them to the input images. We use deep learning-based algorithms to obtain the 3D model present in the databases which is closest to the object seen in the images. We formulate two optimizations using all available information to finely position and orient the model and adapt it to image contours (see Fig. <ref xlink:href="#uid50" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>(1.)).
Our method provides much higher quality rendering results of such objects compared to previous solutions as seen in Fig. <ref xlink:href="#uid50" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>(2.(b)).</p>
        <p>This work is a collaboration with Francisco Massa and Mathieu Aubry from École des Ponts ParisTech.
The work was published in the International Conference in 3D Vision (3DV) - 2016 <ref xlink:href="#graphdeco-2016-bid5" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
        <object id="uid50">
          <table>
            <tr>
              <td>
                <ressource xlink:href="IMG/overview_car.png" type="float" width="256.2026pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
          </table>
          <caption>Overview of our pre-processing and rendering pipeline for Mixed Image-Based Rendering.</caption>
        </object>
      </subsection>
      <subsection id="cid3" level="2">
        <bodyTitle>Multi-View Inpainting for Image-Based Scene Editing and Rendering</bodyTitle>
        <participants>
          <person key="graphdeco-2015-idp70872">
            <firstname>Theo</firstname>
            <lastname>Thonat</lastname>
          </person>
          <person key="reves-2014-idm29368">
            <firstname>George</firstname>
            <lastname>Drettakis</lastname>
          </person>
        </participants>
        <p>We propose a method to remove objects such as people and cars
from multi-view urban image datasets (Figure <ref xlink:href="#uid51" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>), enabling free-viewpoint Image-Based Rendering (IBR)
in the edited scenes.
Our method combines information from multi-view 3D reconstruction with
image inpainting techniques, by formulating the problem as an
optimization of a global patch-based objective function.
We use IBR techniques to reproject
information from neighboring views, and 3D multi-view stereo reconstruction
to perform multi-view coherent initialization for inpainting
of pixels not filled by reprojection.
Our algorithm performs multi-view consistent inpainting
for color and 3D by blending reprojections with
patch-based image inpainting.
We run our algorithm on casually captured datasets, and Google Street View data, removing objects such as cars, people and pillars, showing
that our approach produces results of sufficient quality for
free-viewpoint IBR on “cleaned up” scenes, as well as
IBR scene editing, such as limited displacement of real objects.</p>
        <p>This work is a collaboration with Eli Shechtman and Sylvain Paris from Adobe Research.
It has been published in the International Conference on 3D Vision (3DV) - 2016 <ref xlink:href="#graphdeco-2016-bid4" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
        <object id="uid51">
          <table>
            <tr>
              <td>
                <ressource xlink:href="IMG/inpainting.png" type="float" width="427.0pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
          </table>
          <caption>Our system takes as input a set of images from the same scene (top row). The method then removes all the vehicules in a multi-view coherent way (bottom row).</caption>
        </object>
      </subsection>
      <subsection id="cid4" level="2">
        <bodyTitle>Gaze Prediction using Machine Learning for Dynamic Stereo Manipulation in Games</bodyTitle>
        <participants>
          <person key="reves-2014-idp85432">
            <firstname>George</firstname>
            <lastname>Koulieris</lastname>
          </person>
          <person key="reves-2014-idm29368">
            <firstname>George</firstname>
            <lastname>Drettakis</lastname>
          </person>
        </participants>
        <p>Comfortable, high-quality 3D stereo viewing has become a requirement for interactive applications. Previous research shows that manipulating disparity can alleviate some of the discomfort caused by 3D stereo, but it is best to do this locally, around the object the user is gazing at. The main challenge is thus to develop a gaze predictor in the demanding context of real-time, heavily task-oriented applications such as games. Our key observation is that player actions are highly correlated with the present state of a game, encoded by game variables. Based on this, we trained a classifier to learn these correlations using an eye-tracker which provides the ground-truth object being looked at. The classifier is used at runtime to predict object category – and thus gaze – during game play, based on the current state of game variables. We used this prediction to propose a dynamic disparity manipulation method, which provided rich and comfortable depth. We evaluated the quality of our gaze predictor numerically and experimentally, showing that it predicts gaze more accurately than previous approaches. A subjective rating study demonstrates that our localized disparity manipulation is preferred over previous methods.</p>
        <p>This work is a collaboration with Katerina Mania from the Technical University of Crete and Douglas Cunningham from the Technical University of Cottbus.
The work was presented at the IEEE conference for Virtual Reality (IEEE VR) 2016 <ref xlink:href="#graphdeco-2016-bid7" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
        <object id="uid52">
          <table>
            <tr>
              <td>
                <ressource xlink:href="IMG/pSense.png" type="float" width="427.0pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
          </table>
          <caption>We propose a gaze predictor (a), used to perform localized stereo grading (b). We compare to no stereo grading (c) and prior work (d). We provide rich and comfortable depth. Please use red/cyan anaglyph glasses.</caption>
        </object>
      </subsection>
      <subsection id="cid5" level="2">
        <bodyTitle>A Feasibility Study with Image-Based Rendered Virtual Reality in Patients with Mild Cognitive Impairment</bodyTitle>
        <participants>
          <person key="reves-2014-idm29368">
            <firstname>George</firstname>
            <lastname>Drettakis</lastname>
          </person>
        </participants>
        <p>Virtual Reality (VR) has emerged as a promising tool in many domains of therapy and rehabilitation, and has recently attracted the attention of researchers and clinicians working with
elderly people with MCI, Alzheimer's disease and related disorders.
In this study we tested the feasibility of using highly realistic image-based rendered VR with patients
with MCI and dementia. We designed an attentional task to train selective and sustained
attention, and we tested a VR and a paper version of this task (see Fig. <ref xlink:href="#uid53" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>) in a single-session within-subjects design. Results showed that participants with MCI and dementia reported to be highly
satisfied and interested in the task, and they reported high feelings of security, low discomfort, anxiety and fatigue. In addition, participants reported a preference for the VR condition
compared to the paper condition, even if the task was more difficult. Interestingly, apathetic
participants showed a preference for the VR condition stronger than that of non-apathetic
participants. These findings suggest that VR-based training can be considered as an interesting tool to improve adherence to cognitive training for elderly people with cognitive
impairment.</p>
        <p>This work was a collaboration with EA CoBTek/IA, CMRR (memory center) of the CHU (University Hospital) of Nice, Disney Research and Trinity College Dublin, as part of the (completed) VERVE EU project. The work was published in the PLoS ONE journal <ref xlink:href="#graphdeco-2016-bid8" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
        <object id="uid53">
          <table>
            <tr>
              <td>
                <ressource xlink:href="IMG/feasibility.jpg" type="float" width="256.2026pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
          </table>
          <caption>The VR and paper conditions of the study</caption>
        </object>
      </subsection>
      <subsection id="cid6" level="2">
        <bodyTitle>Scalable Inside-Out Image-Based Rendering</bodyTitle>
        <participants>
          <person key="reves-2014-idm29368">
            <firstname>George</firstname>
            <lastname>Drettakis</lastname>
          </person>
        </participants>
        <p>The goal of this project was to provide high-quality free-viewpoing rendering of indoors environments.
captured with off-the-shelf equipment such as a high-quality color camera and a commodity depth sensor.
Image-based Rendering (IBR) can provide the realistic imagery required at real-time speed.
For indoor scenes however, two challenges are especially prominent.
First, the reconstructed 3D geometry must be compact, but faithful enough to respect occlusion relationships when viewed up close.
Second, man-made materials call for view-dependent texturing, but using too many input photographs reduces performance.</p>
        <p>We customize a typical RGB-D 3D surface reconstruction pipeline to produce a coarse global 3D surface, and local, per-view geometry for each input image.
Our tiled IBR preserves quality by economizing on the expected contributions that entire groups of input pixels make to a final image.
The two components are designed to work together, giving real-time performance, while hardly sacrificing quality.
Testing on a variety of challenging scenes shows that our inside-out IBR scales favorably with the number of input images.</p>
        <p>This work was a collaboration with P. Hedman, G. Brostow and T. Ritschel at UCL, as part
of the CR-PLAY project. It was published in ACM Transactions on Graphics (Proc. SIGGRAPH Asia) <ref xlink:href="#graphdeco-2016-bid1" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
        <object id="uid54">
          <table>
            <tr>
              <td>
                <ressource xlink:href="IMG/inside_out.jpg" type="float" width="427.0pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
          </table>
          <caption>Images from our method rendered in 1080p at 55 Hz on an Nvidia Titan X GPU. Input is an RGB-D video and 298 high-quality photos of `Dr Johnson's house', London. With no wheelchair access to this floor, curators were keen to have their rooms digitized.</caption>
        </object>
      </subsection>
      <subsection id="cid7" level="2">
        <bodyTitle>Measuring Accommodation and Comfort in Head-Mounted Displays</bodyTitle>
        <participants>
          <person key="reves-2014-idp85432">
            <firstname>George</firstname>
            <lastname>Koulieris</lastname>
          </person>
          <person key="reves-2014-idm29368">
            <firstname>George</firstname>
            <lastname>Drettakis</lastname>
          </person>
        </participants>
        <p>Head-mounted displays (HMDs) are rapidly becoming the preferred display for stereo viewing in virtual environments, but they often cause discomfort and even sickness. Previous studies have shown that a major cause of these adverse symptoms is the vergence-accommodation (VA) conflict. Specifically, the eyes use the distance to the screen to accommodate, while they use the distance to the fixated virtual object to converge. The VA conflict is the difference between those distances. The magnitude of the conflict is well correlated with subjective reports of discomfort. Many methods have been proposed for reducing the VA conflict and thereby reducing discomfort by making accommodation more consistent with vergence. But no one has actually measured accommodation in HMDs to see how well a given method is able to drive it to the desired distance. We built the first device for measuring accommodation in an HMD, using a modular design with off-the-shelf components, focus-adjustable lenses, and an autorefractor. We conducted experiments using the device to determine how well accommodation is driven with various combinations of HMD properties and viewing conditions: focus-adjustable lenses, depth-of-field rendering, binocular viewing, and “monovision" (setting the two eyes' focal distances to quite different values). We found that focus-adjustable lenses drive accommodation appropriately across many conditions. The other techniques were much less effective in driving accommodation. We also investigated whether the ability to drive accommodation predicts viewer comfort. We did this by conducting a discomfort study with most of the conditions in the accommodation study. We found that the ability to drive accommodation did in fact predict the amount of discomfort. Specifically, the most comfortable conditions were the ones that generated accommodation consistent with vergence. Together, these results illustrate the potential benefit of focus-adjustable lenses: They enable stimulation of accommodation and thereby comfortable viewing. In contrast, monovision neither enable accurate accommodation nor comfortable viewing.</p>
        <p>This work is an ongoing collaboration with Martin S. Banks from UC Berkeley, in the context of the CRISP Inria associate team.</p>
      </subsection>
      <subsection id="cid8" level="2">
        <bodyTitle>Beyond Gaussian Noise-Based Texture Synthesis</bodyTitle>
        <participants>
          <person key="reves-2014-idp79168">
            <firstname>Kenneth</firstname>
            <lastname>Vanhoey</lastname>
          </person>
          <person key="graphdeco-2015-idp85992">
            <firstname>Georgios</firstname>
            <lastname>Kopanas</lastname>
          </person>
          <person key="reves-2014-idm29368">
            <firstname>George</firstname>
            <lastname>Drettakis</lastname>
          </person>
        </participants>
        <p>Texture synthesis methods based on noise functions have many nice properties: they are continuous (thus resolution-independent), infinite (can be evaluated at any point) and compact (only functional parameters need to be stored).
A good method is also non-repetitive and aperiodic.
Current techniques, like Gabor Noise, fail to produce structured content.
They are limited to so-called “Gaussian textures”, characterized by second-order statistics like mean and variance only.
This is suitable for noise-like patterns (e.g., marble, wood veins, sand) but not for structured ones (e.g., brick
wall, mountain rocks, woven yarn).
Other techniques, like Local Random-Phase noise, leverage some structure but as a trade-off with repetitiveness and periodicity.</p>
        <p>In this project, we model higher-order statistics produced by noise functions in a parametric model.
Then we define an algorithm for sampling of the noise functions' parameters so as to produce a texture that meets prescribed statistics.
This sampling ensures both the reproduction of higher-order visual features with high probability, like edges and ridges, and non-repetitiveness plus aperiodicity thanks to the stochastic sampling method.
Moreover a (deep) learning algorithm has been established to infer the prescribed statistics from an input examplar image.</p>
        <p>This project is a collaboration with Ian H. Jermyn (Durham University, UK, former Inria) and Mathieu Aubry (ENPC, France).</p>
      </subsection>
      <subsection id="cid9" level="2">
        <bodyTitle>Fences in Image Based Rendering</bodyTitle>
        <participants>
          <person key="morpheo-2014-idp109816">
            <firstname>Abdelaziz</firstname>
            <lastname>Djelouah</lastname>
          </person>
          <person key="graphdeco-2016-idp111504">
            <firstname>Frederic</firstname>
            <lastname>Durand</lastname>
          </person>
          <person key="reves-2014-idm29368">
            <firstname>George</firstname>
            <lastname>Drettakis</lastname>
          </person>
        </participants>
        <p>One of the key problem in Image Based Rendering (IBR) methods
is the rendering of regions with incorrect 3D reconstruction.
Some methods try to overcome the issue in the case of reflections
and transparencies trough the estimation of two planes
or the usage of 3D stock models. Fences with their thin repetitive
structures are another important common source of 3D reconstruction
errors but have received very little attention in the context of
image based rendering. They are present in most urban pictures
and represent a standard failure case for reconstruction algorithms,
and state of the art rendering methods exhibit strong artifacts.</p>
        <p>In this project, we propose to detect and segment fences in urban
pictures for IBR applications. Similarly to related
methods in image <i>de-fencing</i>, we use the assumptions that
fences are thin repetitive structures lying on a 3D plane.
To address this problem we consider the following steps:
First we propose a multi-view approach to estimate the plane
supporting the fences using repetition candidates.
Second, we estimate image matting taking advantage of the multi-view constraints and the repetitive patterns.
Finally, the estimated 3D plane and matting masks are
used in a new rendering algorithm.</p>
      </subsection>
      <subsection id="cid10" level="2">
        <bodyTitle>Handling reflections in Image-Based Rendering</bodyTitle>
        <participants>
          <person key="graphdeco-2015-idp70872">
            <firstname>Theo</firstname>
            <lastname>Thonat</lastname>
          </person>
          <person key="graphdeco-2016-idp111504">
            <firstname>Frederic</firstname>
            <lastname>Durand</lastname>
          </person>
          <person key="reves-2014-idm29368">
            <firstname>George</firstname>
            <lastname>Drettakis</lastname>
          </person>
        </participants>
        <p>In order to render new viewpoints, current Image Based Rendering techniques (IBR) use an approximate geometry to warp and blend photographs from close viewpoints.
They assume the scene materials are diffuse, so geometry colors are independent of the viewpoint, an assumption that fails in the case of specular surfaces such as windows.
Dealing with reflections in an IBR context first requires identifying what are the diffuse and the specular color layers in the input images. The challenge is then to correctly warp the specular layers since their associated geometry is not available and since the normals of the reflective surfaces might be not reliable.</p>
      </subsection>
    </subsection>
  </resultats>
  <contrats id="uid55">
    <bodyTitle>Bilateral Contracts and Grants with Industry</bodyTitle>
    <subsection id="uid56" level="1">
      <bodyTitle>Bilateral Contracts with Industry</bodyTitle>
      <subsection id="uid57" level="2">
        <bodyTitle>Optis</bodyTitle>
        <participants>
          <person key="graphdeco-2016-idp123936">
            <firstname>Valentin</firstname>
            <lastname>Deschaintre</lastname>
          </person>
          <person key="reves-2014-idm27888">
            <firstname>Adrien</firstname>
            <lastname>Bousseau</lastname>
          </person>
          <person key="reves-2014-idm29368">
            <firstname>George</firstname>
            <lastname>Drettakis</lastname>
          </person>
        </participants>
        <p>Valentin Deschaintre is starting a CIFRE PhD in collaboration with Optis, a company specialized in material acquisition and rendering.</p>
      </subsection>
    </subsection>
    <subsection id="uid58" level="1">
      <bodyTitle>Bilateral Grants with Industry</bodyTitle>
      <subsection id="uid59" level="2">
        <bodyTitle>Technicolor</bodyTitle>
        <participants>
          <person key="reves-2014-idm29368">
            <firstname>George</firstname>
            <lastname>Drettakis</lastname>
          </person>
          <person key="reves-2014-idm27888">
            <firstname>Adrien</firstname>
            <lastname>Bousseau</lastname>
          </person>
        </participants>
        <p>We have initiated a collaboration with Technicolor on the use of deep
learning for computational photography and video tasks. This will
involve the use of our synthetic ground truth data generation platform (see Sec. <ref xlink:href="#uid30" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>)
to tasks such as color grading and white balance. This is a collaboration
with P. Pérez and E. Reinhard of Technicolor.</p>
      </subsection>
    </subsection>
  </contrats>
  <partenariat id="uid60">
    <bodyTitle>Partnerships and Cooperations</bodyTitle>
    <subsection id="uid61" level="1">
      <bodyTitle>National Initiatives</bodyTitle>
      <subsection id="uid62" level="2">
        <bodyTitle>ANR</bodyTitle>
        <subsection id="uid63" level="3">
          <bodyTitle>ANR DRAO</bodyTitle>
          <participants>
            <person key="reves-2014-idm27888">
              <firstname>Adrien</firstname>
              <lastname>Bousseau</lastname>
            </person>
          </participants>
          <p>
            <ref xlink:href="https://www-sop.inria.fr/members/Adrien.Bousseau/drao/" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>www-sop.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>members/<allowbreak/>Adrien.<allowbreak/>Bousseau/<allowbreak/>drao/</ref>
          </p>
          <p>The ANR DRAO is a young researcher project coordinated by Adrien Bousseau,
in collaboration with the InSitu project team at Inria Saclay - Ile de France (W. Mackay and T. Tsandilas)
and the MANAO project team (P. Barla and G. Guennebaud) and POTIOC project team (M. Hachet) at Inria Bordeaux - Sud Ouest.
The goal of this collaboration is to develop novel drawing tools
for amateurs as well as for expert designers and illustrators, combining expertise in
Computer Graphics (REVES and MANAO) and Human-Computer Interaction (InSitu, POTIOC).
This ANR project funds the PhD of Emmanuel Iarussi.</p>
          <p>While the ANR DRAO ended in 2015, it has resulted in a publication at ACM Conference on Human Factors in Computing Systems (CHI) 2016 <ref xlink:href="#graphdeco-2016-bid3" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.
Adrien Bousseau received an ANR Young Researcher Award for coordinating this project.</p>
        </subsection>
        <subsection id="uid64" level="3">
          <bodyTitle>ANR SEMAPOLIS</bodyTitle>
          <participants>
            <person key="reves-2014-idm29368">
              <firstname>George</firstname>
              <lastname>Drettakis</lastname>
            </person>
            <person key="morpheo-2014-idp109816">
              <firstname>Abdelaziz</firstname>
              <lastname>Djelouah</lastname>
            </person>
            <person key="graphdeco-2015-idp70872">
              <firstname>Theo</firstname>
              <lastname>Thonat</lastname>
            </person>
          </participants>
          <p>This ANR project started in October 2013. The goal is to use semantic information to improve
urban reconstruction and rendering. The consortium is led by ENPC (R. Marlet) and includes
the Inria Willow team and the GREY-C laboratory on image processing. Our contribution
will be in the rendering of urban models, in particular using image-based rendering algorithms.</p>
          <p>This year, the ANR SEMAPOLIS resulted in five publications on multi-view segmentation <ref xlink:href="#graphdeco-2016-bid6" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>,
multi-view inpainting <ref xlink:href="#graphdeco-2016-bid4" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, image-based rendering of cars <ref xlink:href="#graphdeco-2016-bid5" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> and interiors <ref xlink:href="#graphdeco-2016-bid1" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>,
and procedural modeling of buildings <ref xlink:href="#graphdeco-2016-bid2" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.
Two of these projects rely on a deep learning method from the ENPC group to identify semantic object categories in images (e.g., cars, people etc.) <ref xlink:href="#graphdeco-2016-bid4" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, <ref xlink:href="#graphdeco-2016-bid5" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. In <ref xlink:href="#graphdeco-2016-bid5" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> we also collaborated with the ENPC group to use a deep learning method to
allow the use of rendered images to identify objects in photographs. These collaborations have been extremely fruitful for our group, and have opened the
way to several new collaborations with ENPC.</p>
        </subsection>
      </subsection>
    </subsection>
    <subsection id="uid65" level="1">
      <bodyTitle>European Initiatives</bodyTitle>
      <subsection id="uid66" level="2">
        <bodyTitle>FP7 &amp; H2020 Projects</bodyTitle>
        <subsection id="uid67" level="3">
          <bodyTitle>CR-PLAY – Capture Reconstruct Play</bodyTitle>
          <p>
            <ref xlink:href="http://www.cr-play.eu" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>www.<allowbreak/>cr-play.<allowbreak/>eu</ref>
          </p>
          <sanspuceslist>
            <li id="uid68">
              <p noindent="true">Type: COOPERATION (ICT)</p>
            </li>
            <li id="uid69">
              <p noindent="true">Instrument: Specific Targeted Research Project</p>
            </li>
            <li id="uid70">
              <p noindent="true">Objectif: Creativity</p>
            </li>
            <li id="uid71">
              <p noindent="true">Duration: November 2013 - October 2016</p>
            </li>
            <li id="uid72">
              <p noindent="true">Coordinator: Testaluna SA (IT)</p>
            </li>
            <li id="uid73">
              <p noindent="true">Partner: TU Darmstadt (DE), UC London (UK), U. Patras (GR), Miniclip UK, Cursor Oy (FI)</p>
            </li>
            <li id="uid74">
              <p noindent="true">Inria contact: George Drettakis</p>
            </li>
            <li id="uid75">
              <p noindent="true">Abstract:
The goal of this project is to use image- and video-based rendering and relighting techniques
in the context of games and in particular mobile or casual games.
The computer graphics and vision partners (UCL, TUD) are leaders in their
fields, and have developed algorithms allowing easy capture of scenes using images
and video, and reconstruction using vision algorithms. UCL and Inria have developed
image- and video-based rendering algorithms which can be useful for games.
These tools need to be perfected, reducing artifacts and difficulty of use
so that they can be useful and productive for games companies.
For evaluation, the HCI lab of the University of Patras will provide cutting-edge
methodologies to make the resulting systems useable.
The consortium is led by the games company Testaluna, based in Genova Italy.
Other industrial partners include Cursor Oy (a regional group of games companies in Finland, which is
a leader in Europe in Casual games) and Miniclip, which is one of the major players in the
online game market.</p>
              <p>This year we had four results related to CR-PLAY on multi-view segmentation <ref xlink:href="#graphdeco-2016-bid6" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, multi-view inpainting <ref xlink:href="#graphdeco-2016-bid4" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, image-based rendering of cars <ref xlink:href="#graphdeco-2016-bid5" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> and indoors <ref xlink:href="#graphdeco-2016-bid1" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.
The work on indoors rendering was led by the CR-PLAY partner UCL.
The CR-PLAY project ended in October, and was successfully evaluated in November.
The project has resulted in a number of technological developments related to the Ph.D.
work of R. Ortiz-Cayon and T. Thonat, as well as the postdoc of A. Djelouah which
will be the object of a market study in the goal of a technology transfer.</p>
            </li>
          </sanspuceslist>
        </subsection>
        <subsection id="uid76" level="3">
          <bodyTitle>EMOTIVE</bodyTitle>
          <sanspuceslist>
            <li id="uid77">
              <p noindent="true">Type: COOPERATION (ICT)</p>
            </li>
            <li id="uid78">
              <p noindent="true">Instrument: Reseach Innovation Action</p>
            </li>
            <li id="uid79">
              <p noindent="true">Objectif: Virtual Heritage</p>
            </li>
            <li id="uid80">
              <p noindent="true">Duration: November 2016 - October 2019</p>
            </li>
            <li id="uid81">
              <p noindent="true">Coordinator: EXUS SA (UK)</p>
            </li>
            <li id="uid82">
              <p noindent="true">Partner: Diginext (FR), ATHENA (GR), Noho (IRL), U Glasgow (UK), U York (UK)</p>
            </li>
            <li id="uid83">
              <p noindent="true">Inria contact: George Drettakis</p>
            </li>
            <li id="uid84">
              <p noindent="true">Abstract:
Storytelling applies to nearly everything we do. Everybody uses stories, from educators to marketers and from politicians to journalists to inform, persuade, entertain, motivate or inspire. In the cultural heritage sector, however, narrative tends to be used narrowly, as a method to communicate to the public the findings and research conducted by the domain experts of a cultural site or collection.
The principal objective of the EMOTIVE project is to research, design, develop and evaluate methods and tools that can support the cultural and creative industries in creating Virtual Museums which draw on the power of `emotive storytelling'. This means storytelling that can engage visitors, trigger their emotions, connect them to other people around the world, and enhance their understanding, imagination and, ultimately, their experience of cultural sites and content. EMOTIVE will do this by providing the means to authors of cultural products to create high-quality, interactive, personalized digital stories.</p>
              <p>GRAPHDECO will contribute by developing novel image-based rendering techniques to help museum curators and archeologists provide more engaging experiences.</p>
            </li>
          </sanspuceslist>
        </subsection>
      </subsection>
    </subsection>
    <subsection id="uid85" level="1">
      <bodyTitle>International Initiatives</bodyTitle>
      <subsection id="uid86" level="2">
        <bodyTitle>Inria Associate Teams Not Involved in an Inria International Labs</bodyTitle>
        <subsection id="uid87" level="3">
          <bodyTitle>
            <ref xlink:href="http://www-sop.inria.fr/reves/crisp/" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">CRISP2 </ref>
          </bodyTitle>
          <sanspuceslist>
            <li id="uid88">
              <p noindent="true">Title: Creating and Rendering Images based on the Study of Perception</p>
            </li>
            <li id="uid89">
              <p noindent="true">International Partner (Institution - Laboratory - Researcher):</p>
              <sanspuceslist>
                <li id="uid90">
                  <p noindent="true">University of California Berkeley (United States)
- Electrical Engineering and Computer Science Department (EECS) - Maneesh Agrawala</p>
                </li>
              </sanspuceslist>
            </li>
            <li id="uid91">
              <p noindent="true">Start year: 2014</p>
            </li>
            <li id="uid92">
              <p noindent="true">See also: <ref xlink:href="http://www-sop.inria.fr/reves/crisp/" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>www-sop.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>reves/<allowbreak/>crisp/</ref></p>
            </li>
            <li id="uid93">
              <p noindent="true">The CRISP collaboration aims at developing novel techniques to create and manipulate effective numerical imagery. We adopt a multidisciplinary approach, focusing on understanding how people create and perceive images, on developing new rendering algorithms based on this understanding, and on building interactive tools that enable users to efficiently produce the images they have in mind. The participants of CRISP share complementary expertise in computer graphics, human computer interaction and human visual perception.
In 2016, the CRISP collaboration supported the postdoc of George Koulieris, who spent 6 months at UC Berkeley and is now at Inria. Johanna Delanoy also spent 2 weeks at UC Berkeley to collaborate with Alexei Efros.</p>
            </li>
          </sanspuceslist>
        </subsection>
      </subsection>
      <subsection id="uid94" level="2">
        <bodyTitle>Inria International Partners</bodyTitle>
        <subsection id="uid95" level="3">
          <bodyTitle>Informal International Partners</bodyTitle>
          <p><b>Canada.</b> A. Bousseau collaborates regularly with the University of Toronto (K. Singh) and the University of
British Columbia (A. Sheffer).</p>
          <p><b>India.</b> A. Bousseau collaborates with Vinay Namboodiri from IIT Kanpur and hosted several of his students for internships (Rahul Arora and Srinivasa Rao Gadhamchetty).</p>
          <p><b>UK.</b> G. Drettakis collaborates with UCL in the context of the CR-PLAY projects, resulting in a publication on indoor image-based rendering <ref xlink:href="#graphdeco-2016-bid1" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
          <p><b>United States.</b> We regularly collaborate with Adobe Research. Most recently, G. Drettakis worked with Eli Shechtman and
Sylvain Paris on multi-view inpainting <ref xlink:href="#graphdeco-2016-bid4" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. We also collaborate with Daniel Aliaga from Purdue University on sketch-based procedural modeling <ref xlink:href="#graphdeco-2016-bid2" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
        </subsection>
      </subsection>
    </subsection>
    <subsection id="uid96" level="1">
      <bodyTitle>International Research Visitors</bodyTitle>
      <subsection id="uid97" level="2">
        <bodyTitle>Visits of International Scientists</bodyTitle>
        <p>Martin Banks (UC Berkeley) visited our group for two weeks in the context of the Associate Team CRISP (Sec. <ref xlink:href="#uid87" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>).
We also hosted Philip Isola, Richard Zhang and Alexei Efros (UC Berkeley) for two days, also in the context of CRISP.
Several international researchers made short visits and talks: Elena Garces (University of Zaragoza), Yulia Gryditskaya (MPI), Jan Jermyn (Durham University) , Christian Lessig ( Otto-von-Guericke Universitat Magdeburg), Marc Stamminger (Erlangen University).
Finally, we hosted Frédo Durand from MIT (10 months) and Eugene Fiume from university of Toronto (6 months) for their sabbatical.</p>
        <subsection id="uid98" level="3">
          <bodyTitle>Internships</bodyTitle>
          <p>Sai Praveen Bangaru and Srinivasa Rao Gadhamchetty were master interns from IIT Madras and IIT Kampur respectively.</p>
        </subsection>
      </subsection>
      <subsection id="uid99" level="2">
        <bodyTitle>Visits to International Teams</bodyTitle>
        <subsection id="uid100" level="3">
          <bodyTitle>Research Stays Abroad</bodyTitle>
          <p>George Koulieris spent 6 months at UC Berkeley (Feb. 1 - Jul. 31) to work with Martin S. Banks in the context of the CRISP Inria associate team.
Johanna Delanoy also visited UC Berkeley for two weeks to work with Alexei Efros.</p>
        </subsection>
      </subsection>
    </subsection>
  </partenariat>
  <diffusion id="uid101">
    <bodyTitle>Dissemination</bodyTitle>
    <subsection id="uid102" level="1">
      <bodyTitle>Promoting Scientific Activities</bodyTitle>
      <subsection id="uid103" level="2">
        <bodyTitle>Scientific Events Organisation</bodyTitle>
        <subsection id="uid104" level="3">
          <bodyTitle>General Chair, Scientific Chair</bodyTitle>
          <sanspuceslist>
            <li id="uid105">
              <p noindent="true">George Drettakis chairs the Eurographics working group on Rendering.</p>
            </li>
          </sanspuceslist>
        </subsection>
      </subsection>
      <subsection id="uid106" level="2">
        <bodyTitle>Scientific Events Selection</bodyTitle>
        <subsection id="uid107" level="3">
          <bodyTitle>Chair of Conference Program Committees</bodyTitle>
          <sanspuceslist>
            <li id="uid108">
              <p noindent="true">Adrien Bousseau is co-chair of the Tutorial program of Eurographics 2017.</p>
            </li>
          </sanspuceslist>
        </subsection>
        <subsection id="uid109" level="3">
          <bodyTitle>Member of Conference Program Committees</bodyTitle>
          <sanspuceslist>
            <li id="uid110">
              <p noindent="true">Adrien Bousseau was on the program committee of Eurographics 2017 and of SIGGRAPH Asia 2016 technical briefs and posters.</p>
            </li>
          </sanspuceslist>
        </subsection>
      </subsection>
      <subsection id="uid111" level="2">
        <bodyTitle>Journal</bodyTitle>
        <subsection id="uid112" level="3">
          <bodyTitle>Member of the Editorial Boards</bodyTitle>
          <sanspuceslist>
            <li id="uid113">
              <p noindent="true">Adrien Bousseau is Associate Editor of The Visual Computer journal (Springer)</p>
            </li>
            <li id="uid114">
              <p noindent="true">George Drettakis is Associate Editor of ACM Transactions on Graphics (TOG) and of Computational Visual Media (CVM).</p>
            </li>
          </sanspuceslist>
        </subsection>
        <subsection id="uid115" level="3">
          <bodyTitle>Reviewer - Reviewing Activities</bodyTitle>
          <sanspuceslist>
            <li id="uid116">
              <p noindent="true">Adrien Bousseau was reviewer for ACM TOG, IEEE TVCG, SIGGRAPH, SIGGRAPH Asia, Computers &amp; Graphics, ACM CHI, ACM UIST, AFIG.</p>
            </li>
            <li id="uid117">
              <p noindent="true">George Drettakis was reviewer for SIGGRAPH, Computer Graphics Forum.</p>
            </li>
          </sanspuceslist>
        </subsection>
      </subsection>
      <subsection id="uid118" level="2">
        <bodyTitle>Leadership within the Scientific Community</bodyTitle>
        <sanspuceslist>
          <li id="uid119">
            <p noindent="true">George Drettakis chairs the local “Jacques Morgenstern” Colloquium organizing committee.</p>
          </li>
        </sanspuceslist>
      </subsection>
      <subsection id="uid120" level="2">
        <bodyTitle>Scientific Expertise</bodyTitle>
        <p>G. Drettakis is a member of the Scientific Advisory Board (SAB) of Aalto University, Finland, and participated in the SAB meeting at Aalto in October.</p>
      </subsection>
    </subsection>
    <subsection id="uid121" level="1">
      <bodyTitle>Teaching - Supervision - Juries</bodyTitle>
      <subsection id="uid122" level="2">
        <bodyTitle>Teaching</bodyTitle>
        <sanspuceslist>
          <li id="uid123">
            <p noindent="true">Licence : Abdelaziz Djelouah, TP système d'information, niveau L1, Université Nice Sophia Antipolis, 24h</p>
          </li>
          <li id="uid124">
            <p noindent="true">Licence : Johanna Delanoy, TP Bases de données and personal project, niveau L1, IUT - Université Nice Sophia Antipolis, 64h</p>
          </li>
          <li id="uid125">
            <p noindent="true">Masters II: G. Drettakis, A. Bousseau: Foundations of Image Synthesis, CentralSupelec, Paris, 28h</p>
          </li>
          <li id="uid126">
            <p noindent="true">Masters I: A. Bousseau, G. Drettakis: Image Synthesis International Masters I, University of Nice Sophia-Antipolis, 12h</p>
          </li>
          <li id="uid127">
            <p noindent="true">Masters I: G. Drettakis, A. Bousseau: Image Synthesis, Masters MAPI Cannes, University of Nice Sophia-Antipolis, 12h</p>
          </li>
        </sanspuceslist>
      </subsection>
      <subsection id="uid128" level="2">
        <bodyTitle>Supervision</bodyTitle>
        <sanspuceslist>
          <li id="uid129">
            <p noindent="true">PhD in progress : Rodrigo Ortiz-Cayon, Mixed image-based rendering, Nice Sophia Antipolis University, started Dec. 2013, to be defended in Feb. 2017, advisor: G. Drettakis</p>
          </li>
          <li id="uid130">
            <p noindent="true">PhD in progress : Théo Thonat, Multi-view image processing for image-based rendering, Nice Sophia Antipolis University, started Oct. 2015, advisor: G. Drettakis</p>
          </li>
          <li id="uid131">
            <p noindent="true">PhD in progress : Johanna Delanoy, Data-driven sketch-based modeling, Nice Sophia Antipolis University, started Oct. 2015, advisor: A. Bousseau</p>
          </li>
          <li id="uid132">
            <p noindent="true">PhD in progress : Jean-Dominique Favreau, geometric analysis of line drawings, Nice Sophia Antipolis University, started Oct. 2014, advisor: A. Bousseau and F. Lafarge (Titane)</p>
          </li>
          <li id="uid133">
            <p noindent="true">PhD in progress : Valentin Deschaintre, lightweight material capture, Nice Sophia Antipolis University, started Nov. 2016, advisor: A. Bousseau and G. Drettakis</p>
          </li>
          <li id="uid134">
            <p noindent="true">PhD in progress : Julien Philip, Mixed rendering for cultural heritage, Nice Sophia Antipolis University, started Nov. 2016, advisor: G. Drettakis</p>
          </li>
          <li id="uid135">
            <p noindent="true">PhD in progress : Simon Rodriguez, Combining image-based and procedural modeling, Nice Sophia Antipolis University, started Nov. 2016, advisor: G. Drettakis</p>
          </li>
        </sanspuceslist>
      </subsection>
      <subsection id="uid136" level="2">
        <bodyTitle>Juries</bodyTitle>
        <sanspuceslist>
          <li id="uid137">
            <p noindent="true">Adrien Bousseau was in the PhD committees of Boris Raymond (Inria Bordeaux) and Elena Garces (University of Zaragoza)</p>
          </li>
          <li id="uid138">
            <p noindent="true">George Drettakis was in the PhD committees Oriel Frigo (Paris Descartes) and was an evaluator (rapporteur) for Kaan Yucer (ETH Zurich) and Yitzchak Lockerman (Yale).</p>
          </li>
        </sanspuceslist>
      </subsection>
    </subsection>
    <subsection id="uid139" level="1">
      <bodyTitle>Popularization</bodyTitle>
      <p>George Koulieris described his research in an interview for the Inria@SiliconValley Newsletter <footnote id="uid140" id-text="1"><ref xlink:href="https://project.inria.fr/siliconvalley/2016/06/21/post-doc-george-koulieris/" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>project.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>siliconvalley/<allowbreak/>2016/<allowbreak/>06/<allowbreak/>21/<allowbreak/>post-doc-george-koulieris/</ref></footnote>.</p>
      <p>Adrien Bousseau described his research in two interviews for the Inria website <footnote id="uid141" id-text="2"><ref xlink:href="https://www.inria.fr/en/centre/sophia/news/adrien-bousseau-receives-an-erc-starting-grant" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>www.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>en/<allowbreak/>centre/<allowbreak/>sophia/<allowbreak/>news/<allowbreak/>adrien-bousseau-receives-an-erc-starting-grant</ref></footnote> <footnote id="uid142" id-text="3"><ref xlink:href="https://www.inria.fr/centre/sophia/actualites/adrien-bousseau-laureat-du-prix-jeune-chercheur" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>www.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>centre/<allowbreak/>sophia/<allowbreak/>actualites/<allowbreak/>adrien-bousseau-laureat-du-prix-jeune-chercheur</ref></footnote>.</p>
      <p>Adrien Bousseau gave demos of his tool to help people practice drawing-by-observation techniques
during the “Rencontres du numérique” organized by ANR (Nov. 16th) and
during the “Rencontres Inria-Industrie” on e-learning (Dec. 1st).</p>
    </subsection>
  </diffusion>
  <biblio id="bibliography" html="bibliography" numero="10" titre="Bibliography">
    
    <biblStruct id="graphdeco-2016-bid11" type="article" rend="refer" n="refercite:CDSD13">
      <analytic>
        <title level="a">Depth Synthesis and Local Warps for Plausible Image-based Navigation</title>
        <author>
          <persName key="reves-2014-idp70392">
            <foreName>Gaurav</foreName>
            <surname>Chaurasia</surname>
            <initial>G.</initial>
          </persName>
          <persName key="reves-2014-idp71648">
            <foreName>Sylvain</foreName>
            <surname>Duchêne</surname>
            <initial>S.</initial>
          </persName>
          <persName>
            <foreName>Olga</foreName>
            <surname>Sorkine-Hornung</surname>
            <initial>O.</initial>
          </persName>
          <persName key="reves-2014-idm29368">
            <foreName>George</foreName>
            <surname>Drettakis</surname>
            <initial>G.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">ACM Transactions on Graphics</title>
        <imprint>
          <biblScope type="volume">32</biblScope>
          <dateStruct>
            <year>2013</year>
          </dateStruct>
          <ref xlink:href="http://www-sop.inria.fr/reves/Basilic/2013/CDSD13" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>www-sop.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>reves/<allowbreak/>Basilic/<allowbreak/>2013/<allowbreak/>CDSD13</ref>
        </imprint>
      </monogr>
      <note type="bnote">to be presented at SIGGRAPH 2013</note>
    </biblStruct>
    
    <biblStruct id="graphdeco-2016-bid10" type="article" rend="refer" n="refercite:DRCLLPD15">
      <analytic>
        <title level="a">Multi-View Intrinsic Images of Outdoors Scenes with an Application to Relighting</title>
        <author>
          <persName key="reves-2014-idp71648">
            <foreName>Sylvain</foreName>
            <surname>Duchêne</surname>
            <initial>S.</initial>
          </persName>
          <persName key="reves-2014-idp67832">
            <foreName>Clement</foreName>
            <surname>Riant</surname>
            <initial>C.</initial>
          </persName>
          <persName key="reves-2014-idp70392">
            <foreName>Gaurav</foreName>
            <surname>Chaurasia</surname>
            <initial>G.</initial>
          </persName>
          <persName>
            <foreName>Jorge</foreName>
            <surname>Lopez-Moreno</surname>
            <initial>J.</initial>
          </persName>
          <persName>
            <foreName>Pierre-Yves</foreName>
            <surname>Laffont</surname>
            <initial>P.-Y.</initial>
          </persName>
          <persName key="reves-2014-idp76648">
            <foreName>Stefan</foreName>
            <surname>Popov</surname>
            <initial>S.</initial>
          </persName>
          <persName key="reves-2014-idm27888">
            <foreName>Adrien</foreName>
            <surname>Bousseau</surname>
            <initial>A.</initial>
          </persName>
          <persName key="reves-2014-idm29368">
            <foreName>George</foreName>
            <surname>Drettakis</surname>
            <initial>G.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">ACM Transactions on Graphics</title>
        <imprint>
          <dateStruct>
            <year>2015</year>
          </dateStruct>
          <ref xlink:href="http://www-sop.inria.fr/reves/Basilic/2015/DRCLLPD15" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>www-sop.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>reves/<allowbreak/>Basilic/<allowbreak/>2015/<allowbreak/>DRCLLPD15</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="graphdeco-2016-bid12" type="article" rend="refer" n="refercite:PRDD15b">
      <analytic>
        <title level="a">Probabilistic Connections for Bidirectional Path Tracing</title>
        <author>
          <persName key="reves-2014-idp76648">
            <foreName>Stefan</foreName>
            <surname>Popov</surname>
            <initial>S.</initial>
          </persName>
          <persName>
            <foreName>Ravi</foreName>
            <surname>Ramamoorthi</surname>
            <initial>R.</initial>
          </persName>
          <persName key="graphdeco-2016-idp111504">
            <foreName>Frédo</foreName>
            <surname>Durand</surname>
            <initial>F.</initial>
          </persName>
          <persName key="reves-2014-idm29368">
            <foreName>George</foreName>
            <surname>Drettakis</surname>
            <initial>G.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">Computer Graphics Forum (Proceedings of the Eurographics Symposium on Rendering)</title>
        <imprint>
          <biblScope type="volume">34</biblScope>
          <biblScope type="number">4</biblScope>
          <dateStruct>
            <year>2015</year>
          </dateStruct>
          <ref xlink:href="http://www-sop.inria.fr/reves/Basilic/2015/PRDD15b" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>www-sop.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>reves/<allowbreak/>Basilic/<allowbreak/>2015/<allowbreak/>PRDD15b</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="graphdeco-2016-bid13" type="article" rend="refer" n="refercite:XCSBMS14">
      <analytic>
        <title level="a">True2Form: 3D Curve Networks from 2D Sketches via Selective Regularization</title>
        <author>
          <persName>
            <foreName>Baoxuan</foreName>
            <surname>Xu</surname>
            <initial>B.</initial>
          </persName>
          <persName>
            <foreName>William</foreName>
            <surname>Chang</surname>
            <initial>W.</initial>
          </persName>
          <persName>
            <foreName>Alla</foreName>
            <surname>Sheffer</surname>
            <initial>A.</initial>
          </persName>
          <persName key="reves-2014-idm27888">
            <foreName>Adrien</foreName>
            <surname>Bousseau</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>James</foreName>
            <surname>McCrae</surname>
            <initial>J.</initial>
          </persName>
          <persName>
            <foreName>Karan</foreName>
            <surname>Singh</surname>
            <initial>K.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">ACM Transactions on Graphics (SIGGRAPH Conference Proceedings)</title>
        <imprint>
          <biblScope type="volume">33</biblScope>
          <biblScope type="number">4</biblScope>
          <dateStruct>
            <year>2014</year>
          </dateStruct>
          <ref xlink:href="http://www-sop.inria.fr/reves/Basilic/2014/XCSBMS14" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>www-sop.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>reves/<allowbreak/>Basilic/<allowbreak/>2014/<allowbreak/>XCSBMS14</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="graphdeco-2016-bid0" type="article" rend="year" n="cite:favreau:hal-01309271">
      <identifiant type="doi" value="10.1145/2897824.2925946"/>
      <identifiant type="hal" value="hal-01309271"/>
      <analytic>
        <title level="a">Fidelity vs. Simplicity: a Global Approach to Line Drawing Vectorization</title>
        <author>
          <persName key="titane-2014-idp70536">
            <foreName>Jean-Dominique</foreName>
            <surname>Favreau</surname>
            <initial>J.-D.</initial>
          </persName>
          <persName key="titane-2014-idm27920">
            <foreName>Florent</foreName>
            <surname>Lafarge</surname>
            <initial>F.</initial>
          </persName>
          <persName key="reves-2014-idm27888">
            <foreName>Adrien</foreName>
            <surname>Bousseau</surname>
            <initial>A.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid00024">
        <idno type="issn">0730-0301</idno>
        <title level="j">ACM Transactions on Graphics</title>
        <imprint>
          <dateStruct>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01309271" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01309271</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="graphdeco-2016-bid1" type="article" rend="year" n="cite:hedman:hal-01369382">
      <identifiant type="hal" value="hal-01369382"/>
      <analytic>
        <title level="a">Scalable Inside-Out Image-Based Rendering</title>
        <author>
          <persName>
            <foreName>Peter</foreName>
            <surname>Hedman</surname>
            <initial>P.</initial>
          </persName>
          <persName>
            <foreName>Tobias</foreName>
            <surname>Ritschel</surname>
            <initial>T.</initial>
          </persName>
          <persName key="reves-2014-idm29368">
            <foreName>George</foreName>
            <surname>Drettakis</surname>
            <initial>G.</initial>
          </persName>
          <persName>
            <foreName>Gabriel</foreName>
            <surname>Brostow</surname>
            <initial>G.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid00024">
        <idno type="issn">0730-0301</idno>
        <title level="j">ACM Transactions on Graphics</title>
        <imprint>
          <biblScope type="volume">35</biblScope>
          <biblScope type="number">6</biblScope>
          <dateStruct>
            <month>December</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01369382" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01369382</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="graphdeco-2016-bid8" type="article" rend="year" n="cite:manera:hal-01292254">
      <identifiant type="doi" value="10.1371/journal.pone.0151487"/>
      <identifiant type="hal" value="hal-01292254"/>
      <analytic>
        <title level="a">A Feasibility Study with Image-Based Rendered Virtual Reality in Patients with Mild Cognitive Impairment and Dementia</title>
        <author>
          <persName>
            <foreName>Valeria</foreName>
            <surname>Manera</surname>
            <initial>V.</initial>
          </persName>
          <persName key="reves-2014-idp69088">
            <foreName>Emmanuelle</foreName>
            <surname>Chapoulie</surname>
            <initial>E.</initial>
          </persName>
          <persName>
            <foreName>Jérémy</foreName>
            <surname>Bourgeois</surname>
            <initial>J.</initial>
          </persName>
          <persName key="stars-2014-idp78352">
            <foreName>Rachid</foreName>
            <surname>Guerchouche</surname>
            <initial>R.</initial>
          </persName>
          <persName>
            <foreName>Renaud</foreName>
            <surname>David</surname>
            <initial>R.</initial>
          </persName>
          <persName>
            <foreName>Jan</foreName>
            <surname>Ondrej</surname>
            <initial>J.</initial>
          </persName>
          <persName key="reves-2014-idm29368">
            <foreName>George</foreName>
            <surname>Drettakis</surname>
            <initial>G.</initial>
          </persName>
          <persName key="rap-2014-idm40528">
            <foreName>Philippe</foreName>
            <surname>Robert</surname>
            <initial>P.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid01553">
        <idno type="issn">1932-6203</idno>
        <title level="j">PLoS ONE</title>
        <imprint>
          <biblScope type="volume">11</biblScope>
          <biblScope type="number">3</biblScope>
          <dateStruct>
            <month>March</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">14</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01292254" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01292254</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="graphdeco-2016-bid2" type="article" rend="year" n="cite:nishida:hal-01378388">
      <identifiant type="doi" value="10.1145/2897824.2925951"/>
      <identifiant type="hal" value="hal-01378388"/>
      <analytic>
        <title level="a">Interactive sketching of urban procedural models</title>
        <author>
          <persName>
            <foreName>Gen G</foreName>
            <surname>Nishida</surname>
            <initial>G. G.</initial>
          </persName>
          <persName>
            <foreName>Ignacio G</foreName>
            <surname>Garcia-Dorado</surname>
            <initial>I. G.</initial>
          </persName>
          <persName key="graphdeco-2015-idp74568">
            <foreName>Daniel G</foreName>
            <surname>Aliaga</surname>
            <initial>D. G.</initial>
          </persName>
          <persName>
            <foreName>Bedrich</foreName>
            <surname>Benes</surname>
            <initial>B.</initial>
          </persName>
          <persName key="reves-2014-idm27888">
            <foreName>Adrien</foreName>
            <surname>Bousseau</surname>
            <initial>A.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid00024">
        <idno type="issn">0730-0301</idno>
        <title level="j">ACM Transactions on Graphics</title>
        <imprint>
          <biblScope type="volume">35</biblScope>
          <biblScope type="number">4</biblScope>
          <dateStruct>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">1 - 11</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01378388" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01378388</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="graphdeco-2016-bid3" type="inproceedings" rend="year" n="cite:bousseau:hal-01272187">
      <identifiant type="doi" value="10.1145/2858036.2858159"/>
      <identifiant type="hal" value="hal-01272187"/>
      <analytic>
        <title level="a">How Novices Sketch and Prototype Hand-Fabricated Objects</title>
        <author>
          <persName key="reves-2014-idm27888">
            <foreName>Adrien</foreName>
            <surname>Bousseau</surname>
            <initial>A.</initial>
          </persName>
          <persName key="in-situ-2014-idp65368">
            <foreName>Theophanis</foreName>
            <surname>Tsandilas</surname>
            <initial>T.</initial>
          </persName>
          <persName key="in-situ-2014-idp96944">
            <foreName>Lora</foreName>
            <surname>Oehlberg</surname>
            <initial>L.</initial>
          </persName>
          <persName key="in-situ-2014-idm29240">
            <foreName>Wendy E.</foreName>
            <surname>Mackay</surname>
            <initial>W. E.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">Conference on Human Factors in Computing Systems (CHI)</title>
        <loc>San Jose, United States</loc>
        <imprint>
          <publisher>
            <orgName type="organisation">ACM</orgName>
          </publisher>
          <dateStruct>
            <month>May</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01272187" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01272187</ref>
        </imprint>
        <meeting id="cid34258">
          <title>Annual SIGCHI Conference on Human Factors in Computing Systems</title>
          <num>34</num>
          <abbr type="sigle">CHI</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="graphdeco-2016-bid6" type="inproceedings" rend="year" n="cite:djelouah:hal-01367430">
      <identifiant type="hal" value="hal-01367430"/>
      <analytic>
        <title level="a">Cotemporal Multi-View Video Segmentation</title>
        <author>
          <persName key="morpheo-2014-idp109816">
            <foreName>Abdelaziz</foreName>
            <surname>Djelouah</surname>
            <initial>A.</initial>
          </persName>
          <persName key="morpheo-2014-idp107336">
            <foreName>Jean-Sébastien</foreName>
            <surname>Franco</surname>
            <initial>J.-S.</initial>
          </persName>
          <persName key="morpheo-2014-idp13752">
            <foreName>Edmond</foreName>
            <surname>Boyer</surname>
            <initial>E.</initial>
          </persName>
          <persName>
            <foreName>Patrick</foreName>
            <surname>Pérez</surname>
            <initial>P.</initial>
          </persName>
          <persName key="reves-2014-idm29368">
            <foreName>George</foreName>
            <surname>Drettakis</surname>
            <initial>G.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">International Conference on 3D Vision</title>
        <loc>Stanford, United States</loc>
        <imprint>
          <dateStruct>
            <month>October</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01367430" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01367430</ref>
        </imprint>
        <meeting id="cid624201">
          <title>International Conference on 3D Vision</title>
          <num>2015</num>
          <abbr type="sigle">3DV</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="graphdeco-2016-bid7" type="inproceedings" rend="year" n="cite:koulieris:hal-01290273">
      <identifiant type="hal" value="hal-01290273"/>
      <analytic>
        <title level="a">Gaze Prediction using Machine Learning for Dynamic Stereo Manipulation in Games</title>
        <author>
          <persName key="reves-2014-idp85432">
            <foreName>George Alex</foreName>
            <surname>Koulieris</surname>
            <initial>G. A.</initial>
          </persName>
          <persName key="reves-2014-idm29368">
            <foreName>George</foreName>
            <surname>Drettakis</surname>
            <initial>G.</initial>
          </persName>
          <persName>
            <foreName>Douglas</foreName>
            <surname>Cunningham</surname>
            <initial>D.</initial>
          </persName>
          <persName>
            <foreName>Katerina</foreName>
            <surname>Mania</surname>
            <initial>K.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="no" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">IEEE Virtual Reality</title>
        <loc>Greenville, United States</loc>
        <imprint>
          <dateStruct>
            <month>March</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01290273" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01290273</ref>
        </imprint>
        <meeting id="cid86390">
          <title>IEEE International Conference on Virtual Reality</title>
          <num>2007</num>
          <abbr type="sigle">IEEE VR</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="graphdeco-2016-bid9" type="inproceedings" rend="year" n="cite:negin:hal-01416372">
      <identifiant type="hal" value="hal-01416372"/>
      <analytic>
        <title level="a">Praxis and Gesture Recognition</title>
        <author>
          <persName key="stars-2014-idp125016">
            <foreName>Farhood</foreName>
            <surname>Negin</surname>
            <initial>F.</initial>
          </persName>
          <persName>
            <foreName>Jérémy</foreName>
            <surname>Bourgeois</surname>
            <initial>J.</initial>
          </persName>
          <persName key="reves-2014-idp69088">
            <foreName>Emmanuelle</foreName>
            <surname>Chapoulie</surname>
            <initial>E.</initial>
          </persName>
          <persName key="rap-2014-idm40528">
            <foreName>Philippe</foreName>
            <surname>Robert</surname>
            <initial>P.</initial>
          </persName>
          <persName key="stars-2014-idp60040">
            <foreName>Francois</foreName>
            <surname>Bremond</surname>
            <initial>F.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">The 10th World Conference of Gerontechnology (ISG 2016)</title>
        <loc>Nice, France</loc>
        <imprint>
          <dateStruct>
            <month>September</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01416372" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01416372</ref>
        </imprint>
        <meeting id="cid45809">
          <title>Conference of the International Society for Gerontechnology</title>
          <num>10</num>
          <abbr type="sigle">ISG</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="graphdeco-2016-bid5" type="inproceedings" rend="year" n="cite:ortizcayon:hal-01368355">
      <identifiant type="hal" value="hal-01368355"/>
      <analytic>
        <title level="a">Automatic 3D Car Model Alignment for Mixed Image-Based Rendering</title>
        <author>
          <persName>
            <foreName>Rodrigo</foreName>
            <surname>Ortiz-Cayon</surname>
            <initial>R.</initial>
          </persName>
          <persName key="morpheo-2014-idp109816">
            <foreName>Abdelaziz</foreName>
            <surname>Djelouah</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Francisco</foreName>
            <surname>Massa</surname>
            <initial>F.</initial>
          </persName>
          <persName key="willow-2014-idp112336">
            <foreName>Mathieu</foreName>
            <surname>Aubry</surname>
            <initial>M.</initial>
          </persName>
          <persName key="reves-2014-idm29368">
            <foreName>George</foreName>
            <surname>Drettakis</surname>
            <initial>G.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">2016 International Conference on 3D Vision (3DV)</title>
        <loc>Stanford, United States</loc>
        <imprint>
          <dateStruct>
            <month>October</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01368355" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01368355</ref>
        </imprint>
        <meeting id="cid624201">
          <title>International Conference on 3D Vision</title>
          <num>2016</num>
          <abbr type="sigle">3DV</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="graphdeco-2016-bid4" type="inproceedings" rend="year" n="cite:thonat:hal-01369554">
      <identifiant type="hal" value="hal-01369554"/>
      <analytic>
        <title level="a">Multi-View Inpainting for Image-Based Scene Editing and Rendering</title>
        <author>
          <persName key="graphdeco-2015-idp70872">
            <foreName>Theo</foreName>
            <surname>Thonat</surname>
            <initial>T.</initial>
          </persName>
          <persName>
            <foreName>Eli</foreName>
            <surname>Shechtman</surname>
            <initial>E.</initial>
          </persName>
          <persName>
            <foreName>Sylvain</foreName>
            <surname>Paris</surname>
            <initial>S.</initial>
          </persName>
          <persName key="reves-2014-idm29368">
            <foreName>George</foreName>
            <surname>Drettakis</surname>
            <initial>G.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">2016 International Conference on 3D Vision (3DV)</title>
        <loc>Stanford, United States</loc>
        <imprint>
          <dateStruct>
            <month>October</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01369554" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01369554</ref>
        </imprint>
        <meeting id="cid624201">
          <title>International Conference on 3D Vision</title>
          <num>2016</num>
          <abbr type="sigle">3DV</abbr>
        </meeting>
      </monogr>
    </biblStruct>
  </biblio>
</raweb>
