<?xml version="1.0" encoding="utf-8"?>
<raweb xmlns:xlink="http://www.w3.org/1999/xlink" xml:lang="en" year="2016">
  <identification id="sequel" isproject="true">
    <shortname>SEQUEL</shortname>
    <projectName>Sequential Learning</projectName>
    <theme-de-recherche>Optimization, machine learning and statistical methods</theme-de-recherche>
    <domaine-de-recherche>Applied Mathematics, Computation and Simulation</domaine-de-recherche>
    <urlTeam>https://team.inria.fr/sequel/</urlTeam>
    <structure_exterieure type="Labs">
      <libelle>Centre de Recherche en Informatique, Signal et Automatique de Lille</libelle>
    </structure_exterieure>
    <structure_exterieure type="Organism">
      <libelle>Universit√© Charles de Gaulle (Lille 3)</libelle>
    </structure_exterieure>
    <structure_exterieure type="Organism">
      <libelle>Universit√© des sciences et technologies de Lille (Lille 1)</libelle>
    </structure_exterieure>
    <header_dates_team>Creation of the Project-Team: 2007 July 01</header_dates_team>
    <LeTypeProjet>Project-Team</LeTypeProjet>
    <keywordsSdN>
      <term>3. - Data and knowledge</term>
      <term>3.1. - Data</term>
      <term>3.1.1. - Modeling, representation</term>
      <term>3.1.4. - Uncertain data</term>
      <term>3.3. - Data and knowledge analysis</term>
      <term>3.3.1. - On-line analytical processing</term>
      <term>3.3.2. - Data mining</term>
      <term>3.3.3. - Big data analysis</term>
      <term>3.4. - Machine learning and statistics</term>
      <term>3.4.1. - Supervised learning</term>
      <term>3.4.2. - Unsupervised learning</term>
      <term>3.4.3. - Reinforcement learning</term>
      <term>3.4.4. - Optimization and learning</term>
      <term>3.4.6. - Neural networks</term>
      <term>3.4.8. - Deep learning</term>
      <term>3.5.2. - Recommendation systems</term>
      <term>4.8. - Privacy-enhancing technologies</term>
      <term>5.1. - Human-Computer Interaction</term>
      <term>8. - Artificial intelligence</term>
      <term>8.2. - Machine learning</term>
      <term>8.3. - Signal analysis</term>
      <term>8.7. - AI algorithmics</term>
    </keywordsSdN>
    <keywordsSecteurs>
      <term>5.8. - Learning and training</term>
      <term>6.1. - Software industry</term>
      <term>6.1.1. - Software engineering</term>
      <term>6.1.2. - Software evolution, maintenance</term>
      <term>9.1.1. - E-learning, MOOC</term>
      <term>9.4. - Sciences</term>
      <term>9.4.5. - Data science</term>
    </keywordsSecteurs>
    <UR name="Lille"/>
  </identification>
  <team id="uid1">
    <person key="sequel-2014-idm27568">
      <firstname>Philippe</firstname>
      <lastname>Preux</lastname>
      <categoryPro>Enseignant</categoryPro>
      <research-centre>Lille</research-centre>
      <moreinfo>Team leader, Univ. Lille III, Professor</moreinfo>
      <hdr>oui</hdr>
    </person>
    <person key="dyogene-2014-idp72312">
      <firstname>Emilie</firstname>
      <lastname>Kaufmann</lastname>
      <categoryPro>Chercheur</categoryPro>
      <research-centre>Lille</research-centre>
      <moreinfo>CNRS, Researcher</moreinfo>
    </person>
    <person key="sequel-2014-idm26088">
      <firstname>Alessandro</firstname>
      <lastname>Lazaric</lastname>
      <categoryPro>Chercheur</categoryPro>
      <research-centre>Lille</research-centre>
      <moreinfo>Inria, Researcher</moreinfo>
    </person>
    <person key="tao-2015-idp83360">
      <firstname>Odalric</firstname>
      <lastname>Maillard</lastname>
      <categoryPro>Chercheur</categoryPro>
      <research-centre>Lille</research-centre>
      <moreinfo>Inria, Researcher, moved to <span class="smallcap" align="left">SequeL</span> on 11/1/2016</moreinfo>
    </person>
    <person key="sequel-2014-idp67360">
      <firstname>R√©mi</firstname>
      <lastname>Munos</lastname>
      <categoryPro>Chercheur</categoryPro>
      <research-centre>Lille</research-centre>
      <moreinfo>secondment at Google/Deepmind, Senior Researcher</moreinfo>
      <hdr>oui</hdr>
    </person>
    <person key="sequel-2014-idp68800">
      <firstname>Daniil</firstname>
      <lastname>Ryabko</lastname>
      <categoryPro>Chercheur</categoryPro>
      <research-centre>Lille</research-centre>
      <moreinfo>Inria, Researcher</moreinfo>
      <hdr>oui</hdr>
    </person>
    <person key="sequel-2014-idp70232">
      <firstname>Michal</firstname>
      <lastname>Valko</lastname>
      <categoryPro>Chercheur</categoryPro>
      <research-centre>Lille</research-centre>
      <moreinfo>Inria, Researcher</moreinfo>
      <hdr>oui</hdr>
    </person>
    <person key="sequel-2015-idp103832">
      <firstname>Christos</firstname>
      <lastname>Dimitrakakis</lastname>
      <categoryPro>Enseignant</categoryPro>
      <research-centre>Lille</research-centre>
      <moreinfo>Univ. Lille III, Associate Professor</moreinfo>
      <hdr>oui</hdr>
    </person>
    <person key="sequel-2014-idp75664">
      <firstname>Romaric</firstname>
      <lastname>Gaudel</lastname>
      <categoryPro>Enseignant</categoryPro>
      <research-centre>Lille</research-centre>
      <moreinfo>Univ. Lille III, Associate Professor</moreinfo>
    </person>
    <person key="sequel-2014-idp76928">
      <firstname>J√©r√©mie</firstname>
      <lastname>Mary</lastname>
      <categoryPro>Enseignant</categoryPro>
      <research-centre>Lille</research-centre>
      <moreinfo>Univ. Lille III, Associate Professor</moreinfo>
      <hdr>oui</hdr>
    </person>
    <person key="sequel-2014-idp79648">
      <firstname>Bilal</firstname>
      <lastname>Piot</lastname>
      <categoryPro>Enseignant</categoryPro>
      <research-centre>Lille</research-centre>
      <moreinfo>Univ. Lille I, Associate Professor</moreinfo>
    </person>
    <person key="sequel-2014-idp97464">
      <firstname>Marc</firstname>
      <lastname>Abeille</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Lille</research-centre>
      <moreinfo>Univ. Lille I</moreinfo>
    </person>
    <person key="sequel-2015-idp113016">
      <firstname>Merwan</firstname>
      <lastname>Barlier</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Lille</research-centre>
      <moreinfo>Orange Labs, granted by CIFRE</moreinfo>
    </person>
    <person key="sequel-2014-idp99936">
      <firstname>Alexandre</firstname>
      <lastname>Berard</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Lille</research-centre>
      <moreinfo>Univ. Lille I</moreinfo>
    </person>
    <person key="sequel-2016-idp147888">
      <firstname>Lilian</firstname>
      <lastname>Besson</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Lille</research-centre>
      <moreinfo>ENS Cachan, from Oct 2016</moreinfo>
    </person>
    <person key="sequel-2014-idp101176">
      <firstname>Daniele</firstname>
      <lastname>Calandriello</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Lille</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="sequel-2015-idp116704">
      <firstname>Ronan</firstname>
      <lastname>Fruit</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Lille</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="sequel-2014-idp102400">
      <firstname>Pratik</firstname>
      <lastname>Gajane</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Lille</research-centre>
      <moreinfo>Orange Labs, granted by CIFRE</moreinfo>
    </person>
    <person key="i4s-2015-idp77480">
      <firstname>Guillaume</firstname>
      <lastname>Gautier</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Lille</research-centre>
      <moreinfo>Inria and CNRS</moreinfo>
    </person>
    <person key="sequel-2015-idp120424">
      <firstname>Jean-Bastien</firstname>
      <lastname>Grill</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Lille</research-centre>
      <moreinfo>granted by ENS Paris</moreinfo>
    </person>
    <person key="sequel-2014-idp106104">
      <firstname>Fr√©d√©ric</firstname>
      <lastname>Guillou</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Lille</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="sequel-2014-idp108624">
      <firstname>Tom√°≈°</firstname>
      <lastname>Koc√°k</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Lille</research-centre>
      <moreinfo>Inria, until Nov 2016</moreinfo>
    </person>
    <person key="sequel-2014-idp109840">
      <firstname>Julien</firstname>
      <lastname>Perolat</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Lille</research-centre>
      <moreinfo>Univ. Lille I</moreinfo>
    </person>
    <person key="sequel-2015-idp110544">
      <firstname>Florian</firstname>
      <lastname>Strub</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Lille</research-centre>
      <moreinfo>Univ. Lille I</moreinfo>
    </person>
    <person key="sequel-2016-idp172336">
      <firstname>Romain</firstname>
      <lastname>Warlop</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Lille</research-centre>
      <moreinfo>55</moreinfo>
    </person>
    <person key="sequel-2016-idp174768">
      <firstname>James</firstname>
      <lastname>Ridgway</lastname>
      <categoryPro>PostDoc</categoryPro>
      <research-centre>Lille</research-centre>
      <moreinfo>Inria, from Oct 2016</moreinfo>
    </person>
    <person key="sequel-2016-idp177264">
      <firstname>Maryam</firstname>
      <lastname>Aziz</lastname>
      <categoryPro>Visiteur</categoryPro>
      <research-centre>Lille</research-centre>
      <moreinfo>Northeastern University, from May 2016 until Aug 2016</moreinfo>
    </person>
    <person key="sequel-2016-idp179696">
      <firstname>Kamyar</firstname>
      <lastname>Azizzadenesheli</lastname>
      <categoryPro>Visiteur</categoryPro>
      <research-centre>Lille</research-centre>
      <moreinfo>University of California at Irvine, from Aug 2016</moreinfo>
    </person>
    <person key="sequel-2015-idp131664">
      <firstname>Cricia Zilda</firstname>
      <lastname>Felicio Paixao</lastname>
      <categoryPro>Visiteur</categoryPro>
      <research-centre>Lille</research-centre>
      <moreinfo>University Uberlandia, Brasil, until Aug 2016</moreinfo>
    </person>
    <person key="sequel-2016-idp184720">
      <firstname>Yao</firstname>
      <lastname>Ma</lastname>
      <categoryPro>Visiteur</categoryPro>
      <research-centre>Lille</research-centre>
      <moreinfo>University of Tokyo, Japan, until Mar 2016</moreinfo>
    </person>
    <person key="sequel-2016-idp187168">
      <firstname>Aristide</firstname>
      <lastname>Tossou</lastname>
      <categoryPro>Visiteur</categoryPro>
      <research-centre>Lille</research-centre>
      <moreinfo>Chalmers University, Sweden</moreinfo>
    </person>
    <person key="sequel-2014-idp88536">
      <firstname>Amelie</firstname>
      <lastname>Supervielle</lastname>
      <categoryPro>Assistant</categoryPro>
      <research-centre>Lille</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="sequel-2016-idp192128">
      <firstname>Mehdi</firstname>
      <lastname>Abbana Bennani</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Lille</research-centre>
      <moreinfo>Univ. Lille III, intern, from Jun 2016 until Aug 2016</moreinfo>
    </person>
    <person key="sequel-2016-idp194624">
      <firstname>Remi</firstname>
      <lastname>Bardenet</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Lille</research-centre>
      <moreinfo>CNRS, Researcher</moreinfo>
    </person>
    <person key="sequel-2014-idp71472">
      <firstname>Pierre</firstname>
      <lastname>Chainais</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Lille</research-centre>
      <moreinfo>Ecole Centrale de Lille, Associate professor</moreinfo>
      <hdr>oui</hdr>
    </person>
    <person key="sequel-2015-idp142056">
      <firstname>Pierre-Victor</firstname>
      <lastname>Chaumier</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Lille</research-centre>
      <moreinfo>Inria, intern, from Feb 2016 until Jun 2016</moreinfo>
    </person>
    <person key="sequel-2016-idp202464">
      <firstname>Quentin</firstname>
      <lastname>Co√´t</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Lille</research-centre>
      <moreinfo>Univ. Lille I, intern, from Mar 2016 until Aug 2016</moreinfo>
    </person>
    <person key="sequel-2016-idp204960">
      <firstname>Jean-Beno√Æt</firstname>
      <lastname>Delbrouck</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Lille</research-centre>
      <moreinfo>Univ. Lille I, intern, from Mar 2016 until Aug 2016</moreinfo>
    </person>
    <person key="sequel-2016-idp207472">
      <firstname>Eddy</firstname>
      <lastname>El Khatib</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Lille</research-centre>
      <moreinfo>Univ. Lille I, intern, from Apr 2016 until Jul 2016</moreinfo>
    </person>
    <person key="sequel-2014-idp111080">
      <firstname>Olivier</firstname>
      <lastname>Pietquin</lastname>
      <categoryPro>Enseignant</categoryPro>
      <research-centre>Lille</research-centre>
      <moreinfo>Univ. Lille I, Professor, currently in secondment at Google/Deepmind since May 2016</moreinfo>
      <hdr>oui</hdr>
    </person>
  </team>
  <presentation id="uid2">
    <bodyTitle>Overall Objectives</bodyTitle>
    <subsection id="uid3" level="1">
      <bodyTitle>Presentation</bodyTitle>
      <p><span class="smallcap" align="left">SequeL</span> means ‚ÄúSequential Learning‚Äù. As such, <span class="smallcap" align="left">SequeL</span> focuses on the task of learning in artificial systems (either hardware, or software) that gather information along time. Such systems are named <i>(learning) agents</i> (or learning machines) in the following.
These data may be used to estimate some parameters of a model, which in turn, may be used for selecting actions in order to perform some long-term optimization task.</p>
      <p>For the purpose of model building, the agent needs to represent information collected so far in some compact form and use it to process newly available data.</p>
      <p>The acquired data may result from an observation process of an agent in interaction with its environment (the data thus represent a perception). This is the case when the agent makes decisions (in order to attain a certain objective) that impact the environment, and thus the observation process itself.</p>
      <p>Hence, in <span class="smallcap" align="left">SequeL</span>, the term <b>sequential</b> refers to two aspects:</p>
      <simplelist>
        <li id="uid4">
          <p noindent="true">The <b>sequential acquisition of data</b>, from which a model is learned (supervised and non supervised learning),</p>
        </li>
        <li id="uid5">
          <p noindent="true">the <b>sequential decision making task</b>, based on the learned model (reinforcement learning).</p>
        </li>
      </simplelist>
      <p>Examples of sequential learning problems include:</p>
      <descriptionlist>
        <label>Supervised learning</label>
        <li id="uid6">
          <p noindent="true">tasks deal with the prediction of some response given a certain set of observations of input variables and responses. New sample points keep on being observed.</p>
        </li>
        <label>Unsupervised learning</label>
        <li id="uid7">
          <p noindent="true">tasks deal with clustering objects, these latter making a flow of objects. The (unknown) number of clusters typically evolves during time, as new objects are observed.</p>
        </li>
        <label>Reinforcement learning</label>
        <li id="uid8">
          <p noindent="true">tasks deal with the control (a policy) of some system which has to be optimized (see <ref xlink:href="#sequel-2016-bid0" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>). We do not assume the availability of a model of the system to be controlled.</p>
        </li>
      </descriptionlist>
      <p>In all these cases, we mostly assume that the process can be considered stationary for at least a certain amount of time, and slowly evolving.</p>
      <p>We wish to have any-time algorithms, that is, at any moment, a prediction may be required/an action may be selected making full use, and hopefully, the best use, of the experience already gathered by the learning agent.</p>
      <p>The perception of the environment by the learning agent (using its sensors) is generally neither the best one to make a prediction, nor to take a decision (we deal with Partially Observable Markov Decision Problem). So, the perception has to be mapped in some way to a better, and relevant, state (or input) space.</p>
      <p>Finally, an important issue of prediction regards its evaluation: how wrong may we be when we perform a prediction? For real systems to be controlled, this issue can not be simply left unanswered.</p>
      <p spacebefore="6.0pt">To sum-up, in <span class="smallcap" align="left">SequeL</span>, the main issues regard:</p>
      <simplelist>
        <li id="uid9">
          <p noindent="true">the learning of a model: we focus on models that map some
input space <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msup><mi>‚Ñù</mi><mi>P</mi></msup></math></formula> to <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>‚Ñù</mi></math></formula>,</p>
        </li>
        <li id="uid10">
          <p noindent="true">the observation to state mapping,</p>
        </li>
        <li id="uid11">
          <p noindent="true">the choice of the action to perform (in the case of sequential
decision problem),</p>
        </li>
        <li id="uid12">
          <p noindent="true">the performance guarantees,</p>
        </li>
        <li id="uid13">
          <p noindent="true">the implementation of usable algorithms,</p>
        </li>
      </simplelist>
      <p>all that being understood in a <i>sequential</i> framework.</p>
    </subsection>
  </presentation>
  <fondements id="uid14">
    <bodyTitle>Research Program</bodyTitle>
    <subsection id="uid15" level="1">
      <bodyTitle>In Short</bodyTitle>
      <p><span class="smallcap" align="left">SequeL</span> is primarily grounded on two domains:</p>
      <simplelist>
        <li id="uid16">
          <p noindent="true">the problem of decision under uncertainty,</p>
        </li>
        <li id="uid17">
          <p noindent="true">statistical analysis and statistical learning, which provide the general concepts and tools to solve this problem.</p>
        </li>
      </simplelist>
      <p>To help the reader who is unfamiliar with these questions, we briefly present key ideas below.</p>
    </subsection>
    <subsection id="uid18" level="1">
      <bodyTitle>Decision-making Under Uncertainty</bodyTitle>
      <p>The phrase ‚ÄúDecision under uncertainty‚Äù refers to the problem of taking decisions when we do not have a full knowledge neither of the situation, nor of the consequences of the decisions, as well as when the consequences of decision are non deterministic.</p>
      <p>We introduce two specific sub-domains, namely the Markov decision processes which models sequential decision problems, and bandit problems.</p>
      <subsection id="uid19" level="2">
        <bodyTitle>Reinforcement Learning</bodyTitle>
        <p>Sequential decision processes occupy the heart of the <span class="smallcap" align="left">SequeL</span> project; a detailed presentation of this problem may be found in Puterman's book <ref xlink:href="#sequel-2016-bid1" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
        <p>A Markov Decision Process (MDP) is defined as the tuple <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mo>(</mo><mi>ùí≥</mi><mo>,</mo><mi>ùíú</mi><mo>,</mo><mi>P</mi><mo>,</mo><mi>r</mi><mo>)</mo></mrow></math></formula> where <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>ùí≥</mi></math></formula> is the state space, <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>ùíú</mi></math></formula> is the action space, <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>P</mi></math></formula> is the probabilistic transition kernel, and <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mi>r</mi><mo>:</mo><mi>ùí≥</mi><mo>√ó</mo><mi>ùíú</mi><mo>√ó</mo><mi>ùí≥</mi><mo>‚Üí</mo><mi>I</mi><mspace width="-0.166667em"/><mspace width="-0.166667em"/><mi>R</mi></mrow></math></formula> is the reward function. For the sake of simplicity, we assume in this introduction that the state and action spaces are finite. If the current state (at time <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>t</mi></math></formula>) is <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mi>x</mi><mo>‚àà</mo><mi>ùí≥</mi></mrow></math></formula> and the chosen action is <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mi>a</mi><mo>‚àà</mo><mi>ùíú</mi></mrow></math></formula>, then the Markov assumption means that the transition probability to a new state <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><msup><mi>x</mi><mo>'</mo></msup><mo>‚àà</mo><mi>ùí≥</mi></mrow></math></formula> (at time <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></math></formula>) only depends on <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mo>(</mo><mi>x</mi><mo>,</mo><mi>a</mi><mo>)</mo></mrow></math></formula>. We write <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mi>p</mi><mo>(</mo><msup><mi>x</mi><mo>'</mo></msup><mo>|</mo><mi>x</mi><mo>,</mo><mi>a</mi><mo>)</mo></mrow></math></formula> the corresponding transition probability. During a transition <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mrow><mo>(</mo><mi>x</mi><mo>,</mo><mi>a</mi><mo>)</mo></mrow><mo>‚Üí</mo><msup><mi>x</mi><mo>'</mo></msup></mrow></math></formula>, a reward <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mi>r</mi><mo>(</mo><mi>x</mi><mo>,</mo><mi>a</mi><mo>,</mo><msup><mi>x</mi><mo>'</mo></msup><mo>)</mo></mrow></math></formula> is incurred.</p>
        <p>In the MDP (<formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mi>ùí≥</mi><mo>,</mo><mi>ùíú</mi><mo>,</mo><mi>P</mi><mo>,</mo><mi>r</mi><mo>)</mo></mrow></math></formula>, each initial state <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msub><mi>x</mi><mn>0</mn></msub></math></formula> and action sequence <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><msub><mi>a</mi><mn>0</mn></msub><mo>,</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><mo>...</mo></mrow></math></formula> gives rise to a sequence of states <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mo>...</mo></mrow></math></formula>, satisfying <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mi>‚Ñô</mi><mfenced separators="" open="(" close=")"><msub><mi>x</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msup><mi>x</mi><mo>'</mo></msup><mrow><mo>|</mo></mrow><msub><mi>x</mi><mi>t</mi></msub><mo>=</mo><mi>x</mi><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo>=</mo><mi>a</mi></mfenced><mo>=</mo><mi>p</mi><mrow><mo>(</mo><msup><mi>x</mi><mo>'</mo></msup><mo>|</mo><mi>x</mi><mo>,</mo><mi>a</mi><mo>)</mo></mrow><mo>,</mo></mrow></math></formula> and rewards <footnote id="uid20" id-text="1">Note that for simplicity, we considered the case of a deterministic reward function, but in many applications, the reward <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msub><mi>r</mi><mi>t</mi></msub></math></formula> itself is a random variable.</footnote> <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><msub><mi>r</mi><mn>1</mn></msub><mo>,</mo><msub><mi>r</mi><mn>2</mn></msub><mo>,</mo><mo>...</mo></mrow></math></formula> defined by <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><msub><mi>r</mi><mi>t</mi></msub><mo>=</mo><mi>r</mi><mrow><mo>(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo>,</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>)</mo></mrow></mrow></math></formula>.</p>
        <p>The history of the process up to time <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>t</mi></math></formula> is defined to be <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><msub><mi>H</mi><mi>t</mi></msub><mo>=</mo><mrow><mo>(</mo><msub><mi>x</mi><mn>0</mn></msub><mo>,</mo><msub><mi>a</mi><mn>0</mn></msub><mo>,</mo><mo>...</mo><mo>,</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>a</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>x</mi><mi>t</mi></msub><mo>)</mo></mrow></mrow></math></formula>. A policy <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>œÄ</mi></math></formula> is a sequence of functions <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><msub><mi>œÄ</mi><mn>0</mn></msub><mo>,</mo><msub><mi>œÄ</mi><mn>1</mn></msub><mo>,</mo><mo>...</mo></mrow></math></formula>, where <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msub><mi>œÄ</mi><mi>t</mi></msub></math></formula> maps the space of possible histories at time <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>t</mi></math></formula> to the space of probability distributions over the space of actions <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>ùíú</mi></math></formula>. To follow a policy means that, in each time step, we assume that the process history up to time <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>t</mi></math></formula> is <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><msub><mi>x</mi><mn>0</mn></msub><mo>,</mo><msub><mi>a</mi><mn>0</mn></msub><mo>,</mo><mo>...</mo><mo>,</mo><msub><mi>x</mi><mi>t</mi></msub></mrow></math></formula> and the probability of selecting an action <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>a</mi></math></formula> is equal to <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><msub><mi>œÄ</mi><mi>t</mi></msub><mrow><mo>(</mo><msub><mi>x</mi><mn>0</mn></msub><mo>,</mo><msub><mi>a</mi><mn>0</mn></msub><mo>,</mo><mo>...</mo><mo>,</mo><msub><mi>x</mi><mi>t</mi></msub><mo>)</mo></mrow><mrow><mo>(</mo><mi>a</mi><mo>)</mo></mrow></mrow></math></formula>. A policy is called stationary (or Markovian) if <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msub><mi>œÄ</mi><mi>t</mi></msub></math></formula> depends only on the last visited state. In other words, a policy <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mi>œÄ</mi><mo>=</mo><mo>(</mo><msub><mi>œÄ</mi><mn>0</mn></msub><mo>,</mo><msub><mi>œÄ</mi><mn>1</mn></msub><mo>,</mo><mo>...</mo><mo>)</mo></mrow></math></formula> is called stationary if <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><msub><mi>œÄ</mi><mi>t</mi></msub><mrow><mo>(</mo><msub><mi>x</mi><mn>0</mn></msub><mo>,</mo><msub><mi>a</mi><mn>0</mn></msub><mo>,</mo><mo>...</mo><mo>,</mo><msub><mi>x</mi><mi>t</mi></msub><mo>)</mo></mrow><mo>=</mo><msub><mi>œÄ</mi><mn>0</mn></msub><mrow><mo>(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>)</mo></mrow></mrow></math></formula> holds for all <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mi>t</mi><mo>‚â•</mo><mn>0</mn></mrow></math></formula>. A policy is called deterministic if the probability distribution prescribed by the policy for any history is concentrated on a single action. Otherwise it is called a stochastic policy.</p>
        <p>We move from an MD process to an MD problem by formulating the goal of the agent, that is what the sought policy <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>œÄ</mi></math></formula> has to optimize? It is very often formulated as maximizing (or minimizing), in expectation, some functional of the sequence of future rewards. For example, an usual functional is the infinite-time horizon sum of discounted rewards. For a given (stationary) policy <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>œÄ</mi></math></formula>, we define the value function <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><msup><mi>V</mi><mi>œÄ</mi></msup><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></math></formula> of that policy <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>œÄ</mi></math></formula> at a state <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mi>x</mi><mo>‚àà</mo><mi>ùí≥</mi></mrow></math></formula> as the expected sum of discounted future rewards given that we state from the initial state <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>x</mi></math></formula> and follow the policy <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>œÄ</mi></math></formula>:</p>
        <formula id-text="1" id="uid21" textype="equation" type="display">
          <math xmlns="http://www.w3.org/1998/Math/MathML" mode="display" overflow="scroll">
            <mrow>
              <msup>
                <mi>V</mi>
                <mi>œÄ</mi>
              </msup>
              <mrow>
                <mo>(</mo>
                <mi>x</mi>
                <mo>)</mo>
              </mrow>
              <mo>=</mo>
              <mi>ùîº</mi>
              <mfenced separators="" open="[" close="]">
                <munderover>
                  <mo>‚àë</mo>
                  <mrow>
                    <mi>t</mi>
                    <mo>=</mo>
                    <mn>0</mn>
                  </mrow>
                  <mi>‚àû</mi>
                </munderover>
                <msup>
                  <mi>Œ≥</mi>
                  <mi>t</mi>
                </msup>
                <msub>
                  <mi>r</mi>
                  <mi>t</mi>
                </msub>
                <mo>|</mo>
                <msub>
                  <mi>x</mi>
                  <mn>0</mn>
                </msub>
                <mo>=</mo>
                <mi>x</mi>
                <mo>,</mo>
                <mi>œÄ</mi>
              </mfenced>
              <mo>,</mo>
            </mrow>
          </math>
        </formula>
        <p>where <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>ùîº</mi></math></formula> is the expectation operator and <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mi>Œ≥</mi><mo>‚àà</mo><mo>(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>)</mo></mrow></math></formula> is the discount factor. This value function <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msup><mi>V</mi><mi>œÄ</mi></msup></math></formula> gives an evaluation of the performance of a given policy <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>œÄ</mi></math></formula>. Other functionals of the sequence of future rewards may be considered, such as the undiscounted reward (see the stochastic shortest path problems <ref xlink:href="#sequel-2016-bid2" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>) and average reward settings. Note also that, here, we considered the problem of maximizing a reward functional, but a formulation in terms of minimizing some cost or risk functional would be equivalent.</p>
        <p>In order to maximize a given functional in a sequential framework, one usually applies Dynamic Programming (DP)¬† <ref xlink:href="#sequel-2016-bid3" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, which introduces the optimal value function <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><msup><mi>V</mi><mo>*</mo></msup><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></math></formula>, defined as the optimal expected sum of rewards when the agent starts from a state <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>x</mi></math></formula>. We have <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><msup><mi>V</mi><mo>*</mo></msup><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>=</mo><msub><mo movablelimits="true" form="prefix">sup</mo><mi>œÄ</mi></msub><msup><mi>V</mi><mi>œÄ</mi></msup><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></math></formula>. Now, let us give two definitions about policies:</p>
        <simplelist>
          <li id="uid22">
            <p noindent="true">We say that a policy <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>œÄ</mi></math></formula> is optimal, if it attains the optimal values <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><msup><mi>V</mi><mo>*</mo></msup><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></math></formula> for any state <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mi>x</mi><mo>‚àà</mo><mi>ùí≥</mi></mrow></math></formula>, <i>i.e.</i>, if <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><msup><mi>V</mi><mi>œÄ</mi></msup><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>=</mo><msup><mi>V</mi><mo>*</mo></msup><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></math></formula> for all <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mi>x</mi><mo>‚àà</mo><mi>ùí≥</mi></mrow></math></formula>. Under mild conditions, deterministic stationary optimal policies exist <ref xlink:href="#sequel-2016-bid4" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. Such an optimal policy is written <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msup><mi>œÄ</mi><mo>*</mo></msup></math></formula>.</p>
          </li>
          <li id="uid23">
            <p noindent="true">We say that a (deterministic stationary) policy <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>œÄ</mi></math></formula> is greedy with respect to (w.r.t.) some function <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>V</mi></math></formula> (defined on <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>ùí≥</mi></math></formula>) if, for all <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mi>x</mi><mo>‚àà</mo><mi>ùí≥</mi></mrow></math></formula>,</p>
            <formula type="display">
              <math xmlns="http://www.w3.org/1998/Math/MathML" mode="display" overflow="scroll">
                <mrow>
                  <mi>œÄ</mi>
                  <mrow>
                    <mo>(</mo>
                    <mi>x</mi>
                    <mo>)</mo>
                  </mrow>
                  <mo>‚àà</mo>
                  <mo form="prefix">arg</mo>
                  <munder>
                    <mo movablelimits="true" form="prefix">max</mo>
                    <mrow>
                      <mi>a</mi>
                      <mo>‚àà</mo>
                      <mi>ùíú</mi>
                    </mrow>
                  </munder>
                  <munder>
                    <mo>‚àë</mo>
                    <mrow>
                      <msup>
                        <mi>x</mi>
                        <mo>'</mo>
                      </msup>
                      <mo>‚àà</mo>
                      <mi>ùí≥</mi>
                    </mrow>
                  </munder>
                  <mi>p</mi>
                  <mrow>
                    <mo>(</mo>
                    <msup>
                      <mi>x</mi>
                      <mo>'</mo>
                    </msup>
                    <mo>|</mo>
                    <mi>x</mi>
                    <mo>,</mo>
                    <mi>a</mi>
                    <mo>)</mo>
                  </mrow>
                  <mfenced separators="" open="[" close="]">
                    <mi>r</mi>
                    <mrow>
                      <mo>(</mo>
                      <mi>x</mi>
                      <mo>,</mo>
                      <mi>a</mi>
                      <mo>,</mo>
                      <msup>
                        <mi>x</mi>
                        <mo>'</mo>
                      </msup>
                      <mo>)</mo>
                    </mrow>
                    <mo>+</mo>
                    <mi>Œ≥</mi>
                    <mi>V</mi>
                    <mrow>
                      <mo>(</mo>
                      <msup>
                        <mi>x</mi>
                        <mo>'</mo>
                      </msup>
                      <mo>)</mo>
                    </mrow>
                  </mfenced>
                  <mo>.</mo>
                </mrow>
              </math>
            </formula>
            <p>¬†</p>
            <p noindent="true">where <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mo form="prefix">arg</mo><msub><mo movablelimits="true" form="prefix">max</mo><mrow><mi>a</mi><mo>‚àà</mo><mi>ùíú</mi></mrow></msub><mi>f</mi><mrow><mo>(</mo><mi>a</mi><mo>)</mo></mrow></mrow></math></formula> is the set of <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mi>a</mi><mo>‚àà</mo><mi>ùíú</mi></mrow></math></formula> that maximizes <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mi>f</mi><mo>(</mo><mi>a</mi><mo>)</mo></mrow></math></formula>. For any function <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>V</mi></math></formula>, such a greedy policy always exists because <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>ùíú</mi></math></formula> is finite.</p>
          </li>
        </simplelist>
        <p>The goal of Reinforcement Learning (RL), as well as that of dynamic programming, is to design an optimal policy (or a good approximation of it).</p>
        <p spacebefore="6.0pt">The well-known Dynamic Programming equation (also called the Bellman equation) provides a relation between the optimal value function at a state <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>x</mi></math></formula> and the optimal value function at the successors states <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msup><mi>x</mi><mo>'</mo></msup></math></formula> when choosing an optimal action: for all <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mi>x</mi><mo>‚àà</mo><mi>ùí≥</mi></mrow></math></formula>,</p>
        <formula id-text="2" id="uid24" textype="equation" type="display">
          <math xmlns="http://www.w3.org/1998/Math/MathML" mode="display" overflow="scroll">
            <mrow>
              <msup>
                <mi>V</mi>
                <mo>*</mo>
              </msup>
              <mrow>
                <mo>(</mo>
                <mi>x</mi>
                <mo>)</mo>
              </mrow>
              <mo>=</mo>
              <munder>
                <mo movablelimits="true" form="prefix">max</mo>
                <mrow>
                  <mi>a</mi>
                  <mo>‚àà</mo>
                  <mi>ùíú</mi>
                </mrow>
              </munder>
              <munder>
                <mo>‚àë</mo>
                <mrow>
                  <msup>
                    <mi>x</mi>
                    <mo>'</mo>
                  </msup>
                  <mo>‚àà</mo>
                  <mi>ùí≥</mi>
                </mrow>
              </munder>
              <mi>p</mi>
              <mrow>
                <mo>(</mo>
                <msup>
                  <mi>x</mi>
                  <mo>'</mo>
                </msup>
                <mo>|</mo>
                <mi>x</mi>
                <mo>,</mo>
                <mi>a</mi>
                <mo>)</mo>
              </mrow>
              <mfenced separators="" open="[" close="]">
                <mi>r</mi>
                <mrow>
                  <mo>(</mo>
                  <mi>x</mi>
                  <mo>,</mo>
                  <mi>a</mi>
                  <mo>,</mo>
                  <msup>
                    <mi>x</mi>
                    <mo>'</mo>
                  </msup>
                  <mo>)</mo>
                </mrow>
                <mo>+</mo>
                <mi>Œ≥</mi>
                <msup>
                  <mi>V</mi>
                  <mo>*</mo>
                </msup>
                <mrow>
                  <mo>(</mo>
                  <msup>
                    <mi>x</mi>
                    <mo>'</mo>
                  </msup>
                  <mo>)</mo>
                </mrow>
              </mfenced>
              <mo>.</mo>
            </mrow>
          </math>
        </formula>
        <p>The benefit of introducing this concept of optimal value function relies on the property that, from the optimal value function <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msup><mi>V</mi><mo>*</mo></msup></math></formula>, it is easy to derive an optimal behavior by choosing the actions according to a policy greedy w.r.t. <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msup><mi>V</mi><mo>*</mo></msup></math></formula>. Indeed, we have the property that a policy greedy w.r.t. the optimal value function is an optimal policy:</p>
        <formula id-text="3" id="uid25" textype="equation" type="display">
          <math xmlns="http://www.w3.org/1998/Math/MathML" mode="display" overflow="scroll">
            <mrow>
              <msup>
                <mi>œÄ</mi>
                <mo>*</mo>
              </msup>
              <mrow>
                <mo>(</mo>
                <mi>x</mi>
                <mo>)</mo>
              </mrow>
              <mo>‚àà</mo>
              <mo form="prefix">arg</mo>
              <munder>
                <mo movablelimits="true" form="prefix">max</mo>
                <mrow>
                  <mi>a</mi>
                  <mo>‚àà</mo>
                  <mi>ùíú</mi>
                </mrow>
              </munder>
              <munder>
                <mo>‚àë</mo>
                <mrow>
                  <msup>
                    <mi>x</mi>
                    <mo>'</mo>
                  </msup>
                  <mo>‚àà</mo>
                  <mi>ùí≥</mi>
                </mrow>
              </munder>
              <mi>p</mi>
              <mrow>
                <mo>(</mo>
                <msup>
                  <mi>x</mi>
                  <mo>'</mo>
                </msup>
                <mo>|</mo>
                <mi>x</mi>
                <mo>,</mo>
                <mi>a</mi>
                <mo>)</mo>
              </mrow>
              <mfenced separators="" open="[" close="]">
                <mi>r</mi>
                <mrow>
                  <mo>(</mo>
                  <mi>x</mi>
                  <mo>,</mo>
                  <mi>a</mi>
                  <mo>,</mo>
                  <msup>
                    <mi>x</mi>
                    <mo>'</mo>
                  </msup>
                  <mo>)</mo>
                </mrow>
                <mo>+</mo>
                <mi>Œ≥</mi>
                <msup>
                  <mi>V</mi>
                  <mo>*</mo>
                </msup>
                <mrow>
                  <mo>(</mo>
                  <msup>
                    <mi>x</mi>
                    <mo>'</mo>
                  </msup>
                  <mo>)</mo>
                </mrow>
              </mfenced>
              <mo>.</mo>
            </mrow>
          </math>
        </formula>
        <p>In short, we would like to mention that most of the reinforcement learning methods developed so far are built on one (or both) of the two following approaches ( <ref xlink:href="#sequel-2016-bid5" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>):</p>
        <simplelist>
          <li id="uid26">
            <p noindent="true">Bellman's dynamic programming approach, based on the introduction of the value function. It consists in learning a ‚Äúgood‚Äù approximation of the optimal value function, and then using it to derive a greedy policy w.r.t. this approximation. The hope (well justified in several cases) is that the performance <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msup><mi>V</mi><mi>œÄ</mi></msup></math></formula> of the policy <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>œÄ</mi></math></formula> greedy w.r.t. an approximation <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>V</mi></math></formula> of <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msup><mi>V</mi><mo>*</mo></msup></math></formula> will be close to optimality. This approximation issue of the optimal value function is one of the major challenges inherent to the reinforcement learning problem. <b>Approximate dynamic programming</b> addresses the problem of estimating performance bounds (<i>e.g.</i> the loss in performance <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mrow><mo>|</mo><mo>|</mo></mrow><msup><mi>V</mi><mo>*</mo></msup><mo>-</mo><msup><mi>V</mi><mi>œÄ</mi></msup><mrow><mo>|</mo><mo>|</mo></mrow></mrow></math></formula> resulting from using a policy <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>œÄ</mi></math></formula>-greedy w.r.t. some approximation <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>V</mi></math></formula>- instead of an optimal policy) in terms of the approximation error <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mrow><mo>|</mo><mo>|</mo></mrow><msup><mi>V</mi><mo>*</mo></msup><mo>-</mo><mi>V</mi><mrow><mo>|</mo><mo>|</mo></mrow></mrow></math></formula> of the optimal value function <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msup><mi>V</mi><mo>*</mo></msup></math></formula> by <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>V</mi></math></formula>. Approximation theory and Statistical Learning theory provide us with bounds in terms of the number of sample data used to represent the
functions, and the capacity and approximation power of the considered function spaces.</p>
          </li>
          <li id="uid27">
            <p noindent="true">Pontryagin's maximum principle approach, based on sensitivity analysis of the performance measure w.r.t. some control parameters. This approach, also called <b>direct policy search</b> in the Reinforcement Learning community aims at directly finding a good feedback control law in a parameterized policy space without trying to approximate the value function. The method consists in estimating the so-called <b>policy gradient</b>, <i>i.e.</i> the sensitivity of the performance measure (the value function) w.r.t. some parameters of the current policy. The idea being that an optimal control problem is replaced by a parametric optimization problem in the space of parameterized policies. As such, deriving a policy gradient estimate would lead to performing a stochastic gradient method in order to search for a local optimal parametric policy.</p>
          </li>
        </simplelist>
        <p>Finally, many extensions of the Markov decision processes exist, among which the Partially Observable MDPs (POMDPs) is the case where the current state does not contain all the necessary information required to decide for sure of the best action.</p>
      </subsection>
      <subsection id="uid28" level="2">
        <bodyTitle>Multi-arm Bandit Theory</bodyTitle>
        <p>Bandit problems illustrate the fundamental difficulty of decision making in the face of uncertainty: A decision maker must choose between what seems to be the best choice (‚Äúexploit‚Äù), or to test (‚Äúexplore‚Äù) some alternative, hoping to discover a choice that beats the current best choice.</p>
        <p>The classical example of a bandit problem is deciding what treatment to give each patient in a clinical trial when the effectiveness of the treatments are initially unknown and the patients arrive sequentially. These bandit problems became popular with the seminal paper <ref xlink:href="#sequel-2016-bid6" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, after which they have found applications in diverse fields, such as control, economics, statistics, or learning theory.</p>
        <p>Formally, a K-armed bandit problem (<formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mi>K</mi><mo>‚â•</mo><mn>2</mn></mrow></math></formula>) is specified by K real-valued distributions. In each time step a decision maker can select one of the distributions to obtain a sample from it. The samples obtained are considered as rewards. The distributions are initially unknown to the decision maker, whose goal is to maximize the sum of the rewards received, or equivalently, to minimize the regret which is defined as the loss compared to the total payoff that can be achieved given full knowledge of the problem, <i>i.e.</i>, when the arm giving the highest expected reward is pulled all the time.</p>
        <p>The name ‚Äúbandit‚Äù comes from imagining a gambler playing with K slot machines. The gambler can pull the arm of any of the machines, which produces a random payoff as a result: When arm k is pulled, the random payoff is drawn from the distribution associated to k. Since the payoff distributions are initially unknown, the gambler must use exploratory actions to learn the utility of the individual arms. However, exploration has to be carefully controlled since excessive exploration may lead to unnecessary losses. Hence, to play well, the gambler must carefully balance exploration and exploitation. Auer <i>et al.</i> <ref xlink:href="#sequel-2016-bid7" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> introduced the algorithm UCB (Upper Confidence Bounds) that follows what is now called the ‚Äúoptimism in the face of uncertainty principle‚Äù. Their algorithm works by computing upper confidence bounds for all the arms and then choosing the arm with the highest such bound. They proved that the expected regret of their algorithm increases at most at a logarithmic rate
with the number of trials, and that the algorithm achieves the smallest possible regret up to some sub-logarithmic factor (for the considered family of distributions).</p>
      </subsection>
    </subsection>
    <subsection id="uid29" level="1">
      <bodyTitle>Statistical analysis of time series</bodyTitle>
      <p>Many of the problems of machine learning can be seen as extensions of classical problems of mathematical statistics to their (extremely) non-parametric and model-free cases. Other machine learning problems are founded on such statistical problems. Statistical problems of sequential learning are mainly those that are concerned with the analysis of time series. These problems are as follows.</p>
      <subsection id="uid30" level="2">
        <bodyTitle>Prediction of Sequences of Structured and Unstructured Data</bodyTitle>
        <p>Given a series of observations <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mo>‚ãØ</mo><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub></mrow></math></formula> it is required to give forecasts concerning the distribution of the future observations <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><msub><mi>x</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>x</mi><mrow><mi>n</mi><mo>+</mo><mn>2</mn></mrow></msub><mo>,</mo><mo>‚ãØ</mo></mrow></math></formula>; in the simplest case, that of the next outcome <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msub><mi>x</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub></math></formula>.
Then <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msub><mi>x</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub></math></formula> is revealed and the process continues. Different goals can be formulated in this setting. One can either make some assumptions on the probability
measure that generates the sequence <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mo>‚ãØ</mo><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub><mo>,</mo><mo>‚ãØ</mo></mrow></math></formula>, such as that the outcomes are independent and identically distributed (i.i.d.),
or that the sequence is a Markov chain, that it is a stationary process, etc.
More generally, one can assume that the data is generated by a probability measure that belongs to a certain set <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>ùíû</mi></math></formula>.
In these cases the goal is to have the discrepancy between the predicted and the ‚Äútrue‚Äù probabilities to go to zero, if possible, with guarantees
on the speed of convergence.</p>
        <p>Alternatively, rather than making some assumptions on the data, one can change the goal: the predicted probabilities should be asymptotically as good as those given by the best reference predictor from a certain pre-defined set.</p>
        <p>Another dimension of complexity in this problem concerns the nature of observations <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msub><mi>x</mi><mi>i</mi></msub></math></formula>. In the simplest case,
they come from a finite space, but already basic applications often require real-valued observations. Moreover,
function or even graph-valued observations often arise in practice, in particular in applications concerning Web data.
In these settings estimating even simple characteristics of probability distributions of the future outcomes becomes
non-trivial, and new learning algorithms for solving these problems are in order.</p>
      </subsection>
      <subsection id="uid31" level="2">
        <bodyTitle>Hypothesis testing</bodyTitle>
        <p>Given a series of observations of <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mo>‚ãØ</mo><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub><mo>,</mo><mo>‚ãØ</mo></mrow></math></formula> generated by some unknown probability measure <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>Œº</mi></math></formula>, the problem is to test a certain given hypothesis <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msub><mi>H</mi><mn>0</mn></msub></math></formula> about <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>Œº</mi></math></formula>, versus a given alternative hypothesis <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msub><mi>H</mi><mn>1</mn></msub></math></formula>. There are many different examples of this problem. Perhaps the simplest one is testing a simple hypothesis ‚Äú<formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>Œº</mi></math></formula> is Bernoulli i.i.d. measure with probability of 0 equals 1/2‚Äù versus ‚Äú<formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>Œº</mi></math></formula> is Bernoulli i.i.d. with the parameter different from 1/2‚Äù. More
interesting cases include the problems of model verification: for example, testing that <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>Œº</mi></math></formula> is a Markov chain, versus that it is a stationary ergodic process but not a Markov chain. In the case when we have not one but several series of observations, we may wish to test the hypothesis that they are independent, or that they are generated by the same distribution. Applications of these problems to a more general class of machine learning tasks include the problem of feature selection, the problem of testing that a certain behaviour (such as pulling a certain arm of a bandit, or using a certain policy) is better (in terms of achieving some goal, or collecting some rewards) than another behaviour, or than a class of other behaviours.</p>
        <p>The problem of hypothesis testing can also be studied in its general formulations: given two (abstract) hypothesis <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msub><mi>H</mi><mn>0</mn></msub></math></formula> and <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msub><mi>H</mi><mn>1</mn></msub></math></formula> about the unknown measure that generates the data, find out whether it is possible to test <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msub><mi>H</mi><mn>0</mn></msub></math></formula> against <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msub><mi>H</mi><mn>1</mn></msub></math></formula> (with confidence), and if yes then how can one do it.</p>
      </subsection>
      <subsection id="uid32" level="2">
        <bodyTitle>Change Point Analysis</bodyTitle>
        <p>A stochastic process is generating the data. At some point, the process distribution changes.
In the ‚Äúoffline‚Äù situation, the statistician observes the resulting sequence of outcomes and has
to estimate the point or the points at which the change(s) occurred. In online setting, the goal is to
detect the change as quickly as possible.</p>
        <p>These are the classical problems in mathematical statistics, and probably among the last remaining statistical problems
not adequately addressed by machine learning methods. The reason for the latter is perhaps in that the problem is rather
challenging. Thus, most methods available so far are parametric methods concerning piece-wise constant distributions, and the
change in distribution is associated with the change in the mean. However, many applications, including DNA analysis,
the analysis of (user) behaviour data, etc., fail to comply with this kind of assumptions. Thus, our goal here is to provide completely non-parametric
methods allowing for any kind of changes in the time-series distribution.</p>
      </subsection>
      <subsection id="uid33" level="2">
        <bodyTitle>Clustering Time Series, Online and Offline</bodyTitle>
        <p>The problem of clustering, while being a classical problem of mathematical statistics, belongs to the realm of unsupervised learning. For time series, this problem can be formulated as follows: given several samples <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><msup><mi>x</mi><mn>1</mn></msup><mo>=</mo><mrow><mo>(</mo><msubsup><mi>x</mi><mn>1</mn><mn>1</mn></msubsup><mo>,</mo><mo>‚ãØ</mo><mo>,</mo><msubsup><mi>x</mi><msub><mi>n</mi><mn>1</mn></msub><mn>1</mn></msubsup><mo>)</mo></mrow><mo>,</mo><mo>‚ãØ</mo><mo>,</mo><msup><mi>x</mi><mi>N</mi></msup><mo>=</mo><mrow><mo>(</mo><msubsup><mi>x</mi><mi>N</mi><mn>1</mn></msubsup><mo>,</mo><mo>‚ãØ</mo><mo>,</mo><msubsup><mi>x</mi><msub><mi>n</mi><mi>N</mi></msub><mi>N</mi></msubsup><mo>)</mo></mrow></mrow></math></formula>, we wish to group similar objects together. While this is of course not a precise formulation, it can be made precise if we assume that the samples were generated by <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>k</mi></math></formula> different distributions.</p>
        <p>The online version of the problem allows for the number of observed time series to grow with time, in general, in an arbitrary manner.</p>
      </subsection>
      <subsection id="uid34" level="2">
        <bodyTitle>Online Semi-Supervised Learning</bodyTitle>
        <p>Semi-supervised learning (SSL) is a field of machine learning that studies
learning from both labeled and unlabeled examples. This learning
paradigm is extremely useful for solving real-world problems, where
data is often abundant but the resources to label them are limited.</p>
        <p>Furthermore, <i>online</i> SSL is suitable for adaptive machine
learning systems. In the classification case, learning is viewed as a
repeated game against a potentially adversarial nature. At each step
<formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>t</mi></math></formula> of this game, we observe an example <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msub><mi>ùê±</mi><mi>ùê≠</mi></msub></math></formula>, and then predict
its label <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msub><mover accent="true"><mi>y</mi><mo>^</mo></mover><mi>t</mi></msub></math></formula>.</p>
        <p>The challenge of the game is that we only exceptionally observe the true label
<formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msub><mi>y</mi><mi>t</mi></msub></math></formula>. In the extreme case, which we also study, only a handful of labeled
examples are provided in advance and set the initial bias of the system while
unlabeled examples are gathered online and update the bias continuously.
Thus, if we want to adapt to changes in the environment, we have to rely on
indirect forms of feedback, such as the structure of data.</p>
      </subsection>
      <subsection id="uid35" level="2">
        <bodyTitle>Online Kernel and Graph-Based Methods</bodyTitle>
        <p>Large-scale kernel ridge regression is limited by the need to store a
large kernel matrix. Similarly, large-scale graph-based learning is
limited by storing the graph Laplacian. Furthermore, if the data come
online, at some point no finite storage is sufficient and per step
operations become slow.</p>
        <p>Our challenge is to design sparsification methods that give guaranteed
approximate solutions with a reduced storage requirements.</p>
      </subsection>
    </subsection>
  </fondements>
  <domaine id="uid36">
    <bodyTitle>Application Domains</bodyTitle>
    <subsection id="uid37" level="1">
      <bodyTitle>Sequential decision making under uncertainty and prediction</bodyTitle>
      <p>The spectrum of applications of our research is very wide: it ranges from the core of our research, that is sequential decision making under uncertainty, to the application of components used to solve this decision making problem.</p>
      <p>To be more specific, we work on computational advertizing and recommandation systems; these problems are considered as a sequential matching problem in which resources available in a limited amount have to be matched to meet some users' expectations. The sequential approach we advocate paves the way to better tackle the cold-start problem, and non stationary environments. More generally, these approaches are applied to the optimization of budgeted resources under uncertainty, in a time-varying environment, including constraints on computational times (typically, a decision has to be made in less than 1 ms in a recommandation system). An other field of applications of our research is related to education which we consider as a sequential matching problem between a student, and educational contents.</p>
      <p>The algorithms to solve these tasks heavily rely on tools from machine learning, statistics, and optimization. Henceforth, we also apply our work to more classical supervised learning, and prediction tasks, as well as unsupervised learning tasks. The whole range of methods is used, from decision forests, to kernel methods, to deep learning. For instance, we have recently used deep learning on images. We also have a line of works related to software development studying how machine learning can improve the quality of software being developed. More generally, we apply our research to data science.
</p>
    </subsection>
  </domaine>
  <highlights id="uid38">
    <bodyTitle>Highlights of the Year</bodyTitle>
    <subsection id="uid39" level="1">
      <bodyTitle>Highlights of the Year</bodyTitle>
      <simplelist>
        <li id="uid40">
          <p noindent="true">Grill, Valko &amp; Munos gave an oral presentation at NIPS. Oral presentations at NIPS are rare: out of 2500+ submissions, only 1.8% are presented orally.</p>
        </li>
        <li id="uid41">
          <p noindent="true">Using a deep learning approach (sparse denoising autoencoders), Strub, Mary &amp; Gaudel have obtained the best ever published results on the data from the Netflix challenge on recommendation systems. 10 years ago, such an achievement was worth 1M$.</p>
        </li>
      </simplelist>
    </subsection>
  </highlights>
  <logiciels id="uid42">
    <bodyTitle>New Software and Platforms</bodyTitle>
    <subsection id="uid43" level="1">
      <bodyTitle>BAC</bodyTitle>
      <p>Bayesian Policy Gradient and Actor-Critic Algorithms</p>
      <p noindent="true"><span class="smallcap" align="left">Keywords:</span> Machine learning - Incremental learning - Policy Learning</p>
      <p noindent="true">
        <span class="smallcap" align="left">Functional Description</span>
      </p>
      <p>To address this issue, we proceed to supplement our Bayesian policy gradient framework with a new actor-critic learning model in which a Bayesian class of non-parametric critics, based on Gaussian process temporal difference learning, is used. Such critics model the action-value function as a Gaussian process, allowing Bayes‚Äô rule to be used in computing the posterior distribution over action-value functions, conditioned on the observed data. Appropriate choices of the policy parameterization and of the prior covariance (kernel) between action-values allow us to obtain closed-form expressions for the posterior distribution of the gradient of the expected return with respect to the policy parameters. We perform detailed experimental comparisons of the proposed Bayesian policy gradient and actor-critic algorithms with classic Monte-Carlo based policy gradient methods, as well as with each other, on a number of reinforcement learning problems.</p>
      <simplelist>
        <li id="uid44">
          <p noindent="true">Contact: Michal Valko</p>
        </li>
        <li id="uid45">
          <p noindent="true">URL: <ref xlink:href="https://team.inria.fr/sequel/Software/BAC/" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>team.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>sequel/<allowbreak/>Software/<allowbreak/>BAC/</ref></p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid46" level="1">
      <bodyTitle>Collaborative Filtering Network</bodyTitle>
      <p><span class="smallcap" align="left">Keywords:</span> Recommender system - Neural networks - Deep learning</p>
      <p noindent="true">
        <span class="smallcap" align="left">Functional Description</span>
      </p>
      <p>Recommendation systems advise users on which items (movies, musics, books etc.) they are more likely to be interested in. A good recommendation system may dramatically increase the amount of sales of a firm or retain customers. For instance, 80% of movies watched on Netflix come from the recommender system of the company. Colaborative Filtering (CF) aims at recommending an item to a user by predicting how a user would rate this item. To do so, the feedback of one user on some items is combined with the feedback of all other users on all items to predict a new rating. For instance, if someone rated a few books, CF objective is to estimate the ratings he would have given to thousands of other books by using the ratings of all the other readers.</p>
      <p>The following module tackles Collaborative Filtering tasks by using a novel approach based on neural networks (sparse denoising autoencoders). In a few words, the module lets the user train neural networks to predict unknown entries in a history files.</p>
      <p>The input files are classic csv files. The output files can either be the full matrix of ratings and/or the network weights. The root mean square error is computed to assess the quality of the training.</p>
      <p>This module is based on Lua/Torch Framework. It works on both CPU/GPU and it is multithreaded.</p>
      <simplelist>
        <li id="uid47">
          <p noindent="true">Contact: Florian Strub</p>
        </li>
        <li id="uid48">
          <p noindent="true">URL: <ref xlink:href="https://github.com/fstrub95/Autoencoders_cf" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>github.<allowbreak/>com/<allowbreak/>fstrub95/<allowbreak/>Autoencoders_cf</ref></p>
        </li>
      </simplelist>
    </subsection>
  </logiciels>
  <resultats id="uid49">
    <bodyTitle>New Results</bodyTitle>
    <subsection id="uid50" level="1">
      <bodyTitle>Decision-making Under Uncertainty</bodyTitle>
      <subsection id="uid51" level="2">
        <bodyTitle>Reinforcement Learning</bodyTitle>
        <p><b>Analysis of Classification-based Policy Iteration Algorithms</b>, <ref xlink:href="#sequel-2016-bid8" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
        <p>We introduce a variant of the classification-based approach to policy iteration which uses a cost-sensitive loss function weighting each classification mistake by its actual regret, that is, the difference between the action-value of the greedy action and of the action chosen by the classifier. For this algorithm, we provide a full finite-sample analysis. Our results state a performance bound in terms of the number of policy improvement steps, the number of rollouts used in each iteration, the capacity of the considered policy space (classifier), and a capacity measure which indicates how well the policy space can approximate policies that are greedy with respect to any of its members. The analysis reveals a tradeoff between the estimation and approximation errors in this classification-based policy iteration setting. Furthermore it confirms the intuition that classification-based policy iteration algorithms could be favorably compared to value-based approaches when the policies can be approximated more easily than their corresponding value functions. We also study the consistency of the algorithm when there exists a sequence of policy spaces with increasing capacity.</p>
        <p><b>Reinforcement Learning of POMDPs using Spectral Methods</b>, <ref xlink:href="#sequel-2016-bid9" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
        <p>We propose a new reinforcement learning algorithm for partially observable Markov decision processes (POMDP) based on spectral decomposition methods. While spectral methods have been previously employed for consistent learning of (passive) latent variable models such as hidden Markov models, POMDPs are more challenging since the learner interacts with the environment and possibly changes the future observations in the process. We devise a learning algorithm running through episodes, in each episode we employ spectral techniques to learn the POMDP parameters from a trajectory generated by a fixed policy. At the end of the episode, an optimization oracle returns the optimal memoryless planning policy which maximizes the expected reward based on the estimated POMDP model. We prove an order-optimal regret bound w.r.t. the optimal memoryless policy and efficient scaling with respect to the dimensionality of observation and action spaces.</p>
        <p><b>Bayesian Policy Gradient and Actor-Critic Algorithms</b>, <ref xlink:href="#sequel-2016-bid10" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
        <p>Policy gradient methods are reinforcement learning algorithms that adapt a parameterized policy by following a performance gradient estimate. Many conventional policy gradient methods use Monte-Carlo techniques to estimate this gradient. The policy is improved by adjusting the parameters in the direction of the gradient estimate. Since Monte-Carlo methods tend to have high variance, a large number of samples is required to attain accurate estimates, resulting in slow convergence. In this paper, we first propose a Bayesian framework for policy gradient, based on modeling the policy gradient as a Gaussian process. This reduces the number of samples needed to obtain accurate gradient estimates. Moreover, estimates of the natural gradient as well as a measure of the uncertainty in the gradient estimates, namely, the gradient covariance, are provided at little extra cost. Since the proposed Bayesian framework considers system trajectories as its basic observable unit, it does not require the dynamics within trajectories to be of any particular form, and thus, can be easily extended to partially observable problems. On the downside, it cannot take advantage of the Markov property when the system is Markovian. To address this issue, we proceed to supplement our Bayesian policy gradient framework with a new actor-critic learning model in which a Bayesian class of non-parametric critics, based on Gaussian process temporal difference learning, is used. Such critics model the action-value function as a Gaussian process, allowing Bayes‚Äô rule to be used in computing the posterior distribution over action-value functions, conditioned on the observed data. Appropriate choices of the policy parameterization and of the prior covariance (kernel) between action-values allow us to obtain closed-form expressions for the posterior distribution of the gradient of the expected return with respect to the policy parameters. We perform detailed experimental comparisons of the proposed Bayesian policy gradient and actor-critic algorithms with classic Monte-Carlo based policy gradient methods, as well as with each other, on a number of reinforcement learning problems.</p>
      </subsection>
      <subsection id="uid52" level="2">
        <bodyTitle>Multi-arm Bandit Theory</bodyTitle>
        <p><b>Improved Learning Complexity in Combinatorial Pure Exploration Bandits</b>, <ref xlink:href="#sequel-2016-bid11" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
        <p>We study the problem of combinatorial pure exploration in the stochastic multi-armed bandit problem. We first construct a new measure of complexity that provably characterizes the learning performance of the algorithms we propose for the fixed confidence and the fixed budget setting. We show that this complexity is never higher than the one in existing work and illustrate a number of configurations in which it can be significantly smaller. While in general this improvement comes at the cost of increased computational complexity, we provide a series of examples , including a planning problem, where this extra cost is not significant.</p>
        <p><b>Online learning with noisy side observations</b>, <ref xlink:href="#sequel-2016-bid12" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
        <p>We propose a new partial-observability model for online learning problems where the learner, besides its own loss, also observes some noisy feedback about the other actions, depending on the underlying structure of the problem. We represent this structure by a weighted directed graph, where the edge weights are related to the quality of the feedback shared by the connected nodes. Our main contribution is an efficient algorithm that guarantees a regret of <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mi>O</mi><mo>(</mo><msqrt><mrow><mi>Œ±</mi><mo>*</mo><mi>T</mi></mrow></msqrt><mo>)</mo></mrow></math></formula> after T rounds, where <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>Œ±</mi></math></formula> * is a novel graph property that we call the effective independence number. Our algorithm is completely parameter-free and does not require knowledge (or even estimation) of <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>Œ±</mi></math></formula> *. For the special case of binary edge weights, our setting reduces to the partial-observability models of Mannor &amp; Shamir (2011) and Alon et al. (2013) and our algorithm recovers the near-optimal regret bounds.</p>
        <p><b>Online learning with Erd√∂s-R√©nyi side-observation graphs</b>, <ref xlink:href="#sequel-2016-bid13" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
        <p>We consider adversarial multi-armed bandit problems where the learner is allowed to observe losses of a number of arms beside the arm that it actually chose. We study the case where all non-chosen arms reveal their loss with an unknown probability rt, independently of each other and the action of the learner. Moreover, we allow rt to change in every round t, which rules out the possibility of estimating rt by a well-concentrated sample average. We propose an algorithm which operates under the assumption that rt is large enough to warrant at least one side observation with high probability. We show that after T rounds in a bandit problem with N arms, the expected regret of our algorithm is of order O(sqrt(sum(t=1)T (1/rt) log N )), given that rt less than log T / (2N-2) for all t. All our bounds are within logarithmic factors of the best achievable performance of any algorithm that is even allowed to know exact values of rt.</p>
        <p><b>Revealing graph bandits for maximizing local influence</b>, <ref xlink:href="#sequel-2016-bid14" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
        <p>We study a graph bandit setting where the objective of the learner is to detect the most influential node of a graph by requesting as little information from the graph as possible. One of the relevant applications for this setting is marketing in social networks, where the marketer aims at finding and taking advantage of the most influential customers. The existing approaches for bandit problems on graphs require either partial or complete knowledge of the graph. In this paper, we do not assume any knowledge of the graph, but we consider a setting where it can be gradually discovered in a sequential and active way. At each round, the learner chooses a node of the graph and the only information it receives is a stochastic set of the nodes that the chosen node is currently influencing. To address this setting, we propose BARE, a bandit strategy for which we prove a regret guarantee that scales with the detectable dimension, a problem dependent quantity that is often much smaller than the number of nodes.</p>
        <p><b>Algorithms for Differentially Private Multi-Armed Bandits</b>, <ref xlink:href="#sequel-2016-bid15" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
        <p>We present differentially private algorithms for the stochastic Multi-Armed Bandit (MAB) problem. This is a problem for applications such as adaptive clinical trials, experiment design, and user-targeted advertising where private information is connected to individual rewards. Our major contribution is to show that there exist <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mo>(</mo><mi>œµ</mi><mo>,</mo><mi>Œ¥</mi><mo>)</mo></mrow></math></formula> differentially private variants of Upper Confidence Bound algorithms which have optimal regret, <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mi>O</mi><mo>(</mo><msup><mi>œµ</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>+</mo><mo form="prefix">log</mo><mi>T</mi><mo>)</mo></mrow></math></formula>. This is a significant improvement over previous results, which only achieve poly-log regret <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mi>O</mi><mo>(</mo><msup><mi>œµ</mi><mrow><mo>-</mo><mn>2</mn></mrow></msup><msup><mo form="prefix">log</mo><mn>2</mn></msup><mi>T</mi><mo>)</mo></mrow></math></formula>, because of our use of a novel interval-based mechanism. We also substantially improve the bounds of previous family of algorithms which use a continual release mechanism. Experiments clearly validate our theoretical bounds.</p>
        <p><b>On the Complexity of Best Arm Identification in Multi-Armed Bandit Models</b>, <ref xlink:href="#sequel-2016-bid16" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
        <p>The stochastic multi-armed bandit model is a simple abstraction that has proven useful in many different contexts in statistics and machine learning. Whereas the achievable limit in terms of regret minimization is now well known, our aim is to contribute to a better understanding of the performance in terms of identifying the m best arms. We introduce generic notions of complexity for the two dominant frameworks considered in the literature: fixed-budget and fixed-confidence settings. In the fixed-confidence setting, we provide the first known distribution-dependent lower bound on the complexity that involves information-theoretic quantities and holds when m is larger than 1 under general assumptions. In the specific case of two armed-bandits, we derive refined lower bounds in both the fixed-confidence and fixed-budget settings, along with matching algorithms for Gaussian and Bernoulli bandit models. These results show in particular that the complexity of the fixed-budget setting may be smaller than the complexity of the fixed-confidence setting, contradicting the familiar behavior observed when testing fully specified alternatives. In addition, we also provide improved sequential stopping rules that have guaranteed error probabilities and shorter average running times. The proofs rely on two technical results that are of independent interest : a deviation lemma for self-normalized sums (Lemma 19) and a novel change of measure inequality for bandit models (Lemma 1).</p>
        <p><b>Optimal Best Arm Identification with Fixed Confidence</b>, <ref xlink:href="#sequel-2016-bid17" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
        <p>We give a complete characterization of the complexity of best-arm identification in one-parameter bandit problems. We prove a new, tight lower bound on the sample complexity. We propose the `Track-and-Stop' strategy, which we prove to be asymptotically optimal. It consists in a new sampling rule (which tracks the optimal proportions of arm draws highlighted by the lower bound) and in a stopping rule named after Chernoff, for which we give a new analysis.</p>
        <p><b>On Explore-Then-Commit Strategies</b>, <ref xlink:href="#sequel-2016-bid18" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
        <p>We study the problem of minimising regret in two-armed bandit problems with Gaussian rewards. Our objective is to use this simple setting to illustrate that strategies based on an exploration phase (up to a stopping time) followed by exploitation are necessarily suboptimal. The results hold regardless of whether or not the difference in means between the two arms is known. Besides the main message, we also refine existing deviation inequalities, which allow us to design fully sequential strategies with finite-time regret guarantees that are (a) asymptotically optimal as the horizon grows and (b) order-optimal in the minimax sense. Furthermore we provide empirical evidence that the theory also holds in practice and discuss extensions to non-gaussian and multiple-armed case.</p>
      </subsection>
      <subsection id="uid53" level="2">
        <bodyTitle>Recommendation systems</bodyTitle>
        <p><b>Scalable explore-exploit Collaborative Filtering</b>, <ref xlink:href="#sequel-2016-bid19" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
        <p>Recommender Systems (RS) aim at suggesting to users one or several items in which they might have interest. These systems have to update themselves as users provide new ratings, but also as new users/items enter the system. While this adaptation makes recommendation an intrinsically sequential task, most researches about RS based on Collaborative Filtering are omitting this fact, as well as the ensuing exploration/exploitation dilemma: should the system recommend items which bring more information about the users (explore), or should it try to get an immediate feedback as high as possible (exploit)? Recently, a few approaches were proposed to solve that dilemma, but they do not meet requirements to scale up to real life applications which is a crucial point as the number of items available on RS and the number of users in these systems explode. In this paper, we present an explore-exploit Collaborative Filtering RS which is both efficient and scales well. Extensive experiments on some of the largest available real-world datasets show that the proposed approach performs accurate personalized recommendations in less than a millisecond per recommendation, which makes it a good candidate for true applications.</p>
        <p><b>Large-scale Bandit Recommender System</b>, <ref xlink:href="#sequel-2016-bid20" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
        <p>The main target of Recommender Systems (RS) is to propose to users one or several items in which they might be interested. However, as users provide more feedback, the recommendation process has to take these new data into consideration. The necessity of this update phase makes recommendation an intrinsically sequential task. A few approaches were recently proposed to address this issue, but they do not meet the need to scale up to real life applications. In this paper , we present a Collaborative Filtering RS method based on Matrix Factorization and Multi-Armed Bandits. This approach aims at good recommendations with a narrow computation time. Several experiments on large datasets show that the proposed approach performs personalized recommendations in less than a millisecond per recommendation.</p>
        <p><b>Sequential Collaborative Ranking Using (No-)Click Implicit Feedback</b>, <ref xlink:href="#sequel-2016-bid21" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
        <p>We study Recommender Systems in the context where they suggest a list of items to users. Several crucial issues are raised in such a setting: first, identify the relevant items to recommend; second, account for the feedback given by the user after he clicked and rated an item; third, since new feedback arrive into the system at any moment, incorporate such information to improve future recommendations. In this paper, we take these three aspects into consideration and present an approach handling click/no-click feedback information. Experiments on real-world datasets show that our approach outperforms state of the art algorithms.</p>
        <p><b>Hybrid Recommender System based on Autoencoders</b>, <ref xlink:href="#sequel-2016-bid22" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
        <p>A standard model for Recommender Systems is the Matrix Completion setting: given partially known matrix of ratings given by users (rows) to items (columns), infer the unknown ratings. In the last decades, few attempts where done to handle that objective with Neural Networks, but recently an architecture based on Autoencoders proved to be a promising approach. In current paper, we enhanced that architecture (i) by using a loss function adapted to input data with missing values, and (ii) by incorporating side information. The experiments demonstrate that while side information only slightly improve the test error averaged on all users/items, it has more impact on cold users/items.</p>
        <p><b>Compromis exploration-exploitation pour syst√®me de recommandation √† grande √©chelle</b>, <ref xlink:href="#sequel-2016-bid23" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
        <p>Les syst√®mes de recommandation recommandent √† des utilisateurs un ou des produits qui pourraient les int√©resser. La recommandation se fonde sur les retours des utilisateurs par le pass√©, lors des pr√©c√©dentes recommandations. La recommandation est donc un probl√®me s√©quentiel et le syst√®me de recommandation recommande (i) pour obtenir une bonne r√©compense, mais aussi (ii) pour mieux cern√© l'utilisateur/les produits et ainsi obtenir de meilleures r√©compenses par la suite. Quelques approches r√©centes ciblent ce double objectif mais elles sont trop gourmandes en temps de calcul pour s'appliquer √† certaines applications de la vie r√©elle. Dans cet article, nous pr√©sentons un syst√®me de recommandation fond√© sur la factorisation de matrice et les bandits manchots. Plusieurs exp√©riences sur de grandes base de donn√©es montrent que l'approche propos√©e fournit de bonnes recommendations en moins d'une milli-seconde par recommandation.</p>
        <p><b>Filtrage Collaboratif Hybride avec des Auto-encodeurs</b>, <ref xlink:href="#sequel-2016-bid24" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
        <p>Le filtrage collaboratif (CF) exploite les retours des utilisateurs pour leur fournir des recommandations personnalis√©es. Lorsque ces algorithmes ont acc√®s √† des informations compl√©mentaires, ils ont de meilleurs r√©sultats et g√®rent plus efficacement le d√©marrage √† froid. Bien que les r√©seaux de neurones (NN) remportent de nombreux succ√®s en traitement d'images, ils ont re√ßu beaucoup moins d'attention dans la communaut√© du CF. C'est d'autant plus surprenant que les NN apprennent comme les algorithme de CF une repr√©sentation latente des donn√©es. Dans cet article, nous introduisons une architecture de NN adapt√©e au CF (nomm√©e CFN) qui prend en compte la parcimonie des donn√©es et les informations compl√©mentaires. Nous montrons empiriquement sur les bases de donn√©es MovieLens et Douban que CFN b√¢t l'√©tat de l'art et profite des informations compl√©mentaires. Nous fournissons une impl√©mentation de l'algorithme sous forme d'un plugin pour Torch.</p>
      </subsection>
      <subsection id="uid54" level="2">
        <bodyTitle>Nonparametric statistics of time series</bodyTitle>
        <p><b>Things Bayes can't do</b>, <ref xlink:href="#sequel-2016-bid25" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
        <p>The problem of forecasting conditional probabilities of the next event given the past is consideredin a general probabilistic setting. Given an arbitrary (large, uncountable) set C of predictors, we would like to construct a single predictor that performs asymptotically as well as the best predictor in C, on any data. Here we show that there are sets C for which such predictors exist, but none of them is a Bayesian predictor with a prior concentrated on C.In other words, there is a predictor with sublinear regret, but every Bayesian predictor must have a linear regret. This negative finding is in sharp contrast with previous resultsthat establish the opposite for the case when one of the predictors in C achieves asymptotically vanishing error.In such a case, if there is a predictor that achieves asymptotically vanishing error for any measure in C, then there is a Bayesian predictor that also has this property, and whose prior is concentrated on (a countable subset of) C.</p>
      </subsection>
      <subsection id="uid55" level="2">
        <bodyTitle>Imitation and Inverse Reinforcement Learning</bodyTitle>
        <p><b>Score-based Inverse Reinforcement Learning</b>, <ref xlink:href="#sequel-2016-bid26" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
        <p>This paper reports theoretical and empirical results obtained for the score-based Inverse Reinforcement Learning (IRL) algorithm. It relies on a non-standard setting for IRL consisting of learning a reward from a set of globally scored trajec-tories. This allows using any type of policy (optimal or not) to generate trajectories without prior knowledge during data collection. This way, any existing database (like logs of systems in use) can be scored a posteriori by an expert and used to learn a reward function. Thanks to this reward function, it is shown that a near-optimal policy can be computed. Being related to least-square regression, the algorithm (called SBIRL) comes with theoretical guarantees that are proven in this paper. SBIRL is compared to standard IRL algorithms on synthetic data showing that annotations do help under conditions on the quality of the trajectories. It is also shown to be suitable for real-world applications such as the optimisation of a spoken dialogue system.</p>
      </subsection>
      <subsection id="uid56" level="2">
        <bodyTitle>Stochastic Games</bodyTitle>
        <p><b>Blazing the trails before beating the path: Sample-efficient Monte-Carlo planning</b>, <ref xlink:href="#sequel-2016-bid27" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
        <p>You are a robot and you live in a Markov decision process (MDP) with a finite or an infinite number of transitions from state-action to next states. You got brains and so you plan before you act. Luckily, your roboparents equipped you with a generative model to do some Monte-Carlo planning. The world is waiting for you and you have no time to waste. You want your planning to be efficient. Sample-efficient. Indeed, you want to exploit the possible structure of the MDP by exploring only a subset of states reachable by following near-optimal policies. You want guarantees on sample complexity that depend on a measure of the quantity of near-optimal states. You want something, that is an extension of Monte-Carlo sampling (for estimating an expectation) to problems that alternate maximization (over actions) and expectation (over next states). But you do not want to StOP with exponential running time, you want something simple to implement and computationally efficient. You want it all and you want it now. You want TrailBlazer.</p>
        <p><b>Maximin Action Identification: A New Bandit Framework for Games</b>, <ref xlink:href="#sequel-2016-bid28" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
        <p>We study an original problem of pure exploration in a strategic bandit model motivated by Monte Carlo Tree Search. It consists in identifying the best action in a game, when the player may sample random outcomes of sequentially chosen pairs of actions. We propose two strategies for the fixed-confidence setting: Maximin-LUCB, based on lower-and upper-confidence bounds; and Maximin-Racing, which operates by successively eliminating the sub-optimal actions. We discuss the sample complexity of both methods and compare their performance empirically. We sketch a lower bound analysis, and possible connections to an optimal algorithm.</p>
      </subsection>
    </subsection>
    <subsection id="uid57" level="1">
      <bodyTitle>Statistical analysis of time series</bodyTitle>
      <subsection id="uid58" level="2">
        <bodyTitle>Change Point Analysis</bodyTitle>
        <p><b>Nonparametric multiple change point estimation in highly dependent time series</b>, <ref xlink:href="#sequel-2016-bid29" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
        <p>Given a heterogeneous time-series sample, the objective is to find points in time, called change points, where the probability distribution generating the data has changed. The data are assumed to have been generated by arbitrary unknown stationary ergodic distributions. No modelling, independence or mixing assumptions are made. A novel, computationally efficient, nonparametric method is proposed, and is shown to be asymptotically consistent in this general framework. The theoretical results are complemented with experimental evaluations.</p>
      </subsection>
      <subsection id="uid59" level="2">
        <bodyTitle>Clustering Time Series, Online and Offline</bodyTitle>
        <p><b>Consistent Algorithms for Clustering Time Series</b>, <ref xlink:href="#sequel-2016-bid30" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
        <p>The problem of clustering is considered for the case where every point is a time series. The time series are either given in one batch (offline setting), or they are allowed to grow with time and new time series can be added along the way (online setting). We propose a natural notion of consistency for this problem, and show that there are simple, com-putationally efficient algorithms that are asymptotically consistent under extremely weak assumptions on the distributions that generate the data. The notion of consistency is as follows. A clustering algorithm is called consistent if it places two time series into the same cluster if and only if the distribution that generates them is the same. In the considered framework the time series are allowed to be highly dependent, and the dependence can have arbitrary form. If the number of clusters is known, the only assumption we make is that the (marginal) distribution of each time series is stationary ergodic. No paramet-ric, memory or mixing assumptions are made. When the number of clusters is unknown, stronger assumptions are provably necessary, but it is still possible to devise nonparametric algorithms that are consistent under very general conditions. The theoretical findings of this work are illustrated with experiments on both synthetic and real data.</p>
      </subsection>
      <subsection id="uid60" level="2">
        <bodyTitle>Automata Learning</bodyTitle>
        <p><b>PAC learning of Probabilistic Automaton based on the Method of Moments</b>, <ref xlink:href="#sequel-2016-bid31" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
        <p>Probabilitic Finite Automata (PFA) are gener-ative graphical models that define distributions with latent variables over finite sequences of symbols, a.k.a. stochastic languages. Traditionally , unsupervised learning of PFA is performed through algorithms that iteratively improves the likelihood like the Expectation-Maximization (EM) algorithm. Recently, learning algorithms based on the so-called Method of Moments (MoM) have been proposed as a much faster alternative that comes with PAC-style guarantees. However, these algorithms do not ensure the learnt automata to model a proper distribution , limiting their applicability and preventing them to serve as an initialization to iterative algorithms. In this paper, we propose a new MoM-based algorithm with PAC-style guarantees that learns automata defining proper distributions. We assess its performances on synthetic problems from the PAutomaC challenge and real datasets extracted from Wikipedia against previous MoM-based algorithms and EM algorithm.</p>
      </subsection>
      <subsection id="uid61" level="2">
        <bodyTitle>Online Kernel and Graph-Based Methods</bodyTitle>
        <p><b>Analysis of Nystr√∂m method with sequential ridge leverage score sampling</b>, <ref xlink:href="#sequel-2016-bid32" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
        <p>Large-scale kernel ridge regression (KRR) is limited by the need to store a large kernel matrix Kt. To avoid storing the entire matrix Kt, NystroÃàm methods subsample a subset of columns of the kernel matrix, and efficiently find an approximate KRR solution on the reconstructed Kt . The chosen subsampling distribution in turn affects the statistical and computational tradeoffs. For KRR problems, [15, 1] show that a sampling distribution proportional to the ridge leverage scores (RLSs) provides strong reconstruction guarantees for Kt. While exact RLSs are as difficult to compute as a KRR solution, we may be able to approximate them well enough. In this paper, we study KRR problems in a sequential setting and introduce the INK-ESTIMATE algorithm, that incrementally computes the RLSs estimates. INK-ESTIMATE maintains a small sketch of Kt, that at each step is used to compute an intermediate estimate of the RLSs. First, our sketch update does not require access to previously seen columns, and therefore a single pass over the kernel matrix is sufficient. Second, the algorithm requires a fixed, small space budget to run dependent only on the effective dimension of the kernel matrix. Finally, our sketch provides strong approximation guarantees on the distance <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mrow><mo>|</mo><mo>|</mo><mi>K</mi><mi>t</mi></mrow><mo>-</mo><msup><mrow><mi>K</mi><mi>t</mi><mo>|</mo><mo>|</mo></mrow><mn>2</mn></msup></mrow></math></formula> , and on the statistical risk of the approximate KRR solution at any time, because all our guarantees hold at any intermediate step.</p>
      </subsection>
    </subsection>
    <subsection id="uid62" level="1">
      <bodyTitle>Statistical Learning and Bayesian Analysis</bodyTitle>
      <subsection id="uid63" level="2">
        <bodyTitle>Non-parametric methods for Function Approximation</bodyTitle>
        <p><b>Pliable rejection sampling</b>, <ref xlink:href="#sequel-2016-bid33" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
        <p>Rejection sampling is a technique for sampling from difficult distributions. However, its use is limited due to a high rejection rate. Common adaptive rejection sampling methods either work only for very specific distributions or without performance guarantees. In this paper, we present pliable rejection sampling (PRS), a new approach to rejection sampling, where we learn the sampling proposal using a kernel estimator. Since our method builds on rejection sampling, the samples obtained are with high probability i.i.d. and distributed according to f. Moreover, PRS comes with a guarantee on the number of accepted samples.</p>
      </subsection>
      <subsection id="uid64" level="2">
        <bodyTitle>Non-parametric methods for functional supervised learning</bodyTitle>
        <p><b>Operator-valued Kernels for Learning from Functional Response Data</b>, <ref xlink:href="#sequel-2016-bid34" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
        <p>In this paper we consider the problems of supervised classification and regression in the case where attributes and labels are functions: a data is represented by a set of functions, and the label is also a function. We focus on the use of reproducing kernel Hilbert space theory to learn from such functional data. Basic concepts and properties of kernel-based learning are extended to include the estimation of function-valued functions. In this setting, the representer theorem is restated, a set of rigorously defined infinite-dimensional operator-valued kernels that can be valuably applied when the data are functions is described, and a learning algorithm for nonlinear functional data analysis is introduced. The methodology is illustrated through speech and audio signal processing experiments.</p>
      </subsection>
      <subsection id="uid65" level="2">
        <bodyTitle>Differential privacy</bodyTitle>
        <p><b>On the Differential Privacy of Bayesian Inference</b>, <ref xlink:href="#sequel-2016-bid35" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
        <p>We study how to communicate findings of Bayesian inference to third parties, while preserving the strong guarantee of differential privacy. Our main contributions are four different algorithms for private Bayesian inference on proba-bilistic graphical models. These include two mechanisms for adding noise to the Bayesian updates, either directly to the posterior parameters, or to their Fourier transform so as to preserve update consistency. We also utilise a recently introduced posterior sampling mechanism, for which we prove bounds for the specific but general case of discrete Bayesian networks; and we introduce a maximum-a-posteriori private mechanism. Our analysis includes utility and privacy bounds, with a novel focus on the influence of graph structure on privacy. Worked examples and experiments with Bayesian na√Øve Bayes and Bayesian linear regression illustrate the application of our mechanisms.</p>
        <p><b>Algorithms for Differentially Private Multi-Armed Bandits</b>, <ref xlink:href="#sequel-2016-bid15" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
        <p>We present differentially private algorithms for the stochastic Multi-Armed Bandit (MAB) problem. This is a problem for applications such as adaptive clinical trials, experiment design, and user-targeted advertising where private information is connected to individual rewards. Our major contribution is to show that there exist <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mo>(</mo><mi>œµ</mi><mo>,</mo><mi>Œ¥</mi><mo>)</mo></mrow></math></formula> differentially private variants of Upper Confidence Bound algorithms which have optimal regret, <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mi>O</mi><mo>(</mo><msup><mi>œµ</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>+</mo><mo form="prefix">log</mo><mi>T</mi><mo>)</mo></mrow></math></formula>. This is a significant improvement over previous results, which only achieve poly-log regret <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mi>O</mi><mo>(</mo><msup><mi>œµ</mi><mrow><mo>-</mo><mn>2</mn></mrow></msup><msup><mo form="prefix">log</mo><mn>2</mn></msup><mi>T</mi><mo>)</mo></mrow></math></formula>, because of our use of a novel interval-based mechanism. We also substantially improve the bounds of previous family of algorithms which use a continual release mechanism. Experiments clearly validate our theoretical bounds.</p>
      </subsection>
    </subsection>
    <subsection id="uid66" level="1">
      <bodyTitle>Applications</bodyTitle>
      <subsection id="uid67" level="2">
        <bodyTitle>Spoken Dialogue Systems</bodyTitle>
        <p><b>Compact and Interpretable Dialogue State Representation with Genetic Sparse Distributed Memory</b>, <ref xlink:href="#sequel-2016-bid36" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
        <p>t User satisfaction is often considered as the objective that should be achieved by spoken dialogue systems. This is why, the reward function of Spoken Dialogue Systems (SDS) trained by Reinforcement Learning (RL) is often designed to reflect user satisfaction. To do so, the state space representation should be based on features capturing user satisfaction characteristics such as the mean speech recognition confidence score for instance. On the other hand, for deployment in industrial systems, there is a need for state representations that are understandable by system engineers. In this paper, we propose to represent the state space using a Genetic Sparse Distributed Memory. This is a state aggregation method computing state prototypes which are selected so as to lead to the best linear representation of the value function in RL. To do so, previous work on Genetic Sparse Distributed Memory for classification is adapted to the Reinforcement Learning task and a new way of building the prototypes is proposed. The approach is tested on a corpus of dialogues collected with an appointment scheduling system. The results are compared to a grid-based linear parametrisation. It is shown that learning is accelerated and made more memory efficient. It is also shown that the framework is calable in that it is possible to include many dialogue features in the representation, interpret the resulting policy and identify the most important dialogue features.</p>
        <p><b>A Stochastic Model for Computer-Aided Human-Human Dialogue</b>, <ref xlink:href="#sequel-2016-bid37" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
        <p>In this paper we introduce a novel model for computer-aided human-human dialogue. In this context, the computer aims at improving the outcome of a human-human task-oriented dialogue by intervening during the course of the interaction. While dialogue state and topic tracking in human-human dialogue have already been studied, few work has been devoted to the sequential part of the problem, where the impact of the system's actions on the future of the conversation is taken into account. This paper addresses this issue by first modelling human-human dialogue as a Markov Reward Process. The task of purposely taking part into the conversation is then optimised within the Linearly Solvable Markov Decision Process framework. Utterances of the Conversational Agent are seen as perturbations in this process, which aim at satisfying the user's long-term goals while keeping the conversation natural. Finally, results obtained by simulation suggest that such an approach is suitable for computer-aided human-human dialogue and is a first step towards three-party dialogue.</p>
        <p><b>Learning Dialogue Dynamics with the Method of Moments</b>, <ref xlink:href="#sequel-2016-bid38" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
        <p>In this paper, we introduce a novel framework to encode the dynamics of dialogues into a probabilistic graphical model. Traditionally, Hidden Markov Models (HMMs) would be used to address this problem, involving a first step of hand-crafting to build a dialogue model (e.g. defining potential hidden states) followed by applying expectation-maximisation (EM) algorithms to refine it. Recently, an alternative class of algorithms based on the Method of Moments (MoM) has proven successful in avoiding issues of the EM-like algorithms such as convergence towards local optima, tractability issues, initialization issues or the lack of theoretical guarantees. In this work, we show that dialogues may be modeled by SP-RFA, a class of graphical models efficiently learnable within the MoM and directly usable in planning algorithms (such as reinforcement learning). Experiments are led on the Ubuntu corpus and dialogues are considered as sequences of dialogue acts, represented by their Latent Dirichlet Allocation (LDA) and Latent Semantic Analysis (LSA). We show that a MoM-based algorithm can learn a compact model of sequences of such acts.</p>
      </subsection>
      <subsection id="uid68" level="2">
        <bodyTitle>Software development</bodyTitle>
        <p><b>Mutation-Based Graph Inference for Fault Localization</b>, <ref xlink:href="#sequel-2016-bid39" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
        <p>We present a new fault localization algorithm, called Vautrin, built on an approximation of causality based on call graphs. The approximation of causality is done using software mutants. The key idea is that if a mutant is killed by a test, certain call graph edges within a path between the mutation point and the failing test are likely causal. We evaluate our approach on the fault localization benchmark by Steimann et al. totaling 5,836 faults. The causal graphs are extracted from 88,732 nodes connected by 119,531 edges. Vautrin improves the fault localization effectiveness for all subjects of the benchmark. Considering the wasted effort at the method level, a classical fault localization evaluation metric, the improvement ranges from 3</p>
        <p><b>A Large-scale Study of Call Graph-based Impact Prediction using Mutation Testing</b>, <ref xlink:href="#sequel-2016-bid40" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
        <p>In software engineering, impact analysis consists in predicting the software elements (e.g. modules, classes, methods) potentially impacted by a change in the source code. Impact analysis is required to optimize the testing effort. In this paper, we propose a framework to predict error propagation. Based on 10 open-source Java projects and 5 classical mutation operators, we create 17000 mutants and study how the error they introduce propagates. This framework enables us to analyze impact prediction based on four types of call graph. Our results show that the sophistication indeed increases completeness of impact prediction. However, and surprisingly to us, the most basic call graph gives the highest trade-off between precision and recall for impact prediction.</p>
        <p><b>A Learning Algorithm for Change Impact Prediction</b>, <ref xlink:href="#sequel-2016-bid41" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
        <p>Change impact analysis (CIA) consists in predicting the impact of a code change in a software application. In this paper, the artifacts that are considered for CIA are methods of object-oriented software; the change under study is a change in the code of the method, the impact is the test methods that fail because of the change that has been performed. We propose LCIP, a learning algorithm that learns from past impacts to predict future impacts. To evaluate LCIP, we consider Java software applications that are strongly tested. We simulate 6000 changes and their actual impact through code mutations, as done in mutation testing. We find that LCIP can predict the impact with a precision of 74</p>
      </subsection>
    </subsection>
  </resultats>
  <contrats id="uid69">
    <bodyTitle>Bilateral Contracts and Grants with Industry</bodyTitle>
    <subsection id="uid70" level="1">
      <bodyTitle>Bilateral Contracts with Industry</bodyTitle>
      <simplelist>
        <li id="uid71">
          <p noindent="true">contract with ‚Äú500px‚Äù; PI: Romaric Gaudel.</p>
          <p>Title: Recommender System for Photos</p>
          <p>Duration: May 2016 ‚Äì Oct. 2016 (6 months)</p>
          <p>Abstract: Recommender Systems aim at recommending items to
users. Advances in that field are targeting more and more
personalized recommendation. From a recommendation based on market
segment to a recommendation based on individual user taste. From a
recommendation based on user‚Äôs information to a recommendation
based on any feedback from any user. From a recommendation based
on logged data to a recommendation including latest trends...
500px is a Canadian company which is part of this trend. 500px
offers solutions to store pictures online, to share pictures, and
to browse among pictures exhibited by other users. Given the huge
amount of pictures stored by 500px, users need help to find
pictures which corresponds to their tastes. 500px offers several
tools to filter the content presented to users. But the tools
allowing exploration of the pictures landscape are not
personalized, the selection is mostly based on the popularity of
pictures/galleries. The most personalized recommendations are
obtained by following other users: you see recent pictures of that
users. But such recommendations requires you (i) to discover by
yourself relevant users, (ii) to explicitly tag these users. The
aim of the project is to scan state of the art in Collaborative
Filtering and to design a tool which recommends pictures to users
based on their implicit actions: given the list of followed users,
famed pictures, commented pictures, browsed pictures, ..., infer
user‚Äôs tastes and recommend to that user pictures and/or other
user to look at. The system would also make use of informations
on the pictures and of user profiles.</p>
        </li>
        <li id="uid72">
          <p noindent="true">contract with ‚ÄúOrange Labs‚Äù; PI: Philippe Preux</p>
          <p>Title: Sequential Learning and Decision Making under Partial Monitoring</p>
          <p>Duration: Oct. 2014 ‚Äì Sep. 2017</p>
          <p>Abstract:
In applications such as recommendation systems, or computational advertising, the return collected from the user is partial: (s)he clicks on one item, or no item at all. We study this setting in which only a ‚Äúpartial‚Äù information is gathered in particular how to learn to behave optimaly in such a setting.</p>
        </li>
        <li id="uid73">
          <p noindent="true">contract with ‚Äú55‚Äù; PI: J√©r√©mie Mary</p>
          <p>Title: Novel Learning and Exploration-Exploitation Methods for Effective Recommender Systems</p>
          <p>Duration: Oct. 2015 ‚Äì Sep. 2018</p>
          <p>Abstract: In this Ph.D. thesis we intend to deal with this problem by developing novel and more sophisticated recommendation strategies in which the collection of data and the improvement of the performance are considered as a unique process, where the trade-off between the quality of the data and the performance of the recommendation strategy is optimized over time. This work also consider tensor methods (one layer of the tensor can be the time) with the goal to scale them at RS level.</p>
        </li>
        <li id="uid74">
          <p noindent="true">contract with ‚ÄúWhat a nice place‚Äù ; PI: J√©r√©mie Mary</p>
          <p>Title: Deduplication of pictures</p>
          <p>Duration: Mar.  2016 ‚Äì Jan.  2017</p>
          <p>Abstract: ‚ÄúWhat is nice place‚Äù is a start up which aggregates products from different sources in order to provide some home staging advises. Uniqueness of presence for the items in their database can be hard to achieve because of the differences over names and variations of a product. Here we build a classification and deduplication system based on deep neural networks. In this contract we received support from Inria Tech and transferred them some knowledge about deep neural networks.</p>
        </li>
        <li id="uid75">
          <p noindent="true">contract with ‚ÄúWhat a nice place‚Äù and ‚ÄúLeroy Merlin‚Äù; PI: J√©r√©mie Mary</p>
          <p>Title: New Shopping Experience - Virtual Coach</p>
          <p>Duration: Jun. 2016 ‚Äì Fev. 2017</p>
          <p>Abstract: The goal of this project is to use pictures of house interiors in order to propose automatically some products which would fit in nicely. The relations are learnt automatically using deep neural networks and recommendation systems techniques. We made a first version which focuses on lamps which is available for demonstration at <ref xlink:href="https://whataniceplace.leroymerlin.fr/" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>whataniceplace.<allowbreak/>leroymerlin.<allowbreak/>fr/</ref></p>
        </li>
      </simplelist>
    </subsection>
  </contrats>
  <partenariat id="uid76">
    <bodyTitle>Partnerships and Cooperations</bodyTitle>
    <subsection id="uid77" level="1">
      <bodyTitle>National Initiatives</bodyTitle>
      <subsection id="uid78" level="2">
        <bodyTitle>ANR BoB</bodyTitle>
        <participants>
          <person key="sequel-2014-idp70232">
            <firstname>Michal</firstname>
            <lastname>Valko</lastname>
          </person>
        </participants>
        <simplelist>
          <li id="uid79">
            <p noindent="true"><i>Title</i>: Bayesian statistics for expensive models and tall data</p>
          </li>
          <li id="uid80">
            <p noindent="true"><i>Type</i>: National Research Agency</p>
          </li>
          <li id="uid81">
            <p noindent="true"><i>Coordinator</i>: CNRS (R. Bardenet)</p>
          </li>
          <li id="uid82">
            <p noindent="true"><i>Duration</i>: 2016-2020</p>
          </li>
          <li id="uid83">
            <p noindent="true"><i>Abstract</i>:</p>
            <p>Bayesian methods are a popular class of statistical algorithms for updating scientific beliefs. They turn
data into decisions and models, taking into account uncertainty about models and their parameters. This
makes Bayesian methods popular among applied scientists such as biologists, physicists, or engineers.
However, at the heart of Bayesian analysis lie 1) repeated sweeps over the full dataset considered, and
2) repeated evaluations of the model that describes the observed physical process. The current trends to
large-scale data collection and complex models thus raises two main issues.
Experiments, observations, and numerical simulations in many areas of science nowadays generate terabytes
of data, as does the LHC in particle physics for instance. Simultaneously, knowledge creation
is becoming more and more data-driven, which requires new paradigms addressing how data are captured,
processed, discovered, exchanged, distributed, and analyzed. For statistical algorithms to scale up,
reaching a given performance must require as few iterations and as little access to data as possible.
It is not only experimental measurements that are growing at a rapid pace. Cell biologists tend to have
scarce data but large-scale models of tens of nonlinear differential equations to describe complex dynamics.
In such settings, evaluating the model once requires numerically solving a large system of differential
equations, which may take minutes for some tens of differential equations on today‚Äôs hardware. Iterative
statistical processing that requires a million sequential runs of the model is thus out of the question.
In this project, we tackle the fundamental cost-accuracy trade-off for Bayesian methods, in order to
produce generic inference algorithms that scale favourably with the number of measurements in an experiment
and the number of runs of a statistical model. We propose a collection of objectives with different
risk-reward trade-offs to tackle these two goals. In particular, for experiments with large numbers of
measurements, we further develop existing subsampling-based Monte Carlo methods, while developing
a novel decision theory framework that includes data constraints. For expensive models, we build an
ambitious programme around Monte Carlo methods that leverage determinantal processes, a rich class of
probabilistic tools that lead to accurate inference with limited model evaluations. In short, using innovative
techniques such as subsampling-based Monte Carlo and determinantal point processes, we propose
in this project to push the boundaries of the applicability of Bayesian inference.</p>
          </li>
        </simplelist>
      </subsection>
      <subsection id="uid84" level="2">
        <bodyTitle>ANR Badass</bodyTitle>
        <participants>
          <person key="tao-2015-idp83360">
            <firstname>Odalric</firstname>
            <lastname>Maillard</lastname>
          </person>
          <person key="dyogene-2014-idp72312">
            <firstname>Emilie</firstname>
            <lastname>Kaufmann</lastname>
          </person>
        </participants>
        <simplelist>
          <li id="uid85">
            <p noindent="true"><i>Title</i>:</p>
          </li>
          <li id="uid86">
            <p noindent="true"><i>Type</i>: National Research Agency</p>
          </li>
          <li id="uid87">
            <p noindent="true"><i>Coordinator</i>: Inria Lille (O. Maillard)</p>
          </li>
          <li id="uid88">
            <p noindent="true"><i>Duration</i>: 2016-2020</p>
          </li>
          <li id="uid89">
            <p noindent="true"><i>Abstract</i>: Motivated by the fact that a number of modern applications of sequential decision making require developing strategies that are especially robust to change in the stationarity of the signal, and in order to anticipate and impact the next generation of applications of the field, the BADASS project intends to push theory and application of MAB to the next level by incorporating non-stationary observations while retaining near optimality against the best not necessarily constant decision strategy. Since a non-stationary process typically decomposes into chunks associated with some possibly hidden variables (states), each corresponding to a stationary process, handling non-stationarity crucially requires exploiting the (possibly hidden) structure of the decision problem. For the same reason, a MAB for which arms can be arbitrary non-stationary processes is powerful enough to capture MDPs and even partially observable MDPs as special cases, and it is thus important to jointly address the issue of non-stationarity together with that of structure.
In order to advance these two nested challenges from a solid theoretical standpoint, we intend to focus on the following objectives:
<i>(i)</i> To broaden the range of optimal strategies for stationary MABs: current strategies are only known to be provably optimal in a limited range of scenarios for which the class of distribution (structure) is perfectly known; also, recent heuristics possibly adaptive to the class need to be further analyzed.
<i>(ii)</i> To strengthen the literature on pure sequential prediction (focusing on a single arm) for non-stationary signals via the construction of adaptive confidence sets and a novel measure of complexity: traditional approaches consider a worst-case scenario and are thus overly conservative and non-adaptive to simpler signals.
<i>(iii)</i> To embed the low-rank matrix completion and spectral methods in the context of reinforcement learning, and further study models of structured environments: promising heuristics in the context of e.g. contextual MABs or Predictive State Representations require stronger theoretical guarantees.</p>
            <p>This project will result in the development of a novel generation of strategies to handle non-stationarity and structure that will be evaluated in a number of test beds and validated by a rigorous theoretical analysis. Beyond the significant advancement of the state of the art in MAB and RL theory and the mathematical value of the program, this JCJC BADASS is expected to strategically impact societal and industrial applications, ranging from personalized health-care and e-learning to computational sustainability or rain-adaptive river-bank management to cite a few.</p>
          </li>
        </simplelist>
      </subsection>
      <subsection id="uid90" level="2">
        <bodyTitle>ANR ExTra-Learn</bodyTitle>
        <participants>
          <person key="sequel-2014-idm26088">
            <firstname>Alessandro</firstname>
            <lastname>Lazaric</lastname>
          </person>
          <person key="sequel-2014-idp76928">
            <firstname>J√©r√©mie</firstname>
            <lastname>Mary</lastname>
          </person>
          <person key="sequel-2014-idp67360">
            <firstname>R√©mi</firstname>
            <lastname>Munos</lastname>
          </person>
          <person key="sequel-2014-idp70232">
            <firstname>Michal</firstname>
            <lastname>Valko</lastname>
          </person>
        </participants>
        <simplelist>
          <li id="uid91">
            <p noindent="true"><i>Title</i>: Extraction and Transfer of Knowledge in Reinforcement Learning</p>
          </li>
          <li id="uid92">
            <p noindent="true"><i>Type</i>: National Research Agency (ANR-9011)</p>
          </li>
          <li id="uid93">
            <p noindent="true"><i>Coordinator</i>: Inria Lille (A. Lazaric)</p>
          </li>
          <li id="uid94">
            <p noindent="true"><i>Duration</i>: 2014-2018</p>
          </li>
          <li id="uid95">
            <p noindent="true"><i>Abstract</i>:
ExTra-Learn is directly motivated by the evidence that one of the key features that allows humans to accomplish complicated tasks is their ability of building knowledge from past experience and transfer it while learning new tasks. We believe that integrating transfer of learning in machine learning algorithms will dramatically improve their learning performance and enable them to solve complex tasks. We identify in the reinforcement learning (RL) framework the most suitable candidate for this integration. RL formalizes the problem of learning an optimal control policy from the experience directly collected from an unknown environment. Nonetheless, practical limitations of current algorithms encouraged research to focus on how to integrate prior knowledge into the learning process. Although this improves the performance of RL algorithms, it dramatically reduces their autonomy. In this project we pursue a paradigm shift from designing RL algorithms incorporating prior knowledge, to methods able to incrementally discover, construct, and transfer ‚Äúprior‚Äù knowledge in a fully automatic way. More in detail, three main elements of RL algorithms would significantly benefit from transfer of knowledge. <i>(i)</i> For every new task, RL algorithms need exploring the environment for a long time, and this corresponds to slow learning processes for large environments. Transfer learning would enable RL algorithms to dramatically reduce the exploration of each new task by exploiting its resemblance with tasks solved in the past.
<i>(ii)</i> RL algorithms evaluate the quality of a policy by computing its state-value function. Whenever the number of states is too large, approximation is needed. Since approximation may cause instability, designing suitable approximation schemes is particularly critical. While this is currently done by a domain expert, we propose to perform this step automatically by constructing features that incrementally adapt to the tasks encountered over time. This would significantly reduce human supervision and increase the accuracy and stability of RL algorithms across different tasks.
<i>(iii)</i> In order to deal with complex environments, hierarchical RL solutions have been proposed, where state representations and policies are organized over a hierarchy of subtasks. This requires a careful definition of the hierarchy, which, if not properly constructed, may lead to very poor learning performance. The ambitious goal of transfer learning is to automatically construct a hierarchy of skills, which can be effectively reused over a wide range of similar tasks.</p>
          </li>
          <li id="uid96">
            <p noindent="true"><i>Activity Report</i>: Research in ExTra-Learn continued in investigating how knowledge can be transferred into reinforcement learning algorithms to improve their performance. Pierre-Victor Chaumier did a 4 months internship in SequeL studying how to perform transfer neural networks across different games in the Atari platform. Unfortunately, the preliminary results we obtained were not very positive. We investigated different transfer models, from basic transfer of a fully trained network, to co-train over multiple games and retrain with initialization from a previous network. In most of the cases, the improvement from transfer was rather limited and in some cases even negative transfer effects appeared. This seems to be intrinsic in the neural network architecture which tends to overfit on one single task and it poorly generlizes over alternative tasks. Another activity was related to the study of macro-actions in RL. We proved for the first time under which conditions macro-actions can actually improve the learning speed of an RL exploration-exploitation algorithm. This is the first step towards the automatic identification and construction of useful macro-actions across multiple tasks.</p>
          </li>
        </simplelist>
      </subsection>
      <subsection id="uid97" level="2">
        <bodyTitle>ANR KEHATH</bodyTitle>
        <participants>
          <person key="sequel-2014-idp111080">
            <firstname>Olivier</firstname>
            <lastname>Pietquin</lastname>
          </person>
        </participants>
        <simplelist>
          <li id="uid98">
            <p noindent="true"><i>Acronym</i>: KEHATH</p>
          </li>
          <li id="uid99">
            <p noindent="true"><i>Title</i>: Advanced Quality Methods for Post-Edition of Machine Translation</p>
          </li>
          <li id="uid100">
            <p noindent="true"><i>Type</i>: ANR</p>
          </li>
          <li id="uid101">
            <p noindent="true"><i>Coordinator</i>: Lingua &amp; Machina</p>
          </li>
          <li id="uid102">
            <p noindent="true"><i>Duration</i>: 2014-2017</p>
          </li>
          <li id="uid103">
            <p noindent="true"><i>Other partners</i>: Univ. Lille 1, Laboratoire d'Informatique de Grenoble (LIG)</p>
          </li>
          <li id="uid104">
            <p noindent="true"><i>Abstract</i>: The translation community has seen a major
change over the last five years. Thanks to progress in the
training of statistical machine translation engines on corpora of
existing translations, machine translation has become good enough
so that it has become advantageous for translators to post-edit
machine outputs rather than translate from scratch. However,
current enhancement of machine translation (MT) systems from human
post-edition (PE) are rather basic: the post-edited output is
added to the training corpus and the translation model and
language model are re-trained, with no clear view of how much has
been improved and how much is left to be improved. Moreover, the
final PE result is the only feedback used: available technologies
do not take advantages of logged sequences of post-edition
actions, which inform on the cognitive processes of the
post-editor. The KEHATH project intends to address these issues
in two ways. Firstly, we will optimise advanced machine learning
techniques in the MT+PE loop. Our goal is to boost the impact of
PE, that is, reach the same performance with less PE or better
performance with the same amount of PE. In other words, we want to
improve machine translation learning curves. For this purpose,
active learning and reinforcement learning techniques will be
proposed and evaluated. Along with this, we will have to face
challenges such as MT systems heterogeneity (statistical and/or
rule-based), and ML scalability so as to improve domain-specific
MT. Secondly, since quality prediction (QP) on MT outputs is
crucial for translation project managers, we will implement and
evaluate in real-world conditions several confidence estimation
and error detection techniques previously developed at a
laboratory scale. A shared concern will be to work on continuous
domain-specific data flows to improve both MT and the performance
of indicators for quality prediction. The overall goal of the
KEHATH project is straightforward: gain additional machine
translation performance as fast as possible in each and every new
industrial translation project, so that post-edition time and cost
is drastically reduced. Basic research is the best way to reach
this goal, for an industrial impact that is powerful and
immediate.</p>
          </li>
        </simplelist>
      </subsection>
      <subsection id="uid105" level="2">
        <bodyTitle>ANR MaRDi</bodyTitle>
        <participants>
          <person key="sequel-2014-idp111080">
            <firstname>Olivier</firstname>
            <lastname>Pietquin</lastname>
          </person>
          <person key="sequel-2014-idp79648">
            <firstname>Bilal</firstname>
            <lastname>Piot</lastname>
          </person>
        </participants>
        <simplelist>
          <li id="uid106">
            <p noindent="true"><i>Acronym</i>: MaRDi</p>
          </li>
          <li id="uid107">
            <p noindent="true"><i>Title</i>: Man-Robot Dialogue</p>
          </li>
          <li id="uid108">
            <p noindent="true"><i>Type</i>: ANR</p>
          </li>
          <li id="uid109">
            <p noindent="true"><i>Coordinator</i>: Univ. Lille 1 (Olivier Pietquin)</p>
          </li>
          <li id="uid110">
            <p noindent="true"><i>Duration</i>: 2012-2016</p>
          </li>
          <li id="uid111">
            <p noindent="true"><i>Other partners</i>: Laboratoire d'Informatique d'Avignon (LIA), CNRS - LAAS (Toulouse), Acapela group (Toulouse)</p>
          </li>
          <li id="uid112">
            <p noindent="true"><i>Abstract</i>: In the MaRDi project, we study the
interaction between humans and machines as a situated problem in
which human users and machines share the same
environment. Especially, we investigate how the physical
environment of robots interacting with humans can be used to
improve the performance of spoken interaction which is known to be
imperfect and sensible to noise. To achieve this objectif, we
study three main problems. First, how to interactively build a
multimodal representation of the current dialogue context from
perception and proprioception signals. Second, how to
automatically learn a strategy of interaction using methods such
as reinforcement learning. Third, how to provide expressive
feedbacks to users about how the machine is confident about its
behaviour and to reflect its current state (also the physical
state).</p>
          </li>
        </simplelist>
      </subsection>
      <subsection id="uid113" level="2">
        <bodyTitle>National Partners</bodyTitle>
        <simplelist>
          <li id="uid114">
            <p noindent="true">CentraleSup√©lec</p>
            <simplelist>
              <li id="uid115">
                <p noindent="true">J.Perolat, B.Piot and O.Pietquin worked with M.Geist on Stochastic Games. it led to a conference publication in ICML 2016.</p>
              </li>
            </simplelist>
          </li>
          <li id="uid116">
            <p noindent="true">Inria Nancy - Grand Est</p>
            <simplelist>
              <li id="uid117">
                <p noindent="true">J.Perolat, B.Piot and O.Pietquin worked with Bruno Scherrer on Stochastic Games. It led to a conference publication in AISTATS 2016¬†<ref xlink:href="#sequel-2016-bid42" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> and ICML 2016.</p>
              </li>
            </simplelist>
          </li>
          <li id="uid118">
            <p noindent="true">Institut de Math√©matiques de Toulouse</p>
            <simplelist>
              <li id="uid119">
                <p noindent="true">√â. Kaufmann had publications at COLT, ALT and NIPS with Aur√©lie Garivier.</p>
              </li>
            </simplelist>
          </li>
        </simplelist>
      </subsection>
    </subsection>
    <subsection id="uid120" level="1">
      <bodyTitle>European Initiatives</bodyTitle>
      <subsection id="uid121" level="2">
        <bodyTitle>FP7 &amp; H2020 Projects</bodyTitle>
        <sanspuceslist>
          <li id="uid122">
            <p noindent="true">Program: H2020</p>
          </li>
          <li id="uid123">
            <p noindent="true">Project acronym: BabyRobot</p>
          </li>
          <li id="uid124">
            <p noindent="true">Project title: Child-Robot Communication and Collaboration</p>
          </li>
          <li id="uid125">
            <p noindent="true">Duration: 01/2016 - 12/2018</p>
          </li>
          <li id="uid126">
            <p noindent="true">Coordinator: Alexandros Potamianos (Athena Research and Innovation Center in Information Communication and Knowledge Technologies, Greece)</p>
          </li>
          <li id="uid127">
            <p noindent="true">Other partners: Institute of Communication and Computer Systems
(Greece), The University of Hertfordshire Higher Education
Corporation (UK), Universitaet Bielefeld (Germany), Kunlgliga
Tekniska Hoegskolan (Sweden), Blue Ocean Robotics ApS (Denmark),
Univ. Lille (France), Furhat Robotics AB (Sweden)</p>
          </li>
          <li id="uid128">
            <p noindent="true">Abstract: The crowning achievement of human communication is our
unique ability to share intentionality, create and execute on joint
plans. Using this paradigm we model human-robot communication as a
three step process: sharing attention, establishing common ground
and forming shared goals. Prerequisites for successful communication
are being able to decode the cognitive state of people around us
(mindreading) and building trust. Our main goal is to create robots
that analyze and track human behavior over time in the context of
their surroundings (situational) using audio-visual monitoring in
order to establish common ground and mind-reading capabilities. On
BabyRobot we focus on the typically developing and autistic spectrum
children user population. Children have unique communication skills,
are quick and adaptive learners, eager to embrace new robotic
technologies. This is especially relevant for special eduation where
the development of social skills is delayed or never fully develops
without intervention or therapy. Thus our second goal is to define,
implement and evaluate child-robot interaction application scenarios
for developing specific socio-affective, communication and
collaboration skills in typically developing and autistic spectrum
children. We will support not supplant the therapist or educator,
working hand-inhand to create a low risk environment for learning
and cognitive development. Breakthroughs in core robotic
technologies are needed to support this research mainly in the areas
of motion planning and control in constrained spaces, gestural
kinematics, sensorimotor learning and adaptation. Our third goal is
to push beyond the state-of-the-art in core robotic technologies to
support natural human-robot interaction and collaboration for
edutainment and healthcare applications. Creating robots that can
establish communication protocols and form collaboration plans on
the fly will have impact beyond the application scenarios
investigated here.</p>
          </li>
        </sanspuceslist>
      </subsection>
      <subsection id="uid129" level="2">
        <bodyTitle>Collaborations in European Programs, Except FP7 &amp; H2020</bodyTitle>
        <sanspuceslist>
          <li id="uid130">
            <p noindent="true">Program: CHIST-ERA</p>
          </li>
          <li id="uid131">
            <p noindent="true">Project acronym: IGLU</p>
          </li>
          <li id="uid132">
            <p noindent="true">Project title: Interactively Grounded Language Understanding</p>
          </li>
          <li id="uid133">
            <p noindent="true">Duration: 11/2015 - 10/2018</p>
          </li>
          <li id="uid134">
            <p noindent="true">Coordinator: Jean Rouat (Universit√© de Sherbrooke, Canada)</p>
          </li>
          <li id="uid135">
            <p noindent="true">Other partners: UMONS (Belgique), Inria (France), Univ-Lille (France), KTH (sweden), Universidad de Zaragoza (Spain)</p>
          </li>
          <li id="uid136">
            <p noindent="true">Abstract: Language is an ability that develops in young children
through joint interaction with their caretakers and their physical
environment. At this level, human language understanding could be
referred as interpreting and expressing semantic concepts
(e.g. objects, actions and relations) through what can be perceived
(or inferred) from current context in the environment. Previous work
in the field of artificial intelligence has failed to address the
acquisition of such perceptually-grounded knowledge in virtual
agents (avatars), mainly because of the lack of physical embodiment
(ability to interact physically) and dialogue, communication skills
(ability to interact verbally). We believe that robotic agents are
more appropriate for this task, and that interaction is a so
important aspect of human language learning and understanding that
pragmatic knowledge (identifying or conveying intention) must be
present to complement semantic knowledge. Through a developmental
approach where knowledge grows in complexity while driven by
multimodal experience and language interaction with a human, we
propose an agent that will incorporate models of dialogues, human
emotions and intentions as part of its decision-making process. This
will lead anticipation and reaction not only based on its internal
state (own goal and intention, perception of the environment), but
also on the perceived state and intention of the human
interactant. This will be possible through the development of
advanced machine learning methods (combining developmental, deep and
reinforcement learning) to handle large-scale multimodal inputs,
besides leveraging state-of-the-art technological components
involved in a language-based dialog system available within the
consortium. Evaluations of learned skills and knowledge will be
performed using an integrated architecture in a culinary use-case,
and novel databases enabling research in grounded human language
understanding will be released.</p>
          </li>
        </sanspuceslist>
      </subsection>
    </subsection>
    <subsection id="uid137" level="1">
      <bodyTitle>International Initiatives</bodyTitle>
      <subsection id="uid138" level="2">
        <bodyTitle>Inria Associate Teams Not Involved in an Inria International Labs</bodyTitle>
        <subsection id="uid139" level="3">
          <bodyTitle>
            <ref xlink:href="https://project.inria.fr/eduband/" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">EduBand</ref>
          </bodyTitle>
          <sanspuceslist>
            <li id="uid140">
              <p noindent="true">Title: Educational Bandits</p>
            </li>
            <li id="uid141">
              <p noindent="true">International Partner (Institution - Laboratory - Researcher):</p>
              <sanspuceslist>
                <li id="uid142">
                  <p noindent="true">Carnegie Mellon University (United States)
- Department of Computer Science, Theory of computation lab - Emma Brunskill</p>
                </li>
              </sanspuceslist>
            </li>
            <li id="uid143">
              <p noindent="true">Start year: 2015</p>
            </li>
            <li id="uid144">
              <p noindent="true">See also: <ref xlink:href="https://project.inria.fr/eduband/" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>project.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>eduband/</ref></p>
            </li>
            <li id="uid145">
              <p noindent="true">Education can transform an individual's capacity and the
opportunities available to him. The proposed collaboration will
build on and develop novel machine learning approaches towards
enhancing (human) learning. Massive open online classes (MOOCs) are
enabling many more people to access education, but mostly operate
using status quo teaching methods. Even more important than access
is the opportunity for online software to radically improve the
efficiency, engagement and effectiveness of education. Existing
intelligent tutoring systems (ITSs) have had some promising
successes, but mostly rely on learning sciences research to
construct hand-built strategies for automated teaching. Online
systems make it possible to actively collect substantial amount of
data about how people learn, and offer a huge opportunity to
substantially accelerate progress in improving education. An
essential aspect of teaching is providing the right learning
experience for the student, but it is often unknown a priori
exactly how this should be achieved. This challenge can often be
cast as an instance of decision-making under uncertainty. In
particular, prior work by Brunskill and colleagues demonstrated
that reinforcement learning (RL) and multi-arm bandit (MAB) can be
very effective approaches to solve the problem of automated
teaching. The proposed collaboration is thus intended to explore
the potential interactions of the fields of online education and RL
and MAB. On the one hand, we will define novel RL and MAB settings
and problems in online education. On the other hand, we will
investigate how solutions developed in RL and MAB could be
integrated in ITS and MOOCs and improve their effectiveness.</p>
            </li>
          </sanspuceslist>
        </subsection>
      </subsection>
      <subsection id="uid146" level="2">
        <bodyTitle>Inria International Partners</bodyTitle>
        <subsection id="uid147" level="3">
          <bodyTitle>With CWI</bodyTitle>
          <sanspuceslist>
            <li id="uid148">
              <p noindent="true">Title: Learning theory</p>
            </li>
            <li id="uid149">
              <p noindent="true">‚ÄúNorth-European Associate Team‚Äù</p>
              <sanspuceslist>
                <li id="uid150">
                  <p noindent="true">Centrum Wiskunde &amp; Informatica (CWI), Amsterdam (NL) - Peter Gr√ºnwald</p>
                </li>
              </sanspuceslist>
            </li>
            <li id="uid151">
              <p noindent="true">Duration: 2016 - 2018</p>
            </li>
            <li id="uid152">
              <p noindent="true">Start year: 2016</p>
            </li>
            <li id="uid153">
              <p noindent="true">ABSTRACT: The aim is to develop the theory of learning for sequential decision making under uncertainty problems.</p>
              <p>In 2016, this collaboration involved D. Ryabko, √â. Kaufmann, J. Ridgway, M. Valko, A. Lazaric, O. Maillard. A post-doc funded by Inria has been recruited in Fall 2016.</p>
              <p>This collaboration aims at developing through the Inria International Laboratory with CWI.</p>
            </li>
          </sanspuceslist>
        </subsection>
        <subsection id="uid154" level="3">
          <bodyTitle>With University of Leoben</bodyTitle>
          <sanspuceslist>
            <li id="uid155">
              <p noindent="true">Title: The multi-armed bandit problem</p>
            </li>
            <li id="uid156">
              <p noindent="true">International Partner (Institution - Laboratory - Researcher):</p>
              <sanspuceslist>
                <li id="uid157">
                  <p noindent="true">University of Leoben (Austria) - Peter Auer</p>
                </li>
              </sanspuceslist>
            </li>
            <li id="uid158">
              <p noindent="true">Duration: 2016 - 2016</p>
            </li>
            <li id="uid159">
              <p noindent="true">Start year: 2016</p>
            </li>
            <li id="uid160">
              <p noindent="true">ABSTRACT: Study of the multi-armed bandit problem.</p>
            </li>
          </sanspuceslist>
        </subsection>
        <subsection id="uid161" level="3">
          <bodyTitle>Informal International Partners</bodyTitle>
          <simplelist>
            <li id="uid162">
              <p noindent="true">University of California Irvine (USA)</p>
              <sanspuceslist>
                <li id="uid163">
                  <p noindent="true">Anima Anandkumar <i>Collaborator</i></p>
                </li>
                <li id="uid164">
                  <p noindent="true">A. Lazaric collaborates with A. Anandkumar on the use of spectral methods for reinforcement learning.</p>
                </li>
              </sanspuceslist>
            </li>
            <li id="uid165">
              <p noindent="true">University of Lancaster (UK)</p>
              <sanspuceslist>
                <li id="uid166">
                  <p noindent="true">Borja Balle <i>Collaborator</i></p>
                </li>
                <li id="uid167">
                  <p noindent="true">O-A. Maillard collaborates with B. Balle on concentration inequalities for Hankel matrices.</p>
                </li>
              </sanspuceslist>
            </li>
          </simplelist>
        </subsection>
      </subsection>
    </subsection>
    <subsection id="uid168" level="1">
      <bodyTitle>International Research Visitors</bodyTitle>
      <subsection id="uid169" level="2">
        <bodyTitle>Visits of International Scientists</bodyTitle>
        <subsection id="uid170" level="3">
          <bodyTitle>Internships</bodyTitle>
          <simplelist>
            <li id="uid171">
              <p noindent="true">Cricia Zilda Felicio Paixao, University Uberlandia, Brasil, Sep. 2015-Jul. 2016, working on recommendation systems in collaboration with Philippe Preux</p>
            </li>
            <li id="uid172">
              <p noindent="true">Maryam Aziz, Northeastern University, May-Aug. 2016, working on multi-armed bandits for clinical trials in collaboration with Emilie Kaufmann</p>
            </li>
            <li id="uid173">
              <p noindent="true">Kamyar Azizzadenesheli, University of California at Irvine, Aug-Oct. 2016, working on latent variable models for reinforcement learning in collaboration with Alessandro Lazaric</p>
            </li>
            <li id="uid174">
              <p noindent="true">Pierre-Victor Chaumier, Ecole Polytechnique, Jan-Jun. 2016, working on transfer learning in collaboration with Alessandro Lazaric</p>
            </li>
            <li id="uid175">
              <p noindent="true">Firas Jarboui, ENSTA ParisTech, France, May-July.@ 2016, working on Human-AI co-operation, in collaboration with Christos Dimitrakakis.</p>
            </li>
          </simplelist>
        </subsection>
      </subsection>
      <subsection id="uid176" level="2">
        <bodyTitle>Visits to International Teams</bodyTitle>
        <subsection id="uid177" level="3">
          <bodyTitle>Research Stays Abroad</bodyTitle>
          <simplelist>
            <li id="uid178">
              <p noindent="true">Christos Dimitrakakis visited SEAS, Harvard University, USA in the context of a Swedish/EU project ‚ÄúMarket Mechanisms for Multiple Minds‚Äù, and the future of life institute project ‚ÄúMechanism Design for Multiple AIs‚Äù, May-June, September-December 2016.</p>
            </li>
            <li id="uid179">
              <p noindent="true">Christos Dimitrakakis visited ETHZ, Switzerland, in the context of the Swiss SNSF project ‚ÄúDifferential Privacy and Approximate Decision Making‚Äù, July-September 2016.</p>
            </li>
          </simplelist>
        </subsection>
      </subsection>
    </subsection>
  </partenariat>
  <diffusion id="uid180">
    <bodyTitle>Dissemination</bodyTitle>
    <subsection id="uid181" level="1">
      <bodyTitle>Promoting Scientific Activities</bodyTitle>
      <subsection id="uid182" level="2">
        <bodyTitle>Scientific Events Organisation</bodyTitle>
        <subsection id="uid183" level="3">
          <bodyTitle>Member of the Organizing Committees</bodyTitle>
          <simplelist>
            <li id="uid184">
              <p noindent="true">C. Dimitrakakis, ICML Workshop on the theory and practice of differential privacy.</p>
            </li>
            <li id="uid185">
              <p noindent="true">Ph. Preux, ‚ÄúBig Data : Modelisation, Estimation and Selection‚Äù, June 2016, Villeneuve d'Ascq.</p>
            </li>
          </simplelist>
        </subsection>
      </subsection>
      <subsection id="uid186" level="2">
        <bodyTitle>Scientific Events Selection</bodyTitle>
        <subsection id="uid187" level="3">
          <bodyTitle>Member of the Conference Program Committees</bodyTitle>
          <simplelist>
            <li id="uid188">
              <p noindent="true">Conference on Learning Theory (COLT)</p>
            </li>
            <li id="uid189">
              <p noindent="true">International Joint Conference on Artificial Intelligence (IJCAI)</p>
            </li>
            <li id="uid190">
              <p noindent="true">European Conference on Machine Learning (ECML)</p>
            </li>
            <li id="uid191">
              <p noindent="true">ICPRAM</p>
            </li>
            <li id="uid192">
              <p noindent="true">French conferences:</p>
              <simplelist>
                <li id="uid193">
                  <p noindent="true">Extraction et Gestion de Conaissances (EGC),</p>
                </li>
                <li id="uid194">
                  <p noindent="true">Journ√©es Francophones de Planification, D√©cision, Apprentissage (JFPDA),</p>
                </li>
                <li id="uid195">
                  <p noindent="true">Apprentissage Automatique et Fouille de Donn√©es &amp; Soci√©t√© Fran√ßaise de Classification</p>
                </li>
              </simplelist>
            </li>
          </simplelist>
        </subsection>
        <subsection id="uid196" level="3">
          <bodyTitle>Reviewer</bodyTitle>
          <simplelist>
            <li id="uid197">
              <p noindent="true">Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)</p>
            </li>
            <li id="uid198">
              <p noindent="true">Conference on Learning Theory (COLT 2016)</p>
            </li>
            <li id="uid199">
              <p noindent="true">European Workshop on Reinforcement Learning (EWRL 2016)</p>
            </li>
            <li id="uid200">
              <p noindent="true">European Conference on Machine Learning (ECML 2016)</p>
            </li>
            <li id="uid201">
              <p noindent="true">International Conference on Machine Learning (ICML 2016)</p>
            </li>
            <li id="uid202">
              <p noindent="true">Neural Information Processing Systems (NIPS 2016)</p>
            </li>
            <li id="uid203">
              <p noindent="true">International Joint Conference on Artificial Intelligence (IJCAI 2016)</p>
            </li>
            <li id="uid204">
              <p noindent="true">Conference on Autonomous Agents and Multia-Agent Systems (AAMAS 2016)</p>
            </li>
            <li id="uid205">
              <p noindent="true">International Conference on Artificial Intelligence and Statistics (AISTATS 2016)</p>
            </li>
            <li id="uid206">
              <p noindent="true">French conferences:</p>
              <simplelist>
                <li id="uid207">
                  <p noindent="true">Extraction et Gestion de Conaissances (EGC),</p>
                </li>
                <li id="uid208">
                  <p noindent="true">Journ√©es Francophones de Planification, D√©cision, Apprentissage (JFPDA),</p>
                </li>
                <li id="uid209">
                  <p noindent="true">conf√©rence francophone sur l'Apprentissage Automatique (CAp),</p>
                </li>
                <li id="uid210">
                  <p noindent="true">Apprentissage Automatique et Fouille de Donn√©es &amp; Soci√©t√© Fran√ßaise de Classification</p>
                </li>
                <li id="uid211">
                  <p noindent="true">Conf√©rence Nationale d'Intelligence Artificielle (CNIA)</p>
                </li>
              </simplelist>
            </li>
          </simplelist>
        </subsection>
      </subsection>
      <subsection id="uid212" level="2">
        <bodyTitle>Journal</bodyTitle>
        <subsection id="uid213" level="3">
          <bodyTitle>Member of the Editorial Boards</bodyTitle>
          <simplelist>
            <li id="uid214">
              <p noindent="true">Journal of Games.</p>
            </li>
            <li id="uid215">
              <p noindent="true">Neurocomputing.</p>
            </li>
            <li id="uid216">
              <p noindent="true">Revue d'Intelligence Artificielle</p>
            </li>
          </simplelist>
        </subsection>
        <subsection id="uid217" level="3">
          <bodyTitle>Reviewer - Reviewing Activities</bodyTitle>
          <simplelist>
            <li id="uid218">
              <p noindent="true">Automatica</p>
            </li>
            <li id="uid219">
              <p noindent="true">Artificial Intelligence Journal</p>
            </li>
            <li id="uid220">
              <p noindent="true">Machine Learning Journal</p>
            </li>
            <li id="uid221">
              <p noindent="true">Journal of Artificial Intelligence Research</p>
            </li>
            <li id="uid222">
              <p noindent="true">Journal of Machine Learning Research</p>
            </li>
            <li id="uid223">
              <p noindent="true">AMS Mathematical Review</p>
            </li>
            <li id="uid224">
              <p noindent="true">IEEE Transaction on Signal Processing</p>
            </li>
            <li id="uid225">
              <p noindent="true">IEEE Tansaction on Cybernetics</p>
            </li>
          </simplelist>
        </subsection>
      </subsection>
      <subsection id="uid226" level="2">
        <bodyTitle>Invited Talks</bodyTitle>
        <simplelist>
          <li id="uid227">
            <p noindent="true">R. Gaudel,
<i>From Bandits to Recommender Systems</i>, Presented on September 29th, 2016 at ENSAI, Rennes, France</p>
          </li>
          <li id="uid228">
            <p noindent="true">R. Gaudel,
<i>Recommendation as a sequential process</i>, Presented on December 12th, 2016 at CMLA Math√©matiques Appliqu√©es, Cachan, France</p>
          </li>
          <li id="uid229">
            <p noindent="true">E. Kaufmann, <i>The information complexity of best arm identification</i>, Multi-armed Bandit Workshop 2016 at STOR-i, Lancaster University, UK, January 2016.</p>
          </li>
          <li id="uid230">
            <p noindent="true">E. Kaufmann, <i>The information complexity of sequential resource allocation</i>, seminar of the Collegio Carlo Alberto, Turin, March 2016.</p>
          </li>
          <li id="uid231">
            <p noindent="true">E. Kaufmann, <i>Optimal Best Arm Identification with Fixed Confidence</i>, Workshop on Computational and Statistical Trade-offs in Learning , Institut des Hautes Etudes Scientifiques (Orsay), March 2016.</p>
          </li>
          <li id="uid232">
            <p noindent="true">E. Kaufmann, <i>The information complexity of sequential resource allocation</i>, Stalab seminar, University of Cambridge, UK, April 2016.</p>
          </li>
          <li id="uid233">
            <p noindent="true">E. Kaufmann, <i>Strat√©gies bay√©siennes et fr√©quentistes dans un mod√®le de bandit</i>, 1er congr√®s de la Soci√©t√© Math√©matique de France, Tours, June 2016.</p>
          </li>
          <li id="uid234">
            <p noindent="true">E. Kaufmann, <i>Strat√©gies bay√©siennes et fr√©quentistes dans un mod√®le de bandit</i>, Journ√©es MAS, Grenoble, August 2016.</p>
          </li>
          <li id="uid235">
            <p noindent="true">E. Kaufmann, <i>Revisiting the Exploration-Exploitation Tradeoff in Bandit Models</i>, Workshop on Optimization and Decision-Making in Uncertainty,
Simons Institute, Berkeley, September 2016.</p>
          </li>
          <li id="uid236">
            <p noindent="true">A. Lazaric, <i>Spectral Methods for Learning in POMDPs</i>, University of Li√®ge, Belgium, February 2016.</p>
          </li>
          <li id="uid237">
            <p noindent="true">A. Lazaric, <i>Spectral Methods for Learning in POMDPs</i>, CMLA Math√©matiques Appliqu√©es, Cachan, France, February 2016.</p>
          </li>
          <li id="uid238">
            <p noindent="true">A. Lazaric, <i>Incremental Kernel Regression with Ridge Leverage Score Sampling</i>, ‚ÄúData Learning and Inference‚Äù (DALI), Sestri Levante, Italy, April 2016.</p>
          </li>
          <li id="uid239">
            <p noindent="true">A. Lazaric, <i>Optimism and Randomness in Linear Multi-armed Bandit</i>, ‚ÄúInternational Conference on Monte-Carlo Techniques‚Äù, July 2016.</p>
          </li>
          <li id="uid240">
            <p noindent="true">J. Mary, <i>Structured Bandits</i>, ‚ÄúUniversity of Strasbourg‚Äù, May. 2016.</p>
          </li>
          <li id="uid241">
            <p noindent="true">J. Mary, <i>Tutorial on Deep Neural Networks</i>, ‚ÄúJourn√©es Big Data‚Äù, by the Laboratoire Painlev√©. Jun. 2016.</p>
          </li>
          <li id="uid242">
            <p noindent="true">J. Mary, <i>Machine Learning and AI</i>, ‚ÄúEDF Seminar‚Äù, Dec. 2016.</p>
          </li>
          <li id="uid243">
            <p noindent="true">O. Pietquin,
<i>Closing the Interaction Loop with (Inverse) Reinforcement Learning</i>, Presented on November 15, 2016 at AWRL, Hamilton, New-Zealand</p>
          </li>
          <li id="uid244">
            <p noindent="true">O. Pietquin,
<i>Challenges of End-to-End Spoken Dialogue Systems</i>, Presented on December 10, 2016 at FILM@NIPS Workshop, Barcelona, Spain</p>
          </li>
          <li id="uid245">
            <p noindent="true">O. Pietquin,
<i>Keeping the Human in the Loop: Challenges for Machine Learning</i>, Presented on March 10, 2016 at Xerox Research Center in Europe, Grenoble, France</p>
          </li>
          <li id="uid246">
            <p noindent="true">M. Valko, <i>Spectral Methods for Learning in POMDPs</i>, University of Li√®ge, Belgium, February 2016.</p>
          </li>
          <li id="uid247">
            <p noindent="true">M. Valko,
<i>Where is Justin Bieber?</i>, Presented on September 22nd, 2016 at Comenius University in Bratislava, Slovakia
(<i>FMFI 2016</i>)</p>
          </li>
          <li id="uid248">
            <p noindent="true">M. Valko,
<i>Bandit learning</i>, Presented on September 15‚Äì19th, 2016 at Information technologies - Applications and Theory, at Tatransk√© Matliare, High Tatras, Slovakia
(<i>ITAT 2016</i>)</p>
          </li>
          <li id="uid249">
            <p noindent="true">M. Valko,
<i>Decision-making on graphs without graphs</i>, Presented on June 16-17th, 2016 at Graph-based Learning and Graph Mining workshop, at Inria Lille, France
(<i>GBLGM 2016</i>)</p>
          </li>
          <li id="uid250">
            <p noindent="true">M. Valko,
<i>Sequential learning on graphs with limited feedback</i>, Presented on May 11‚Äì13th, 2016 at Data Driven Approach to Networks and Language, at ENS Lyon, France
(<i>NETSpringLyon 2016</i>)</p>
          </li>
          <li id="uid251">
            <p noindent="true">M. Valko,
<i>Benefits of Graphs in Bandit Settings</i>, Presented on January 11‚Äì12th, 2016 at Multi-armed Bandit Workshop 2016 at STOR-i, Lancaster University, UK
(<i>STOR-i 2016</i>)</p>
          </li>
        </simplelist>
      </subsection>
      <subsection id="uid252" level="2">
        <bodyTitle>Scientific Expertise</bodyTitle>
        <simplelist>
          <li id="uid253">
            <p noindent="true">Agence Nationale pour la Recherche (ANR)</p>
          </li>
          <li id="uid254">
            <p noindent="true">ANRT</p>
          </li>
          <li id="uid255">
            <p noindent="true">D2RT Ile de France</p>
          </li>
          <li id="uid256">
            <p noindent="true">Institut National de Recherche en Agronomie (INRA)</p>
          </li>
          <li id="uid257">
            <p noindent="true">Fonds National pour la Recherche Scientifique (FNRS), Belgium</p>
          </li>
          <li id="uid258">
            <p noindent="true">H2020 program</p>
          </li>
          <li id="uid259">
            <p noindent="true"><i>A. Lazaric</i> was a member of the hiring committee for junior researchers at Inria Lille (2016).</p>
          </li>
          <li id="uid260">
            <p noindent="true"><i>M. Valko</i> is an elected member of the evaluation committee and participates in the hiring, promotion, and evaluation juries of Inria, notably</p>
            <simplelist>
              <li id="uid261">
                <p noindent="true">Hiring committee for junior researchers at Inria Sophia Antipolis (2016)</p>
              </li>
              <li id="uid262">
                <p noindent="true">Selection committee for Inria award for scientific excellence, junior and senior (2016)</p>
              </li>
              <li id="uid263">
                <p noindent="true">Selection committee for CR promotions (2016)</p>
              </li>
            </simplelist>
          </li>
          <li id="uid264">
            <p noindent="true">Ph. Preux has chaired the hiring committee for an associate professor position at Universit√© de Lille 3</p>
          </li>
          <li id="uid265">
            <p noindent="true">J. Mary was webpage chair for ICML'2016 in NYC</p>
          </li>
        </simplelist>
      </subsection>
      <subsection id="uid266" level="2">
        <bodyTitle>Research Administration</bodyTitle>
        <simplelist>
          <li id="uid267">
            <p noindent="true">Philippe Preux is:</p>
            <simplelist>
              <li id="uid268">
                <p noindent="true">D√©l√©gu√© Scientifique Adjoint (DSA) at Inria Lille</p>
              </li>
              <li id="uid269">
                <p noindent="true">member of the Evaluation Committee (CE) at Inria</p>
              </li>
              <li id="uid270">
                <p noindent="true">member of the Project Committee Board (BCP, Bureau du Comit√© des Projets) at Inria Lille</p>
              </li>
              <li id="uid271">
                <p noindent="true">head of the ‚ÄúData Intelligence‚Äù (DatInG) thematic group at CRIStAL.</p>
              </li>
              <li id="uid272">
                <p noindent="true">member of the Scientific Committee of CRIStAL.</p>
              </li>
            </simplelist>
          </li>
          <li id="uid273">
            <p noindent="true">R. Gaudel is member of the board of CRIStAL.</p>
          </li>
          <li id="uid274">
            <p noindent="true">R. Gaudel is manager of proml mailing list. This mailing list gathers French-speaking researchers from Machine Learning community.</p>
          </li>
          <li id="uid275">
            <p noindent="true">J. Mary is member of the "Commission D√©veloppement Technologique" at Inria Lille.</p>
          </li>
        </simplelist>
      </subsection>
    </subsection>
    <subsection id="uid276" level="1">
      <bodyTitle>Teaching - Supervision - Juries</bodyTitle>
      <subsection id="uid277" level="2">
        <bodyTitle>Teaching</bodyTitle>
        <sanspuceslist>
          <li id="uid278">
            <p noindent="true">Licence: C. Dimitrakakis, C2i, 25h eqTD, L1-2, Universit√© de Lille 3, France.</p>
          </li>
          <li id="uid279">
            <p noindent="true">Licence: C. Dimitrakakis, Traitement de donn√©es, Universit√© de Lille 3, France.</p>
          </li>
          <li id="uid280">
            <p noindent="true">Licence: C. Dimitrakakis, Mod√©lisation de bases de donn√©es, Universit√© de Lille 3, France.</p>
          </li>
          <li id="uid281">
            <p noindent="true">Licence: C. Dimitrakakis, Fonctionnement des r√©seaux, Universit√© de Lille 3, France.</p>
          </li>
          <li id="uid282">
            <p noindent="true">Master: A. Lazaric, Reinforcement Learning, 25h eqTD, M2, ENS Cachan, France</p>
          </li>
          <li id="uid283">
            <p noindent="true">Master: A. Lazaric, Reinforcement Learning, 25h eqTD, M2, Ecole Centrale Lille, France</p>
          </li>
          <li id="uid284">
            <p noindent="true">Master: Ph. Preux, Advanced data mining, 30h eqTD, M2, Universit√© de Lille 3, France</p>
          </li>
          <li id="uid285">
            <p noindent="true">Master: Ph. Preux, Fundamental algorithms for data mining, 30h eqTD, M1, Universit√© de Lille 3, France</p>
          </li>
          <li id="uid286">
            <p noindent="true">Licence: Ph. Preux, Neural Networks, 28h eqTD, L3, Universit√© de Lille 3, France</p>
          </li>
          <li id="uid287">
            <p noindent="true">Licence: Ph. Preux, Graph Theory, 28h eqTD, L3, Universit√© de Lille 3, France</p>
          </li>
          <li id="uid288">
            <p noindent="true">Licence: Ph. Preux, C2i, 25h eqTD, L1-2, Universit√© de Lille 3, France.</p>
          </li>
          <li id="uid289">
            <p noindent="true">Master: M. Valko, 2016/2017 Fall: Graphs in Machine Learning, 27h eqTD, M2, ENS Cachan</p>
          </li>
          <li id="uid290">
            <p noindent="true">Licence: R. Gaudel, 2016/2017 Spring: programmation R pour statistiques et sociologie quantitative, 44h eqTD, L1, universit√© Lille 3, France</p>
          </li>
          <li id="uid291">
            <p noindent="true">Licence: R. Gaudel, 2016/2017 Fall: pr√©paration au C2i niveau 1, 30h eqTD, L1-3, universit√© Lille 3, France</p>
          </li>
          <li id="uid292">
            <p noindent="true">Licence: R. Gaudel, 2016/2017 Spring: pr√©paration au C2i niveau 1, 25h eqTD, L1-3, universit√© Lille 3, France</p>
          </li>
          <li id="uid293">
            <p noindent="true">Licence: R. Gaudel, 2016/2017 Fall: travail collaboratif et √† distance dans un monde num√©rique, 13h eqTD, L1-3 (enseignement √† distance), universit√© Lille 3, France</p>
          </li>
          <li id="uid294">
            <p noindent="true">Licence: R. Gaudel, 2016/2017 Fall: algorithmes fondamentaux de la fouille de donn√©es, 30h eqTD, M1, universit√© Lille 3, France</p>
          </li>
          <li id="uid295">
            <p noindent="true">Licence: R. Gaudel, 2016/2017 Fall: Fouille de donn√©es avanc√©e, 30h eqTD, M2, universit√© Lille 3, France</p>
          </li>
          <li id="uid296">
            <p noindent="true">Master: B. Piot, 2016/2017 Spring: Web Design, 60h eqTD, M2, Univ. Lille, France</p>
          </li>
          <li id="uid297">
            <p noindent="true">Master: B. Piot, 2016/2017 Spring: Object Programming, 70h eqTD, M2, Univ. Lille, France</p>
          </li>
          <li id="uid298">
            <p noindent="true">Master: B. Piot, 2016/2017 Fall: Web Design, 30h eqTD, M2, Univ. Lille, France</p>
          </li>
          <li id="uid299">
            <p noindent="true">Master: B. Piot, 2016/2017 Fall: Object Programming, 22h eqTD, M2, Univ. Lille, France</p>
          </li>
          <li id="uid300">
            <p noindent="true">Master: B. Piot, 2016/2017 Fall: Databases, 30h eqTD, M1, Univ. Lille, France</p>
          </li>
          <li id="uid301">
            <p noindent="true">Master: J. Mary, 2016/2017 Fall: algorithmes fondamentaux de la fouille de donn√©es, 30h eqTD, M1, Univ. Lille, France</p>
          </li>
          <li id="uid302">
            <p noindent="true">Master: J. Mary, 2016/2017 Fall: Programmation Web Avanc√©e, 30h eqTD, M2, Univ. Lille, France</p>
          </li>
          <li id="uid303">
            <p noindent="true">Master: J. Mary, 2016/2017 Fall: Reinforcement Learning, 16h eqTD, M2, Univ. Lille, France</p>
          </li>
        </sanspuceslist>
      </subsection>
      <subsection id="uid304" level="2">
        <bodyTitle>Supervision</bodyTitle>
        <sanspuceslist>
          <li id="uid305">
            <p noindent="true">HdR: Michal Valko, Bandits on graphs and structures, ENS-Cachan, June 15th, 2016</p>
          </li>
          <li id="uid306">
            <p noindent="true">PhD: Fr√©d√©ric Guillou, On recommendation systems in sequential context, University of Lille, Dec. 2nd, 2016, advisors: Philippe Preux, Romaric Gaudel, J√©r√©mie Mary</p>
          </li>
          <li id="uid307">
            <p noindent="true">PhD: Tomas Kocak, Apprentissage s√©quentiel avec similitudes, University of Lille, Nov. 28th, 2016, advisor: Michal Valko, R√©mi Munos</p>
          </li>
          <li id="uid308">
            <p noindent="true">PhD: Hadrien Glaude, M√©thodes des moments pour l‚Äôinf√©rence de syst√®mes s√©quentiels lin√©aires rationnels, University of Lille, July. 8th, 2016, advisor: Olivier Pietquin</p>
          </li>
          <li id="uid309">
            <p noindent="true">PhD in progress: Pratik Gajane, Sequential Learning and Decision Making under Partial Monitoring, University of Lille, started Oct. 2014, advisor: Philippe Preux</p>
          </li>
          <li id="uid310">
            <p noindent="true">PhD in progress: Marc Abeille, Randomized Exploration-exploration Stretegies, University of Lille, started Oct. 2014, advisor: Alessandro Lazaric</p>
          </li>
          <li id="uid311">
            <p noindent="true">PhD in progress: Merwan Barlier, Dialogues intelligents bas√©s sur l'√©coute de conversations homme/homme, University of Lille, started Oct. 2014, advisor: Olivier Pietquin</p>
          </li>
          <li id="uid312">
            <p noindent="true">PhD in progress: Alexandre Berard, Learning from post-editing for machine translation, University of Lille, started Oct. 2014, advisor: Olivier Pietquin</p>
          </li>
          <li id="uid313">
            <p noindent="true">PhD in progress: Lilian Besson, Apprentissage s√©quentiel multi-joueurs pour la radio intelligente, CentraleSup√©lec Rennes, started Oct. 2016, advisor: Emilie Kaufmann</p>
          </li>
          <li id="uid314">
            <p noindent="true">PhD in progress: Reda Alami, Bandit √† M√©moire pour la prise de d√©cision en environnement dynamique, Orange LABS, University of Paris-Saclay, started Oct. 2016, advisor: Odalric-Ambrym Maillard, Rapha√´l Feraud</p>
          </li>
          <li id="uid315">
            <p noindent="true">PhD in progress: Daniele Calandriello, Efficient Sequential Learning in Structured and Constrained Environment, Inria, started Oct. 2014, advisor: Michal Valko, Alessandro Lazaric</p>
          </li>
          <li id="uid316">
            <p noindent="true">PhD in progress: Ronan Fruit, Transfer in Hierarchical Reinforcement Learning, University of Lille, started Dec. 2015, advisor: Alessandro Lazaric</p>
          </li>
          <li id="uid317">
            <p noindent="true">PhD in progress: Guillaume Gautier, DPPs in ML, started Oct. 2016, advisor: Michal Valko; R√©mi Bardenet</p>
          </li>
          <li id="uid318">
            <p noindent="true">PhD in progress: Jean-Bastien Grill, Cr√©ation et analyse d'algorithmes efficaces pour la prise de d√©cision dans un environnement inconnu et incertain, Inria/ENS Paris/Lille 1, started Oct. 2014, advisor: R√©mi Munos, Michal Valko</p>
          </li>
          <li id="uid319">
            <p noindent="true">PhD in progress: Julien Perolat, Reinforcement learning: the 2-player case, University of Lille, started Oct. 2014, advisor: Olivier Pietquin, Bilal Piot</p>
          </li>
          <li id="uid320">
            <p noindent="true">PhD in progress: Florian Strub, Deep sequential learning and its application to human-robot interaction, University of Lille, started Jan. 2016, advisor: Olivier Pietquin, J√©r√©mie Mary</p>
          </li>
          <li id="uid321">
            <p noindent="true">PhD in progress: Romain Warlop, Novel Learning and Exploration-Exploitation Methods for Effective Recommender Systems, University of Lille, started Sep. 2015, advisor: J√©r√©mie Mary</p>
          </li>
          <li id="uid322">
            <p noindent="true">PhD in progress: Aristide Tossou, Privacy in Sequential Decision Making (provional), Chalmers, started Feb. 2015, advisor: Christos Dimitrakakis</p>
          </li>
        </sanspuceslist>
      </subsection>
      <subsection id="uid323" level="2">
        <bodyTitle>Juries</bodyTitle>
        <p>PhD and hdr juries:</p>
        <simplelist>
          <li id="uid324">
            <p noindent="true">E. Kaufmann: Marie-Liesse Cauwet, LRI, Orsay.</p>
          </li>
          <li id="uid325">
            <p noindent="true">A. Lazaric: Matteo Pirotta, Politecnico di Milano, Italy.</p>
          </li>
          <li id="uid326">
            <p noindent="true">J. Mary: examinator for Raph√´l Puget, universit√© Paris 6.</p>
          </li>
          <li id="uid327">
            <p noindent="true">J. Mary: reviewer for Robin Allesiardo, universit√© Paris Saclay</p>
          </li>
          <li id="uid328">
            <p noindent="true">Ph. Preux: reviewer for Hongliang Zhong, Laboratoire d'Informatique Fondamentale, Marseille</p>
          </li>
          <li id="uid329">
            <p noindent="true">Ph. Preux: president of the defense jury of the HDR of Matthieu Geist, Universit√© de Lille</p>
          </li>
          <li id="uid330">
            <p noindent="true">O. Pietquin: advisor of the HDR of Matthieu Gesit, Universit√© de Lille</p>
          </li>
          <li id="uid331">
            <p noindent="true">O. Pietquin: Hatim Khouzami, University of Avignon</p>
          </li>
        </simplelist>
        <p>PhD mid-term evaluation:</p>
        <simplelist>
          <li id="uid332">
            <p noindent="true">A. Lazaric: Claire Vernade (mid-term evaluation), Telecom ParisTech, France.</p>
          </li>
          <li id="uid333">
            <p noindent="true">E. Kaufmann: opponent for the licenciate thesis of Stefan Magureanu, KTH, Stockholm, Sweden.</p>
          </li>
        </simplelist>
      </subsection>
    </subsection>
    <subsection id="uid334" level="1">
      <bodyTitle>Popularization</bodyTitle>
      <simplelist>
        <li id="uid335">
          <p noindent="true">A. Lazaric was interviewed for Inria-Lille magazine (December issue).</p>
        </li>
        <li id="uid336">
          <p noindent="true">J. Mary gave a 55 min talk in front of students of Lyc√©e Sainte-Famille at Amiens.</p>
        </li>
        <li id="uid337">
          <p noindent="true">Ph. Preux gave 2 talks on ‚ÄúArtificial Intelligence‚Äù in a high-school in Villeneuve d'Ascq within the ‚ÄúF√™te de la science‚Äù.</p>
        </li>
        <li id="uid338">
          <p noindent="true">O. Pietquin was interviewed by France Culture (Supersonic) on March 23rd, 2016</p>
        </li>
      </simplelist>
    </subsection>
  </diffusion>
  <biblio id="bibliography" html="bibliography" numero="10" titre="Bibliography">
    
    <biblStruct id="sequel-2016-bid65" type="article" rend="refer" n="refercite:cappe:hal-00738209">
      <identifiant type="hal" value="hal-00738209"/>
      <analytic>
        <title level="a">Kullback-Leibler Upper Confidence Bounds for Optimal Sequential Allocation</title>
        <author>
          <persName>
            <foreName>Olivier</foreName>
            <surname>Capp√©</surname>
            <initial>O.</initial>
          </persName>
          <persName>
            <foreName>Aur√©lien</foreName>
            <surname>Garivier</surname>
            <initial>A.</initial>
          </persName>
          <persName key="tao-2015-idp83360">
            <foreName>Odalric-Ambrym</foreName>
            <surname>Maillard</surname>
            <initial>O.-A.</initial>
          </persName>
          <persName key="sequel-2014-idp67360">
            <foreName>R√©mi</foreName>
            <surname>Munos</surname>
            <initial>R.</initial>
          </persName>
          <persName>
            <foreName>Gilles</foreName>
            <surname>Stoltz</surname>
            <initial>G.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">Annals of Statistics</title>
        <imprint>
          <biblScope type="volume">41</biblScope>
          <biblScope type="number">3</biblScope>
          <dateStruct>
            <year>2013</year>
          </dateStruct>
          <biblScope type="pages">1516-1541</biblScope>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-00738209" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-00738209</ref>
        </imprint>
      </monogr>
      <note type="bnote">Accepted, to appear in Annals of Statistics</note>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid67" type="inproceedings" rend="refer" n="refercite:carpentier:hal-01304020">
      <identifiant type="hal" value="hal-01304020"/>
      <analytic>
        <title level="a">Revealing graph bandits for maximizing local influence</title>
        <author>
          <persName>
            <foreName>Alexandra</foreName>
            <surname>Carpentier</surname>
            <initial>A.</initial>
          </persName>
          <persName key="sequel-2014-idp70232">
            <foreName>Michal</foreName>
            <surname>Valko</surname>
            <initial>M.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="m">International Conference on Artificial Intelligence and Statistics</title>
        <loc>Seville, Spain</loc>
        <imprint>
          <dateStruct>
            <month>May</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01304020" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01304020</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid61" type="article" rend="refer" n="refercite:gatti:hal-01237670">
      <identifiant type="hal" value="hal-01237670"/>
      <analytic>
        <title level="a">Truthful Learning Mechanisms for Multi‚ÄìSlot Sponsored Search Auctions with Externalities</title>
        <author>
          <persName>
            <foreName>Nicola</foreName>
            <surname>Gatti</surname>
            <initial>N.</initial>
          </persName>
          <persName key="sequel-2014-idm26088">
            <foreName>Alessandro</foreName>
            <surname>Lazaric</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Marco</foreName>
            <surname>Rocco</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>Francesco</foreName>
            <surname>Trov√≤</surname>
            <initial>F.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">Artificial Intelligence</title>
        <imprint>
          <biblScope type="volume">227</biblScope>
          <dateStruct>
            <month>October</month>
            <year>2015</year>
          </dateStruct>
          <biblScope type="pages">93-139</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01237670" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01237670</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid58" type="article" rend="refer" n="refercite:ghavamzadeh:hal-00776608">
      <identifiant type="hal" value="hal-00776608"/>
      <analytic>
        <title level="a">Bayesian Policy Gradient and Actor-Critic Algorithms</title>
        <author>
          <persName key="sequel-2014-idp65928">
            <foreName>Mohammad</foreName>
            <surname>Ghavamzadeh</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>Yaakov</foreName>
            <surname>Engel</surname>
            <initial>Y.</initial>
          </persName>
          <persName key="sequel-2014-idp70232">
            <foreName>Michal</foreName>
            <surname>Valko</surname>
            <initial>M.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">Journal of Machine Learning Research</title>
        <imprint>
          <biblScope type="volume">17</biblScope>
          <biblScope type="number">66</biblScope>
          <dateStruct>
            <month>January</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">1-53</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-00776608" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-00776608</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid60" type="article" rend="refer" n="refercite:kadri:hal-01221329">
      <identifiant type="hal" value="hal-01221329"/>
      <analytic>
        <title level="a">Operator-valued Kernels for Learning from Functional Response Data</title>
        <author>
          <persName>
            <foreName>Hachem</foreName>
            <surname>Kadri</surname>
            <initial>H.</initial>
          </persName>
          <persName key="sequel-2014-idp74216">
            <foreName>Emmanuel</foreName>
            <surname>Duflos</surname>
            <initial>E.</initial>
          </persName>
          <persName key="sequel-2014-idm27568">
            <foreName>Philippe</foreName>
            <surname>Preux</surname>
            <initial>P.</initial>
          </persName>
          <persName>
            <foreName>St√©phane</foreName>
            <surname>Canu</surname>
            <initial>S.</initial>
          </persName>
          <persName>
            <foreName>Alain</foreName>
            <surname>Rakotomamonjy</surname>
            <initial>A.</initial>
          </persName>
          <persName key="sequel-2014-idp120064">
            <foreName>Julien</foreName>
            <surname>Audiffren</surname>
            <initial>J.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">Journal of Machine Learning Research (JMLR)</title>
        <imprint>
          <dateStruct>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01221329" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01221329</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid66" type="article" rend="refer" n="refercite:kaufmann:hal-01024894">
      <identifiant type="hal" value="hal-01024894"/>
      <analytic>
        <title level="a">On the Complexity of Best Arm Identification in Multi-Armed Bandit Models</title>
        <author>
          <persName key="dyogene-2014-idp72312">
            <foreName>Emilie</foreName>
            <surname>Kaufmann</surname>
            <initial>E.</initial>
          </persName>
          <persName>
            <foreName>Olivier</foreName>
            <surname>Capp√©</surname>
            <initial>O.</initial>
          </persName>
          <persName>
            <foreName>Aur√©lien</foreName>
            <surname>Garivier</surname>
            <initial>A.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">Journal of Machine Learning Research</title>
        <imprint>
          <biblScope type="volume">17</biblScope>
          <dateStruct>
            <month>January</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">1-42</biblScope>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01024894" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01024894</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid59" type="article" rend="refer" n="refercite:lazaric:hal-01401513">
      <identifiant type="hal" value="hal-01401513"/>
      <analytic>
        <title level="a">Analysis of Classification-based Policy Iteration Algorithms</title>
        <author>
          <persName key="sequel-2014-idm26088">
            <foreName>Alessandro</foreName>
            <surname>Lazaric</surname>
            <initial>A.</initial>
          </persName>
          <persName key="sequel-2014-idp65928">
            <foreName>Mohammad</foreName>
            <surname>Ghavamzadeh</surname>
            <initial>M.</initial>
          </persName>
          <persName key="sequel-2014-idp67360">
            <foreName>R√©mi</foreName>
            <surname>Munos</surname>
            <initial>R.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">Journal of Machine Learning Research</title>
        <imprint>
          <biblScope type="volume">17</biblScope>
          <dateStruct>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">1 - 30</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01401513" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01401513</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid63" type="techreport" rend="refer" n="refercite:munos:hal-00747575">
      <identifiant type="hal" value="hal-00747575"/>
      <monogr>
        <title level="m">From Bandits to Monte-Carlo Tree Search: The Optimistic Principle Applied to Optimization and Planning</title>
        <author>
          <persName key="sequel-2014-idp67360">
            <foreName>R√©mi</foreName>
            <surname>Munos</surname>
            <initial>R.</initial>
          </persName>
        </author>
        <imprint>
          <dateStruct>
            <year>2014</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-00747575" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-00747575</ref>
        </imprint>
      </monogr>
      <note type="bnote">130 pages</note>
      <note type="typdoc">Technical report</note>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid62" type="article" rend="refer" n="refercite:ortner:hal-01074077">
      <identifiant type="doi" value="10.1016/j.tcs.2014.09.026"/>
      <identifiant type="hal" value="hal-01074077"/>
      <analytic>
        <title level="a">Regret bounds for restless Markov bandits</title>
        <author>
          <persName>
            <foreName>Ronald</foreName>
            <surname>Ortner</surname>
            <initial>R.</initial>
          </persName>
          <persName key="sequel-2014-idp68800">
            <foreName>Daniil</foreName>
            <surname>Ryabko</surname>
            <initial>D.</initial>
          </persName>
          <persName>
            <foreName>Peter</foreName>
            <surname>Auer</surname>
            <initial>P.</initial>
          </persName>
          <persName key="sequel-2014-idp67360">
            <foreName>R√©mi</foreName>
            <surname>Munos</surname>
            <initial>R.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">Journal of Theoretical Computer Science (TCS)</title>
        <imprint>
          <biblScope type="volume">558</biblScope>
          <dateStruct>
            <year>2014</year>
          </dateStruct>
          <biblScope type="pages">62-76</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01074077" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01074077</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid64" type="article" rend="refer" n="refercite:ryabko:hal-00913240">
      <identifiant type="hal" value="hal-00913240"/>
      <analytic>
        <title level="a">A Binary-Classification-Based Metric between Time-Series Distributions and Its Use in Statistical and Learning Problems</title>
        <author>
          <persName key="sequel-2014-idp68800">
            <foreName>Daniil</foreName>
            <surname>Ryabko</surname>
            <initial>D.</initial>
          </persName>
          <persName key="sequel-2014-idp76928">
            <foreName>J√©r√©mie</foreName>
            <surname>Mary</surname>
            <initial>J.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">Journal of Machine Learning Research</title>
        <imprint>
          <biblScope type="volume">14</biblScope>
          <dateStruct>
            <year>2013</year>
          </dateStruct>
          <biblScope type="pages">2837-2856</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-00913240" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-00913240</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid50" type="phdthesis" rend="year" n="cite:glaude:tel-01374080">
      <identifiant type="hal" value="tel-01374080"/>
      <monogr>
        <title level="m">Learning rational linear sequential systems using the method of moments</title>
        <author>
          <persName key="sequel-2014-idp103632">
            <foreName>Hadrien</foreName>
            <surname>Glaude</surname>
            <initial>H.</initial>
          </persName>
        </author>
        <imprint>
          <publisher>
            <orgName type="school">Universit√© de Lille 1 - Sciences et Technologies</orgName>
          </publisher>
          <dateStruct>
            <month>July</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://tel.archives-ouvertes.fr/tel-01374080" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>tel.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>tel-01374080</ref>
        </imprint>
      </monogr>
      <note type="typdoc">Theses</note>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid51" type="phdthesis" rend="year" n="cite:guillou:tel-01407336">
      <identifiant type="hal" value="tel-01407336"/>
      <monogr>
        <title level="m">On Recommendation Systems in a Sequential Context</title>
        <author>
          <persName key="sequel-2014-idp106104">
            <foreName>Fr√©d√©ric</foreName>
            <surname>Guillou</surname>
            <initial>F.</initial>
          </persName>
        </author>
        <imprint>
          <publisher>
            <orgName type="school">Universit√© Lille 3</orgName>
          </publisher>
          <dateStruct>
            <month>December</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://tel.archives-ouvertes.fr/tel-01407336" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>tel.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>tel-01407336</ref>
        </imprint>
      </monogr>
      <note type="typdoc">Theses</note>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid54" type="phdthesis" rend="year" n="cite:musco:tel-01398903">
      <identifiant type="hal" value="tel-01398903"/>
      <monogr>
        <title level="m">Propagation Analysis based on Software Graphs and Synthetic Data</title>
        <author>
          <persName key="sequel-2014-idp117600">
            <foreName>Vincenzo</foreName>
            <surname>Musco</surname>
            <initial>V.</initial>
          </persName>
        </author>
        <imprint>
          <publisher>
            <orgName type="school">Universit√© Lille 3</orgName>
          </publisher>
          <dateStruct>
            <month>November</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://tel.archives-ouvertes.fr/tel-01398903" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>tel.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>tel-01398903</ref>
        </imprint>
      </monogr>
      <note type="typdoc">Theses</note>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid52" type="hdrthesis" rend="year" n="cite:valko:tel-01359757">
      <identifiant type="hal" value="tel-01359757"/>
      <monogr>
        <title level="m">Bandits on graphs and structures</title>
        <author>
          <persName key="sequel-2014-idp70232">
            <foreName>Michal</foreName>
            <surname>Valko</surname>
            <initial>M.</initial>
          </persName>
        </author>
        <imprint>
          <publisher>
            <orgName type="school">√âcole normale sup√©rieure de Cachan - ENS Cachan</orgName>
          </publisher>
          <dateStruct>
            <month>June</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/tel-01359757" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>tel-01359757</ref>
        </imprint>
      </monogr>
      <note type="typdoc">Habilitation √† diriger des recherches</note>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid10" type="article" rend="year" n="cite:ghavamzadeh:hal-00776608">
      <identifiant type="hal" value="hal-00776608"/>
      <analytic>
        <title level="a">Bayesian Policy Gradient and Actor-Critic Algorithms</title>
        <author>
          <persName key="sequel-2014-idp65928">
            <foreName>Mohammad</foreName>
            <surname>Ghavamzadeh</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>Yaakov</foreName>
            <surname>Engel</surname>
            <initial>Y.</initial>
          </persName>
          <persName key="sequel-2014-idp70232">
            <foreName>Michal</foreName>
            <surname>Valko</surname>
            <initial>M.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid01187">
        <idno type="issn">1532-4435</idno>
        <title level="j">Journal of Machine Learning Research</title>
        <imprint>
          <biblScope type="volume">17</biblScope>
          <biblScope type="number">66</biblScope>
          <dateStruct>
            <month>January</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">1-53</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-00776608" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-00776608</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid34" type="article" rend="year" n="cite:kadri:hal-01221329">
      <identifiant type="hal" value="hal-01221329"/>
      <analytic>
        <title level="a">Operator-valued Kernels for Learning from Functional Response Data</title>
        <author>
          <persName>
            <foreName>Hachem</foreName>
            <surname>Kadri</surname>
            <initial>H.</initial>
          </persName>
          <persName key="sequel-2014-idp74216">
            <foreName>Emmanuel</foreName>
            <surname>Duflos</surname>
            <initial>E.</initial>
          </persName>
          <persName key="sequel-2014-idm27568">
            <foreName>Philippe</foreName>
            <surname>Preux</surname>
            <initial>P.</initial>
          </persName>
          <persName>
            <foreName>St√©phane</foreName>
            <surname>Canu</surname>
            <initial>S.</initial>
          </persName>
          <persName>
            <foreName>Alain</foreName>
            <surname>Rakotomamonjy</surname>
            <initial>A.</initial>
          </persName>
          <persName key="sequel-2014-idp120064">
            <foreName>Julien</foreName>
            <surname>Audiffren</surname>
            <initial>J.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid01187">
        <idno type="issn">1532-4435</idno>
        <title level="j">Journal of Machine Learning Research (JMLR)</title>
        <imprint>
          <dateStruct>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01221329" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01221329</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid16" type="article" rend="year" n="cite:kaufmann:hal-01024894">
      <identifiant type="hal" value="hal-01024894"/>
      <analytic>
        <title level="a">On the Complexity of Best Arm Identification in Multi-Armed Bandit Models</title>
        <author>
          <persName key="dyogene-2014-idp72312">
            <foreName>Emilie</foreName>
            <surname>Kaufmann</surname>
            <initial>E.</initial>
          </persName>
          <persName>
            <foreName>Olivier</foreName>
            <surname>Capp√©</surname>
            <initial>O.</initial>
          </persName>
          <persName>
            <foreName>Aur√©lien</foreName>
            <surname>Garivier</surname>
            <initial>A.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid01187">
        <idno type="issn">1532-4435</idno>
        <title level="j">Journal of Machine Learning Research</title>
        <imprint>
          <biblScope type="volume">17</biblScope>
          <dateStruct>
            <month>January</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">1-42</biblScope>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01024894" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01024894</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid29" type="article" rend="year" n="cite:khaleghi:hal-01235330">
      <identifiant type="doi" value="10.1016/j.tcs.2015.10.041"/>
      <identifiant type="hal" value="hal-01235330"/>
      <analytic>
        <title level="a">Nonparametric multiple change point estimation in highly dependent time series</title>
        <author>
          <persName>
            <foreName>Azadeh</foreName>
            <surname>Khaleghi</surname>
            <initial>A.</initial>
          </persName>
          <persName key="sequel-2014-idp68800">
            <foreName>Daniil</foreName>
            <surname>Ryabko</surname>
            <initial>D.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid01855">
        <idno type="issn">0304-3975</idno>
        <title level="j">Theoretical Computer Science</title>
        <imprint>
          <biblScope type="volume">620</biblScope>
          <dateStruct>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">119-133</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01235330" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01235330</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid30" type="article" rend="year" n="cite:khaleghi:hal-01399613">
      <identifiant type="hal" value="hal-01399613"/>
      <analytic>
        <title level="a">Consistent Algorithms for Clustering Time Series</title>
        <author>
          <persName>
            <foreName>Azadeh</foreName>
            <surname>Khaleghi</surname>
            <initial>A.</initial>
          </persName>
          <persName key="sequel-2014-idp68800">
            <foreName>Daniil</foreName>
            <surname>Ryabko</surname>
            <initial>D.</initial>
          </persName>
          <persName key="sequel-2014-idp76928">
            <foreName>Jeremie</foreName>
            <surname>Mary</surname>
            <initial>J.</initial>
          </persName>
          <persName key="sequel-2014-idm27568">
            <foreName>Philippe</foreName>
            <surname>Preux</surname>
            <initial>P.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid01187">
        <idno type="issn">1532-4435</idno>
        <title level="j">Journal of Machine Learning Research</title>
        <imprint>
          <biblScope type="volume">17</biblScope>
          <biblScope type="number">3</biblScope>
          <dateStruct>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">1 - 32</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01399613" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01399613</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid8" type="article" rend="year" n="cite:lazaric:hal-01401513">
      <identifiant type="hal" value="hal-01401513"/>
      <analytic>
        <title level="a">Analysis of Classification-based Policy Iteration Algorithms</title>
        <author>
          <persName key="sequel-2014-idm26088">
            <foreName>Alessandro</foreName>
            <surname>Lazaric</surname>
            <initial>A.</initial>
          </persName>
          <persName key="sequel-2014-idp65928">
            <foreName>Mohammad</foreName>
            <surname>Ghavamzadeh</surname>
            <initial>M.</initial>
          </persName>
          <persName key="sequel-2014-idp67360">
            <foreName>R√©mi</foreName>
            <surname>Munos</surname>
            <initial>R.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid01187">
        <idno type="issn">1532-4435</idno>
        <title level="j">Journal of Machine Learning Research</title>
        <imprint>
          <biblScope type="volume">17</biblScope>
          <dateStruct>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">1 - 30</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01401513" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01401513</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid40" type="article" rend="year" n="cite:musco:hal-01346046">
      <identifiant type="doi" value="10.1007/s11219-016-9332-8"/>
      <identifiant type="hal" value="hal-01346046"/>
      <analytic>
        <title level="a">A Large-scale Study of Call Graph-based Impact Prediction using Mutation Testing</title>
        <author>
          <persName key="sequel-2014-idp117600">
            <foreName>Vincenzo</foreName>
            <surname>Musco</surname>
            <initial>V.</initial>
          </persName>
          <persName key="spirals-2014-idp82576">
            <foreName>Martin</foreName>
            <surname>Monperrus</surname>
            <initial>M.</initial>
          </persName>
          <persName key="sequel-2014-idm27568">
            <foreName>Philippe</foreName>
            <surname>Preux</surname>
            <initial>P.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid01783">
        <idno type="issn">0963-9314</idno>
        <title level="j">Software Quality Journal</title>
        <imprint>
          <dateStruct>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01346046" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01346046</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid44" type="article" rend="year" n="cite:neu:hal-01380278">
      <identifiant type="hal" value="hal-01380278"/>
      <analytic>
        <title level="a">Importance Weighting Without Importance Weights: An Efficient Algorithm for Combinatorial Semi-Bandits</title>
        <author>
          <persName key="sequel-2014-idp82200">
            <foreName>Gergely</foreName>
            <surname>Neu</surname>
            <initial>G.</initial>
          </persName>
          <persName>
            <foreName>Bart√≥k</foreName>
            <surname>G√°bor</surname>
            <initial>B.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid01187">
        <idno type="issn">1532-4435</idno>
        <title level="j">Journal of Machine Learning Research</title>
        <imprint>
          <biblScope type="volume">17</biblScope>
          <biblScope type="number">154</biblScope>
          <dateStruct>
            <month>August</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">1 - 21</biblScope>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01380278" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01380278</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid9" type="inproceedings" rend="year" n="cite:azizzadenesheli:hal-01322207">
      <identifiant type="hal" value="hal-01322207"/>
      <analytic>
        <title level="a">Reinforcement Learning of POMDPs using Spectral Methods</title>
        <author>
          <persName key="sequel-2016-idp179696">
            <foreName>Kamyar</foreName>
            <surname>Azizzadenesheli</surname>
            <initial>K.</initial>
          </persName>
          <persName key="sequel-2014-idm26088">
            <foreName>Alessandro</foreName>
            <surname>Lazaric</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Animashree</foreName>
            <surname>Anandkumar</surname>
            <initial>A.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">Proceedings of the 29th Annual Conference on Learning Theory (COLT2016)</title>
        <loc>New York City, United States</loc>
        <imprint>
          <dateStruct>
            <month>June</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01322207" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01322207</ref>
        </imprint>
        <meeting id="cid29437">
          <title>Annual Conference on Learning Theory</title>
          <num>29</num>
          <abbr type="sigle">COLT</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid37" type="inproceedings" rend="year" n="cite:barlier:hal-01406894">
      <identifiant type="hal" value="hal-01406894"/>
      <analytic>
        <title level="a">A Stochastic Model for Computer-Aided Human-Human Dialogue</title>
        <author>
          <persName key="sequel-2015-idp113016">
            <foreName>Merwan</foreName>
            <surname>Barlier</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>Romain</foreName>
            <surname>Laroche</surname>
            <initial>R.</initial>
          </persName>
          <persName key="sequel-2014-idp111080">
            <foreName>Olivier</foreName>
            <surname>Pietquin</surname>
            <initial>O.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">Interspeech 2016</title>
        <loc>San Francisco, United States</loc>
        <imprint>
          <biblScope type="volume">2016</biblScope>
          <dateStruct>
            <month>September</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">2051 - 2055</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01406894" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01406894</ref>
        </imprint>
        <meeting id="cid29182">
          <title>Annual Conference of the International Speech Communication Association</title>
          <num>17</num>
          <abbr type="sigle">INTERSPEECH</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid38" type="inproceedings" rend="year" n="cite:barlier:hal-01406904">
      <identifiant type="hal" value="hal-01406904"/>
      <analytic>
        <title level="a">Learning Dialogue Dynamics with the Method of Moments</title>
        <author>
          <persName key="sequel-2015-idp113016">
            <foreName>Merwan</foreName>
            <surname>Barlier</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>Romain</foreName>
            <surname>Laroche</surname>
            <initial>R.</initial>
          </persName>
          <persName key="sequel-2014-idp111080">
            <foreName>Olivier</foreName>
            <surname>Pietquin</surname>
            <initial>O.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">Workshop on Spoken Language Technologie (SLT 2016)</title>
        <loc>San Diego, United States</loc>
        <imprint>
          <dateStruct>
            <month>December</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01406904" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01406904</ref>
        </imprint>
        <meeting id="cid391795">
          <title>Spoken Language Technologies Workshop</title>
          <num>2016</num>
          <abbr type="sigle">SLT</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid48" type="inproceedings" rend="year" n="cite:berard:hal-01335930">
      <identifiant type="hal" value="hal-01335930"/>
      <analytic>
        <title level="a">MultiVec: a Multilingual and Multilevel Representation Learning Toolkit for NLP</title>
        <author>
          <persName key="sequel-2014-idp99936">
            <foreName>Alexandre</foreName>
            <surname>B√©rard</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Christophe</foreName>
            <surname>Servan</surname>
            <initial>C.</initial>
          </persName>
          <persName key="sequel-2014-idp111080">
            <foreName>Olivier</foreName>
            <surname>Pietquin</surname>
            <initial>O.</initial>
          </persName>
          <persName>
            <foreName>Laurent</foreName>
            <surname>Besacier</surname>
            <initial>L.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="no" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">The 10th edition of the Language Resources and Evaluation Conference (LREC)</title>
        <loc>Portoroz, Slovenia</loc>
        <imprint>
          <dateStruct>
            <month>May</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01335930" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01335930</ref>
        </imprint>
        <meeting id="cid289398">
          <title>International Conference on Language Resources and Evaluation</title>
          <num>10</num>
          <abbr type="sigle">LREC</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid32" type="inproceedings" rend="year" n="cite:calandriello:hal-01343674">
      <identifiant type="hal" value="hal-01343674"/>
      <analytic>
        <title level="a">Analysis of Nystr√∂m method with sequential ridge leverage score sampling</title>
        <author>
          <persName key="sequel-2014-idp101176">
            <foreName>Daniele</foreName>
            <surname>Calandriello</surname>
            <initial>D.</initial>
          </persName>
          <persName key="sequel-2014-idm26088">
            <foreName>Alessandro</foreName>
            <surname>Lazaric</surname>
            <initial>A.</initial>
          </persName>
          <persName key="sequel-2014-idp70232">
            <foreName>Michal</foreName>
            <surname>Valko</surname>
            <initial>M.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">Uncertainty in Artificial Intelligence</title>
        <loc>New York City, United States</loc>
        <imprint>
          <dateStruct>
            <month>June</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01343674" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01343674</ref>
        </imprint>
        <meeting id="cid49628">
          <title>Conference on Uncertainty in Artificial Intelligence</title>
          <num>2016</num>
          <abbr type="sigle">UAI</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid14" type="inproceedings" rend="year" n="cite:carpentier:hal-01304020">
      <identifiant type="hal" value="hal-01304020"/>
      <analytic>
        <title level="a">Revealing graph bandits for maximizing local influence</title>
        <author>
          <persName>
            <foreName>Alexandra</foreName>
            <surname>Carpentier</surname>
            <initial>A.</initial>
          </persName>
          <persName key="sequel-2014-idp70232">
            <foreName>Michal</foreName>
            <surname>Valko</surname>
            <initial>M.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">International Conference on Artificial Intelligence and Statistics</title>
        <loc>Seville, Spain</loc>
        <imprint>
          <dateStruct>
            <month>May</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01304020" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01304020</ref>
        </imprint>
        <meeting id="cid388734">
          <title>International Conference on Artificial Intelligence and Statistics</title>
          <num>14</num>
          <abbr type="sigle">AISTATS</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid36" type="inproceedings" rend="year" n="cite:elasri:hal-01406873">
      <identifiant type="hal" value="hal-01406873"/>
      <analytic>
        <title level="a">Compact and Interpretable Dialogue State Representation with Genetic Sparse Distributed Memory</title>
        <author>
          <persName key="sequel-2014-idp86016">
            <foreName>Layla</foreName>
            <surname>El Asri</surname>
            <initial>L.</initial>
          </persName>
          <persName>
            <foreName>Romain</foreName>
            <surname>Laroche</surname>
            <initial>R.</initial>
          </persName>
          <persName key="sequel-2014-idp111080">
            <foreName>Olivier</foreName>
            <surname>Pietquin</surname>
            <initial>O.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">7th International Workshop on Spoken Dialogue Systems (IWSDS 2016)</title>
        <loc>Saariselka, Finland</loc>
        <imprint>
          <dateStruct>
            <month>January</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01406873" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01406873</ref>
        </imprint>
        <meeting id="cid625330">
          <title>IWSDS International Workshop on Spoken Dialogue Systems</title>
          <num>7</num>
          <abbr type="sigle">IWSDS</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid26" type="inproceedings" rend="year" n="cite:elasri:hal-01406886">
      <identifiant type="hal" value="hal-01406886"/>
      <analytic>
        <title level="a">Score-based Inverse Reinforcement Learning</title>
        <author>
          <persName key="sequel-2014-idp86016">
            <foreName>Layla</foreName>
            <surname>El Asri</surname>
            <initial>L.</initial>
          </persName>
          <persName key="sequel-2014-idp79648">
            <foreName>Bilal</foreName>
            <surname>Piot</surname>
            <initial>B.</initial>
          </persName>
          <persName>
            <foreName>Matthieu</foreName>
            <surname>Geist</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>Romain</foreName>
            <surname>Laroche</surname>
            <initial>R.</initial>
          </persName>
          <persName key="sequel-2014-idp111080">
            <foreName>Olivier</foreName>
            <surname>Pietquin</surname>
            <initial>O.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2016)</title>
        <loc>Singapore, Singapore</loc>
        <imprint>
          <dateStruct>
            <month>May</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01406886" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01406886</ref>
        </imprint>
        <meeting id="cid112748">
          <title>International Conference on Autonomous Agents and Multiagent Systems</title>
          <num>15</num>
          <abbr type="sigle">AAMAS</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid33" type="inproceedings" rend="year" n="cite:erraqabi:hal-01322168">
      <identifiant type="hal" value="hal-01322168"/>
      <analytic>
        <title level="a">Pliable rejection sampling</title>
        <author>
          <persName>
            <foreName>Akram</foreName>
            <surname>Erraqabi</surname>
            <initial>A.</initial>
          </persName>
          <persName key="sequel-2014-idp70232">
            <foreName>Michal</foreName>
            <surname>Valko</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>Alexandra</foreName>
            <surname>Carpentier</surname>
            <initial>A.</initial>
          </persName>
          <persName key="tao-2015-idp83360">
            <foreName>Odalric-Ambrym</foreName>
            <surname>Maillard</surname>
            <initial>O.-A.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">International Conference on Machine Learning</title>
        <loc>New York City, United States</loc>
        <imprint>
          <dateStruct>
            <month>June</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01322168" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01322168</ref>
        </imprint>
        <meeting id="cid32516">
          <title>International Conference on Machine Learning</title>
          <num>27</num>
          <abbr type="sigle">ICML</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid53" type="inproceedings" rend="year" n="cite:felicio:hal-01390762">
      <identifiant type="hal" value="hal-01390762"/>
      <analytic>
        <title level="a">Preference-like Score to Cope with Cold-Start User in Recommender Systems</title>
        <author>
          <persName>
            <foreName>Cr√≠cia Z</foreName>
            <surname>Fel√≠cio</surname>
            <initial>C. Z.</initial>
          </persName>
          <persName>
            <foreName>Kl√©risson V R</foreName>
            <surname>Paix√£o</surname>
            <initial>K. V. R.</initial>
          </persName>
          <persName>
            <foreName>Celia A Z</foreName>
            <surname>Barcelos</surname>
            <initial>C. A. Z.</initial>
          </persName>
          <persName key="sequel-2014-idm27568">
            <foreName>Philippe</foreName>
            <surname>Preux</surname>
            <initial>P.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">28th International Conference on Tools with Artificial Intelligence (ICTAI)</title>
        <loc>San Jose, United States</loc>
        <title level="s">Proceedings of the IEEE 28th International Conference on Tools with Artificial Intelligence (ICTAI)</title>
        <imprint>
          <dateStruct>
            <month>November</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01390762" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01390762</ref>
        </imprint>
        <meeting id="cid86108">
          <title>IEEE International Conference on Tools with Artificial Intelligence</title>
          <num>28</num>
          <abbr type="sigle">ICTAI</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid11" type="inproceedings" rend="year" n="cite:gabillon:hal-01322198">
      <identifiant type="hal" value="hal-01322198"/>
      <analytic>
        <title level="a">Improved Learning Complexity in Combinatorial Pure Exploration Bandits</title>
        <author>
          <persName key="sequel-2014-idp116352">
            <foreName>Victor</foreName>
            <surname>Gabillon</surname>
            <initial>V.</initial>
          </persName>
          <persName key="sequel-2014-idm26088">
            <foreName>Alessandro</foreName>
            <surname>Lazaric</surname>
            <initial>A.</initial>
          </persName>
          <persName key="sequel-2014-idp65928">
            <foreName>Mohammad</foreName>
            <surname>Ghavamzadeh</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>Ronald</foreName>
            <surname>Ortner</surname>
            <initial>R.</initial>
          </persName>
          <persName>
            <foreName>Peter</foreName>
            <surname>Bartlett</surname>
            <initial>P.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">Proceedings of the 19th International Conference on Artificial Intelligence (AISTATS)</title>
        <loc>Cadiz, Spain</loc>
        <imprint>
          <dateStruct>
            <month>May</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01322198" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01322198</ref>
        </imprint>
        <meeting id="cid388734">
          <title>International Conference on Artificial Intelligence and Statistics</title>
          <num>19</num>
          <abbr type="sigle">AISTATS</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid17" type="inproceedings" rend="year" n="cite:garivier:hal-01273838">
      <identifiant type="hal" value="hal-01273838"/>
      <analytic>
        <title level="a">Optimal Best Arm Identification with Fixed Confidence</title>
        <author>
          <persName>
            <foreName>Aur√©lien</foreName>
            <surname>Garivier</surname>
            <initial>A.</initial>
          </persName>
          <persName key="dyogene-2014-idp72312">
            <foreName>Emilie</foreName>
            <surname>Kaufmann</surname>
            <initial>E.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">29th Annual Conference on Learning Theory (COLT)</title>
        <loc>New York, United States</loc>
        <title level="s">JMLR Workshop and Conference Proceedings</title>
        <imprint>
          <biblScope type="volume">49</biblScope>
          <dateStruct>
            <month>June</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01273838" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01273838</ref>
        </imprint>
        <meeting id="cid29437">
          <title>Annual Conference on Learning Theory</title>
          <num>29</num>
          <abbr type="sigle">COLT</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid28" type="inproceedings" rend="year" n="cite:garivier:hal-01273842">
      <identifiant type="hal" value="hal-01273842"/>
      <analytic>
        <title level="a">Maximin Action Identification: A New Bandit Framework for Games</title>
        <author>
          <persName>
            <foreName>Aur√©lien</foreName>
            <surname>Garivier</surname>
            <initial>A.</initial>
          </persName>
          <persName key="dyogene-2014-idp72312">
            <foreName>Emilie</foreName>
            <surname>Kaufmann</surname>
            <initial>E.</initial>
          </persName>
          <persName>
            <foreName>Wouter M.</foreName>
            <surname>Koolen</surname>
            <initial>W. M.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">29th Annual Conference on Learning Theory (COLT)</title>
        <loc>New-York, United States</loc>
        <title level="s">JMLR Workshop and Conference Proceedings</title>
        <imprint>
          <biblScope type="volume">49</biblScope>
          <dateStruct>
            <month>June</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01273842" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01273842</ref>
        </imprint>
        <meeting id="cid29437">
          <title>Annual Conference on Learning Theory</title>
          <num>29</num>
          <abbr type="sigle">COLT</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid18" type="inproceedings" rend="year" n="cite:garivier:hal-01322906">
      <identifiant type="hal" value="hal-01322906"/>
      <analytic>
        <title level="a">On Explore-Then-Commit Strategies</title>
        <author>
          <persName>
            <foreName>Aur√©lien</foreName>
            <surname>Garivier</surname>
            <initial>A.</initial>
          </persName>
          <persName key="dyogene-2014-idp72312">
            <foreName>Emilie</foreName>
            <surname>Kaufmann</surname>
            <initial>E.</initial>
          </persName>
          <persName>
            <foreName>Tor</foreName>
            <surname>Lattimore</surname>
            <initial>T.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">NIPS</title>
        <loc>Barcelona, Spain</loc>
        <title level="s">Advances in Neural Information Processing Systems (NIPS)</title>
        <imprint>
          <biblScope type="volume">29</biblScope>
          <dateStruct>
            <month>December</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01322906" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01322906</ref>
        </imprint>
        <meeting id="cid29560">
          <title>Annual Conference on Neural Information Processing Systems</title>
          <num>23</num>
          <abbr type="sigle">NIPS</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid31" type="inproceedings" rend="year" n="cite:glaude:hal-01406889">
      <identifiant type="hal" value="hal-01406889"/>
      <analytic>
        <title level="a">PAC learning of Probabilistic Automaton based on the Method of Moments</title>
        <author>
          <persName key="sequel-2014-idp103632">
            <foreName>Hadrien</foreName>
            <surname>Glaude</surname>
            <initial>H.</initial>
          </persName>
          <persName key="sequel-2014-idp111080">
            <foreName>Olivier</foreName>
            <surname>Pietquin</surname>
            <initial>O.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">International Conference on Machine Learning (ICML 2016)</title>
        <loc>New York, United States</loc>
        <imprint>
          <dateStruct>
            <month>June</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01406889" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01406889</ref>
        </imprint>
        <meeting id="cid32516">
          <title>International Conference on Machine Learning</title>
          <num>33</num>
          <abbr type="sigle">ICML</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid27" type="inproceedings" rend="year" n="cite:grill:hal-01389107">
      <identifiant type="hal" value="hal-01389107"/>
      <analytic>
        <title level="a">Blazing the trails before beating the path: Sample-efficient Monte-Carlo planning</title>
        <author>
          <persName key="sequel-2015-idp120424">
            <foreName>Jean-Bastien</foreName>
            <surname>Grill</surname>
            <initial>J.-B.</initial>
          </persName>
          <persName key="sequel-2014-idp70232">
            <foreName>Michal</foreName>
            <surname>Valko</surname>
            <initial>M.</initial>
          </persName>
          <persName key="sequel-2014-idp67360">
            <foreName>R√©mi</foreName>
            <surname>Munos</surname>
            <initial>R.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">NIPS 2016 - Thirtieth Annual Conference on Neural Information Processing Systems</title>
        <loc>Barcelona, Spain</loc>
        <imprint>
          <dateStruct>
            <month>December</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01389107" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01389107</ref>
        </imprint>
        <meeting id="cid29560">
          <title>Annual Conference on Neural Information Processing Systems</title>
          <num>30</num>
          <abbr type="sigle">NIPS</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid23" type="inproceedings" rend="year" n="cite:guillou:hal-01406439">
      <identifiant type="hal" value="hal-01406439"/>
      <analytic>
        <title level="a">Compromis exploration-exploitation pour syst√®me de recommandation √† grande √©chelle</title>
        <author>
          <persName key="sequel-2014-idp106104">
            <foreName>Fr√©d√©ric</foreName>
            <surname>Guillou</surname>
            <initial>F.</initial>
          </persName>
          <persName key="sequel-2014-idp75664">
            <foreName>Romaric</foreName>
            <surname>Gaudel</surname>
            <initial>R.</initial>
          </persName>
          <persName key="sequel-2014-idm27568">
            <foreName>Philippe</foreName>
            <surname>Preux</surname>
            <initial>P.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="no" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">Conf√©rence francophone sur l'Apprentissage Automatique (CAp'16)</title>
        <loc>Marseille, France</loc>
        <imprint>
          <dateStruct>
            <month>July</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01406439" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01406439</ref>
        </imprint>
        <meeting id="cid50509">
          <title>Conf√©rence Francophone sur l'Apprentissage Automatique</title>
          <num>2016</num>
          <abbr type="sigle">CAP</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid20" type="inproceedings" rend="year" n="cite:guillou:hal-01406389">
      <identifiant type="hal" value="hal-01406389"/>
      <analytic>
        <title level="a">Large-scale Bandit Recommender System</title>
        <author>
          <persName key="sequel-2014-idp106104">
            <foreName>Fr√©d√©ric</foreName>
            <surname>Guillou</surname>
            <initial>F.</initial>
          </persName>
          <persName key="sequel-2014-idp75664">
            <foreName>Romaric</foreName>
            <surname>Gaudel</surname>
            <initial>R.</initial>
          </persName>
          <persName key="sequel-2014-idm27568">
            <foreName>Philippe</foreName>
            <surname>Preux</surname>
            <initial>P.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">the 2nd International Workshop on Machine Learning, Optimization and Big Data (MOD'16)</title>
        <loc>Volterra, Italy</loc>
        <imprint>
          <dateStruct>
            <month>August</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01406389" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01406389</ref>
        </imprint>
        <meeting id="cid624970">
          <title>International Workshop on Machine Learning, Optimization, and Big Data</title>
          <num>2</num>
          <abbr type="sigle">MOD</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid19" type="inproceedings" rend="year" n="cite:guillou:hal-01406418">
      <identifiant type="hal" value="hal-01406418"/>
      <analytic>
        <title level="a">Scalable explore-exploit Collaborative Filtering</title>
        <author>
          <persName key="sequel-2014-idp106104">
            <foreName>Fr√©d√©ric</foreName>
            <surname>Guillou</surname>
            <initial>F.</initial>
          </persName>
          <persName key="sequel-2014-idp75664">
            <foreName>Romaric</foreName>
            <surname>Gaudel</surname>
            <initial>R.</initial>
          </persName>
          <persName key="sequel-2014-idm27568">
            <foreName>Philippe</foreName>
            <surname>Preux</surname>
            <initial>P.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">Pacific Asia Conference on Information Systems (PACIS'16)</title>
        <loc>Chiayi, Taiwan</loc>
        <imprint>
          <dateStruct>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01406418" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01406418</ref>
        </imprint>
        <meeting id="cid625331">
          <title>Pacific Asia Conference on Information Systems</title>
          <num>2016</num>
          <abbr type="sigle">PACIS</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid21" type="inproceedings" rend="year" n="cite:guillou:hal-01406338">
      <identifiant type="doi" value="10.1007/978-3-319-46672-9_33"/>
      <identifiant type="hal" value="hal-01406338"/>
      <analytic>
        <title level="a">Sequential Collaborative Ranking Using (No-)Click Implicit Feedback</title>
        <author>
          <persName key="sequel-2014-idp106104">
            <foreName>Fr√©d√©ric</foreName>
            <surname>Guillou</surname>
            <initial>F.</initial>
          </persName>
          <persName key="sequel-2014-idp75664">
            <foreName>Romaric</foreName>
            <surname>Gaudel</surname>
            <initial>R.</initial>
          </persName>
          <persName key="sequel-2014-idm27568">
            <foreName>Philippe</foreName>
            <surname>Preux</surname>
            <initial>P.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">The 23rd International Conference on Neural Information Processing (ICONIP'16)</title>
        <loc>Kyoto, Japan</loc>
        <title level="s">Lecture Notes in Computer Science</title>
        <imprint>
          <biblScope type="volume">9948</biblScope>
          <dateStruct>
            <month>October</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">288 - 296</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01406338" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01406338</ref>
        </imprint>
        <meeting id="cid293445">
          <title>International Conference on Neural Information Processing</title>
          <num>23</num>
          <abbr type="sigle">ICONIP</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid43" type="inproceedings" rend="year" n="cite:kaufmann:hal-01163147">
      <identifiant type="doi" value="10.1007/978-3-319-46379-7_24"/>
      <identifiant type="hal" value="hal-01163147"/>
      <analytic>
        <title level="a">A Spectral Algorithm with Additive Clustering for the Recovery of Overlapping Communities in Networks</title>
        <author>
          <persName key="dyogene-2014-idp72312">
            <foreName>Emilie</foreName>
            <surname>Kaufmann</surname>
            <initial>E.</initial>
          </persName>
          <persName>
            <foreName>Thomas</foreName>
            <surname>Bonald</surname>
            <initial>T.</initial>
          </persName>
          <persName key="dyogene-2014-idp61656">
            <foreName>Marc</foreName>
            <surname>Lelarge</surname>
            <initial>M.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <editor role="editor">
          <persName>
            <foreName>Ronald</foreName>
            <surname>Ortner</surname>
            <initial>R.</initial>
          </persName>
          <persName>
            <foreName>Hans Ulrich</foreName>
            <surname>Simon</surname>
            <initial>H. U.</initial>
          </persName>
          <persName>
            <foreName>Sandra</foreName>
            <surname>Zilles</surname>
            <initial>S.</initial>
          </persName>
        </editor>
        <title level="m">ALT 2016 - Algorithmic Learning Theory</title>
        <loc>Bari, Italy</loc>
        <title level="s">Lecture Notes in Computer Science</title>
        <imprint>
          <biblScope type="volume">9925</biblScope>
          <publisher>
            <orgName>Springer</orgName>
          </publisher>
          <dateStruct>
            <month>October</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">355-370</biblScope>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01163147" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01163147</ref>
        </imprint>
        <meeting id="cid110465">
          <title>International Conference on Algorithmic Learning Theory</title>
          <num>2016</num>
          <abbr type="sigle">ALT</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid13" type="inproceedings" rend="year" n="cite:kocak:hal-01320588">
      <identifiant type="hal" value="hal-01320588"/>
      <analytic>
        <title level="a">Online learning with Erd≈ës-R√©nyi side-observation graphs</title>
        <author>
          <persName key="sequel-2014-idp108624">
            <foreName>Tom√°≈°</foreName>
            <surname>Koc√°k</surname>
            <initial>T.</initial>
          </persName>
          <persName key="sequel-2014-idp82200">
            <foreName>Gergely</foreName>
            <surname>Neu</surname>
            <initial>G.</initial>
          </persName>
          <persName key="sequel-2014-idp70232">
            <foreName>Michal</foreName>
            <surname>Valko</surname>
            <initial>M.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">Uncertainty in Artificial Intelligence</title>
        <loc>New York City, United States</loc>
        <imprint>
          <dateStruct>
            <month>June</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01320588" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01320588</ref>
        </imprint>
        <meeting id="cid49628">
          <title>Conference on Uncertainty in Artificial Intelligence</title>
          <num>2016</num>
          <abbr type="sigle">UAI</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid12" type="inproceedings" rend="year" n="cite:kocak:hal-01303377">
      <identifiant type="hal" value="hal-01303377"/>
      <analytic>
        <title level="a">Online learning with noisy side observations</title>
        <author>
          <persName key="sequel-2014-idp108624">
            <foreName>Tom√°≈°</foreName>
            <surname>Koc√°k</surname>
            <initial>T.</initial>
          </persName>
          <persName key="sequel-2014-idp82200">
            <foreName>Gergely</foreName>
            <surname>Neu</surname>
            <initial>G.</initial>
          </persName>
          <persName key="sequel-2014-idp70232">
            <foreName>Michal</foreName>
            <surname>Valko</surname>
            <initial>M.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">International Conference on Artificial Intelligence and Statistics</title>
        <loc>Seville, Spain</loc>
        <imprint>
          <dateStruct>
            <month>May</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01303377" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01303377</ref>
        </imprint>
        <meeting id="cid388734">
          <title>International Conference on Artificial Intelligence and Statistics</title>
          <num>14</num>
          <abbr type="sigle">AISTATS</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid41" type="inproceedings" rend="year" n="cite:musco:hal-01279620">
      <identifiant type="hal" value="hal-01279620"/>
      <analytic>
        <title level="a">A Learning Algorithm for Change Impact Prediction</title>
        <author>
          <persName key="sequel-2014-idp117600">
            <foreName>Vincenzo</foreName>
            <surname>Musco</surname>
            <initial>V.</initial>
          </persName>
          <persName key="sequel-2015-idp136704">
            <foreName>Antonin</foreName>
            <surname>Carette</surname>
            <initial>A.</initial>
          </persName>
          <persName key="spirals-2014-idp82576">
            <foreName>Martin</foreName>
            <surname>Monperrus</surname>
            <initial>M.</initial>
          </persName>
          <persName key="sequel-2014-idm27568">
            <foreName>Philippe</foreName>
            <surname>Preux</surname>
            <initial>P.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">5th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering</title>
        <loc>Austin, United States</loc>
        <imprint>
          <dateStruct>
            <month>May</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01279620" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01279620</ref>
        </imprint>
        <meeting id="cid623554">
          <title>Workshop on Realizing Artificial Intelligence Synergies in Software Engineering</title>
          <num>5</num>
          <abbr type="sigle">RAISE</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid39" type="inproceedings" rend="year" n="cite:musco:hal-01350515">
      <identifiant type="hal" value="hal-01350515"/>
      <analytic>
        <title level="a">Mutation-Based Graph Inference for Fault Localization</title>
        <author>
          <persName key="sequel-2014-idp117600">
            <foreName>Vincenzo</foreName>
            <surname>Musco</surname>
            <initial>V.</initial>
          </persName>
          <persName key="spirals-2014-idp82576">
            <foreName>Martin</foreName>
            <surname>Monperrus</surname>
            <initial>M.</initial>
          </persName>
          <persName key="sequel-2014-idm27568">
            <foreName>Philippe</foreName>
            <surname>Preux</surname>
            <initial>P.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">International Working Conference on Source Code Analysis and Manipulation</title>
        <loc>Raleigh, United States</loc>
        <imprint>
          <dateStruct>
            <month>October</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01350515" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01350515</ref>
        </imprint>
        <meeting id="cid92590">
          <title>IEEE International Workshop on Source Code Analysis and Manipulation</title>
          <num>2016</num>
          <abbr type="sigle">SCAM</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid55" type="inproceedings" rend="year" n="cite:perolat:hal-01393328">
      <identifiant type="hal" value="hal-01393328"/>
      <analytic>
        <title level="a">Softened Approximate Policy Iteration for Markov Games</title>
        <author>
          <persName key="sequel-2014-idp109840">
            <foreName>Julien</foreName>
            <surname>P√©rolat</surname>
            <initial>J.</initial>
          </persName>
          <persName key="sequel-2014-idp79648">
            <foreName>Bilal</foreName>
            <surname>Piot</surname>
            <initial>B.</initial>
          </persName>
          <persName>
            <foreName>Matthieu</foreName>
            <surname>Geist</surname>
            <initial>M.</initial>
          </persName>
          <persName key="maia-2014-idp84944">
            <foreName>Bruno</foreName>
            <surname>Scherrer</surname>
            <initial>B.</initial>
          </persName>
          <persName key="sequel-2014-idp111080">
            <foreName>Olivier</foreName>
            <surname>Pietquin</surname>
            <initial>O.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">ICML 2016 - 33rd International Conference on Machine Learning</title>
        <loc>New York City, United States</loc>
        <imprint>
          <dateStruct>
            <month>June</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01393328" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01393328</ref>
        </imprint>
        <meeting id="cid32516">
          <title>International Conference on Machine Learning</title>
          <num>33</num>
          <abbr type="sigle">ICML</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid42" type="inproceedings" rend="year" n="cite:perolat:hal-01291495">
      <identifiant type="hal" value="hal-01291495"/>
      <analytic>
        <title level="a">On the Use of Non-Stationary Strategies for Solving Two-Player Zero-Sum Markov Games</title>
        <author>
          <persName key="sequel-2014-idp109840">
            <foreName>Julien</foreName>
            <surname>P√©rolat</surname>
            <initial>J.</initial>
          </persName>
          <persName key="sequel-2014-idp79648">
            <foreName>Bilal</foreName>
            <surname>Piot</surname>
            <initial>B.</initial>
          </persName>
          <persName key="maia-2014-idp84944">
            <foreName>Bruno</foreName>
            <surname>Scherrer</surname>
            <initial>B.</initial>
          </persName>
          <persName key="sequel-2014-idp111080">
            <foreName>Olivier</foreName>
            <surname>Pietquin</surname>
            <initial>O.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">19th International Conference on Artificial Intelligence and Statistics (AISTATS 2016)</title>
        <loc>Cadiz, Spain</loc>
        <title level="s">Proceedings of the International Conference on Artificial Intelligences and Statistics</title>
        <imprint>
          <dateStruct>
            <month>May</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01291495" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01291495</ref>
        </imprint>
        <meeting id="cid388734">
          <title>International Conference on Artificial Intelligence and Statistics</title>
          <num>19</num>
          <abbr type="sigle">AISTATS</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid25" type="inproceedings" rend="year" n="cite:ryabko:hal-01380063">
      <identifiant type="doi" value="10.1007/978-3-319-46379-7_17"/>
      <identifiant type="hal" value="hal-01380063"/>
      <analytic>
        <title level="a">Things Bayes can't do</title>
        <author>
          <persName key="sequel-2014-idp68800">
            <foreName>Daniil</foreName>
            <surname>Ryabko</surname>
            <initial>D.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">Proceedings of the 27th International Conference on Algorithmic Learning Theory (ALT'16)</title>
        <loc>Bari, Italy</loc>
        <imprint>
          <biblScope type="volume">LNCS</biblScope>
          <biblScope type="number">9925</biblScope>
          <dateStruct>
            <month>October</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">253-260</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01380063" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01380063</ref>
        </imprint>
        <meeting id="cid110465">
          <title>International Conference on Algorithmic Learning Theory</title>
          <num>2016</num>
          <abbr type="sigle">ALT</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid22" type="inproceedings" rend="year" n="cite:strub:hal-01336912">
      <identifiant type="doi" value="10.1145/2988450.2988456"/>
      <identifiant type="hal" value="hal-01336912"/>
      <analytic>
        <title level="a">Hybrid Recommender System based on Autoencoders</title>
        <author>
          <persName key="sequel-2015-idp110544">
            <foreName>Florian</foreName>
            <surname>Strub</surname>
            <initial>F.</initial>
          </persName>
          <persName key="sequel-2014-idp75664">
            <foreName>Romaric</foreName>
            <surname>Gaudel</surname>
            <initial>R.</initial>
          </persName>
          <persName key="sequel-2014-idp76928">
            <foreName>J√©r√©mie</foreName>
            <surname>Mary</surname>
            <initial>J.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">the 1st Workshop on Deep Learning for Recommender Systems</title>
        <loc>Boston, United States</loc>
        <imprint>
          <dateStruct>
            <month>September</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">11 - 16</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01336912" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01336912</ref>
        </imprint>
        <meeting id="cid625332">
          <title>Workshop on Deep Learning for Recommender Systems</title>
          <num>1</num>
          <abbr type="sigle"/>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid24" type="inproceedings" rend="year" n="cite:strub:hal-01406432">
      <identifiant type="hal" value="hal-01406432"/>
      <analytic>
        <title level="a">Filtrage Collaboratif Hybride avec des Auto-encodeurs</title>
        <author>
          <persName key="sequel-2015-idp110544">
            <foreName>Florian</foreName>
            <surname>Strub</surname>
            <initial>F.</initial>
          </persName>
          <persName key="sequel-2014-idp76928">
            <foreName>J√©r√©mie</foreName>
            <surname>Mary</surname>
            <initial>J.</initial>
          </persName>
          <persName key="sequel-2014-idp75664">
            <foreName>Romaric</foreName>
            <surname>Gaudel</surname>
            <initial>R.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="no" x-proceedings="no" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">Conf√©rence francophone sur l'Apprentissage Automatique (CAp'16)</title>
        <loc>Marseille, France</loc>
        <imprint>
          <dateStruct>
            <month>July</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01406432" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01406432</ref>
        </imprint>
        <meeting id="cid50509">
          <title>Conf√©rence Francophone sur l'Apprentissage Automatique</title>
          <num>2016</num>
          <abbr type="sigle">CAP</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid15" type="inproceedings" rend="year" n="cite:tossou:hal-01234427">
      <identifiant type="hal" value="hal-01234427"/>
      <analytic>
        <title level="a">Algorithms for Differentially Private Multi-Armed Bandits</title>
        <author>
          <persName key="sequel-2016-idp187168">
            <foreName>Aristide C. Y.</foreName>
            <surname>Tossou</surname>
            <initial>A. C. Y.</initial>
          </persName>
          <persName key="sequel-2015-idp103832">
            <foreName>Christos</foreName>
            <surname>Dimitrakakis</surname>
            <initial>C.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">AAAI 2016</title>
        <loc>Phoenix, Arizona, United States</loc>
        <imprint>
          <dateStruct>
            <month>February</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01234427" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01234427</ref>
        </imprint>
        <meeting id="cid355099">
          <title>National Conference on Artificial Intelligence</title>
          <num>30</num>
          <abbr type="sigle">AAAI</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid35" type="inproceedings" rend="year" n="cite:zhang:hal-01234215">
      <identifiant type="hal" value="hal-01234215"/>
      <analytic>
        <title level="a">On the Differential Privacy of Bayesian Inference</title>
        <author>
          <persName>
            <foreName>Zuhe</foreName>
            <surname>Zhang</surname>
            <initial>Z.</initial>
          </persName>
          <persName>
            <foreName>Benjamin</foreName>
            <surname>Rubinstein</surname>
            <initial>B.</initial>
          </persName>
          <persName key="sequel-2015-idp103832">
            <foreName>Christos</foreName>
            <surname>Dimitrakakis</surname>
            <initial>C.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">AAAI 2016</title>
        <loc>Phoenix, Arizona, United States</loc>
        <imprint>
          <dateStruct>
            <month>February</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01234215" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01234215</ref>
        </imprint>
        <meeting id="cid355099">
          <title>National Conference on Artificial Intelligence</title>
          <num>30</num>
          <abbr type="sigle">AAAI</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid47" type="techreport" rend="year" n="cite:danglot:hal-01378523">
      <identifiant type="hal" value="hal-01378523"/>
      <monogr>
        <title level="m">Correctness Attraction: A Study of Stability of Software Behavior Under Runtime Perturbation</title>
        <author>
          <persName key="dreampal-2014-idp76992">
            <foreName>Benjamin</foreName>
            <surname>Danglot</surname>
            <initial>B.</initial>
          </persName>
          <persName key="sequel-2014-idm27568">
            <foreName>Philippe</foreName>
            <surname>Preux</surname>
            <initial>P.</initial>
          </persName>
          <persName key="diverse-2014-idm8000">
            <foreName>Benoit</foreName>
            <surname>Baudry</surname>
            <initial>B.</initial>
          </persName>
          <persName key="spirals-2014-idp82576">
            <foreName>Martin</foreName>
            <surname>Monperrus</surname>
            <initial>M.</initial>
          </persName>
        </author>
        <imprint>
          <biblScope type="number">hal-01378523</biblScope>
          <publisher>
            <orgName type="institution">HAL</orgName>
          </publisher>
          <dateStruct>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01378523" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01378523</ref>
        </imprint>
      </monogr>
      <note type="typdoc">Research Report</note>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid56" type="unpublished" rend="year" n="cite:bubeck:hal-01428950">
      <identifiant type="hal" value="hal-01428950"/>
      <monogr>
        <title level="m">Sampling from a log-concave distribution with Projected Langevin Monte Carlo</title>
        <author>
          <persName>
            <foreName>S√©bastien</foreName>
            <surname>Bubeck</surname>
            <initial>S.</initial>
          </persName>
          <persName>
            <foreName>Ronen</foreName>
            <surname>Eldan</surname>
            <initial>R.</initial>
          </persName>
          <persName>
            <foreName>Joseph</foreName>
            <surname>Lehec</surname>
            <initial>J.</initial>
          </persName>
        </author>
        <imprint>
          <dateStruct>
            <month>January</month>
            <year>2017</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01428950" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01428950</ref>
        </imprint>
      </monogr>
      <note type="bnote">working paper or preprint</note>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid46" type="unpublished" rend="year" n="cite:dimitrakakis:hal-01408294">
      <identifiant type="hal" value="hal-01408294"/>
      <monogr>
        <title level="m">Multi-view Sequential Games: The Helper-Agent Problem</title>
        <author>
          <persName key="sequel-2015-idp103832">
            <foreName>Christos</foreName>
            <surname>Dimitrakakis</surname>
            <initial>C.</initial>
          </persName>
          <persName>
            <foreName>Firas</foreName>
            <surname>Jarboui</surname>
            <initial>F.</initial>
          </persName>
          <persName>
            <foreName>David</foreName>
            <surname>Parkes</surname>
            <initial>D.</initial>
          </persName>
          <persName>
            <foreName>Lior</foreName>
            <surname>Seeman</surname>
            <initial>L.</initial>
          </persName>
        </author>
        <imprint>
          <dateStruct>
            <month>December</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01408294" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01408294</ref>
        </imprint>
      </monogr>
      <note type="bnote">working paper or preprint</note>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid49" type="unpublished" rend="year" n="cite:kaufmann:hal-01251606">
      <identifiant type="hal" value="hal-01251606"/>
      <monogr>
        <title level="m">On Bayesian index policies for sequential resource allocation</title>
        <author>
          <persName key="dyogene-2014-idp72312">
            <foreName>Emilie</foreName>
            <surname>Kaufmann</surname>
            <initial>E.</initial>
          </persName>
        </author>
        <imprint>
          <dateStruct>
            <month>September</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01251606" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01251606</ref>
        </imprint>
      </monogr>
      <note type="bnote">working paper or preprint</note>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid45" type="unpublished" rend="year" n="cite:luedtke:hal-01338733">
      <identifiant type="hal" value="hal-01338733"/>
      <monogr>
        <title level="m">Asymptotically Optimal Algorithms for Multiple Play Bandits with Partial Feedback</title>
        <author>
          <persName>
            <foreName>Alexander R.</foreName>
            <surname>Luedtke</surname>
            <initial>A. R.</initial>
          </persName>
          <persName key="dyogene-2014-idp72312">
            <foreName>Emilie</foreName>
            <surname>Kaufmann</surname>
            <initial>E.</initial>
          </persName>
          <persName>
            <foreName>Antoine</foreName>
            <surname>Chambaz</surname>
            <initial>A.</initial>
          </persName>
        </author>
        <imprint>
          <dateStruct>
            <month>June</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01338733" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01338733</ref>
        </imprint>
      </monogr>
      <note type="bnote">working paper or preprint</note>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid57" type="unpublished" rend="year" n="cite:strub:hal-01281794">
      <identifiant type="hal" value="hal-01281794"/>
      <monogr>
        <title level="m">Hybrid Collaborative Filtering with Autoencoders</title>
        <author>
          <persName key="sequel-2015-idp110544">
            <foreName>Florian</foreName>
            <surname>Strub</surname>
            <initial>F.</initial>
          </persName>
          <persName key="sequel-2014-idp76928">
            <foreName>J√©r√©mie</foreName>
            <surname>Mary</surname>
            <initial>J.</initial>
          </persName>
          <persName key="sequel-2014-idp75664">
            <foreName>Romaric</foreName>
            <surname>Gaudel</surname>
            <initial>R.</initial>
          </persName>
        </author>
        <imprint>
          <dateStruct>
            <month>July</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01281794" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01281794</ref>
        </imprint>
      </monogr>
      <note type="bnote">working paper or preprint</note>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid7" type="article" rend="foot" n="footcite:Aueretal2002">
      <analytic>
        <title level="a">Finite-time analysis of the multi-armed bandit problem</title>
        <author>
          <persName>
            <foreName>Peter</foreName>
            <surname>Auer</surname>
            <initial>P.</initial>
          </persName>
          <persName>
            <foreName>N.</foreName>
            <surname>Cesa-Bianchi</surname>
            <initial>N.</initial>
          </persName>
          <persName>
            <foreName>P.</foreName>
            <surname>Fischer</surname>
            <initial>P.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">Machine Learning</title>
        <imprint>
          <biblScope type="volume">47</biblScope>
          <biblScope type="number">2/3</biblScope>
          <dateStruct>
            <year>2002</year>
          </dateStruct>
          <biblScope type="pages">235‚Äì256</biblScope>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid3" type="book" rend="foot" n="footcite:bellman">
      <monogr>
        <title level="m">Dynamic Programming</title>
        <author>
          <persName>
            <foreName>R.</foreName>
            <surname>Bellman</surname>
            <initial>R.</initial>
          </persName>
        </author>
        <imprint>
          <publisher>
            <orgName>Princeton University Press</orgName>
          </publisher>
          <dateStruct>
            <year>1957</year>
          </dateStruct>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid4" type="book" rend="foot" n="footcite:bertshreve78">
      <monogr>
        <title level="m">Stochastic Optimal Control (The Discrete Time Case)</title>
        <author>
          <persName>
            <foreName>D.P.</foreName>
            <surname>Bertsekas</surname>
            <initial>D.</initial>
          </persName>
          <persName>
            <foreName>S.E.</foreName>
            <surname>Shreve</surname>
            <initial>S.</initial>
          </persName>
        </author>
        <imprint>
          <publisher>
            <orgName>Academic Press, New York</orgName>
          </publisher>
          <dateStruct>
            <year>1978</year>
          </dateStruct>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid2" type="book" rend="foot" n="footcite:Bertsekas96">
      <monogr>
        <title level="m">Neuro-Dynamic Programming</title>
        <author>
          <persName>
            <foreName>D.P.</foreName>
            <surname>Bertsekas</surname>
            <initial>D.</initial>
          </persName>
          <persName>
            <foreName>J.</foreName>
            <surname>Tsitsiklis</surname>
            <initial>J.</initial>
          </persName>
        </author>
        <imprint>
          <publisher>
            <orgName>Athena Scientific</orgName>
          </publisher>
          <dateStruct>
            <year>1996</year>
          </dateStruct>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid1" type="book" rend="foot" n="footcite:puterman94">
      <monogr>
        <title level="m">Markov Decision Processes: Discrete Stochastic Dynamic Programming</title>
        <author>
          <persName>
            <foreName>M.L.</foreName>
            <surname>Puterman</surname>
            <initial>M.</initial>
          </persName>
        </author>
        <imprint>
          <publisher>
            <orgName>John Wiley and Sons</orgName>
          </publisher>
          <dateStruct>
            <year>1994</year>
          </dateStruct>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid6" type="article" rend="foot" n="footcite:Robbins1952">
      <analytic>
        <title level="a">Some aspects of the sequential design of experiments</title>
        <author>
          <persName>
            <foreName>H.</foreName>
            <surname>Robbins</surname>
            <initial>H.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">Bull. Amer. Math. Soc.</title>
        <imprint>
          <biblScope type="volume">55</biblScope>
          <dateStruct>
            <year>1952</year>
          </dateStruct>
          <biblScope type="pages">527‚Äì535</biblScope>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid0" type="book" rend="foot" n="footcite:sb">
      <monogr>
        <title level="m">Reinforcement learning: an introduction</title>
        <author>
          <persName>
            <foreName>R.S.</foreName>
            <surname>Sutton</surname>
            <initial>R.</initial>
          </persName>
          <persName>
            <foreName>A.G.</foreName>
            <surname>Barto</surname>
            <initial>A.</initial>
          </persName>
        </author>
        <imprint>
          <publisher>
            <orgName>MIT Press</orgName>
          </publisher>
          <dateStruct>
            <year>1998</year>
          </dateStruct>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sequel-2016-bid5" type="inbook" rend="foot" n="footcite:werbosHandbookADP">
      <analytic>
        <author>
          <persName>
            <foreName>P.</foreName>
            <surname>Werbos</surname>
            <initial>P.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="m">ADP: Goals, Opportunities and Principles</title>
        <imprint>
          <publisher>
            <orgName>IEEE Press</orgName>
          </publisher>
          <dateStruct>
            <year>2004</year>
          </dateStruct>
          <biblScope type="pages">3‚Äì44</biblScope>
        </imprint>
      </monogr>
      <note type="bnote">Handbook of learning and approximate dynamic programming</note>
    </biblStruct>
  </biblio>
</raweb>
