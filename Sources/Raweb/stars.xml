<?xml version="1.0" encoding="utf-8"?>
<raweb xmlns:xlink="http://www.w3.org/1999/xlink" xml:lang="en" year="2016">
  <identification id="stars" isproject="true">
    <shortname>STARS</shortname>
    <projectName>Spatio-Temporal Activity Recognition Systems</projectName>
    <theme-de-recherche>Vision, perception and multimedia interpretation</theme-de-recherche>
    <domaine-de-recherche>Perception, Cognition and Interaction</domaine-de-recherche>
    <urlTeam>http://team.inria.fr/stars</urlTeam>
    <header_dates_team>Creation of the Team: 2012 January 01, updated into Project-Team: 2013 January 01</header_dates_team>
    <LeTypeProjet>Project-Team</LeTypeProjet>
    <keywordsSdN>
      <term>2.1.8. - Synchronous languages</term>
      <term>2.1.11. - Proof languages</term>
      <term>2.3.3. - Real-time systems</term>
      <term>2.4.2. - Model-checking</term>
      <term>2.4.3. - Proofs</term>
      <term>2.5. - Software engineering</term>
      <term>3.2.1. - Knowledge bases</term>
      <term>3.3.2. - Data mining</term>
      <term>3.4.1. - Supervised learning</term>
      <term>3.4.2. - Unsupervised learning</term>
      <term>3.4.5. - Bayesian methods</term>
      <term>3.4.6. - Neural networks</term>
      <term>4.7. - Access control</term>
      <term>5.1. - Human-Computer Interaction</term>
      <term>5.3.2. - Sparse modeling and image representation</term>
      <term>5.3.3. - Pattern recognition</term>
      <term>5.4.1. - Object recognition</term>
      <term>5.4.2. - Activity recognition</term>
      <term>5.4.3. - Content retrieval</term>
      <term>5.4.5. - Object tracking and motion analysis</term>
      <term>8.1. - Knowledge</term>
      <term>8.2. - Machine learning</term>
      <term>8.3. - Signal analysis</term>
    </keywordsSdN>
    <keywordsSecteurs>
      <term>1.3.2. - Cognitive science</term>
      <term>2.1. - Well being</term>
      <term>7.1.1. - Pedestrian traffic and crowds</term>
      <term>8.1. - Smart building/home</term>
      <term>8.4. - Security and personal assistance</term>
    </keywordsSecteurs>
    <UR name="Sophia"/>
  </identification>
  <team id="uid1">
    <person key="stars-2014-idp60040">
      <firstname>Francois</firstname>
      <lastname>Bremond</lastname>
      <categoryPro>Chercheur</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Team leader, Inria, Research Scientist</moreinfo>
      <hdr>oui</hdr>
    </person>
    <person key="stars-2014-idp64040">
      <firstname>Sabine</firstname>
      <lastname>Moisan</lastname>
      <categoryPro>Chercheur</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria, Research Scientist</moreinfo>
      <hdr>oui</hdr>
    </person>
    <person key="stars-2014-idp65472">
      <firstname>Annie</firstname>
      <lastname>Ressouche</lastname>
      <categoryPro>Chercheur</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria, Research Scientist</moreinfo>
    </person>
    <person key="stars-2015-idp7880">
      <firstname>Jean Paul</firstname>
      <lastname>Rigault</lastname>
      <categoryPro>Enseignant</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Univ. Nice, Professor</moreinfo>
    </person>
    <person key="stars-2014-idp97320">
      <firstname>Jane</firstname>
      <lastname>Desplanques</lastname>
      <categoryPro>Assistant</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="stars-2014-idp112576">
      <firstname>Julien</firstname>
      <lastname>Badie</lastname>
      <categoryPro>PostDoc</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria, granted by IRCA project</moreinfo>
    </person>
    <person key="lagadic-2014-idp89016">
      <firstname>Manikandan</firstname>
      <lastname>Bakthavatchalam</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="stars-2014-idp69392">
      <firstname>Vasanth</firstname>
      <lastname>Bathrinarayanan</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="stars-2015-idp105640">
      <firstname>Carlos Fernando</firstname>
      <lastname>Crispim Junior</lastname>
      <categoryPro>PostDoc</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria, granted by BPIFRANCE FINANCEMENT SA</moreinfo>
    </person>
    <person key="galaad2-2014-idp67104">
      <firstname>Anais</firstname>
      <lastname>Ducoffe</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="stars-2014-idp78352">
      <firstname>Rachid</firstname>
      <lastname>Guerchouche</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="stars-2015-idp112000">
      <firstname>Furqan Muhammad</firstname>
      <lastname>Khan</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="stars-2015-idp113256">
      <firstname>Matias</firstname>
      <lastname>Marin</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="stars-2015-idp114504">
      <firstname>Thanh Hung</firstname>
      <lastname>Nguyen</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="stars-2014-idp85976">
      <firstname>Javier</firstname>
      <lastname>Ortiz</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="stars-2015-idp118264">
      <firstname>Remi</firstname>
      <lastname>Trichet</lastname>
      <categoryPro>PostDoc</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria, granted by MOVEMENT project</moreinfo>
    </person>
    <person key="stars-2015-idp142000">
      <firstname>Kanishka Nithin</firstname>
      <lastname>Dhandapani</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria, granted by Toyota project, until Oct 2016</moreinfo>
    </person>
    <person key="stars-2014-idp121288">
      <firstname>Auriane</firstname>
      <lastname>Gros</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>CHU Nice</moreinfo>
    </person>
    <person key="stars-2014-idp123800">
      <firstname>Michal</firstname>
      <lastname>Koperski</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Toyota, granted by CIFRE</moreinfo>
    </person>
    <person key="stars-2014-idp125016">
      <firstname>Farhood</firstname>
      <lastname>Negin</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="stars-2014-idp126296">
      <firstname>Thi Lan Anh</firstname>
      <lastname>Nguyen</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="stars-2014-idp130088">
      <firstname>Minh Khue</firstname>
      <lastname>Phan Tran</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Genious, granted by CIFRE</moreinfo>
    </person>
    <person key="stars-2014-idp94784">
      <firstname>Ines</firstname>
      <lastname>Sarray</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="stars-2015-idp148376">
      <firstname>Ujjwal</firstname>
      <lastname>Ujjwal</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>VEDCOM, granted by CIFRE</moreinfo>
    </person>
    <person key="stars-2016-idp166720">
      <firstname>Seongro</firstname>
      <lastname>Yoon</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria, from Apr 2016</moreinfo>
    </person>
    <person key="stars-2016-idp169184">
      <firstname>Karel</firstname>
      <lastname>Krehnac</lastname>
      <categoryPro>Visiteur</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Centaur Project, from May 2016 to Jun 2016</moreinfo>
    </person>
    <person key="stars-2016-idp171696">
      <firstname>Jana</firstname>
      <lastname>Trojanova</lastname>
      <categoryPro>Visiteur</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Centaur project, from Mar 2016 to Sep 2016</moreinfo>
    </person>
    <person key="stars-2015-idp130656">
      <firstname>Salwa</firstname>
      <lastname>Baabou</lastname>
      <categoryPro>Visiteur</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Guest PhD, to Nov 2016</moreinfo>
    </person>
    <person key="stars-2015-idp131904">
      <firstname>Siyuan</firstname>
      <lastname>Chen</lastname>
      <categoryPro>Visiteur</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Guest PhD, to Feb 2016</moreinfo>
    </person>
    <person key="stars-2014-idp93496">
      <firstname>Adlen</firstname>
      <lastname>Kerboua</lastname>
      <categoryPro>Visiteur</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>PhD</moreinfo>
    </person>
    <person key="stars-2016-idp181664">
      <firstname>Aimen</firstname>
      <lastname>Neffati</lastname>
      <categoryPro>Visiteur</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria, from Jul 2016 to Aug 2016</moreinfo>
    </person>
    <person key="stars-2016-idp184160">
      <firstname>Luis Emiliano</firstname>
      <lastname>Sanchez</lastname>
      <categoryPro>Visiteur</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>University of Rosario (Argentina), from Sep 2016 to Dec 2016</moreinfo>
    </person>
    <person key="stars-2016-idp186688">
      <firstname>Yashas</firstname>
      <lastname>Annadani</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria, from May 2016 to august 2016</moreinfo>
    </person>
    <person key="stars-2015-idp138160">
      <firstname>Ghada</firstname>
      <lastname>Bahloul</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria, Engineers, granted by EIT project</moreinfo>
    </person>
    <person key="stars-2016-idp191664">
      <firstname>Robin</firstname>
      <lastname>Bermont</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria, research support, from Oct 2016</moreinfo>
    </person>
    <person key="stars-2014-idp113792">
      <firstname>Piotr Tadeusz</firstname>
      <lastname>Bilinski</lastname>
      <categoryPro>PostDoc</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria, granted by Toyota project, until Sep 2016</moreinfo>
    </person>
    <person key="stars-2016-idp196640">
      <firstname>Ulysse</firstname>
      <lastname>Castet</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria, from Apr 2016 to Aug 2016</moreinfo>
    </person>
    <person key="stars-2014-idp116256">
      <firstname>Etienne</firstname>
      <lastname>Corvee</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Associate partner</moreinfo>
    </person>
    <person key="stars-2014-idp88472">
      <firstname>Antitza</firstname>
      <lastname>Dantcheva</lastname>
      <categoryPro>PostDoc</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria, granted by Labex</moreinfo>
    </person>
    <person key="stars-2016-idp204048">
      <firstname>Chandraja</firstname>
      <lastname>Dharmana</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria, from May 2016</moreinfo>
    </person>
    <person key="stars-2016-idp206512">
      <firstname>Margaux</firstname>
      <lastname>Failla</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria, research support, from Oct 2016</moreinfo>
    </person>
    <person key="stars-2016-idp208992">
      <firstname>Loic</firstname>
      <lastname>Franchi</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria, from Jun 2016 until Jul 2016</moreinfo>
    </person>
    <person key="stars-2014-idp62776">
      <firstname>Daniel</firstname>
      <lastname>Gaffe</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>External Collaborator, Univ. Nice, Associate Professor</moreinfo>
    </person>
    <person key="stars-2016-idp213968">
      <firstname>Renaud</firstname>
      <lastname>Heyrendt</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria, from Apr 2016 to Jun 2016</moreinfo>
    </person>
    <person key="stars-2016-idp216448">
      <firstname>Guillaume</firstname>
      <lastname>Lombard</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria, from Apr 2016 to Jul 2016</moreinfo>
    </person>
    <person key="stars-2016-idp218928">
      <firstname>Robinson</firstname>
      <lastname>Menetrey</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria, from Apr 2016 to Jun 2016</moreinfo>
    </person>
    <person key="stars-2016-idp221408">
      <firstname>Nairouz</firstname>
      <lastname>Mrabah</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria, from Apr 2016 to Sep 2016</moreinfo>
    </person>
    <person key="stars-2016-idp223888">
      <firstname>Isabel</firstname>
      <lastname>Rayas</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria, from Jun 2016</moreinfo>
    </person>
    <person key="rap-2014-idm40528">
      <firstname>Philippe</firstname>
      <lastname>Robert</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>External Collaborator, CHU Nice and COBTECK</moreinfo>
    </person>
    <person key="stars-2016-idp228848">
      <firstname>Hugues</firstname>
      <lastname>Thomas</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria,from Apr 2016 to Sep 2016</moreinfo>
    </person>
    <person key="stars-2015-idp172352">
      <firstname>Jean Yves</firstname>
      <lastname>Tigli</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>External Collaborator, Univ. Nice, Associate Professor</moreinfo>
    </person>
    <person key="stars-2016-idp233824">
      <firstname>Shanu</firstname>
      <lastname>Vashishtha</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Sophia</research-centre>
      <moreinfo>Inria, from May 2016 to Jul 2016</moreinfo>
    </person>
  </team>
  <presentation id="uid2">
    <bodyTitle>Overall Objectives</bodyTitle>
    <subsection id="uid3" level="1">
      <bodyTitle>Presentation</bodyTitle>
      <subsection id="uid4" level="2">
        <bodyTitle>Research Themes</bodyTitle>
        <p><b>STARS (Spatio-Temporal Activity Recognition Systems)</b> is focused on the design of cognitive
systems for Activity Recognition.
We aim at endowing cognitive systems with perceptual capabilities
to reason about an observed environment, to provide a variety of services
to people living in this environment while preserving their privacy.
In today world, a huge amount of new sensors
and new hardware devices are currently available, addressing potentially new needs of the modern
society. However the lack of automated processes (with no human interaction) able to extract a meaningful and
accurate information (i.e. a correct understanding of the situation)
has often generated frustrations among the society and especially among older people.
Therefore, Stars objective is to propose novel autonomous systems for the <b>real-time semantic interpretation of dynamic scenes</b> observed by sensors. We
study long-term spatio-temporal activities performed by several
interacting agents such as human beings, animals and vehicles in the physical
world. Such systems also raise fundamental software engineering problems to specify them as well as to adapt them at run time.</p>
        <p>We propose new techniques at the frontier between computer vision, knowledge engineering, machine learning and software engineering.
The major challenge in semantic interpretation of dynamic scenes is to bridge the gap between
the task dependent interpretation of data and the flood of measures provided by sensors.
The problems we address range from physical object detection, activity understanding,
activity learning to vision system design and evaluation.
The two principal classes of human activities we focus on, are assistance to older adults and video analytic.</p>
        <p>A typical example of a complex activity is shown in
Figure <ref xlink:href="#uid5" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> and Figure <ref xlink:href="#uid6" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> for a homecare application. In this example, the
duration of the monitoring of an older person apartment could last several months.
The activities involve interactions between the observed person and several
pieces of equipment. The application goal is to recognize the everyday activities at home
through formal activity models (as shown in Figure <ref xlink:href="#uid7" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>) and data
captured by a network of sensors embedded in the
apartment. Here typical services include an objective assessment of the frailty level of the observed person
to be able to provide a more personalized care and to monitor the effectiveness of a prescribed therapy.
The assessment of the frailty level is performed by an Activity Recognition System
which transmits a textual report (containing only meta-data) to the general practitioner who follows the older person.
Thanks to the recognized activities, the quality of life of the observed people can thus be improved and
their personal information can be preserved.</p>
        <object id="uid5">
          <table>
            <tr>
              <td>
                <ressource xlink:href="IMG/gerhome_plan.jpg" type="float" width="298.8987pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
          </table>
          <caption>Homecare monitoring: the set of sensors embedded in an apartment</caption>
        </object>
        <object id="uid6">
          <table>
            <tr>
              <td>
                <ressource xlink:href="IMG/gerhome_kitchen.jpg" type="inline" width="106.75pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
              <td>
                <ressource xlink:href="IMG/gerhome_living1.jpg" type="inline" width="106.75pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
              <td>
                <ressource xlink:href="IMG/gerhome_living2.jpg" type="inline" width="106.75pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
              <td>
                <ressource xlink:href="IMG/gerhome_bedroom.jpg" type="inline" width="106.75pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
          </table>
          <caption>Homecare monitoring: the different views of the apartment captured by 4 video cameras</caption>
        </object>
        <object id="uid7">
          <table rend="inline">
            <tr style="border-top-style:solid;border-top-width:1px;" top-border="true">
              <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" cols="2" right-border="true" left-border="true" halign="left"><b>Activity</b> (<i>PrepareMeal</i>,</td>
            </tr>
            <tr style="">
              <td style="text-align:left;border-left-style:solid;border-left-width:1px;" left-border="true" halign="left"> <b>PhysicalObjects</b>(</td>
              <td style="text-align:left;border-right-style:solid;border-right-width:1px;" right-border="true" halign="left">(p : Person), (z : Zone), (eq : Equipment))</td>
            </tr>
            <tr style="">
              <td style="text-align:left;border-left-style:solid;border-left-width:1px;" left-border="true" halign="left"> <b>Components</b>(</td>
              <td style="text-align:left;border-right-style:solid;border-right-width:1px;" right-border="true" halign="left">(s_inside : InsideKitchen(p, z))</td>
            </tr>
            <tr style="">
              <td style="text-align:left;border-left-style:solid;border-left-width:1px;" left-border="true" halign="left"/>
              <td style="text-align:left;border-right-style:solid;border-right-width:1px;" right-border="true" halign="left">(s_close : CloseToCountertop(p, eq))</td>
            </tr>
            <tr style="">
              <td style="text-align:left;border-left-style:solid;border-left-width:1px;" left-border="true" halign="left"/>
              <td style="text-align:left;border-right-style:solid;border-right-width:1px;" right-border="true" halign="left">(s_stand : PersonStandingInKitchen(p, z)))</td>
            </tr>
            <tr style="">
              <td style="text-align:left;border-left-style:solid;border-left-width:1px;" left-border="true" halign="left"> <b>Constraints</b>(</td>
              <td style="text-align:left;border-right-style:solid;border-right-width:1px;" right-border="true" halign="left">(z-&gt;Name = Kitchen)</td>
            </tr>
            <tr style="">
              <td style="text-align:left;border-left-style:solid;border-left-width:1px;" left-border="true" halign="left"/>
              <td style="text-align:left;border-right-style:solid;border-right-width:1px;" right-border="true" halign="left">(eq-&gt;Name = Countertop)</td>
            </tr>
            <tr style="">
              <td style="text-align:left;border-left-style:solid;border-left-width:1px;" left-border="true" halign="left"/>
              <td style="text-align:left;border-right-style:solid;border-right-width:1px;" right-border="true" halign="left">(s_close-&gt;Duration &gt;= 100)</td>
            </tr>
            <tr style="">
              <td style="text-align:left;border-left-style:solid;border-left-width:1px;" left-border="true" halign="left"/>
              <td style="text-align:left;border-right-style:solid;border-right-width:1px;" right-border="true" halign="left">(s_stand-&gt;Duration &gt;= 100))</td>
            </tr>
            <tr style="">
              <td style="text-align:left;border-left-style:solid;border-left-width:1px;" left-border="true" halign="left"> <b>Annotation</b>(</td>
              <td style="text-align:left;border-right-style:solid;border-right-width:1px;" right-border="true" halign="left">AText("prepare meal")</td>
            </tr>
            <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
              <td style="text-align:left;border-left-style:solid;border-left-width:1px;" left-border="true" halign="left"/>
              <td style="text-align:left;border-right-style:solid;border-right-width:1px;" right-border="true" halign="left">AType("not urgent")))</td>
            </tr>
            <caption/>
          </table>
          <caption>Homecare monitoring: example of an activity model
describing a scenario related to the preparation of a meal with a high-level language</caption>
        </object>
        <p>The ultimate goal is for cognitive systems to perceive and understand their environment to be able
to provide appropriate services to a potential user.
An important step is to propose
a computational representation of people activities to adapt these services to them.
Up to now, the most effective sensors have been video cameras due to the rich information they can provide on the observed environment.
These sensors are currently perceived as intrusive ones.
A key issue is to capture the pertinent raw data for adapting the services to the people
while preserving their privacy. We plan to study different solutions including
of course the local processing of the data without transmission of images and
the utilization of new compact sensors developed for interaction
(also called RGB-Depth sensors, an example being the Kinect)
or networks of small non visual sensors.</p>
      </subsection>
      <subsection id="uid8" level="2">
        <bodyTitle>International and Industrial Cooperation</bodyTitle>
        <p>Our work has been applied in the context of more than 10 European projects such as
COFRIEND, ADVISOR, SERKET, CARETAKER, VANAHEIM, SUPPORT, DEM@CARE, VICOMO. We had or have industrial collaborations in
several domains: <i>transportation</i> (CCI Airport Toulouse Blagnac,
SNCF, Inrets, Alstom, Ratp, GTT (Italy),
Turin GTT (Italy)), <i>banking</i>
(Crédit Agricole Bank Corporation, Eurotelis and Ciel), <i>security</i>
(Thales R&amp;T FR, Thales Security Syst, EADS, Sagem, Bertin,
Alcatel, Keeneo),
<i>multimedia</i> (Multitel (Belgium), Thales Communications, Idiap
(Switzerland)), <i>civil engineering</i> (Centre Scientifique et Technique
du Bâtiment (CSTB)), <i>computer industry</i> (BULL), <i>software industry</i> (AKKA),
<i>hardware industry</i> (ST-Microelectronics) and <i>health industry</i> (Philips, Link Care Services, Vistek).</p>
        <p>We have international
cooperations with research centers such as Reading University (UK), ENSI Tunis (Tunisia),
National Cheng Kung University, National Taiwan University (Taiwan),
MICA (Vietnam), IPAL, I2R (Singapore),
University of Southern California, University of South Florida,
University of Maryland (USA).</p>
      </subsection>
    </subsection>
  </presentation>
  <fondements id="uid9">
    <bodyTitle>Research Program</bodyTitle>
    <subsection id="uid10" level="1">
      <bodyTitle>Introduction</bodyTitle>
      <p>Stars follows three main research directions: perception for
activity recognition, semantic activity recognition, and software
engineering for activity recognition.
<b>These three research directions are
interleaved</b>: <i>the software engineering</i> research direction provides new
methodologies for building safe activity recognition systems and <i>the perception</i>
and <i>the semantic activity recognition</i> directions provide new activity
recognition techniques which are designed and validated for concrete video analytic and healthcare applications.
Conversely, these concrete systems raise new software issues that
enrich the software engineering research direction.</p>
      <p>Transversely, we consider a <i>new research axis in machine learning</i>,
combining a priori knowledge and learning techniques, to set up the various
models of an activity recognition system. A major objective is to automate model
building or model enrichment at the perception level and at the understanding
level.</p>
    </subsection>
    <subsection id="uid11" level="1">
      <bodyTitle>Perception for Activity Recognition</bodyTitle>
      <participants>
        <person key="PASUSERID">
          <firstname>François</firstname>
          <lastname>Brémond</lastname>
        </person>
        <person key="stars-2014-idp64040">
          <firstname>Sabine</firstname>
          <lastname>Moisan</lastname>
        </person>
        <person key="PASUSERID">
          <firstname>Monique</firstname>
          <lastname>Thonnat</lastname>
        </person>
      </participants>
      <p>Computer Vision; Cognitive Systems; Learning; Activity Recognition.
</p>
      <subsection id="uid12" level="2">
        <bodyTitle>Introduction</bodyTitle>
        <p>Our main goal in perception is to develop vision algorithms able to
address the large variety of conditions characterizing real world
scenes in terms of sensor conditions, hardware requirements, lighting
conditions, physical objects, and application objectives.
We have also several issues related to perception
which combine machine learning and perception techniques:
learning people appearance, parameters for system control and shape statistics.</p>
      </subsection>
      <subsection id="uid13" level="2">
        <bodyTitle>Appearance Models and People Tracking</bodyTitle>
        <p>An important issue is to detect in real-time physical objects from
perceptual features and predefined 3D models. It requires finding a
good balance between efficient methods and precise spatio-temporal
models.
Many improvements and analysis need to be performed in
order to tackle the large range of people detection scenarios.</p>
        <p><b>Appearance models. </b>
In particular, we study the temporal variation of the features characterizing the
appearance of a human. This task could be achieved by
clustering potential candidates depending on their position and their
reliability. This task can provide any people tracking
algorithms with reliable features allowing for instance to (1) better
track people or their body parts during occlusion, or to (2) model
people appearance for re-identification purposes in mono and
multi-camera networks, which is still an open issue.
The underlying challenge of the person re-identification problem
arises from significant differences in illumination, pose and camera
parameters. The re-identification approaches have two aspects: (1)
establishing correspondences between body parts and (2) generating
signatures that are invariant to different color responses. As we
have already several descriptors which are color invariant,
we now focus more on aligning two people detection and on finding their corresponding body parts.
Having detected body parts, the approach can handle pose variations.
Further, different body parts might have different influence on
finding the correct match among a whole gallery dataset. Thus, the
re-identification approaches have to search for matching strategies.
As the results of the re-identification are always
given as the ranking list, re-identification focuses on learning
to rank. "Learning to rank" is a type of machine learning problem, in
which the goal is to automatically construct a ranking model from a
training data.</p>
        <p>Therefore, we work on information fusion to handle perceptual features
coming from various sensors (several cameras covering a large scale
area or heterogeneous sensors capturing more or less precise and rich
information).
New 3D
RGB-D sensors are also investigated, to help in getting an accurate segmentation for specific scene conditions.</p>
        <p><b>Long term tracking. </b> For activity recognition we need robust and coherent object tracking
over long periods of time (often several hours in videosurveillance and several
days in healthcare).
To guarantee the long term coherence of tracked objects,
spatio-temporal reasoning is required. Modeling and managing the uncertainty of these processes
is also an open issue.
In Stars we propose to add a reasoning layer to a classical Bayesian framework
modeling the uncertainty of the tracked objects.
This reasoning layer can take into account the a priori knowledge of the scene
for outlier elimination and long-term coherency checking.</p>
        <p><b>Controlling system parameters. </b>
Another research direction is to manage a library of video processing programs.
We are building a perception library by selecting robust algorithms for feature
extraction, by insuring they work efficiently with real time constraints and by
formalizing their conditions of use within a program supervision model. In
the case of video cameras, at least two problems are still open: robust image
segmentation and meaningful feature extraction. For these issues, we are
developing new learning techniques.</p>
      </subsection>
    </subsection>
    <subsection id="uid14" level="1">
      <bodyTitle>Semantic Activity Recognition</bodyTitle>
      <participants>
        <person key="PASUSERID">
          <firstname>François</firstname>
          <lastname>Brémond</lastname>
        </person>
        <person key="stars-2014-idp64040">
          <firstname>Sabine</firstname>
          <lastname>Moisan</lastname>
        </person>
        <person key="PASUSERID">
          <firstname>Monique</firstname>
          <lastname>Thonnat</lastname>
        </person>
      </participants>
      <p>Activity Recognition, Scene Understanding, Computer Vision
</p>
      <subsection id="uid15" level="2">
        <bodyTitle>Introduction</bodyTitle>
        <p>Semantic activity recognition is a complex process where information is abstracted
through four levels: signal (e.g. pixel, sound), perceptual features, physical
objects and activities. The signal and the feature levels are characterized by strong noise,
ambiguous, corrupted and missing data. The whole process of scene understanding
consists in analyzing this information to bring forth pertinent insight of the
scene and its dynamics while handling the low level noise.
Moreover, to obtain a semantic abstraction, building activity models
is a crucial point. A still open issue consists in
determining whether these models should be given a priori or
learned. Another challenge consists in organizing this knowledge in order
to capitalize experience, share it with others and update it along with
experimentation. To face this challenge, tools in knowledge engineering such as
machine learning or ontology are needed.</p>
        <p>Thus we work along the following research axes:
high level understanding (to recognize the activities of physical objects based on high level
activity models), learning (how to learn the models needed for
activity recognition) and activity recognition and discrete event systems.</p>
      </subsection>
      <subsection id="uid16" level="2">
        <bodyTitle>High Level Understanding</bodyTitle>
        <p>A challenging research axis is to recognize subjective activities of
physical objects (i.e. human beings, animals, vehicles)
based on a priori models and objective perceptual measures (e.g.
robust and coherent object tracks).</p>
        <p>To reach this goal, we have defined original activity recognition algorithms
and activity models. Activity recognition algorithms include the computation
of spatio-temporal relationships between physical objects. All the possible
relationships may correspond to activities of interest and all have to be
explored in an efficient way. The variety of these activities, generally called
video events, is huge and depends on their spatial and temporal granularity, on the
number of physical objects involved in the events, and on the event complexity
(number of components constituting the event).</p>
        <p>Concerning the modeling of activities, we are working towards two directions:
the uncertainty management for representing probability distributions and knowledge
acquisition facilities based on ontological engineering techniques. For the first
direction, we are investigating classical statistical techniques and logical
approaches. For the second direction, we built a language for video event modeling and
a visual concept ontology (including color, texture and spatial concepts) to
be extended with temporal concepts (motion, trajectories, events ...) and other
perceptual concepts (physiological sensor concepts ...).</p>
      </subsection>
      <subsection id="uid17" level="2">
        <bodyTitle>Learning for Activity Recognition</bodyTitle>
        <p>Given the difficulty of building an activity recognition system with a priori
knowledge for a new
application, we study how machine learning techniques
can automate building or completing models at the perception level and
at the understanding level.</p>
        <p>At the understanding level, we are learning primitive event detectors.
This can be done for example by learning visual concept detectors using SVMs
(Support Vector Machines) with perceptual feature samples. An open question is
how far can we go in weakly supervised learning for each type of perceptual concept
(i.e. leveraging the human annotation task).
A second direction is to learn typical composite event models
for frequent activities using trajectory clustering or data mining techniques.
We name composite event a particular combination of
several primitive events.</p>
      </subsection>
      <subsection id="uid18" level="2">
        <bodyTitle>Activity Recognition and Discrete Event Systems</bodyTitle>
        <p>The previous research axes are unavoidable to cope with the semantic
interpretations. However they tend to let aside the pure event driven aspects
of scenario recognition. These aspects have been studied for a long time at a
theoretical level and led to methods and tools that may bring extra value to
activity recognition, the most important being the possibility of formal
analysis, verification and validation.</p>
        <p>We have thus started to specify a formal model to define, analyze, simulate,
and prove scenarios. This model deals with both absolute time (to be realistic
and efficient in the analysis phase) and logical time (to benefit from
well-known mathematical models providing re-usability, easy extension, and
verification). Our purpose is to offer a generic tool to express and recognize
activities associated with a concrete language to specify activities in the
form of a set of scenarios with temporal constraints. The theoretical
foundations and the tools being shared with Software Engineering aspects, they
will be detailed in section <ref xlink:href="#uid19" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
        <p>The results of the research performed in perception and semantic
activity recognition (first and second research directions) produce new
techniques for scene understanding and contribute to specify the needs for
new software architectures (third research direction).
</p>
      </subsection>
    </subsection>
    <subsection id="uid19" level="1">
      <bodyTitle>Software Engineering for Activity Recognition</bodyTitle>
      <participants>
        <person key="stars-2014-idp64040">
          <firstname>Sabine</firstname>
          <lastname>Moisan</lastname>
        </person>
        <person key="stars-2014-idp65472">
          <firstname>Annie</firstname>
          <lastname>Ressouche</lastname>
        </person>
        <person key="stars-2015-idp7880">
          <firstname>Jean-Paul</firstname>
          <lastname>Rigault</lastname>
        </person>
        <person key="PASUSERID">
          <firstname>François</firstname>
          <lastname>Brémond</lastname>
        </person>
      </participants>
      <p>Software Engineering, Generic Components, Knowledge-based Systems, Software
Component Platform, Object-oriented Frameworks, Software Reuse, Model-driven
Engineering
</p>
      <p>The aim of this research axis is to build general solutions and tools to
develop systems dedicated to activity recognition. For this, we rely on
state-of-the art Software Engineering practices to ensure both sound design and easy
use, providing genericity, modularity, adaptability, reusability, extensibility,
dependability, and maintainability.</p>
      <p>This research requires theoretical studies combined with validation based on
concrete experiments conducted in Stars.
We work on the following three research axes: <i>models</i> (adapted to the
activity recognition domain), <i>platform architecture</i> (to cope with deployment
constraints and run time adaptation), and <i>system verification</i> (to
generate dependable systems). For all these tasks we follow state of the
art Software Engineering practices and, if needed, we attempt to set up
new ones.</p>
      <subsection id="uid20" level="2">
        <bodyTitle>Platform Architecture for Activity Recognition</bodyTitle>
        <object id="uid21">
          <table>
            <tr>
              <td>
                <ressource xlink:href="IMG/ar-platform.jpg" type="float" width="298.8987pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
          </table>
          <caption>Global Architecture of an Activity Recognition The gray areas contain software engineering support modules
whereas the other modules correspond to software components (at Task and Component levels) or to generated systems (at Application level).</caption>
        </object>
        <p>In the former project teams Orion and Pulsar, we have developed two platforms, one (VSIP), a library of
real-time video understanding modules and another one, <span class="smallcap" align="left">Lama</span> <ref xlink:href="#stars-2016-bid0" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, a
software platform enabling to design not only knowledge bases, but also
inference engines, and additional tools. <span class="smallcap" align="left">lama</span> offers toolkits to build and to
adapt all the software elements that compose a knowledge-based system.</p>
        <p>Figure <ref xlink:href="#uid21" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> presents our conceptual vision for the architecture
of an activity recognition platform. It consists of three levels:</p>
        <simplelist>
          <li id="uid22">
            <p noindent="true">The <b>Component Level</b>, the lowest one, offers software components
providing elementary operations and data for perception, understanding, and
learning.</p>
            <simplelist>
              <li id="uid23">
                <p noindent="true"><i>Perception components</i> contain algorithms for sensor management, image
and signal analysis, image and video processing (segmentation,
tracking...), etc.</p>
              </li>
              <li id="uid24">
                <p noindent="true"><i>Understanding components</i> provide the building blocks for
Knowledge-based Systems: knowledge representation and management, elements
for controlling inference engine strategies, etc.</p>
              </li>
              <li id="uid25">
                <p noindent="true"><i>Learning components</i> implement different learning strategies, such as
Support Vector Machines (SVM), Case-based Learning (CBL), clustering, etc.</p>
              </li>
            </simplelist>
            <p>An Activity Recognition system is likely to pick components from these three
packages. Hence, tools must be provided to configure (select, assemble),
simulate, verify the resulting component combination. Other support tools may
help to generate task or application dedicated languages or graphic
interfaces.</p>
          </li>
          <li id="uid26">
            <p noindent="true">The <b>Task Level</b>, the middle one, contains executable realizations
of individual tasks that will collaborate in a particular final application.
Of course, the code of these tasks is built on top of the components from the
previous level. We have already identified several of these important tasks:
Object Recognition, Tracking, Scenario Recognition... In the future, other
tasks will probably enrich this level.</p>
            <p>For these tasks to nicely collaborate, communication and interaction
facilities are needed. We shall also add MDE-enhanced tools for configuration
and run-time adaptation.</p>
          </li>
          <li id="uid27">
            <p noindent="true">The <b>Application Level</b> integrates several of these tasks to build
a system for a particular type of application, e.g., vandalism detection,
patient monitoring, aircraft loading/unloading surveillance, etc.. Each
system is parameterized to adapt to its local environment (number, type,
location of sensors, scene geometry, visual parameters, number of objects of
interest...). Thus configuration and deployment facilities are required.</p>
          </li>
        </simplelist>
        <p>The philosophy of this architecture is to offer at each level a balance
between the widest possible genericity and the maximum effective reusability,
in particular at the code level.</p>
        <p>To cope with real application requirements, we shall also investigate
distributed architecture, real time implementation, and user interfaces.</p>
        <p>Concerning implementation issues, we shall use when possible existing open
standard tools such as NuSMV for model-checking, Eclipse for graphic interfaces
or model engineering support, Alloy for constraint representation and SAT
solving for verification, etc. Note that, in Figure <ref xlink:href="#uid21" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, some of the boxes can
be naturally adapted from SUP existing elements (many
perception and understanding components, program supervision, scenario
recognition...) whereas others are to be developed, completely or partially
(learning components, most support and configuration tools).</p>
      </subsection>
      <subsection id="uid28" level="2">
        <bodyTitle>Discrete Event Models of Activities</bodyTitle>
        <p>As mentioned in the previous section (<ref xlink:href="#uid14" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>)
we have started to specify a formal model of scenario dealing with both absolute time and logical time.
Our scenario and time models as well as the platform verification tools rely on
a formal basis, namely the synchronous paradigm.
To recognize scenarios, we consider
activity descriptions as synchronous reactive systems
and we apply general modeling methods to express scenario
behavior.</p>
        <p>Activity recognition systems usually exhibit many safeness issues. From the
software engineering point of view we only consider software security. Our
previous work on verification and validation has to be pursued; in particular,
we need to test its scalability and to develop associated tools.
Model-checking is an appealing technique since it can be
automatized and helps to produce a code
that has been formally proved. Our verification method follows a compositional
approach, a well-known way to cope with scalability problems in
model-checking.</p>
        <p>Moreover, recognizing real scenarios is not a purely deterministic process.
Sensor performance, precision of image analysis, scenario descriptions may
induce various kinds of uncertainty. While taking into account this
uncertainty, we should still keep our model of time deterministic, modular, and
formally verifiable. To formally describe probabilistic timed systems, the
most popular approach involves probabilistic extension of timed
automata. New model checking techniques can be used as
verification means, but relying on model checking
techniques is not sufficient. Model checking is a powerful tool to prove
decidable properties but introducing uncertainty may lead to infinite state or
even undecidable properties. Thus model checking validation has to be completed
with non exhaustive methods such as abstract interpretation.</p>
      </subsection>
      <subsection id="uid29" level="2">
        <bodyTitle>Model-Driven Engineering for Configuration and Control and Control of Video Surveillance systems</bodyTitle>
        <p>Model-driven engineering techniques can support the
configuration and dynamic adaptation of video surveillance systems designed
with our SUP activity recognition platform. The challenge is to cope with the many—functional as
well as nonfunctional—causes of variability both in the video application
specification and in the concrete SUP implementation. We have used <i>feature models</i> to define two models: a generic model of video surveillance
applications and a model of configuration for SUP components and chains. Both
of them express variability factors. Ultimately, we wish to automatically
generate a SUP component assembly from an application specification, using
models to represent transformations <ref xlink:href="#stars-2016-bid1" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. Our models are enriched
with intra- and inter-models constraints. Inter-models constraints specify
models to represent transformations. Feature models are appropriate to describe
variants; they are simple enough for video surveillance experts to express
their requirements. Yet, they are powerful enough to be liable to static
analysis  <ref xlink:href="#stars-2016-bid2" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. In particular, the constraints can be
analyzed as a SAT problem.</p>
        <p>An additional challenge is to manage the possible run-time changes of
implementation due to context variations (e.g., lighting conditions, changes in
the reference scene, etc.). Video surveillance systems have to dynamically
adapt to a changing environment. The use of models at run-time is a
solution. We are defining adaptation rules corresponding to the
dependency constraints between specification elements in one model and software
variants in the other  <ref xlink:href="#stars-2016-bid3" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, <ref xlink:href="#stars-2016-bid4" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, <ref xlink:href="#stars-2016-bid5" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.
</p>
      </subsection>
    </subsection>
  </fondements>
  <domaine id="uid30">
    <bodyTitle>Application Domains</bodyTitle>
    <subsection id="uid31" level="1">
      <bodyTitle>Introduction</bodyTitle>
      <p>While in our research the focus is to develop techniques, models and
platforms that are generic and reusable, we also make effort in the
development of real applications. The motivation is twofold. The
first is to validate the new ideas and approaches we introduce. The
second is to demonstrate how to build working systems for real
applications of various domains based on the techniques and tools
developed. Indeed, Stars focuses on two main domains: <b>video analytic</b>
and <b>healthcare monitoring</b>.
</p>
    </subsection>
    <subsection id="uid32" level="1">
      <bodyTitle>Video Analytics</bodyTitle>
      <p>Our experience in video analytic <ref xlink:href="#stars-2016-bid6" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, <ref xlink:href="#stars-2016-bid7" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, <ref xlink:href="#stars-2016-bid8" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> (also referred to as visual surveillance) is a strong basis which
ensures both a precise view of the research topics to develop and a network of
industrial partners ranging from end-users, integrators and software editors to
provide data, objectives, evaluation and funding.</p>
      <p>For instance, the Keeneo start-up was created in July 2005 for the
industrialization and exploitation of Orion and Pulsar results in
video analytic (VSIP library, which was a previous version of SUP).
Keeneo has been bought by Digital Barriers in August 2011 and is now independent from Inria.
However, Stars continues to maintain a close
cooperation with Keeneo for impact analysis of SUP and for exploitation of new
results.</p>
      <p>Moreover new challenges are arising from the visual surveillance
community. For instance, people detection and tracking in a crowded
environment are still open issues despite the high competition on
these topics. Also detecting abnormal activities may require to discover rare events
from very large video data bases often characterized by noise or incomplete data.</p>
    </subsection>
    <subsection id="uid33" level="1">
      <bodyTitle>Healthcare Monitoring</bodyTitle>
      <p>Since 2011, we have initiated a
strategic partnership (called CobTek) with Nice hospital  <ref xlink:href="#stars-2016-bid9" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, <ref xlink:href="#stars-2016-bid10" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>
(CHU Nice, Prof P. Robert) to start ambitious research activities dedicated to healthcare
monitoring and to assistive technologies.
These new studies address the analysis of more complex spatio-temporal activities (e.g. complex interactions, long term activities).</p>
      <subsection id="uid34" level="2">
        <bodyTitle>Research</bodyTitle>
        <p>To achieve this objective, several topics need to be tackled.
These topics can be summarized within two points: finer activity description and longitudinal experimentation.
Finer activity description is needed for instance, to discriminate the activities
(e.g. sitting, walking, eating) of Alzheimer patients from the ones of healthy older people.
It is essential to be able to pre-diagnose dementia and to provide a better and more specialized care.
Longer analysis is required when people monitoring aims at measuring the evolution of patient behavioral disorders.
Setting up such long experimentation with dementia people has never been tried before but is necessary to have real-world validation.
This is one of the challenge of the European FP7 project Dem@Care where several patient homes should be monitored over several months.</p>
        <p>For this domain, a goal for Stars is to allow people with
dementia to continue living in a self-sufficient manner in their own
homes or residential centers, away from a hospital, as well as to
allow clinicians and caregivers remotely provide effective care and
management.
For all this to become possible, comprehensive
monitoring of the daily life of the person with dementia is deemed
necessary, since caregivers and clinicians will need a comprehensive
view of the person's daily activities, behavioral patterns,
lifestyle, as well as changes in them, indicating the progression of
their condition.</p>
      </subsection>
      <subsection id="uid35" level="2">
        <bodyTitle>Ethical and Acceptability Issues</bodyTitle>
        <p>The development and ultimate use of novel assistive technologies by a
vulnerable user group such as individuals with dementia, and the
assessment methodologies planned by Stars are not free of ethical, or
even legal concerns, even if many studies have shown how these Information and Communication Technologies (ICT)
can be useful and well accepted by older people with or
without impairments. Thus one goal of Stars team is to design the right technologies that can
provide the appropriate information to the medical carers
while preserving people privacy. Moreover, Stars will pay particular attention to
ethical, acceptability, legal and privacy concerns that may arise, addressing them in
a professional way following the corresponding established EU and
national laws and regulations, especially when outside France.
Now, Stars can benefit from the support of the COERLE (Comité Opérationnel d'Evaluation des Risques Légaux et Ethiques)
to help it to respect ethical policies in its applications.</p>
        <p>As presented in <ref xlink:href="#uid10" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, Stars aims at designing cognitive vision systems with
perceptual capabilities to monitor efficiently people activities.
As a matter of fact, vision sensors can be seen as intrusive ones, even if no images are
acquired or transmitted (only meta-data describing activities need to be collected).
Therefore new communication paradigms and other sensors (e.g. accelerometers, RFID, and new sensors to come in the future)
are also envisaged to provide the most appropriate services
to the observed people, while preserving their privacy.
To better understand ethical issues, Stars members are already involved in several ethical organizations.
For instance, F. Brémond has been a member of the ODEGAM - “Commission Ethique et Droit” (a local association in Nice area for ethical issues related to older people)
from 2010 to 2011 and a member of the
French scientific council for the national seminar on “La maladie d'Alzheimer et les nouvelles technologies - Enjeux éthiques et questions de société” in 2011.
This council has in particular proposed a chart and guidelines for conducting researches with dementia patients.</p>
        <p>For addressing the acceptability issues, focus groups and HMI (Human Machine Interaction) experts, will be
consulted on the most adequate range of mechanisms to interact and display information to older people.</p>
      </subsection>
    </subsection>
  </domaine>
  <logiciels id="uid36">
    <bodyTitle>New Software and Platforms</bodyTitle>
    <subsection id="uid37" level="1">
      <bodyTitle>CLEM</bodyTitle>
      <p>
        <span class="smallcap" align="left">Functional Description</span>
      </p>
      <p>The Clem Toolkit is a set of tools devoted to design, simulate, verify and generate code for LE programs. LE is a synchronous language supporting a modular compilation. It also supports automata possibly designed with a dedicated graphical editor and implicit Mealy machine definition.</p>
      <simplelist>
        <li id="uid38">
          <p noindent="true">Participants: Daniel Gaffe and Annie Ressouche</p>
        </li>
        <li id="uid39">
          <p noindent="true">Contact: Annie Ressouche</p>
        </li>
        <li id="uid40">
          <p noindent="true">URL: <ref xlink:href="http://www-sop.inria.fr/teams/pulsar/projects/Clem/" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>www-sop.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>teams/<allowbreak/>pulsar/<allowbreak/>projects/<allowbreak/>Clem/</ref></p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid41" level="1">
      <bodyTitle>EGMM-BGS</bodyTitle>
      <p>
        <span class="smallcap" align="left">Functional Description</span>
      </p>
      <p>This software implements a generic background subtraction algorithm for video and RGB-D cameras, which can take feedback from people detection and tracking processes.
Embedded in a people detection framework, it does not classify foreground / background at pixel level but provides useful information for the framework to remove noise.
Noise is only removed when the framework has all the information from background subtraction, classification and object tracking.
In our experiment, our background subtraction algorithm outperforms GMM, a popular background subtraction algorithm, in detecting people and removing noise.</p>
      <simplelist>
        <li id="uid42">
          <p noindent="true">Participants: Anh Tuan Nghiem, Francois Bremond and Vasanth Bathrinarayanan</p>
        </li>
        <li id="uid43">
          <p noindent="true">Contact: Francois Bremond</p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid44" level="1">
      <bodyTitle>MTS</bodyTitle>
      <p>
        <span class="smallcap" align="left">Functional Description</span>
      </p>
      <p>This software consists of a retrieval tool for a human operator to select a person of interest in a network of cameras.
The multi-camera system can re-identify the person of interest, wherever and whenever (s)he has been observed in the camera network.
This task is particularly hard due to camera variations, different lighting conditions, different color responses and different camera viewpoints.
Moreover, we focus on non-rigid objects (i.e. humans) that change their pose and orientation contributing to the complexity of the problem.
In this work we design two methods for appearance matching across non-overlapping cameras.
One particular aspect is the choice of the image descriptor. A good descriptor should capture the most distinguishing characteristics of an appearance,
while being invariant to camera changes. We chose to describe the object appearance by using the covariance descriptor as its performance is found to be
superior to other methods. By averaging descriptors on a Riemannian manifold, we incorporate information from multiple images. This produces
mean Riemannian covariance that yields a compact and robust representation.
This new software has made digital video surveillance systems a product highly asked by security operators, especially the ones monitoring large critical infrastructures, such as public transportation
(subways, airports, and harbours), industrials (gas plants), and supermarkets.</p>
      <simplelist>
        <li id="uid45">
          <p noindent="true">Participants: Slawomir Bak and Francois Bremond</p>
        </li>
        <li id="uid46">
          <p noindent="true">Contact: Francois Bremond</p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid47" level="1">
      <bodyTitle>Person Manual Tracking in a Static Camera Network (PMT-SCN)</bodyTitle>
      <p>
        <span class="smallcap" align="left">Functional Description</span>
      </p>
      <p>This software allows tracking a person in a heterogeneous camera network. The tracking is done manually. The advantage of this software is to give the opportunity to operators in video-surveillance to focus on tracking the activity
of a person without knowing the positions of the cameras in a considered area. When the tracked person leaves the field-of–view (FOV) of a first camera, and enters the FOV of a second one, the second camera is automatically
showed to the operator.
This software was developed conjointly by Inria and Neosensys.</p>
      <simplelist>
        <li id="uid48">
          <p noindent="true">Participants: Bernard Boulay, Anais Ducoffe, Sofia Zaidenberg, Annunziato Polimeni and Julien Gueytat</p>
        </li>
        <li id="uid49">
          <p noindent="true">Partner: Neosensys</p>
        </li>
        <li id="uid50">
          <p noindent="true">Contact: Anais Ducoffe</p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid51" level="1">
      <bodyTitle>PrintFoot Tracker</bodyTitle>
      <p>
        <span class="smallcap" align="left">Functional Description</span>
      </p>
      <p>This software implements a new algorithm for tracking multiple persons in a single camera. This algorithm computes many different appearance-based
descriptors to characterize the visual appearance of an object and to track it over time. Object tracking quality usually depends on video scene conditions (e.g. illumination, density of
objects, object occlusion level). In order to overcome this limitation, this algorithm presents a new control approach to adapt the object tracking process to the
scene condition variations. More precisely, this approach learns how to tune the tracker parameters to cope with the tracking context variations.
The tracking context, or video context, of a video sequence is defined as a set of six features: density of mobile objects, their occlusion level, their contrast with regard to the
surrounding background, their contrast variance, their 2D area and their 2D area variance. The software has been experimented with three different tracking algorithms and on
long, complex video datasets.</p>
      <simplelist>
        <li id="uid52">
          <p noindent="true">Participants: Duc Phu Chau and Francois Bremond</p>
        </li>
        <li id="uid53">
          <p noindent="true">Contact: Francois Bremond</p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid54" level="1">
      <bodyTitle>Proof Of Concept Néosensys (Poc-NS)</bodyTitle>
      <p>
        <span class="smallcap" align="left">Functional Description</span>
      </p>
      <p>This is a demonstration software which gathers different technologies from Inria and Neosensys: PMT-SCN, re-identification and auto-side switch. This software is used to approach potential clients of Neosensys.</p>
      <simplelist>
        <li id="uid55">
          <p noindent="true">Participants: Bernard Boulay, Sofia Zaidenberg, Julien Gueytat, Slawomir Bak, Francois Bremond, Annunziato Polimeni and Yves Pichon</p>
        </li>
        <li id="uid56">
          <p noindent="true">Partner: Neosensys</p>
        </li>
        <li id="uid57">
          <p noindent="true">Contact: Francois Bremond</p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid58" level="1">
      <bodyTitle>SUP</bodyTitle>
      <p>Scene Understanding Platform</p>
      <p noindent="true"><span class="smallcap" align="left">Keywords:</span> Activity recognition - 3D - Dynamic scene</p>
      <p noindent="true">
        <span class="smallcap" align="left">Functional Description</span>
      </p>
      <p>SUP is a software platform for perceiving, analyzing and interpreting a 3D dynamic scene observed through a network of sensors. It encompasses algorithms allowing for the modeling of interesting activities for users to enable their recognition in real-world applications requiring high-throughput.</p>
      <simplelist>
        <li id="uid59">
          <p noindent="true">Participants: François Brémond, Carlos Fernando Crispim Junior and Etienne Corvée</p>
        </li>
        <li id="uid60">
          <p noindent="true">Partners: CEA - CHU Nice - I2R - Université de Hamburg - USC Californie</p>
        </li>
        <li id="uid61">
          <p noindent="true">Contact: Francois Bremond</p>
        </li>
        <li id="uid62">
          <p noindent="true">URL: <ref xlink:href="https://team.inria.fr/stars/software" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>team.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>stars/<allowbreak/>software</ref></p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid63" level="1">
      <bodyTitle>VISEVAL</bodyTitle>
      <p>
        <span class="smallcap" align="left">Functional Description</span>
      </p>
      <p>ViSEval is a software dedicated to the evaluation and visualization of video processing algorithm outputs. The evaluation of video processing algorithm results is an important step in video analysis research. In video processing, we identify 4 different tasks to evaluate: detection, classification and tracking of physical objects of interest and event recognition.</p>
      <simplelist>
        <li id="uid64">
          <p noindent="true">Participants: Bernard Boulay and Francois Bremond</p>
        </li>
        <li id="uid65">
          <p noindent="true">Contact: Francois Bremond</p>
        </li>
        <li id="uid66">
          <p noindent="true">URL: <ref xlink:href="http://www-sop.inria.fr/teams/pulsar/EvaluationTool/ViSEvAl_Description.html" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>www-sop.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>teams/<allowbreak/>pulsar/<allowbreak/>EvaluationTool/<allowbreak/>ViSEvAl_Description.<allowbreak/>html</ref></p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid67" level="1">
      <bodyTitle>py_ad</bodyTitle>
      <p>py action detection</p>
      <p noindent="true">
        <span class="smallcap" align="left">Functional Description</span>
      </p>
      <p>Action Detection framework
Allows user to detect action in video stream. It uses model trained in py_ar.</p>
      <simplelist>
        <li id="uid68">
          <p noindent="true">Participants: Michal Koperski and Francois Bremond</p>
        </li>
        <li id="uid69">
          <p noindent="true">Contact: Michal Koperski</p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid70" level="1">
      <bodyTitle>py_ar</bodyTitle>
      <p>py action recognition</p>
      <p noindent="true">
        <span class="smallcap" align="left">Functional Description</span>
      </p>
      <p>Action Recognition training/evaluation framework. It allows user do define action recognition experiment (on clipped videos). Train, test model, save the results and print the statistics.</p>
      <simplelist>
        <li id="uid71">
          <p noindent="true">Participants: Michal Koperski and Francois Bremond</p>
        </li>
        <li id="uid72">
          <p noindent="true">Contact: Michal Koperski</p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid73" level="1">
      <bodyTitle>py_sup_reader</bodyTitle>
      <p>
        <span class="smallcap" align="left">Functional Description</span>
      </p>
      <p>This is a library which allows to read video saved in SUP format in Python.</p>
      <simplelist>
        <li id="uid74">
          <p noindent="true">Participant: Michal Koperski</p>
        </li>
        <li id="uid75">
          <p noindent="true">Contact: Michal Koperski</p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid76" level="1">
      <bodyTitle>py_tra3d</bodyTitle>
      <p>py trajectories 3d</p>
      <p noindent="true">
        <span class="smallcap" align="left">Scientific Description</span>
      </p>
      <p>New video descriptor which fuse trajectory information with 3D information from depth sensor.</p>
      <p noindent="true">
        <span class="smallcap" align="left">Functional Description</span>
      </p>
      <p>3D Trajectories descriptor
Compute 3D trajectories descriptor proposed in (http://hal.inria.fr/docs/01/05/49/49/PDF/koperski-icip.pdf)</p>
      <simplelist>
        <li id="uid77">
          <p noindent="true">Participants: Michal Koperski and Francois Bremond</p>
        </li>
        <li id="uid78">
          <p noindent="true">Contact: Michal Koperski</p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid79" level="1">
      <bodyTitle>sup_ad</bodyTitle>
      <p>sup action detection</p>
      <p noindent="true">
        <span class="smallcap" align="left">Scientific Description</span>
      </p>
      <p>This software introduces the framework for online/real-time action recognition using state-of-the-art features and sliding window technique.</p>
      <p noindent="true">
        <span class="smallcap" align="left">Functional Description</span>
      </p>
      <p>SUP Action Detection Plugin
Plugin for SUP platform which performs action detection using sliding window and Bag of Words. It uses an input data model trained in py_ar project.</p>
      <simplelist>
        <li id="uid80">
          <p noindent="true">Participants: Michal Koperski and Francois Bremond</p>
        </li>
        <li id="uid81">
          <p noindent="true">Contact: Michal Koperski</p>
        </li>
      </simplelist>
    </subsection>
  </logiciels>
  <resultats id="uid82">
    <bodyTitle>New Results</bodyTitle>
    <subsection id="uid83" level="1">
      <bodyTitle>Introduction</bodyTitle>
      <p>This year Stars has proposed new results related to its three main research axes :
perception for activity recognition, semantic activity recognition and software engineering for activity recognition.</p>
      <subsection id="uid84" level="2">
        <bodyTitle>Perception for Activity Recognition</bodyTitle>
        <participants>
          <person key="stars-2014-idp113792">
            <firstname>Piotr</firstname>
            <lastname>Bilinski</lastname>
          </person>
          <person key="PASUSERID">
            <firstname>François</firstname>
            <lastname>Brémond</lastname>
          </person>
          <person key="PASUSERID">
            <firstname>Etienne</firstname>
            <lastname>Corvée</lastname>
          </person>
          <person key="PASUSERID">
            <firstname>Antitza</firstname>
            <lastname>Dancheva</lastname>
          </person>
          <person key="stars-2015-idp112000">
            <firstname>Furqan Muhammad</firstname>
            <lastname>Khan</lastname>
          </person>
          <person key="stars-2014-idp123800">
            <firstname>Michal</firstname>
            <lastname>Koperski</lastname>
          </person>
          <person key="stars-2014-idp126296">
            <firstname>Thi Lan Anh</firstname>
            <lastname>Nguyen</lastname>
          </person>
          <person key="stars-2014-idp85976">
            <firstname>Javier</firstname>
            <lastname>Ortiz</lastname>
          </person>
          <person key="stars-2015-idp118264">
            <firstname>Remi</firstname>
            <lastname>Trichet</lastname>
          </person>
          <person key="stars-2016-idp171696">
            <firstname>Jana</firstname>
            <lastname>Trojanova</lastname>
          </person>
          <person key="PASUSERID">
            <firstname>Ujjwal</firstname>
            <lastname>Ujjval</lastname>
          </person>
        </participants>
        <p>The new results for perception for activity recognition are:</p>
        <simplelist>
          <li id="uid85">
            <p noindent="true">Exploring Depth Information for Head Detection with Depth Images (see <ref xlink:href="#uid107" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>)</p>
          </li>
          <li id="uid86">
            <p noindent="true">Modeling Spatial Layout of Features for Real World Scenario RGB-D Action Recognition (see <ref xlink:href="#uid109" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>)</p>
          </li>
          <li id="uid87">
            <p noindent="true">Multi-Object Tracking of Pedestrian Driven by Context (see <ref xlink:href="#uid119" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>)</p>
          </li>
          <li id="uid88">
            <p noindent="true">Pedestrian detection: Training set optimization (see <ref xlink:href="#uid125" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>)</p>
          </li>
          <li id="uid89">
            <p noindent="true">Pedestrian Detection on Crossroads (see <ref xlink:href="#uid129" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>)</p>
          </li>
          <li id="uid90">
            <p noindent="true">Automated Healthcare: Facial-expression-analysis for Alzheimer's patients in Musical Mnemotherapy (see <ref xlink:href="#uid134" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>)</p>
          </li>
          <li id="uid91">
            <p noindent="true">Hybrid Approaches for Gender estimation (see <ref xlink:href="#uid136" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>)</p>
          </li>
          <li id="uid92">
            <p noindent="true">Unsupervised Metric Learning for Multi-shot Person Re-identification (see <ref xlink:href="#uid140" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>)</p>
          </li>
        </simplelist>
      </subsection>
      <subsection id="uid93" level="2">
        <bodyTitle>Semantic Activity Recognition</bodyTitle>
        <participants>
          <person key="PASUSERID">
            <firstname>François</firstname>
            <lastname>Brémond</lastname>
          </person>
          <person key="stars-2015-idp105640">
            <firstname>Carlos Fernando</firstname>
            <lastname>Crispim Junior</lastname>
          </person>
          <person key="stars-2014-idp123800">
            <firstname>Michal</firstname>
            <lastname>Koperski</lastname>
          </person>
          <person key="stars-2014-idp125016">
            <firstname>Farhood</firstname>
            <lastname>Negin</lastname>
          </person>
          <person key="stars-2015-idp114504">
            <firstname>Thanh Hung</firstname>
            <lastname>Nguyen</lastname>
          </person>
          <person key="rap-2014-idm40528">
            <firstname>Philippe</firstname>
            <lastname>Robert</lastname>
          </person>
        </participants>
        <p>For this research axis, the contributions are :</p>
        <simplelist>
          <li id="uid94">
            <p noindent="true">Semi-supervised Understanding of Complex Activities in Large-scale Datasets (see <ref xlink:href="#uid145" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>)</p>
          </li>
          <li id="uid95">
            <p noindent="true">On the Study of the Visual Behavioral Roots of Alzheimer's disease (see <ref xlink:href="#uid148" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>)</p>
          </li>
          <li id="uid96">
            <p noindent="true">Uncertainty Modeling with Ontological Models and Probabilistic Logic Programming (see <ref xlink:href="#uid151" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>)</p>
          </li>
          <li id="uid97">
            <p noindent="true">A Hybrid Framework for Online Recognition of Activities of Daily Living In Real-World Settings (see <ref xlink:href="#uid154" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>)</p>
          </li>
          <li id="uid98">
            <p noindent="true">Praxis and Gesture Recognition (see <ref xlink:href="#uid158" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>)</p>
          </li>
        </simplelist>
      </subsection>
      <subsection id="uid99" level="2">
        <bodyTitle>Software Engineering for Activity Recognition</bodyTitle>
        <participants>
          <person key="stars-2014-idp64040">
            <firstname>Sabine</firstname>
            <lastname>Moisan</lastname>
          </person>
          <person key="stars-2014-idp65472">
            <firstname>Annie</firstname>
            <lastname>Ressouche</lastname>
          </person>
          <person key="stars-2015-idp7880">
            <firstname>Jean-Paul</firstname>
            <lastname>Rigault</lastname>
          </person>
          <person key="stars-2014-idp94784">
            <firstname>Ines</firstname>
            <lastname>Sarray</lastname>
          </person>
          <person key="PASUSERID">
            <firstname>Daniel</firstname>
            <lastname>Gaffé</lastname>
          </person>
          <person key="stars-2014-idp78352">
            <firstname>Rachid</firstname>
            <lastname>Guerchouche</lastname>
          </person>
          <person key="stars-2015-idp113256">
            <firstname>Matias</firstname>
            <lastname>Marin</lastname>
          </person>
          <person key="PASUSERID">
            <firstname>Etienne</firstname>
            <lastname>Corvée</lastname>
          </person>
          <person key="stars-2014-idp112576">
            <firstname>Julien</firstname>
            <lastname>Badie</lastname>
          </person>
          <person key="lagadic-2014-idp89016">
            <firstname>Manikandan</firstname>
            <lastname>Bakthavatchalam</lastname>
          </person>
          <person key="stars-2014-idp69392">
            <firstname>Vasanth</firstname>
            <lastname>Bathrinarayanan</lastname>
          </person>
          <person key="PASUSERID">
            <firstname>Ghada</firstname>
            <lastname>Balhoul</lastname>
          </person>
          <person key="galaad2-2014-idp67104">
            <firstname>Anais</firstname>
            <lastname>Ducoffe</lastname>
          </person>
          <person key="stars-2015-idp172352">
            <firstname>Jean Yves</firstname>
            <lastname>Tigli</lastname>
          </person>
          <person key="PASUSERID">
            <firstname>François</firstname>
            <lastname>Brémond</lastname>
          </person>
        </participants>
        <p>The contributions for this research axis are:</p>
        <simplelist>
          <li id="uid100">
            <p noindent="true">Scenario Recognition (see <ref xlink:href="#uid160" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>)</p>
          </li>
          <li id="uid101">
            <p noindent="true">The <span class="smallcap" align="left">clem</span> Workflow (see <ref xlink:href="#uid163" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>)</p>
          </li>
          <li id="uid102">
            <p noindent="true">Safe Composition in Middleware for Internet of Things (see <ref xlink:href="#uid165" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>)</p>
          </li>
          <li id="uid103">
            <p noindent="true">Verification of Temporal Properties of Neuronal Archetype (see <ref xlink:href="#uid166" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>)</p>
          </li>
          <li id="uid104">
            <p noindent="true">Dynamic Reconfiguration of Feature Models (see <ref xlink:href="#uid167" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>)</p>
          </li>
          <li id="uid105">
            <p noindent="true">Setup and management of SafEE devices (see <ref xlink:href="#uid169" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>)</p>
          </li>
          <li id="uid106">
            <p noindent="true">Brick &amp; Mortar Cookies (see <ref xlink:href="#uid176" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>)</p>
          </li>
        </simplelist>
      </subsection>
    </subsection>
    <subsection id="uid107" level="1">
      <bodyTitle>Exploring Depth Information for Head Detection with Depth Images</bodyTitle>
      <participants>
        <person key="stars-2015-idp114504">
          <firstname>Thanh Hung</firstname>
          <lastname>Nguyen</lastname>
        </person>
        <person key="stars-2015-idp131904">
          <firstname>Siyuan</firstname>
          <lastname>Chen</lastname>
        </person>
      </participants>
      <p>Head detection may be more demanding than face recognition
and pedestrian detection in the scenarios where a
face turns away or body parts are occluded in the view of
a sensor, but when locating people is needed. This year <ref xlink:href="#stars-2016-bid11" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, we
introduce an efficient head detection approach for single
depth images at low computational expense. First, a novel
head descriptor was developed and used to classify pixels as
head or non-head. We used depth values to guide each window
size, to eliminate false positives of head centers, and to
cluster head pixels, which significantly reduce the computation
costs of searching for appropriate parameters. High
head detection performance was achieved in experiments with
90% accuracy for our dataset containing heads with different
body postures, head poses, and distances to a Kinect2
sensor, and above 70% precision on a public dataset composed
of a few daily activities, which is better than using a
head-shoulder detector with HOG feature for depth images (see Figure  <ref xlink:href="#uid108" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>)</p>
      <object id="uid108">
        <table>
          <tr>
            <td>
              <ressource xlink:href="IMG/HungNGUYEN_fg_1.png" type="inline" scale="0.4" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
            </td>
            <td>
              <ressource xlink:href="IMG/HungNGUYEN_fg_2.png" type="inline" scale="0.4" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
            </td>
          </tr>
        </table>
        <caption>Examples of head detection where our algorithm successfully detects head.
Pink square represents the ground truth, green rectangle represents our algorithm.</caption>
      </object>
    </subsection>
    <subsection id="uid109" level="1">
      <bodyTitle>Modeling Spatial Layout of Features for Real World Scenario RGB-D Action Recognition</bodyTitle>
      <participants>
        <person key="stars-2014-idp123800">
          <firstname>Michal</firstname>
          <lastname>Koperski</lastname>
        </person>
        <person key="PASUSERID">
          <firstname>François</firstname>
          <lastname>Brémond</lastname>
        </person>
      </participants>
      <p noindent="true"><b>keywords:</b> computer vision, action recognition</p>
      <p noindent="true">
        <b>Challenges in action representation in real-world scenario using RGB-D sensor</b>
      </p>
      <p>With RGB-D sensor it is easy to take advantage
of real-time skeleton detection. Using skeleton information we can model not only dynamics of
action, but also static features like pose. Skeleton-based methods have been proposed by many
authors, and have reported superior accuracy on various daily activity data-sets. But the main
drawback of skeleton-based methods is that they cannot make the decision when skeleton
is missing.</p>
      <p>We claim that in real world scenario of daily living monitoring, skeleton is very often not
available or is very noisy. This makes skeleton based methods unpractical.
There are several reasons why skeleton detection fails in real-world scenario.
Firstly, the sensor has to work outside of it's working range. Since daily living monitoring
is quite an unconstrained environment, the monitored person is very often too far from sensor,
or is captured from non-optimal viewpoint angle.
In Figure <ref xlink:href="#uid110" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>
we show two examples where skeleton detection fails. In the first example, the person on
the picture wears black jeans which interferes with sensor.
In such a case depth information from lower body parts is missing, making skeleton detection inaccurate.
In the second example (see Figure  <ref xlink:href="#uid111" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>) the person is too far from sensor and basically disappears in the background. In this case depth information is too noisy, thus skeleton detection fails.
All disadvantages mentioned above will affect skeleton-based action recognition methods,
because they strictly require skeleton detection.</p>
      <p>On the other hand, local points-of-interest methods do not require skeleton detection, nor segmentation. That is why they received great amount of interest in RGB based action
recognition where segmentation is much more difficult than with RGB-D. Those methods
rely mostly on detection of points-of-interest usually based on some motion features (eg optical flow).
The features are either based on trajectory of points-of-interest or descriptors are computed around the points-of-interest. One of the main disadvantage of those methods is fact
that they fail when they cannot "harvest" enough points-of-interest. It happens when action has low dynamics eg "reading a book" or "writing on paper". Such actions contain very low amount of
motion coming from hand when writing or turning the pages. In addition local points-of-interest methods
very often ignore the spatial layout of detected features.</p>
      <p noindent="true">
        <b>Proposed method</b>
      </p>
      <object id="uid110">
        <table>
          <tr>
            <td>
              <ressource xlink:href="IMG/outline2_part1a.png" type="float" width="199.16928pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
            </td>
          </tr>
        </table>
        <caption>We show two examples where skeleton detection methods fail. Pictures on the left show RGB frame, pictures on the right show depth map (dark blue indicates missing depth information).</caption>
      </object>
      <object id="uid111">
        <table>
          <tr>
            <td>
              <ressource xlink:href="IMG/outline2_part2.png" type="float" width="199.16928pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
            </td>
          </tr>
        </table>
        <caption>We show proposed method where we use people detection in place of skeleton. Next we propose to encode spatial-layout of visual words computed from motion features. In addition we propose GridHOG descriptor which encodes static appearance information.</caption>
      </object>
      <p>To address those problems we propose to replace skeleton detection by RGB-D based
people detector. Note that person detection is much
easier than skeleton detection. In addition we propose to use two people detectors: RGB and depth based - to take advantage of two information streams.</p>
      <p>We propose to model spatial layout of motion features obtained from a
local points-of-interest based method. We use Dense Trajectories
<ref xlink:href="#stars-2016-bid12" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> as a point of interest detector and MBH (Motion Boundary Histogram <ref xlink:href="#stars-2016-bid13" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>) as a descriptor.
To improve the discriminating power of MBH descriptor we propose to model
spatial-layout of visual words computed based on MBH (Figure <ref xlink:href="#uid111" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>).
We divide a person bounding box into Spatial Grid (SG) and we compute Fisher Vector
representation in each cell. In addition, we show that other spatial-layout encoding
methods also improve recognition accuracy. We propose 2 alternative spatial-layout
encoding methods an we compare them with Spatial Grid.</p>
      <p>To improve recognition of actions with low amount of motion we propose a descriptor which
encodes rough static appearance (Figure <ref xlink:href="#uid111" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>).
This can be interpreted as
rough pose information. We divide the detected person bounding box
into grid cells. Then we compute HOG <ref xlink:href="#stars-2016-bid14" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> descriptor inside each cell to form the GHOG (GridHog) descriptor.</p>
      <p>Further details can be find in the paper <ref xlink:href="#stars-2016-bid15" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.
The contributions of this paper can be listed as follows:</p>
      <simplelist>
        <li id="uid112">
          <p noindent="true">We propose to use two people detectors (RGB and depth based ) to obtain person bounding
box instead of skeleton.</p>
        </li>
        <li id="uid113">
          <p noindent="true">We propose to use Spatial Grid (SG) inside person bounding box. To model spatial-layout of MBH features.</p>
        </li>
        <li id="uid114">
          <p noindent="true">We propose to encode static information by using novel GHOG descriptor.</p>
        </li>
        <li id="uid115">
          <p noindent="true">We propose two other methods which model spatial-layout of MBH features and we compare
them with Spatial Grid.</p>
        </li>
      </simplelist>
      <p noindent="true">
        <b>Experiments</b>
      </p>
      <p>We evaluate our approach on three daily activity data-sets: MSRDailyActivity3D,
CAD-60 and CAD-120. The experiments show that we outperform most of the skeleton-based
methods without requiring difficult in real-world scenario skeleton detection and thus being more robust (see Table<ref xlink:href="#uid116" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, Table<ref xlink:href="#uid117" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> and Table<ref xlink:href="#uid118" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>).</p>
      <table id="uid116" rend="display">
        <tr style="border-top-style:solid;border-top-width:1px;border-bottom-style:solid; border-bottom-width:1px;" top-border="true" bottom-border="true">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">Method</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">Accuracy [%]</td>
        </tr>
        <tr style="">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">NBNN <ref xlink:href="#stars-2016-bid16" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">
            <i>70.00</i>
          </td>
        </tr>
        <tr style="">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">HON4D <ref xlink:href="#stars-2016-bid17" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">
            <i>80.00</i>
          </td>
        </tr>
        <tr style="">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">STIP + skeleton <ref xlink:href="#stars-2016-bid18" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">
            <i>80.00</i>
          </td>
        </tr>
        <tr style="">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">SSFF <ref xlink:href="#stars-2016-bid19" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">
            <i>81.90</i>
          </td>
        </tr>
        <tr style="">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">DSCF <ref xlink:href="#stars-2016-bid20" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">
            <i>83.60</i>
          </td>
        </tr>
        <tr style="">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">Actionlet Ensemble <ref xlink:href="#stars-2016-bid21" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">
            <i>85.80</i>
          </td>
        </tr>
        <tr style="">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">RGGP + fusion <ref xlink:href="#stars-2016-bid22" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">
            <i>85.60</i>
          </td>
        </tr>
        <tr style="">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">Super Normal <ref xlink:href="#stars-2016-bid23" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">
            <i>86.26</i>
          </td>
        </tr>
        <tr style="">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">BHIM <ref xlink:href="#stars-2016-bid24" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">86.88</td>
        </tr>
        <tr style="">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">DCSF + joint <ref xlink:href="#stars-2016-bid20" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">
            <i>88.20</i>
          </td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">
            <b>Our Approach</b>
          </td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">
            <b>85.95</b>
          </td>
        </tr>
        <caption>Recognition Accuracy Comparison for MSRDailyActivity3D data-set. corresponds to methods which require skeleton detection.</caption>
      </table>
      <table id="uid117" rend="display">
        <tr style="border-top-style:solid;border-top-width:1px;border-bottom-style:solid; border-bottom-width:1px;" top-border="true" bottom-border="true">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">Method</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">Accuracy [%]</td>
        </tr>
        <tr style="">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">STIP <ref xlink:href="#stars-2016-bid18" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">62.50</td>
        </tr>
        <tr style="">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">Order Sparse Coding <ref xlink:href="#stars-2016-bid25" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">
            <i>65.30</i>
          </td>
        </tr>
        <tr style="">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">Object Affordance <ref xlink:href="#stars-2016-bid26" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">
            <i>71.40</i>
          </td>
        </tr>
        <tr style="">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">HON4D <ref xlink:href="#stars-2016-bid17" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">
            <i>72.70</i>
          </td>
        </tr>
        <tr style="">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">Actionlet Ensemble <ref xlink:href="#stars-2016-bid21" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">
            <i>74.70</i>
          </td>
        </tr>
        <tr style="">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">JOULE-SVM <ref xlink:href="#stars-2016-bid27" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">
            <i>84.10</i>
          </td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">
            <b>Our Approach</b>
          </td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">
            <b>80.36</b>
          </td>
        </tr>
        <caption>Recognition Accuracy Comparison for CAD-60 data-set. corresponds to methods which require skeleton detection.</caption>
      </table>
      <table id="uid118" rend="display">
        <tr style="border-top-style:solid;border-top-width:1px;border-bottom-style:solid; border-bottom-width:1px;" top-border="true" bottom-border="true">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">Method</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">Accuracy [%]</td>
        </tr>
        <tr style="">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">Salient Proto-Objects <ref xlink:href="#stars-2016-bid28" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">78.20</td>
        </tr>
        <tr style="">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">Object Affordance <ref xlink:href="#stars-2016-bid26" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">
            <i>84.70</i>
          </td>
        </tr>
        <tr style="">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">STS <ref xlink:href="#stars-2016-bid29" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">
            <i>93.50</i>
          </td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">
            <b>Our Approach</b>
          </td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">
            <b>85.48</b>
          </td>
        </tr>
        <caption>Recognition Accuracy Comparison for CAD-120 data-set. corresponds to methods which require skeleton detection.</caption>
      </table>
    </subsection>
    <subsection id="uid119" level="1">
      <bodyTitle>Multi-Object Tracking of Pedestrian Driven by Context</bodyTitle>
      <participants>
        <person key="stars-2014-idp126296">
          <firstname>Thi Lan Anh</firstname>
          <lastname>Nguyen</lastname>
        </person>
        <person key="PASUSERID">
          <firstname>François</firstname>
          <lastname>Brémond</lastname>
        </person>
        <person key="stars-2016-idp171696">
          <firstname>Jana</firstname>
          <lastname>Trojanova</lastname>
        </person>
      </participants>
      <p noindent="true"><b>Keywords:</b> Tracklet fusion, Multi-object tracking</p>
      <p>Multi-object tracking (MOT) is essential to many applications in computer vision. As so many trackers have been proposed in the past, one would expect the tracking task as solved.
It is true for scenarios containing solid background with a low number of objects and few interactions. However, scenarios with appearance changes due to pose variation, abrupt motion changes, and occlusion still represent a big challenge.</p>
      <p>In the state of the art, some sets of efficient methods are proposed to face this challenge: data association (local and global) and tracking parameter adaptation.
A very popular method for local data association is the bipartite matching. The exact solution can be found via Hungarian algorithm <ref xlink:href="#stars-2016-bid30" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. These methods are computationally inexpensive, but can deal only with short term occlusion. An example of global method is the extension of the bipartite matching into network flow <ref xlink:href="#stars-2016-bid31" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. Given the objects detections at each frame, the direct acyclic graph is formed and the solution is found through minimum-cost flow algorithm. The algorithms reduce trajectory fragments and improve trajectory consistency but lack robustness to identity switches of close or intersecting trajectories.</p>
      <object id="uid120">
        <table>
          <tr>
            <td>
              <ressource xlink:href="IMG/framework_lan.png" type="float" width="298.8987pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
            </td>
          </tr>
        </table>
        <caption>Our proposed framework.</caption>
      </object>
      <p>Another set of methods for MOT is online parameter adaptation <ref xlink:href="#stars-2016-bid32" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. They tune automatically the tracking parameters based on the context information,
while methods mentioned above use one appearance and/or one motion feature for the whole video. In <ref xlink:href="#stars-2016-bid32" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, the authors learn the parameters for the scene context offline.
In online phase the tracking parameters are selected from database based on the current context of the scene. These parameters are applied to all objects in the scene. Such a concept assumes discriminative appearance and trajectories among individuals, which is not always the case in real scenarios.</p>
      <p>In order to overcome these limitations, we propose a new long term tracking framework. This framework has several dominant contributions:</p>
      <simplelist>
        <li id="uid121">
          <p noindent="true">We introduce new long term tracking framework which combines short data association and the online parameter tuning for individual tracklets. In contrast to previous methods that used the same setting for all tracklets.</p>
        </li>
        <li id="uid122">
          <p noindent="true">We show that large number of parameters can be efficiently tuned via multiple simulated annealing, whereas previous method could tune only a limited number of parameters and fix the rest to be able to do exhaustive search.</p>
        </li>
        <li id="uid123">
          <p noindent="true">We define the surrounding context around each tracklet and similarity metric among tracklets allowing us to match learned context with unseen video set.</p>
        </li>
      </simplelist>
      <p>The proposed framework was trained on 9 public video sequences and tested on 3 unseen sets. It outperforms the state-of-art pedestrian trackers in scenarios of motion changes, appearance changes and occlusion of objects as shown
in Table <ref xlink:href="#uid124" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. The paper is accepted in conference AVSS-2016 <ref xlink:href="#stars-2016-bid33" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
      <table id="uid124" rend="display">
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="center">Dataset</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">Method</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">MOTA</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">MOTP</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">GT</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">MT</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">PT</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">ML</td>
        </tr>
        <tr style="">
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="center">PETS2009</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">Shitrit et al.   <ref xlink:href="#stars-2016-bid34" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">0.81</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">0.58</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">21</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">–</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">–</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">–</td>
        </tr>
        <tr style="">
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="center"/>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center"><b/><i><b>Bae et al.</b></i>-<i>global association</i>   <ref xlink:href="#stars-2016-bid35" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">0.73</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">0.69</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">23</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <i>100</i>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">0</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <i>0.0</i>
          </td>
        </tr>
        <tr style="">
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="center"/>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">Chau et al.  <ref xlink:href="#stars-2016-bid36" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">0.62</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">0.63</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">21</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">–</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">–</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">–</td>
        </tr>
        <tr style="">
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="center"/>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">Chau   <ref xlink:href="#stars-2016-bid37" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>(   <ref xlink:href="#stars-2016-bid36" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> + parameter tuning for whole video context)</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">0.85</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">0.71</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">21</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">–</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">–</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">–</td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="center"/>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center"><b>Ours</b> (   <ref xlink:href="#stars-2016-bid36" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> + Proposed approach )</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <i>0.86</i>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <i>0.73</i>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">21</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">76.2</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">14.3</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">9.5</td>
        </tr>
        <tr style="">
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="center">TUD-Stadtmitte</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">Andriyenko et al.   <ref xlink:href="#stars-2016-bid38" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">0.62</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">0.63</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">9</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">60.0</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">20.0</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">10.0</td>
        </tr>
        <tr style="">
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="center"/>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">Milan et al.   <ref xlink:href="#stars-2016-bid39" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <i>0.71</i>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <i>0.65</i>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">9</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <i>70.0</i>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">20.0</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <i>0.0</i>
          </td>
        </tr>
        <tr style="">
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="center"/>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">Chau et al.   <ref xlink:href="#stars-2016-bid36" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">0.45</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">0.62</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">10</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">60.0</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">40.0</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <i>0.0</i>
          </td>
        </tr>
        <tr style="">
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="center"/>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">Chau   <ref xlink:href="#stars-2016-bid37" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>(  <ref xlink:href="#stars-2016-bid36" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> + parameter tuning for whole video context)</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">–</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">–</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">10</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <i>70.0</i>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">10.0</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">20.0</td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="center"/>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center"><b>Ours</b> (   <ref xlink:href="#stars-2016-bid36" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> + Proposed approach )</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">0.47</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <i>0.65</i>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">10</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <i>70.0</i>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">30.0</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <i>0.0</i>
          </td>
        </tr>
        <tr style="">
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="center">TUD-Crossing</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">Tang et al.   <ref xlink:href="#stars-2016-bid40" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">–</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">–</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">11</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <i>53.8</i>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">38.4</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">7.8</td>
        </tr>
        <tr style="">
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="center"/>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">Chau et al.   <ref xlink:href="#stars-2016-bid36" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">0.69</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">0.65</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">11</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">46.2</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">53.8</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <i>0.0</i>
          </td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="center"/>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center"><b>Ours</b> (  <ref xlink:href="#stars-2016-bid36" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> + Proposed approach)</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <i>0.72</i>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <i>0.67</i>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">11</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <i>53.8</i>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">46.2</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <i>0.0</i>
          </td>
        </tr>
        <caption>Tracking performance.
The best values are printed in red.</caption>
      </table>
    </subsection>
    <subsection id="uid125" level="1">
      <bodyTitle>Pedestrian detection: Training set optimization</bodyTitle>
      <participants>
        <person key="stars-2015-idp118264">
          <firstname>Remi</firstname>
          <lastname>Trichet</lastname>
        </person>
        <person key="stars-2014-idp85976">
          <firstname>Javier</firstname>
          <lastname>Ortiz</lastname>
        </person>
      </participants>
      <p noindent="true"><b>keywords:</b> computer vision, pedestrian detection, classifier training, data selection, data generation, data weighting</p>
      <p>The emphasis of our work is on data selection. Training for pedestrian detection is, indeed, a peculiar task. It aims to differentiate a few positive samples with relatively low intra-class variation and a swarm of negative samples picturing everything else present in the dataset. Consequently, the training set lacks discrimination and is highly imbalanced. Due to the possible creation of noisy data while oversampling, and the likely loss of information when undersampling, balancing positive and negative instances is a rarely addressed issue in the literature.</p>
      <p noindent="true">Bearing these data selection principles in mind, we introduce a new training methodology, grounded on a two-component contribution. First, it harnesses an expectation-maximization scheme to weight important training data for classification. Second, it improves the cascade-of-rejectors <b><ref xlink:href="#stars-2016-bid41" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/><ref xlink:href="#stars-2016-bid42" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></b> classification by enforcing balanced train and validation sets every step of the way, and optimizing separately for recall and precision. A new data generation technique was developped for this purpose.</p>
      <p noindent="true">The training procedure unfolds as follows. After the initial data selection, we balance the negative and positive sample cardinalities. Then, a set of <i>n</i> negative data rejectors are trained and identified negative data are discarded. The validation set negative samples are iteratively oversampled after each training to ensure a balanced set. The final classifier is learned after careful data selection. Figure <ref xlink:href="#uid126" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> illustrates the process.</p>
      <object id="uid126">
        <table>
          <tr>
            <td>
              <ressource xlink:href="IMG/training_pipeline.png" type="float" width="427.0pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
            </td>
          </tr>
        </table>
        <caption>Training pipeline.</caption>
      </object>
      <p noindent="false">Experiments carried out on the Inria <b><ref xlink:href="#stars-2016-bid14" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></b> and PETS2009 <b><ref xlink:href="#stars-2016-bid43" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></b> datasets, demonstrate the effectiveness of the approach, leading to a simple HoG-based detector
to outperform most of its near real-time competitors.</p>
      <table id="uid127" rend="display">
        <tr style="border-top-style:solid;border-top-width:1px;border-bottom-style:solid; border-bottom-width:1px;" top-border="true" bottom-border="true">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">Method</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">Inria</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">Speed</td>
        </tr>
        <tr style="">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">HoG <b><ref xlink:href="#stars-2016-bid14" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></b></td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">46%</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">21fps</td>
        </tr>
        <tr style="">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">DPM-v1 <b><ref xlink:href="#stars-2016-bid44" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></b></td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">44%</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center"><formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mo>&lt;</mo></math></formula> 1fps</td>
        </tr>
        <tr style="">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">HoG-LBP <b><ref xlink:href="#stars-2016-bid45" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></b></td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">39%</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">Not provided</td>
        </tr>
        <tr style="">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">MultiFeatures <b><ref xlink:href="#stars-2016-bid46" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></b></td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">36%</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center"><formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mo>&lt;</mo></math></formula> 1fps</td>
        </tr>
        <tr style="">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">FeatSynth <b><ref xlink:href="#stars-2016-bid47" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></b></td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">31%</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center"><formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mo>&lt;</mo></math></formula> 1fps</td>
        </tr>
        <tr style="">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">MultiFeatures+CSS <b><ref xlink:href="#stars-2016-bid48" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></b></td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">25%</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">No</td>
        </tr>
        <tr style="">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">
            <i>FairTrain - HoG + Luv</i>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <i>25%</i>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <i>11fps</i>
          </td>
        </tr>
        <tr style="">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">
            <i>FairTrain - HoG</i>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <i>25%</i>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <i>16fps</i>
          </td>
        </tr>
        <tr style="">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">Channel Features <b><ref xlink:href="#stars-2016-bid49" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></b></td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">21%</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">0.5fps</td>
        </tr>
        <tr style="">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">FPDW <b><ref xlink:href="#stars-2016-bid50" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></b></td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">21%</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">2-5fps</td>
        </tr>
        <tr style="">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">DPM-v2 <b><ref xlink:href="#stars-2016-bid51" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></b></td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">20%</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center"><formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mo>&lt;</mo></math></formula> 1fps</td>
        </tr>
        <tr style="">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">VeryFast <b><ref xlink:href="#stars-2016-bid52" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></b></td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">18%</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">8fps(CPU)</td>
        </tr>
        <tr style="">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">VeryFast <b><ref xlink:href="#stars-2016-bid52" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></b></td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">18%</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">135fps(GPU)</td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">WordChannels <b><ref xlink:href="#stars-2016-bid53" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></b></td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">17%</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">8fps(GPU)</td>
        </tr>
        <caption>Comparison with the state-of-the-art on the Inria dataset. Our approach is in italic. Computation time are calculated according to 640×480 resolution frames. The used metric is the log-average miss rate (the lower the better).</caption>
      </table>
      <table id="uid128" rend="display">
        <tr style="border-top-style:solid;border-top-width:1px;border-bottom-style:solid; border-bottom-width:1px;" top-border="true" bottom-border="true">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">Method</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">PETS2009</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">Speed</td>
        </tr>
        <tr style="">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">Arsic <b><ref xlink:href="#stars-2016-bid54" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></b></td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">44%</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">n.a.</td>
        </tr>
        <tr style="">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">Alahi <b><ref xlink:href="#stars-2016-bid55" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></b></td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">73%</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">n.a.</td>
        </tr>
        <tr style="">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">Conte <b><ref xlink:href="#stars-2016-bid56" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></b></td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">85%</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">n.a.</td>
        </tr>
        <tr style="">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">
            <i>FairTrain - HoG</i>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <i>85.38%</i>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <i>29fps</i>
          </td>
        </tr>
        <tr style="">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">
            <i>FairTrain - HoG + Luv</i>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <i>85.49%</i>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <i>18fps</i>
          </td>
        </tr>
        <tr style="">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">Breitenstein <b><ref xlink:href="#stars-2016-bid57" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></b></td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">89%</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">n.a.</td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">Yang <b><ref xlink:href="#stars-2016-bid58" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></b></td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">96%</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">n.a.</td>
        </tr>
        <caption>Comparison with the state-of-the-art on the PETS2009 S2.L1 sequence. Our approach is in italic. The used metric is the MODA (the higher the better).</caption>
      </table>
    </subsection>
    <subsection id="uid129" level="1">
      <bodyTitle>Pedestrian Detection on Crossroads</bodyTitle>
      <participants>
        <person key="PASUSERID">
          <firstname>Ujwal</firstname>
          <lastname>Ujwal</lastname>
        </person>
        <person key="PASUSERID">
          <firstname>François</firstname>
          <lastname>Brémond</lastname>
        </person>
      </participants>
      <p>Pedestrian detection has a specific relevance in the space of object detection problems in computer vision. Due to increasing role of automated surveillance systems in increasing areas, demands for a highly robust and accurate pedestrian detection system is increasing day after day. Recently, deep learning has emerged as an important paradigm to tackle complex object detection problems. This year, we performed our initial studies on pedestrian detection using deep learning techniques. These studies form an important basis for us to extend our work in the future.</p>
      <p noindent="true">
        <b>Evaluation Metrics</b>
      </p>
      <p>The relative comparison of different pedestrian detection systems was done using evaluation metrics. In the area of pedestrian detection, the most widely used evaluation metric is that of <i>miss rate</i>(MR). <i>Miss rate</i> is related to the concept of <i>recall</i>, which is another very commonly used metric in computer vision, especially in problems related to retrieval of images and concepts. <i>Miss Rate</i> is defined as follows:</p>
      <formula id-text="1" id="uid130" textype="equation" type="display">
        <math xmlns="http://www.w3.org/1998/Math/MathML" mode="display" overflow="scroll">
          <mrow>
            <mi>M</mi>
            <mi>i</mi>
            <mi>s</mi>
            <mi>s</mi>
            <mspace width="4pt"/>
            <mi>R</mi>
            <mi>a</mi>
            <mi>t</mi>
            <mi>e</mi>
            <mo>=</mo>
            <mfrac>
              <mrow>
                <mi>F</mi>
                <mi>a</mi>
                <mi>l</mi>
                <mi>s</mi>
                <mi>e</mi>
                <mspace width="4pt"/>
                <mi>N</mi>
                <mi>e</mi>
                <mi>g</mi>
                <mi>a</mi>
                <mi>t</mi>
                <mi>i</mi>
                <mi>v</mi>
                <mi>e</mi>
                <mi>s</mi>
              </mrow>
              <mrow>
                <mi>T</mi>
                <mi>r</mi>
                <mi>u</mi>
                <mi>e</mi>
                <mspace width="4pt"/>
                <mi>P</mi>
                <mi>o</mi>
                <mi>s</mi>
                <mi>i</mi>
                <mi>t</mi>
                <mi>i</mi>
                <mi>v</mi>
                <mi>e</mi>
                <mi>s</mi>
                <mo>+</mo>
                <mi>F</mi>
                <mi>a</mi>
                <mi>l</mi>
                <mi>s</mi>
                <mi>e</mi>
                <mspace width="4pt"/>
                <mi>N</mi>
                <mi>e</mi>
                <mi>g</mi>
                <mi>a</mi>
                <mi>t</mi>
                <mi>i</mi>
                <mi>v</mi>
                <mi>e</mi>
                <mi>s</mi>
              </mrow>
            </mfrac>
          </mrow>
        </math>
      </formula>
      <object id="uid131">
        <table>
          <tr>
            <td>
              <ressource xlink:href="IMG/TPFP.jpg" type="float" width="213.5pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
            </td>
          </tr>
        </table>
        <caption>True and False Positives in pedestrian detection</caption>
      </object>
      <p>In equation <ref xlink:href="#uid130" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, <i>True Positives</i>(TP) and <i>False Negatives</i>(FN) can be understood from figure <ref xlink:href="#uid131" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. A good pedestrian detector should not miss many people in a scene and this aspect is reflected in the definition of equation <ref xlink:href="#uid130" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. A good pedestrian detector is required to detect as few <i>False Positives</i>(FP) as possible. This is expressed in the literature usually in the form of False Positives Per Image(FPPI). FPPI is basically a per-image average of total number of FP detections.</p>
      <p noindent="true">Pedestrian detection systems usually work with a number of parameters. Different values of these parameters may tune a system to different MR and FPPI value. This is usually expressed in the form of a <i>Precision-recall</i>(PR) curve. This curve is created by varying a control parameter of a system and plotting MR and FPPI values. In literature it is customary to report MR value at 0.1 FPPI.</p>
      <p noindent="true">
        <b>Experiments</b>
      </p>
      <p>We considered deep learning based models for our initial set of experiments. This is primarily owing to their popularity and the promise which they have demonstrated in the area of object detection over the past several years.</p>
      <p noindent="true">There are many deep learning based models which have been used for object detection.
The purpose of these experiments was to gain a deeper insight into the performance of deep neural networks for pedestrian detection. We experimented with Faster-RCNN <ref xlink:href="#stars-2016-bid59" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> and SSD detector <ref xlink:href="#stars-2016-bid60" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. These were chosen owing to the fact that they are recent models (2015 for Faster-RCNN and 2016 for SSD Detector), and have displayed state-of-art performance in terms of detection speeds and accuracy across many object categories.</p>
      <p>The results shown in table<ref xlink:href="#uid132" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> were obtained by fine-tuning VGG-16 with imagenet and MS-COCO datasets which did not involve any public dataset specific to pedestrian detection. Hence, we took the fine-tuned model and further fine-tuned it with different pedestrian datasets to study the effectiveness of fine-tuning with pedestrian-specific datasets.</p>
      <p>Each row in the first column of table<ref xlink:href="#uid133" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, reflects the dataset(s) which were used to fine-tune the model. For each row, the model was fine-tuned using the dataset indicated in its first column, as well as the datasets indicated in the first column of all rows preceding it. The model was then evaluated against the test set of each dataset and the miss-rates are indicated in the table.</p>
      <table id="uid132" rend="display">
        <tr style="border-top-style:solid;border-top-width:1px;border-bottom-style:solid; border-bottom-width:1px;" top-border="true" bottom-border="true">
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" cols="3" right-border="true" left-border="true" halign="center">Performance of fine-tuned Faster RCNN</td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="center">
            <b>Dataset</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>Faster RCNN Performance</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>State of Art</b>
          </td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="center">Inria</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">13.47%</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">13%</td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="center">Daimler</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">37.7%</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">29%</td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="center">ETH-Zurich</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">32.1%</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center"/>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="center">Caltech</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">26.7%</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">19%</td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="center">TUD-Brussels</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">52.2%</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">45%</td>
        </tr>
        <caption>Performance of fine-tuned Faster RCNN on pedestrian detection datasets.Numbers indicate the miss-rate.</caption>
      </table>
      <p>-</p>
      <table id="uid133" rend="display">
        <tr style="border-top-style:solid;border-top-width:1px;border-bottom-style:solid; border-bottom-width:1px;" top-border="true" bottom-border="true">
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="center"/>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" cols="5" right-border="true" left-border="true" halign="center">Image datasets</td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="center">
            <b>Trained Model</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>Inria</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>Daimler</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>TUD-Brussels</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>ETH-Zurich</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>Caltech</b>
          </td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="center">+Inria</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">13.4%</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">36.9%</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">52%</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">32.1%</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">28.2%</td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="center">+Daimler</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">13.6%</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">33.7%</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">51.1%</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">32.7%</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">29.1%</td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="center">+ETH-Zurich</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">13.8%</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">34.6%</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">49.3%</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">32%</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">26%</td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="center">+Caltech</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">16%</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">35.4%</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">48%</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">33.2%</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">25.2%</td>
        </tr>
        <caption>Faster-RCNN performance after fine-tuning with pedestrian datasets. Numbers indicate the miss-rate.</caption>
      </table>
      <p>While the initial results as seen from table  <ref xlink:href="#uid132" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> are encouraging, they still need a lot of improvement especially with complex datasets such as TUD-Brussels and Caltech. We also see from table<ref xlink:href="#uid133" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, that fine-tuning with pedestrian datasets tends to improve the performance but the magnitude of improvement varies depending upon the dataset(s) being fine-tuned with and the dataset(s) being tested upon. These observations indicate some important research directions. Data in computer vision applications are highly varied and it is not very easy to capture its complexity and variations with sufficient ease. It is important to proceed to work on better dataset usage by clustering the datasets together based on traits such as viewpoint, resolution etc. Resolution is another important element which significantly affects deep learning based approaches. This is because deep learning involves automated feature extractions from the pixel level and low resolution appearance often makes that problem difficult.</p>
      <p noindent="true">We intend to work upon and cover these issues in subsequent efforts towards solving the pedestrian detection problem.</p>
    </subsection>
    <subsection id="uid134" level="1">
      <bodyTitle>Automated Healthcare: Facial-expression-analysis for Alzheimer's patients in Musical Mnemotherapy</bodyTitle>
      <participants>
        <person key="stars-2014-idp88472">
          <firstname>Antitza</firstname>
          <lastname>Dantcheva</lastname>
        </person>
        <person key="stars-2014-idp113792">
          <firstname>Piotr</firstname>
          <lastname>Bilinski</lastname>
        </person>
        <person key="rap-2014-idm40528">
          <firstname>Philippe</firstname>
          <lastname>Robert</lastname>
        </person>
        <person key="PASUSERID">
          <firstname>François</firstname>
          <lastname>Brémond</lastname>
        </person>
      </participants>
      <p noindent="true"><b>keywords:</b> automated healthcare, healthcare monitoring, expression recognition</p>
      <p>The elderly population has been growing dramatically and future predictions and estimations showcase that by 2050 the number of people over 65 years old will increase by 70%, the number of people over 80 years old will increase by 170%, outnumbering younger generations from 0-14 years. Other studies indicate that around half of the current population of over 75 year old suffer from physical and / or mental impairments and as a result are in need of high level of care. The loss of autonomy can be delayed by maintaining an active life style, which also would lead to reduced healthcare financial costs. With the expected increase of the world elderly population, and on the other hand limited available human resources for care a question arises as "How can we improve health care in an efficient and cost effective manner?".</p>
      <p>Motivated by the above, we propose an approach for detecting facial expressions in Alzheimer's disease (AD) patients that can be a pertinent unit in an automated assisted living system for elderly subjects. Specifically, we have collected video-data of AD patients in musical therapy at the AD center Fondation G.S.F J. L. Noisiez in Biot, France from multiple therapy-sessions for validating our method. We note that in such sessions even AD patients suffering from apathy exhibit a number of emotions and expressions. We propose a spatio-temporal algorithm for facial expression recognition based on dense trajectories, Fisher Vectors and support vector machine classification. We compared the proposed algorithm to a facial-landmark-based algorithm concerning signal displacement of tracked points within the face.</p>
      <p>Our algorithm differentiates between four different facial expressions: (i) neutral, (ii) smile, (iii) talking, and (iv) singing with an accuracy of 56%, outperforming the facial-landmark-based algorithm. Challenging for both algorithms has been the unconstrained setting involving different poses, changes in illumination and camera movement. One expected benefit for AD patients is that positive expressions and their cause could be determined and replicated in order to increase life standard for such patients, which also brings to the fore a delay in the development of AD
(see figure  <ref xlink:href="#uid135" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>).</p>
      <object id="uid135">
        <table>
          <tr>
            <td>
              <ressource xlink:href="IMG/ISG_.png" type="float" width="128.1013pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
            </td>
          </tr>
        </table>
        <caption>Expression recognition in AD patients based on dense trajectories and Fisher vectors. Dense trajectories visualization.</caption>
      </object>
      <p>This work is published in the Gerontolgy Journal.</p>
    </subsection>
    <subsection id="uid136" level="1">
      <bodyTitle>Hybrid Approaches for Gender Estimation</bodyTitle>
      <participants>
        <person key="stars-2014-idp88472">
          <firstname>Antitza</firstname>
          <lastname>Dantcheva</lastname>
        </person>
        <person key="stars-2014-idp113792">
          <firstname>Piotr</firstname>
          <lastname>Bilinski</lastname>
        </person>
      </participants>
      <p noindent="true"><b>keywords:</b> gender estimation, soft biometrics, biometrics, visual attributes</p>
      <p>Automated gender estimation has numerous applications including video surveillance, human computer-interaction, anonymous customized advertisement, and image retrieval. Most commonly, the underlying algorithms analyze facial appearance for clues of gender.</p>
      <p noindent="true">
        <b>Can a smile reveal your gender? <ref xlink:href="#stars-2016-bid61" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, <ref xlink:href="#stars-2016-bid62" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></b>
      </p>
      <p>Deviating from such algorithms in <ref xlink:href="#stars-2016-bid61" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> we proposed a novel method for gender estimation, exploiting dynamic features gleaned from smiles and show that (a) facial dynamics incorporate gender clues, and (b) that while for adults appearance features are more accurate than dynamic features, for subjects under 18, facial dynamics outperform appearance features. While it is known that sexual dimorphism concerning facial appearance is not pronounced in infants and teenagers, it is interesting to see that facial dynamics provide already related clues.
The obtained results (see Table <ref xlink:href="#uid137" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>) show that smile-dynamic include pertinent and complementary to appearance gender information.
Such an approach is instrumental in cases of (a) omitted appearance-information (<i>e.g.</i> low resolution due to poor acquisition), (b) gender spoofing (<i>e.g.</i> makeup-based face alteration), as well as can be utilized to (c) improve the performance of appearance-based algorithms, since it provides complementary information.</p>
      <table id="uid137" rend="display">
        <tr style="border-top-style:solid;border-top-width:1px;border-bottom-style:solid; border-bottom-width:1px;" top-border="true" bottom-border="true">
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="center">
            <b>Age</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>
              <formula type="inline">
                <math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll">
                  <mrow>
                    <mo>&lt;</mo>
                    <mn>20</mn>
                  </mrow>
                </math>
              </formula>
            </b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>
              <formula type="inline">
                <math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll">
                  <mrow>
                    <mo>&gt;</mo>
                    <mn>19</mn>
                  </mrow>
                </math>
              </formula>
            </b>
          </td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="center">Subj. amount</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">143</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">214</td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="center">OpenBR</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <formula type="inline">
              <math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll">
                <mrow>
                  <mn>52</mn>
                  <mo>.</mo>
                  <mn>45</mn>
                  <mo>%</mo>
                </mrow>
              </math>
            </formula>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <formula type="inline">
              <math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll">
                <mrow>
                  <mn>78</mn>
                  <mo>.</mo>
                  <mn>04</mn>
                  <mo>%</mo>
                </mrow>
              </math>
            </formula>
          </td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="center">Dynamics (SVM+PCA) <ref xlink:href="#stars-2016-bid61" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <formula type="inline">
              <math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll">
                <mrow>
                  <mn>60</mn>
                  <mo>.</mo>
                  <mn>1</mn>
                  <mo>%</mo>
                </mrow>
              </math>
            </formula>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <formula type="inline">
              <math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll">
                <mrow>
                  <mn>69</mn>
                  <mo>.</mo>
                  <mn>2</mn>
                  <mo>%</mo>
                </mrow>
              </math>
            </formula>
          </td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="center">Dynamics (AdaBoost) <ref xlink:href="#stars-2016-bid61" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <formula type="inline">
              <math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll">
                <mrow>
                  <mn>59</mn>
                  <mo>.</mo>
                  <mn>4</mn>
                  <mo>%</mo>
                </mrow>
              </math>
            </formula>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <formula type="inline">
              <math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll">
                <mrow>
                  <mn>61</mn>
                  <mo>.</mo>
                  <mn>7</mn>
                  <mo>%</mo>
                </mrow>
              </math>
            </formula>
          </td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="center">OpenBR + Dynamics (Bagged Trees) <ref xlink:href="#stars-2016-bid61" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>
              <formula type="inline">
                <math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll">
                  <mrow>
                    <mn>60</mn>
                    <mo>.</mo>
                    <mn>8</mn>
                    <mo>%</mo>
                  </mrow>
                </math>
              </formula>
            </b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>
              <formula type="inline">
                <math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll">
                  <mrow>
                    <mn>80</mn>
                    <mo>.</mo>
                    <mn>8</mn>
                    <mo>%</mo>
                  </mrow>
                </math>
              </formula>
            </b>
          </td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="center">Motion-based descriptors <ref xlink:href="#stars-2016-bid62" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <formula type="inline">
              <math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll">
                <mrow>
                  <mn>77</mn>
                  <mo>.</mo>
                  <mn>7</mn>
                  <mo>%</mo>
                </mrow>
              </math>
            </formula>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <formula type="inline">
              <math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll">
                <mrow>
                  <mn>80</mn>
                  <mo>.</mo>
                  <mn>11</mn>
                  <mo>%</mo>
                </mrow>
              </math>
            </formula>
          </td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="center">Improved dynamics <ref xlink:href="#stars-2016-bid62" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <formula type="inline">
              <math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll">
                <mrow>
                  <mn mathvariant="bold">86</mn>
                  <mo>.</mo>
                  <mn mathvariant="bold">3</mn>
                  <mo>%</mo>
                </mrow>
              </math>
            </formula>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <formula type="inline">
              <math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll">
                <mrow>
                  <mn mathvariant="bold">91</mn>
                  <mo>.</mo>
                  <mn mathvariant="bold">01</mn>
                  <mo>%</mo>
                </mrow>
              </math>
            </formula>
          </td>
        </tr>
        <caption>True gender classification rates. Age given in years.</caption>
      </table>
      <p>We improve upon the above work by proposing a spatio-temporal features based on dense trajectories, represented by a set of descriptors encoded by Fisher Vectors <ref xlink:href="#stars-2016-bid62" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. Our results suggest that smile-based features include significant gender-clues. The designed algorithm obtains true gender classification rates of <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mn>86</mn><mo>.</mo><mn>3</mn><mo>%</mo></mrow></math></formula> for adolescents, significantly outperforming two state-of-the-art appearance-based algorithms (<i>OpenBR</i> and <i>how-old.net</i>), while for adults we obtain true gender classification rates of <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mn>91</mn><mo>.</mo><mn>01</mn><mo>%</mo></mrow></math></formula>, which is comparably discriminative to the better of these appearance-based algorithms (see Table <ref xlink:href="#uid137" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>).</p>
      <p spacebefore="14.22636pt"/>
      <p noindent="true">
        <b>Distance-based gender prediction: What works in different surveillance scenarios?</b>
      </p>
      <p>In this work <ref xlink:href="#stars-2016-bid63" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> we studied gender estimation based on information deduced jointly from face and body, extracted from single-shot images. The approach addressed challenging settings such as low-resolution-images, as well as settings when faces were occluded. Specifically the face-based features included local binary patterns (LBP) and scale-invariant feature transform (SIFT) features, projected into a PCA space. The features of the novel body-based algorithm proposed in this work included continuous shape information extracted from body silhouettes and texture information retained by HOG descriptors. Support Vector Machines (SVMs) were used for classification for body and face features. We conduct experiments on images extracted from video-sequences of the Multi-Biometric Tunnel database, emphasizing on three distance-settings: close, medium and far, ranging from full body exposure (far setting) to head and shoulders exposure (close setting). The experiments suggested that while face-based gender estimation performs best in the close-distance-setting, body-based gender estimation performs best when a large part of the body is visible. Finally we presented two score-level-fusion schemes of face and body-based features, outperforming the two individual modalities in most cases (see Table<ref xlink:href="#uid138" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> and Table <ref xlink:href="#uid139" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>).</p>
      <table id="uid138" rend="display">
        <tr style="border-top-style:solid;border-top-width:1px;border-bottom-style:solid; border-bottom-width:1px;" top-border="true" bottom-border="true">
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="center">Distance</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" cols="3" right-border="true" left-border="true" halign="center">FGE</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" cols="3" right-border="true" left-border="true" halign="center">BGE</td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="center">Scenario</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">Male TPR</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">Fem. TPR</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">Acc.</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">Male TPR</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">Fem. TPR</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">Acc.</td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="center">Far</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">94.28</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">20</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">57.14</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">87.14</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">88.57</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">87.85</td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="center">Medium</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">71.42</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">90</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">80.71</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">85.71</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">87.14</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">86.42</td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="center">Close</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">88.57</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">90</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">89.28</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">78.57</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">80</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">79.28</td>
        </tr>
        <caption>Performance (%) of the Face Gender Estimation algorithm (FGE) and the Body Gender Estimation algorithm (BGE).</caption>
      </table>
      <table id="uid139" rend="display">
        <tr style="border-top-style:solid;border-top-width:1px;border-bottom-style:solid; border-bottom-width:1px;" top-border="true" bottom-border="true">
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="center">Distance</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" cols="3" right-border="true" left-border="true" halign="center">Sum Fusion</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" cols="3" right-border="true" left-border="true" halign="center">Prop. Sum Fusion</td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="center">Scenario</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">Male TPR</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">Fem TPR</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">Acc.</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">Male TPR</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">Fem TPR</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">Acc.</td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="center">Far</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">87.14</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">88.57</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>87.85</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">87.14</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">88.57</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>87.85</b>
          </td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="center">Medium</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">88.57</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">90</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>89.28</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">88.57</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">90</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>89.28</b>
          </td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="center">Close</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">87.14</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">88.57</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">87.85</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">92.85</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">94.28</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>93.57</b>
          </td>
        </tr>
        <caption>Performance (%) of the Sum fusion and Smarter Sum Fusion of FGE and BGE in terms of True Positive Rate (TPR) for Male and Female (Fem.), overall Accuracy (Acc.). Best performance (in terms of Acc.) of each distance-setting is bolded.</caption>
      </table>
    </subsection>
    <subsection id="uid140" level="1">
      <bodyTitle>Unsupervised Metric Learning for Multi-shot Person Re-identification</bodyTitle>
      <participants>
        <person key="stars-2015-idp112000">
          <firstname>Furqan</firstname>
          <lastname>Khan</lastname>
        </person>
        <person key="PASUSERID">
          <firstname>François</firstname>
          <lastname>Brémond</lastname>
        </person>
      </participants>
      <p noindent="true"><b>keywords:</b> re-identification, long term visual tracking, metric learning, unsupervised labeling</p>
      <p noindent="true">
        <b>Automatic label generation for metric learning</b>
      </p>
      <p>Appearance based person re-identification is a challenging task, specially due to difficulty in capturing high intra-person appearance variance across cameras when inter-person similarity is also high. Metric learning is often used to address deficiency of low-level features by learning view specific re-identification models. The models are often acquired using a supervised algorithm. This is not practical for real-world surveillance systems because annotation effort is view dependent. Therefore, everytime a camera is replaced or added, a significant amount of data has to be annotated again. We propose a strategy to automatically generate labels for person tracks to learn similarity metric for multi-shot person re-identification task. Specifically, we use the fact that non-matching (negative) pairs far out-number matching (positive) pairs in any training set. Therefore, the true class conditional probability of distance given negative class can be estimated using the empirical marginal distribution of distance. This distribution can be used to sample non-matching person pairs for metric learning. A brief overview of the approach is presented below, please refer to <ref xlink:href="#stars-2016-bid64" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> for details.</p>
      <object id="uid141">
        <table>
          <tr>
            <td>
              <ressource xlink:href="IMG/furkan_img.png" type="float" width="427.0pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
            </td>
          </tr>
        </table>
        <caption>Distributions of distances between pairs of signature of randomly selected half of a) PRID, and b) iLIDS-VID datasets for MCM representation using Euclidean distance. The distributions are averaged for 10 trials.</caption>
      </object>
      <p>In figure <ref xlink:href="#uid141" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, empirical distribution of Euclidean distance (using MCM <ref xlink:href="#stars-2016-bid65" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> representation) is plotted for two publicly available datasets. It can be noted that the positive samples lie on one side of distribution mode. Therefore, negative pairs can be sampled according to the probability proportional to the signed distance from the mode. Practically, we only select sample pairs that are farthest away in the distribution as negative pairs. For positive pairs, we use the fact that each track has more than one image for a person. Thus we generate positive pairs using the persons selected for negative pairs.
We evaluated our approach on three publicly available datasets in multi-shot settings: iLIDS-VID, PRID and iLIDS-AA.
Performance comparison of different representations using recognition rates at rank <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>r</mi></math></formula> are detailed in table  <ref xlink:href="#uid142" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, table  <ref xlink:href="#uid143" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>
and table  <ref xlink:href="#uid144" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.
Our results validate the effectiveness of our approach by considerably reducing the performance gap between fully-supervised models using KISSME algorithm and Euclidean distance.</p>
      <table id="uid142" rend="display">
        <tr style="border-top-style:solid;border-top-width:1px;border-bottom-style:solid; border-bottom-width:1px;" top-border="true" bottom-border="true">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">Method</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">r=1</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">r=5</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">r=10</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">r=20</td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">MCM+MPD</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">53.6</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">83.1</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">91.0</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">96.9</td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">MCM+UnKISSME</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">59.2</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">81.7</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">90.6</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">96.1</td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">MCM+KISSME</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">64.3</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">86.1</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">94.5</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">98.0</td>
        </tr>
        <caption>PRID</caption>
      </table>
      <table id="uid143" rend="display">
        <tr style="border-top-style:solid;border-top-width:1px;border-bottom-style:solid; border-bottom-width:1px;" top-border="true" bottom-border="true">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">Method</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">r=1</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">r=5</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">r=10</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">r=20</td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">MCM+MPD</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">34.3</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">61.5</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">74.4</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">83.3</td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">MCM+UnKISSME</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">38.2</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">65.7</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">75.9</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">84.1</td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">MCM+KISSME</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">40.3</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">69.9</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">79.0</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">87.5</td>
        </tr>
        <caption>iLIDA-VID</caption>
      </table>
      <table id="uid144" rend="display">
        <tr style="border-top-style:solid;border-top-width:1px;border-bottom-style:solid; border-bottom-width:1px;" top-border="true" bottom-border="true">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">Method</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">r=1</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">r=5</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">r=10</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">r=20</td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">MCM+MPD</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">56.5</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">79.7</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">90.9</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">95.2</td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">MCM+UnKISSME</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">61.2</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">85.1</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">92.8</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">96.0</td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">MCM+KISSME</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">62.9</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">84.7</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">93.4</td>
          <td style="text-align:right;border-right-style:solid;border-right-width:1px;" right-border="true" halign="right">97.0</td>
        </tr>
        <caption>iLIDS-AA</caption>
      </table>
    </subsection>
    <subsection id="uid145" level="1">
      <bodyTitle>Semi-supervised Understanding of Complex Activities in Large-scale Datasets</bodyTitle>
      <participants>
        <person key="PASUSERID">
          <firstname>Carlos F.</firstname>
          <lastname>Crispim-Junior</lastname>
        </person>
        <person key="stars-2014-idp123800">
          <firstname>Michal</firstname>
          <lastname>Koperski</lastname>
        </person>
        <person key="PASUSERID">
          <firstname>Serhan</firstname>
          <lastname>Cosar</lastname>
        </person>
        <person key="PASUSERID">
          <firstname>François</firstname>
          <lastname>Brémond</lastname>
        </person>
      </participants>
      <p noindent="true"><b>keywords:</b> Semi-supervised methods, activity understanding, probabilistic models, pairwise graphs</p>
      <p noindent="true">
        <b>Informations </b>
      </p>
      <p>Methods for action recognition have evolved considerably over the past years and can now automatically learn and recognize short term actions with satisfactory accuracy. Nonetheless, the recognition of complex activities - compositions of actions and scene objects - is still an open problem due to the complex temporal and composite structure of this category of events. Existing methods focus either on simple activities or oversimplify the modeling of complex activities by targeting only whole-part relations between its sub-parts (<i>e.g.</i>, actions). We study a semi-supervised approach (Fig.  <ref xlink:href="#uid146" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>) that can learn complex activities from the temporal patterns of concept compositions in different arities (<i>e.g.</i>, “slicing-tomato” before “pouring_into-pan”). So far, our semi-supervised, probabilistic model using pairwise relations both in compositional and temporal axis outperforms prior work by 6 % (59% against 53%, mean Average precision, Fig.  <ref xlink:href="#uid147" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>). Our method also stands out from the competition by its capability to handle relation learning in a setting with large number of video sequences (<i>e.g.</i>, 256) and distinct concept classes (Cooking Composite dataset, 218 classes, <ref xlink:href="#stars-2016-bid66" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>), an ability that current state-of-the-art methods lack. Our initial achievements in this line of research has been published in <ref xlink:href="#stars-2016-bid67" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. Further work will focus on learning relations of higher arity.</p>
      <object id="uid146">
        <table>
          <tr>
            <td>
              <ressource xlink:href="IMG/Approach_v2_1.jpg" type="float" scale="0.4" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
            </td>
          </tr>
        </table>
        <caption>Semi-supervised learning of a video representation: 1) video temporal segmentation, 2) concept recognition 3) composite concept generation per time segment, 4) Temporal composite generation between segments.</caption>
      </object>
      <object id="uid147">
        <table>
          <tr>
            <td>
              <ressource xlink:href="IMG/benchmarking.png" type="float" scale="0.5" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
            </td>
          </tr>
        </table>
        <caption>Performance benchmarking of our approach against data set baselines: <i>a</i>) Nearest Neighbor classifier (NN) on concepts, script data, and tf*idf-WN, and <i>b</i>) NN only on concepts.</caption>
      </object>
    </subsection>
    <subsection id="uid148" level="1">
      <bodyTitle>On the Study of the Visual Behavioral Roots of Alzheimer's disease</bodyTitle>
      <participants>
        <person key="PASUSERID">
          <firstname>Carlos F.</firstname>
          <lastname>Crispim-Junior</lastname>
        </person>
        <person key="PASUSERID">
          <firstname>François</firstname>
          <lastname>Brémond</lastname>
        </person>
      </participants>
      <p noindent="true"><b>Keywords:</b> Activities of Daily Living, Dementia prediction, RGBD sensors, Activity Recognition, Cognitive Health</p>
      <p>Existing computer vision studies for the diagnosis of Dementia have focused on extracting discriminative patterns between healthy and people with dementia from neuroimagery exams, like functional MRI and PET scans. Nonetheless, the effects of dementia over human behaviors are a discriminative component that is barely explored by automatic vision-based methods. We studied a framework to automatically recognize the cognitive health of seniors from the visual observation of their activities of daily living (Fig.<ref xlink:href="#uid150" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>). We employ a lightweight activity recognition system based on RGBD sensors to recognize the set of target activities (<i>e.g.</i>, prepare drink, prepare medication, make a payment transaction) performed by a person in a continuous video stream. Then, we summarize the absolute and relative activity patterns present in the video sequence using a novel probabilistic representation of activity patterns. Finally, this representation serves as input to Random Forest classifiers to predict the class of cognitive health that the person in question belongs to. We demonstrate that with the current framework can recognized the cognitive health status of seniors (<i>e.g.</i>, healthy, Mild Cognitive Impairment and Alzheimer's disease) with an average F<formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msub><mrow/><mn>1</mn></msub></math></formula>-score of 69 % in real life scenarios.</p>
      <object id="uid149">
        <table>
          <tr>
            <td>
              <ressource xlink:href="IMG/pipeline.png" type="float" width="427.0pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
            </td>
          </tr>
        </table>
        <caption>Automatic framework for visual recognition of cognitive health status: visual event recognition is responsible to detect and track people in the scene and recognize their events based on spatio-temporal relations with scene objects. Cognitive health classification represents absolute and relative information about the target classes.</caption>
      </object>
      <object id="uid150">
        <table>
          <tr>
            <td>
              <ressource xlink:href="IMG/GAADRD.png" type="float" width="256.2026pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
            </td>
          </tr>
        </table>
        <caption>Monitoring a senior performing at a gait-related event</caption>
      </object>
    </subsection>
    <subsection id="uid151" level="1">
      <bodyTitle>Uncertainty Modeling with Ontological Models and Probabilistic Logic Programming</bodyTitle>
      <participants>
        <person key="PASUSERID">
          <firstname>Carlos F.</firstname>
          <lastname>Crispim-Junior</lastname>
        </person>
        <person key="PASUSERID">
          <firstname>François</firstname>
          <lastname>Brémond</lastname>
        </person>
      </participants>
      <p noindent="true"><b>keywords:</b> probabilistic logic programming, activities of daily living, senior monitoring, ontological models,</p>
      <p>We have been investigating novel probabilistic, knowledge-driven formalisms that can join the representation expressiveness of an ontology-based language with the probabilistic reasoning of probabilistic graphical models, like probabilistic graphical models and probabilistic programming languages. The goal is to support the representation of events (entities, sub-events and constraints) and hierarchical structures (event, sub-events) and at the same time be capable of handling uncertainty related to both entity/sub-event detection and soft constraints. Prior work in probabilistic logic provides support to reasoning either about uncertainty related to entity recognition (probability of entity x in the scene defined in ProbLog2) or to soft-constraint (relevance of violation of constraint <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>i</mi></math></formula> to model <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>y</mi></math></formula> as defined in Markov Logic). In our current work in partnership with KU university of Leuven, we have extended the ontological models of our vision pipeline (Fig.<ref xlink:href="#uid152" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>) with probabilistic logic formalism proposed by ProbLog (Fig.<ref xlink:href="#uid153" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>), a probabilistic logic programming language. Current results on the recognition of daily activities of seniors are promising as they improved the precision of our prior method by 1%. Further work will focus on extending our uncertainty models to be robust to constraint violations.</p>
      <object id="uid152">
        <table>
          <tr>
            <td>
              <ressource xlink:href="IMG/PipelineVision.jpg" type="float" width="341.6013pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
            </td>
          </tr>
        </table>
        <caption>Pipeline for online activity recognition: given an acquisition camera (e.g. a Kinect), it firstly detects people using background subtraction algorithm, then it looks for appearance correspondence between people detected in the current frame with respect to past detections (past-present approach), and thirdly it recognizes the activities performed by each of the tracked people.</caption>
      </object>
      <object id="uid153">
        <table>
          <tr>
            <td>
              <ressource xlink:href="IMG/PipelineProbLog.png" type="float" width="298.8987pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
            </td>
          </tr>
        </table>
        <caption>Temporal Inference using ProbLog engine. It takes as input deterministic observations and frame-wisely it recognizes the target events. Frame-events are aggregated into time intervals to create the time intervals of complex activities.</caption>
      </object>
    </subsection>
    <subsection id="uid154" level="1">
      <bodyTitle>A Hybrid Framework for Online Recognition of Activities of Daily Living In Real-World Settings</bodyTitle>
      <participants>
        <person key="stars-2014-idp125016">
          <firstname>Farhood</firstname>
          <lastname>Negin</lastname>
        </person>
        <person key="PASUSERID">
          <firstname>Serhan</firstname>
          <lastname>Cosar</lastname>
        </person>
        <person key="stars-2014-idp123800">
          <firstname>Michal</firstname>
          <lastname>Koperski</lastname>
        </person>
        <person key="PASUSERID">
          <firstname>Carlos</firstname>
          <lastname>Crispim</lastname>
        </person>
        <person key="PASUSERID">
          <firstname>Konstantinos</firstname>
          <lastname>Avgerinakis</lastname>
        </person>
        <person key="PASUSERID">
          <firstname>François</firstname>
          <lastname>Brémond</lastname>
        </person>
      </participants>
      <p noindent="true"><b>keywords:</b> Supervised and Unsupervised Learning, Activity Recognition</p>
      <p noindent="true">
        <b>State-of-the-art and Current Challenges</b>
      </p>
      <p>Recognizing human actions from videos has been an active
research area for the last two decades. With many application
areas, such as surveillance, smart environments and
video games, human activity recognition is an important
task involving computer vision and machine learning. Not
only the problems related to image acquisition, e.g., camera
view, lighting conditions, but also the complex structure
of human activities makes activity recognition a very challenging
problem.
Traditionally, there are two variants of approach to cope
with these challenges: supervised and unsupervised methods.
Supervised approaches are suitable for recognizing
short-term actions. For training, these approaches require
a huge amount of user interaction to obtain well-clipped
videos that only include a single action. However, Activities of Daily Living (ADL)
consists of many simple actions which form a complex activity.
Therefore, the representation in supervised approaches
are insufficient to model these activities and a training set
of clipped videos for ADL cannot cover all the variations.
In addition, since these methods require manually clipped
videos, they can only follow an offline recognition scheme.
On the other hand, unsupervised approaches are strong in
finding spatio-temporal patterns of motion. However, the
global motion patterns are not enough to obtain a precise
classification of ADL. For long-term activities, there are
many unsupervised approaches that model global motion
patterns and detect abnormal events by finding the trajectories
that do not fit in the pattern <ref xlink:href="#stars-2016-bid68" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, <ref xlink:href="#stars-2016-bid69" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. Many methods have
been applied on traffic surveillance videos to learn the regular
traffic dynamics (e.g. cars passing a cross road) and detect
abnormal patterns (e.g. a pedestrian crossing the road)
<ref xlink:href="#stars-2016-bid70" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
      <p noindent="true">
        <b>Proposed Method</b>
      </p>
      <p>We propose a hybrid method to exploit the benefits of both
approaches. With limited user interaction our framework
recognizes more precise activities compared to available approaches.
We use the term precise to indicate that, unlike
most of trajectory-based approaches which cannot distinguish
between activities under same region, our approach
can be more sensitive in the detection of activities thanks to
local motion patterns.
We can summarize the contributions
of this work as following: i) online recognition of activities
by automatic clipping of long-term videos and ii) obtaining
a comprehensive representation of human activities with
high discriminative power and localization capability.</p>
      <object id="uid155">
        <table>
          <tr>
            <td>
              <ressource xlink:href="IMG/Farhood_picture1.jpg" type="float" width="427.0pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
            </td>
          </tr>
        </table>
        <caption>Architecture of the framework: Training and Testing phases</caption>
      </object>
      <p>Figure <ref xlink:href="#uid155" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> illustrates the flow of the training and testing
phases in the proposed framework. For the training phase,
the algorithm learns relevant zones in the scene and generates
activity models for each zone by complementing the models with information such as duration distribution and
BoW representations of discovered activities. At testing,
the algorithm compares the test instances with the generated
activity models and infers the most similar model.</p>
      <p>The performance of the proposed approach has been
tested on the public GAADRD dataset <ref xlink:href="#stars-2016-bid71" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> and CHU
dataset. Our approach always performs equally or better than online
supervised approach in <ref xlink:href="#stars-2016-bid12" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> (see Table<ref xlink:href="#uid156" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> and Table<ref xlink:href="#uid157" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>). And even
most of the time it outperforms totally supervised approach
(manually clipped) of <ref xlink:href="#stars-2016-bid12" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. This reveals the effectiveness of
our hybrid technique where combining information coming
from both constituents could contribute to enhance recognition.
The paper of this work was accepted in AVSS 2016 conference <ref xlink:href="#stars-2016-bid72" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
      <table id="uid156" rend="display">
        <tr style="border-top-style:solid;border-top-width:1px;" top-border="true">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left"/>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" cols="2" right-border="true" left-border="true" halign="center">Supervised (Manually Clipped)</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" cols="2" right-border="true" left-border="true" halign="center">Online Version</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" cols="2" right-border="true" left-border="true" halign="center">Unsupervised Using</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" cols="2" right-border="true" left-border="true" halign="center">Proposed Approach</td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left"/>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" cols="2" right-border="true" left-border="true" halign="center">of <ref xlink:href="#stars-2016-bid12" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" cols="2" right-border="true" left-border="true" halign="center">of <ref xlink:href="#stars-2016-bid12" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" cols="2" right-border="true" left-border="true" halign="center">Global Motion <ref xlink:href="#stars-2016-bid73" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" cols="2" right-border="true" left-border="true" halign="center"/>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">
            <em style="UNDERLINE">ADLs</em>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <em style="UNDERLINE">Recall (%)</em>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <em style="UNDERLINE">Prec. (%)</em>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <em style="UNDERLINE">Recall (%)</em>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <em style="UNDERLINE">Prec. (%)</em>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <em style="UNDERLINE">Recall (%)</em>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <em style="UNDERLINE">Prec. (%)</em>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <em style="UNDERLINE">Recall (%)</em>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <em style="UNDERLINE">Prec. (%)</em>
          </td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">Answering Phone</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">57</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">78</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>100</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>86</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>100</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">60</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>100</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">81.82</td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">P. Tea + W. Plant</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">89</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>86.5</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">76</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">38</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">84.21</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">80</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>94.73</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">81.81</td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">Using Phar. Basket</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>100</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">83</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>100</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">43</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">90</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>100</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>100</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>100</b>
          </td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">Reading</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">35</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>100</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">92</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">36</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">81.82</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>100</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>100</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">91.67</td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">Using Bus Map</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">90</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>90</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>100</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">50</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>100</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">54.54</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>100</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">83.34</td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">AVERAGE</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">74.2</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">87.5</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">93.6</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">50.6</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">91.2</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">78.9</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>98.94</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>87.72</b>
          </td>
        </tr>
        <caption>The activity recognition results for CHU dataset. Bold values represent the best sensitivity and precision results for each class.</caption>
      </table>
      <table id="uid157" rend="display">
        <tr style="border-top-style:solid;border-top-width:1px;" top-border="true">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left"/>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" cols="2" right-border="true" left-border="true" halign="center">Supervised (Manually Clipped)</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" cols="2" right-border="true" left-border="true" halign="center">Online Version</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" cols="2" right-border="true" left-border="true" halign="center">Classification by</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" cols="2" right-border="true" left-border="true" halign="center">Unsupervised Using</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" cols="2" right-border="true" left-border="true" halign="center">Proposed Approach</td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left"/>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" cols="2" right-border="true" left-border="true" halign="center">Approach <ref xlink:href="#stars-2016-bid12" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" cols="2" right-border="true" left-border="true" halign="center">of <ref xlink:href="#stars-2016-bid12" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" cols="2" right-border="true" left-border="true" halign="center">detection using SSBD <ref xlink:href="#stars-2016-bid74" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" cols="2" right-border="true" left-border="true" halign="center">Global Motion <ref xlink:href="#stars-2016-bid73" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" cols="2" right-border="true" left-border="true" halign="center"/>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">
            <em style="UNDERLINE">ADLs</em>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <em style="UNDERLINE">Recall (%)</em>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <em style="UNDERLINE">Prec. (%)</em>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <em style="UNDERLINE">Recall (%)</em>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <em style="UNDERLINE">Prec. (%)</em>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <em style="UNDERLINE">Recall (%)</em>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <em style="UNDERLINE">Prec. (%)</em>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <em style="UNDERLINE">Recall (%)</em>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <em style="UNDERLINE">Prec. (%)</em>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <em style="UNDERLINE">Recall (%)</em>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <em style="UNDERLINE">Prec. (%)</em>
          </td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">Answering Phone</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>100</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">88</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>100</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">70</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">96</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">34.29</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>100</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>100</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>100</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">88</td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">Establish Acc. Bal.</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">67</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>100</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>100</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">29</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">41.67</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">41.67</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>100</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">86</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">67</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>100</b>
          </td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">Preparing Drink</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>100</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">69</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>100</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">69</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">96</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">80</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">78</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>100</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>100</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">82</td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">Prepare Drug Box</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">58.33</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>100</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">11</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">20</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>86.96</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">51.28</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">33.34</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>100</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">22.0</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>100</b>
          </td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">Watering Plant</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">54.54</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>100</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">0</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">0</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>86.36</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">86.36</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">44.45</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">57</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">44.45</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>80</b>
          </td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">Reading</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>100</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>100</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">88</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">37</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>100</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">31.88</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>100</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>100</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>100</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>100</b>
          </td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">Turn On Radio</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">60</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">86</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <em style="UNDERLINE">
              <b>100</b>
            </em>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">75</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">96.55</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">19.86</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">89</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">89</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">89</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">89</td>
        </tr>
        <tr style="border-bottom-style:solid; border-bottom-width:1px;" bottom-border="true">
          <td style="text-align:left;border-right-style:solid;border-right-width:1px;border-left-style:solid;border-left-width:1px;" right-border="true" left-border="true" halign="left">AVERAGE</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">77.12</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>91.85</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">71.29</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">42.86</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">
            <b>86.22</b>
          </td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">49.33</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">77.71</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">90.29</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">74.57</td>
          <td style="text-align:center;border-right-style:solid;border-right-width:1px;" right-border="true" halign="center">91.29</td>
        </tr>
        <caption>The activity recognition results for GAADRD dataset. Bold values represent the best sensitivity and precision results for each class.</caption>
      </table>
    </subsection>
    <subsection id="uid158" level="1">
      <bodyTitle>Praxis and Gesture Recognition</bodyTitle>
      <participants>
        <person key="stars-2014-idp125016">
          <firstname>Farhood</firstname>
          <lastname>Negin</lastname>
        </person>
        <person key="PASUSERID">
          <firstname>Jeremy</firstname>
          <lastname>Bourgeois</lastname>
        </person>
        <person key="PASUSERID">
          <firstname>Emmanuelle</firstname>
          <lastname>Chapoulie</lastname>
        </person>
        <person key="rap-2014-idm40528">
          <firstname>Philippe</firstname>
          <lastname>Robert</lastname>
        </person>
        <person key="PASUSERID">
          <firstname>François</firstname>
          <lastname>Brémond</lastname>
        </person>
      </participants>
      <p noindent="true"><b>keywords:</b> Gesture Recognition, Dynamic and Static Gesture, Alzheimer Disease, Reaction Time, Motion Descriptors</p>
      <p noindent="true">
        <b>Challenges and Proposed Method</b>
      </p>
      <p>Most of the developed societies are experiencing an aging trend of their population. Aging is correlated with cognitive impairment such as dementia and its most common type: Alzheimer's disease. So, there is an urgent need to develop technological tools to help doctors to do early and precise diagnoses of cognitive decline.
Inability to correctly perform purposeful skilled movements with hands and other forelimbs most commonly is associated with Alzheimerâs disease <ref xlink:href="#stars-2016-bid75" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. These patients have difficulty to correctly imitate hand gestures and mime tool use, e.g. pretend to brush one's hair. They make spatial and temporal errors.
We propose a gesture recognition and evaluation framework as a complementary tool to help doctors to spot symptoms of cognitive impairment at its early stages. It is also useful to assess one's cognitive status. First, the algorithm classifies the defined gestures in the gestures set and then it evaluates gestures of the same category to see how well they perform compared to correct gesture templates.
Methods Shape and motion descriptors such as HOG (histogram of oriented gradient) <ref xlink:href="#stars-2016-bid14" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> and HOF (histogram of optical flow) <ref xlink:href="#stars-2016-bid13" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> are an efficient clue to characterize different gestures (Figure <ref xlink:href="#uid159" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> Left). Extracted descriptors are utilized as input to train the classifiers. We use bag-of-visual-words approach to characterize gestures with descriptors. The classification happens in two steps: first we train a classifier to distinguish different gestures and after, we train another classifier with correct and incorrect samples of the same class. This way, we could recognize which gesture is performed and whether it is performed accurately or not.</p>
      <object id="uid159">
        <table>
          <tr>
            <td>
              <ressource xlink:href="IMG/Farhood_picture2.jpg" type="float" width="427.0pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
            </td>
          </tr>
        </table>
        <caption>Left: Extracted motion descriptors while performing a gesture Right: virtual avatar guides patients in a virtual reality environment</caption>
      </object>
      <p noindent="true">
        <b>Experiments and Results</b>
      </p>
      <p>The framework is fed by input data which come from a depth sensor (Kinect, Microsoft). At first phase, the correct samples of gestures performed by clinicians, are recorded. We train the framework using correct instances of each gesture class. In the second phase, participants were asked to perform the gestures. We use virtual reality as modality to interact with subjects to make the experiments more immersive and realistic experience. First an avatar performs a specific gesture and then she asks the subject to repeat the same gesture (Figure <ref xlink:href="#uid159" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> Right).
In this work, we analyze two categories of gestures. First category is dynamic gestures where the whole motion of the hands is considered as a complete gesture. Second category of gestures is static gestures where only a static pose of hands is the desired gesture. For static gestures, we also need to detect this key frame. Moreover, reaction time which starts after avatar asked the subject to do the gesture, until subject really starts to perform the gesture, could be an important diagnostic factor. Our algorithm uses motion descriptors to detect key frames and reaction time. In the preliminary tests, our framework successfully recognized more than 80% of the dynamic gestures. It also detects key frames and reaction time with a high precision.
Thus the proposed gesture recognition framework helps doctors by providing a complete assessment of gestures performed by subject.</p>
      <p>This work is published in <ref xlink:href="#stars-2016-bid72" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> and will appear in the Gerontechnology Journal.</p>
    </subsection>
    <subsection id="uid160" level="1">
      <bodyTitle>Scenario Recognition</bodyTitle>
      <participants>
        <person key="stars-2014-idp94784">
          <firstname>Inès</firstname>
          <lastname>Sarray</lastname>
        </person>
        <person key="stars-2014-idp64040">
          <firstname>Sabine</firstname>
          <lastname>Moisan</lastname>
        </person>
        <person key="stars-2014-idp65472">
          <firstname>Annie</firstname>
          <lastname>Ressouche</lastname>
        </person>
        <person key="stars-2015-idp7880">
          <firstname>Jean-Paul</firstname>
          <lastname>Rigault</lastname>
        </person>
      </participants>
      <p noindent="true"><b>Keywords:</b> Synchronous Modeling, Model checking, Mealy machine, Cognitive systems.</p>
      <p>Activity recognition systems aim at recognizing the intentions and activities of one or more persons in real life, by analyzing their actions and the evolution of the environment. This is done thanks to a pattern matching and clustering algorithms, combined with adequate knowledge representation (e.g scene topology, temporal constraints) at different abstraction levels ( from raw signal to semantics).Stars has been working to ameliorate and facilitate the generation of these activity recognition systems.
As we can use these systems in a big range of important fields, we propose a generic approach to design activity recognition engine. These engines should continuously and repeatedly interact with their environment and react to its stimuli. On the other hand, we should take into consideration the dependability of these engines which is very important to avoid possible safety issue, that’s why we need also to rely on formal methods that allow us to verify these engines behavior.
Synchronous modeling is a solution that allows us to create formal models that describe clearly the system behavior and its reactions when it detects different stimuli. Using these formal models, we can build effective recognition engines for each formal model and validate them easily using model checking.
This year, we adapted this approach to create a new simple scenario language to express the scenario behaviors and to automatically generate its recognition automata at compile time. This automata will be embedded into the recognition engine at runtime.</p>
      <p noindent="true">
        <b>Scenario description Language</b>
      </p>
      <p>As we work with non-computer-science end-users, we need a friendly description language that helps them to express easily their scenarios. To this aim, we collaborated with Ludotic ergonomists to define the easiest way for a simple user to deal with the new language.
Using AxureRP tool, we defined two types of language:</p>
      <p noindent="true"><i>1- Textual language</i>:</p>
      <object id="uid161">
        <table>
          <tr>
            <td>
              <ressource xlink:href="IMG/figure1_textual_language.png" type="float" width="298.8987pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
            </td>
          </tr>
        </table>
        <caption>Example of the textual language</caption>
      </object>
      <p>For the textual language, we decided to use a simple language. Using 9 operators, and after the definition of the types, roles, and sub-scenarios, the user can describe a scenario in a simple way, such as in figure<ref xlink:href="#uid161" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
      <p>This year, we implemented this textual language and it is under testing.</p>
      <p noindent="true"><i>2)- Graphical language</i>:</p>
      <object id="uid162">
        <table>
          <tr>
            <td>
              <ressource xlink:href="IMG/figure_2_generic_flow_chart.png" type="float" width="427.0pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
            </td>
          </tr>
        </table>
        <caption>Generic flowchart</caption>
      </object>
      <p>The graphical language model has 3 basic interfaces: The first interface allows the user to define the types, roles, and the initial state of the scenario. The second one is dedicated to describe the sub-scenarios and to express simple scenarios using a timeline.
In case of complicated scenarios, the third interface offers users a tool panel that allows them to describe their scenarios in a hierarchical way using a flowchart-like representation (see figure  <ref xlink:href="#uid162" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>).</p>
      <p noindent="true">
        <b>Recognition Automata</b>
      </p>
      <p>This year, we worked also on recognition automata generation. We used the synchronous modeling and semantics to define these engines. The semantics consists in a set of formal rules that describe the behavior of a program. We specified first the language operators: we rely on a 4-valued algebra with a bilattice structure to define two semantics for the recognition engine: a behavioral and equational one. A behavioral semantics defines the behavior of a program and its operators and gives it a clear interpretation. Equational semantics allows us to make a modular compilation of our programs using rules that translate each program into an equation system.
After defining these two semantics, we verified their equivalence for all operators, by proving that these semantics agree on both the set of emitted signals and the termination value for a program P. We implemented these semantics and we are now working on the automatic generation of the recognition automata.</p>
    </subsection>
    <subsection id="uid163" level="1">
      <bodyTitle>The Clem Workflow</bodyTitle>
      <participants>
        <person key="stars-2014-idp65472">
          <firstname>Annie</firstname>
          <lastname>Ressouche</lastname>
        </person>
        <person key="PASUSERID">
          <firstname>Daniel</firstname>
          <lastname>Gaffé</lastname>
        </person>
      </participants>
      <p noindent="true"><b>Keywords:</b> Synchronous languages, Synchronous Modeling, Model checking, Mealy machine.</p>
      <object id="uid164">
        <table>
          <tr>
            <td>
              <ressource xlink:href="IMG/clem-toolkit.jpg" type="float" width="213.5pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
            </td>
          </tr>
        </table>
        <caption>The Clem Toolkit</caption>
      </object>
      <p>This research axis concerns the theoretical study of a synchronous language <span class="smallcap" align="left">le</span> with
modular compilation and the development of a toolkit around the language (see Figure  <ref xlink:href="#uid164" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>) to
design, simulate, verify, and generate code for programs.
The novelty of the approach is the ability to manage both modularity and causality.</p>
      <p>This year, we continued to focus on the improvement of both <span class="smallcap" align="left">le</span> language and compiler concerning data handling and the generation
of back-ends, required by other research axis of the team. We also designed a large application: a mechatronics system in <span class="smallcap" align="left">clem</span> and
we have proved that its main safety properties hold in our modeling. Now, to complete the improvement done these two last years concerning data handling,
we want to extend the verification side of <span class="smallcap" align="left">clem</span>. To this aim, this year we began to replace the fundamental representation of Boolean values as
BDD (Binary Decision Diagrams) with LDD (Logical Decision Diagrams), which allow to encode integer values in a very efficient way. It turns out that
the validation mechanism of <span class="smallcap" align="left">clem</span> could take into account properties over integer data. However, this is a first test and the integration of a model checking technique in <span class="smallcap" align="left">clem</span> remains a challenge.</p>
    </subsection>
    <subsection id="uid165" level="1">
      <bodyTitle>Safe Composition in Middleware for Internet of Things</bodyTitle>
      <participants>
        <person key="stars-2014-idp65472">
          <firstname>Annie</firstname>
          <lastname>Ressouche</lastname>
        </person>
        <person key="PASUSERID">
          <firstname>Daniel</firstname>
          <lastname>Gaffé</lastname>
        </person>
        <person key="stars-2015-idp172352">
          <firstname>Jean-Yves</firstname>
          <lastname>Tigli</lastname>
        </person>
      </participants>
      <p noindent="true"><b>Keywords:</b> Synchronous Modeling, Ubiquitous Computing, middleware, internet of things</p>
      <p>The main concern of this research axis is the dependability of a component-based adaptive middleware which dynamically adapt and recompose assemblies of web components.
Such a middleware plays an important role in the generation of event recognition engines we are currently building in Stars team (see section <ref xlink:href="#uid160" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>).
One of the main challenge is how to guarantee and validate some safety
and integrity properties throughout the system's evolution.
These two last years, we have proposed to rely on synchronous models to represent component behavior and their composition and to verify that these compositions verify some constraints during the dynamic adaptation to appearance and disappearance of components. We defined a generic way to express these constraints and we proposed the Description Constraint Language (<span class="smallcap" align="left">dcl</span>) to express these constraints. Hence, we compile them into <span class="smallcap" align="left">le</span> programs
(see <ref xlink:href="#uid163" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>) and we benefit from <span class="smallcap" align="left">clem</span> model checking facilities to ensure that they are respected <ref xlink:href="#stars-2016-bid76" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.
This year, we improved the <span class="smallcap" align="left">dcl</span> language in order to take into account both the dynamic variation of components and also applications which use these components and we are currently testing the efficiency of our method to
add and remove components.
Moreover, genericity is expressed by the notion of type and we aim at extending this notion to a thinner representation of knowledge about components.
</p>
    </subsection>
    <subsection id="uid166" level="1">
      <bodyTitle>Verification of Temporal Properties of Neuronal Archetypes</bodyTitle>
      <participants>
        <person key="stars-2014-idp65472">
          <firstname>Annie</firstname>
          <lastname>Ressouche</lastname>
        </person>
        <person key="PASUSERID">
          <firstname>Daniel</firstname>
          <lastname>Gaffé</lastname>
        </person>
      </participants>
      <p noindent="true"><b>Keywords:</b> Synchronous Modeling, model-checking, lustre, temporal logic, biologic archetypes</p>
      <p>This year, we began a collaboration with with the I3S CNRS laboratory and Jean Dieudonné CNRS laboratory to verify temporal properties of neuronal archetypes. There exist many ways to connect two, three or more neurons together to form different graphs. We call archetypes only the graphs whose properties can be associated to specific classes of biologically relevant structures and behaviors. These archetypes are supposed to be the basis of typical instances of neuronal information processing.
To model different representative archetypes and express their temporal properties, we use a synchronous programming language dedicated to reactive systems (Lustre). Then, we generate several back ends to interface different
model checkers supporting data types and automatically validate these properties. We compare the respective results. They mainly depend on the underlying abstraction methods used in model checkers.</p>
      <p>These results are published in <ref xlink:href="#stars-2016-bid77" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
    </subsection>
    <subsection id="uid167" level="1">
      <bodyTitle>Dynamic Reconfiguration of Feature Models</bodyTitle>
      <participants>
        <person key="stars-2014-idp64040">
          <firstname>Sabine</firstname>
          <lastname>Moisan</lastname>
        </person>
        <person key="stars-2015-idp7880">
          <firstname>Jean-Paul</firstname>
          <lastname>Rigault</lastname>
        </person>
      </participants>
      <p noindent="true"><b>Keywords:</b> feature models, model at run time, self-adaptive systems</p>
      <p>In video understanding systems, context changes (detected by system sensors)
are often unexpected and can combine in unpredictable ways, making it difficult
to determine in advance (off line) the running configuration suitable for each
context combination. To address this issue, we keep, at run time, a model of
the system and its context together with its current running configuration. We
adopted an enriched Feature Model approach to express the variability of the
architecture as well as of the context. A context change is transformed into a
set of feature modifications (selection/deselection of features) to be
processed on the fly. This year we proposed a fully automatic mechanism to
compute at run time the impact of the current selection/deselection requests.
First, the modifications are checked for consistency; second, they are applied
as a single atomic “transaction” to the current configuration to obtain a new
configuration compliant with the model; finally, the running system architecture
is updated accordingly. This year we implemented the reconfiguration step and
its algorithms and heuristics and we evaluated its run time efficiency.</p>
      <p>Our ultimate goal is to control the system through a feed back loop
from video components and sensor events to feature model manipulation
and back to video components modifications.</p>
      <p>The fully automatic adaptation that we propose is similar to a Feature Model
editor. That is the reason why our previous attempt was to embed a general purpose
feature model editor at run time. This revealed two major differences
between our mechanism and an editor. First, in a fully automatic process there
is no human being to drive a series of edits, hence heuristics are
required. Second, the editor operations are often elementary while we need a
global “transaction-like” application of all the selections/deselections to
avoid temporary unconsistencies.</p>
      <object id="uid168">
        <table>
          <tr>
            <td>
              <ressource xlink:href="IMG/first_test.png" type="float" scale="0.4" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
            </td>
          </tr>
        </table>
        <caption>Computation time of initial models</caption>
      </object>
      <p>In order to evaluate our algorithm performance, we randomly generated feature
models (from 60 to 1400 features). We also randomly generated context changes.
The results are shown on figure <ref xlink:href="#uid168" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>: no processing time
explosion is noticeable; in fact the time seems to grow rather
linearly. Moreover, the computation time of a new initial partial configuration
does not exceed 3ms for a rather big model. The algorithm and its evaluation
are detailed in <ref xlink:href="#stars-2016-bid78" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
    </subsection>
    <subsection id="uid169" level="1">
      <bodyTitle>Setup and management of SafEE devices</bodyTitle>
      <participants>
        <person key="stars-2015-idp113256">
          <firstname>Matias</firstname>
          <lastname>Marin</lastname>
        </person>
        <person key="PASUSERID">
          <firstname>Etienne</firstname>
          <lastname>Corvée</lastname>
        </person>
        <person key="PASUSERID">
          <firstname>François</firstname>
          <lastname>Brémond</lastname>
        </person>
      </participants>
      <p>The aim of the SafEE project (see section <ref xlink:href="#uid192" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>) is to provide assistance for the safety, autonomy and quality of life of elderly people at risk or already presenting Alzheimer's disease or related pathology.</p>
      <p>Within <span class="smallcap" align="left">ehpad</span> building (in Nice), 4 patients participated to our experiment
and we plan to include more patients in the project throughout next years.
Besides, 2 other patients have participated in the project at their own home.</p>
      <p>More precisely, the SafEE project focuses on specific clinical targets: behavior, motricity, cognitive capabilities
For this, the SafEE project includes:</p>
      <simplelist>
        <li id="uid170">
          <p noindent="true">srvsafee(web server): a behavior analysis platform has been created to allow identification of certain daytime behavior disturbances (agitation, for example) and nocturnal disturbances (sleep disorders), locomotor capacities (walking and posture ). It centralizes data saved in each local PC with Kinect2 sensor on the one hand, and postgresql database, on the other hand.
About 30 Gb data are recorded for each patient in a day, which represents a huge
amount to manage in the long run.</p>
        </li>
        <li id="uid171">
          <p noindent="true">Aroma diffuser (AromaCare): for sleep disturbances, using in particular an automated device for diffusing fragrances (aromatherapy) adapted to the perturbations detected by the analysis platform.</p>
        </li>
        <li id="uid172">
          <p noindent="true">Tablet (Serious game, MusicCare): for disturbances in spatial orientation, improved procedural memory and a sense of control and confidence in technological tools, using multimedia interfaces using an application for Android OS.</p>
        </li>
        <li id="uid173">
          <p noindent="true">Kinect2: motion detection for analysis linked to a PC, with a database to store recorded events.</p>
        </li>
        <li id="uid174">
          <p noindent="true">Bed sensor: able to track the sleep by analyzing the movements of the body, the breathing, and the beating of the heart.</p>
        </li>
      </simplelist>
      <p>Fig. <ref xlink:href="#uid175" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> shows the Safee project environment.</p>
      <object id="uid175">
        <table>
          <tr>
            <td>
              <ressource xlink:href="IMG/Safee_img.jpg" type="float" width="298.8987pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
            </td>
          </tr>
        </table>
        <caption>The Safee environment</caption>
      </object>
    </subsection>
    <subsection id="uid176" level="1">
      <bodyTitle>Brick &amp; Mortar Cookies</bodyTitle>
      <participants>
        <person key="stars-2014-idp112576">
          <firstname>Julien</firstname>
          <lastname>Badie</lastname>
        </person>
        <person key="lagadic-2014-idp89016">
          <firstname>Manikandan</firstname>
          <lastname>Bakthavatchalam</lastname>
        </person>
        <person key="stars-2014-idp69392">
          <firstname>Vasanth</firstname>
          <lastname>Bathrinarayanan</lastname>
        </person>
        <person key="PASUSERID">
          <firstname>Ghada</firstname>
          <lastname>Balhoul</lastname>
        </person>
        <person key="galaad2-2014-idp67104">
          <firstname>Anais</firstname>
          <lastname>Ducoffe</lastname>
        </person>
      </participants>
      <p>The objective of the BMC project is to create a software that aims to present attendance and attractiveness of the customer in stores, based on automatic video analysis. The final system should be designed to be used without changing the current camera network of the customer store, dedicated to security purpose. Analysis should be given at different time and space resolutions. For instance, attendance of one particular day can be as interesting as attendance of the entire year. Moreover, shop owners want to be able to compare two given years or months, etc... As space resolution is concerned, the software should be able to give information about the global attractiveness of the store but should also analyze some specific zones.</p>
      <p noindent="true">
        <b>IVA embedded on Bosch cameras</b>
      </p>
      <p>Intelligence Video Analysis (IVA) is embedded in some models of Bosch cameras. The algorithms are composed of human detection and tracking. They can be configured directly on the camera interface via <i>tasks</i>.</p>
      <p noindent="true">We are using a live connection to get metadata directly from the camera stream using a RTSP connection.
Thie year we improved the results of last year using calibration tool embedded in the camera : shape of people detected was better, feet were followed with more precision as bounding boxes were more stable.
We also tested the new IVA developed by BOSCH which was built to better manage changes in scene brightness and crossing of people. In the former version people close to each other were often detected as one person. Our first tests in shop revealed that it reduces the number of false detection but people were detected later than in the previous version. The case of people crossing doesn't seem to be better managed than before.</p>
      <p noindent="true">
        <b>Inria algorithms : people detection and tracking</b>
      </p>
      <p>The previously enumerated tasks use algorithms to detect people and get their trajectories. Stars team has developed similar algorithms and has adapted their parameters values to the specific needs of this software.
To improve results after some tests made during summer, the people detection is now using a deep learning method. People are detected earlier than before with this new algorithm and people crossing and occlusions are far better managed.
The performances and the reliability of those algorithms were tested using an annotation tool developed in Stars Team.</p>
      <p noindent="true">
        <b>Annotation tool</b>
      </p>
      <p>Manual annotation of videos requires major human effort. It can take hours and hours of fastidious work to annotate a tiny set of data. That's why we propose a semi-automatic tool which reduces the time of the annotation.
This new semi automatic annotation tool uses a simple input data format, XML file or XGTF file to describe the video contents and algorithms output. Users only have to correct false or missing detection and to fix some wrong object id of the algorithms results using the annotation tool interface.</p>
      <p noindent="true">
        <b>Tests in real conditions</b>
      </p>
      <p>We tested our video acquisition tool and our people detection and people tracking algorithms during summer in a partner supermarket in Nice. We successfully acquire 2 weeks of the desired metadata.
By the end of summer, our results were highly improved by using a deep learning method to detect people. Moreover we can get results in quasi real-time.
Except for the video stream acquisition tool, which needs to be connected to the camera network, our system is now running on an independent and local network. In case there is a crash of our system, the supermarket network will not be affected. Moreover, sensitive data are protected.
A test is starting soon in SuperU to run and evaluate this new prototype.</p>
      <p noindent="true">
        <b>Metadata storage in database</b>
      </p>
      <p>Last year metadata outputs of our analysis were first stored in XML files. Now to manage the quasi real-time solution, metadata are stored directly in the database we designed last year. We improve architecture of this database to manage simultaneously several connections as the final solution is supposed to be composed of several servers which will manage several video streams at the same time.</p>
      <p noindent="true">
        <b>Web interface (HIM)</b>
      </p>
      <p>The web graphic interface is in progress. User interactions were added and improved so that the interface should be more user-friendly. We also changed some charts and tables so that statistical results should be better understood by users.</p>
    </subsection>
  </resultats>
  <contrats id="uid177">
    <bodyTitle>Bilateral Contracts and Grants with Industry</bodyTitle>
    <subsection id="uid178" level="1">
      <bodyTitle>Bilateral Contracts with Industry</bodyTitle>
      <simplelist>
        <li id="uid179">
          <p noindent="true"><b>Toyota Europ</b>: this project with Toyota runs from the 1st of August 2013 up to 2017 (4 years). It aims at detecting critical situations in the daily life of older adults living home alone. We believe that a system that is able to detect potentially dangerous situations will give peace of mind to frail older people as well as to their caregivers. This will require not only recognition of ADLs but also an evaluation of the way and timing in which they are being carried out. The system we want to develop is intended to help them and their relatives to feel more comfortable because they know potentially dangerous situations will be detected and reported to caregivers if necessary. The system is intended to work with a Partner Robot (to send real-time information to the robot) to better interact with older adults.</p>
        </li>
        <li id="uid180">
          <p noindent="true"><b>LinkCareServices</b>: this project with Link Care Services runs from 2010 upto 2015. It aims at designing a novel system for Fall Detection. This study consists in evaluating the performance of video-based systems for Fall Detection in a large variety of situations. Another goal is to design a novel approach based on RGBD sensors with very low rate of false alarms.</p>
        </li>
      </simplelist>
    </subsection>
  </contrats>
  <partenariat id="uid181">
    <bodyTitle>Partnerships and Cooperations</bodyTitle>
    <subsection id="uid182" level="1">
      <bodyTitle>National Initiatives</bodyTitle>
      <subsection id="uid183" level="2">
        <bodyTitle>ANR</bodyTitle>
        <subsection id="uid184" level="3">
          <bodyTitle>MOVEMENT</bodyTitle>
          <sanspuceslist>
            <li id="uid185">
              <p noindent="true">Program: ANR CSOSG</p>
            </li>
            <li id="uid186">
              <p noindent="true">Project acronym: MOVEMENT</p>
            </li>
            <li id="uid187">
              <p noindent="true">Project title: AutoMatic BiOmetric Verification and PersonnEl Tracking for SeaMless Airport ArEas Security MaNagemenT</p>
            </li>
            <li id="uid188">
              <p noindent="true">Duration: January 2014-June 2017</p>
            </li>
            <li id="uid189">
              <p noindent="true">Coordinator: MORPHO (FR)</p>
            </li>
            <li id="uid190">
              <p noindent="true">Other partners: SAGEM (FR), Inria Sophia-Antipolis (FR), EGIDIUM (FR), EVITECH (FR) and CERAPS (FR)</p>
            </li>
            <li id="uid191">
              <p noindent="true">Abstract: MOVEMENT is focusing on the management of security zones in the non public airport areas. These areas, with a restricted access, are dedicated to service activities such as maintenance, aircraft ground handling, airfreight activities, etc. In these areas, personnel movements tracking and traceability have to be improved in order to facilitate their passage through the different areas, while insuring a high level of security to prevent any unauthorized access. MOVEMENT aims at proposing a new concept for the airport's non public security zones (e.g. customs control rooms or luggage loading/unloading areas) management along with the development of an innovative supervision system prototype.</p>
            </li>
          </sanspuceslist>
        </subsection>
        <subsection id="uid192" level="3">
          <bodyTitle>SafEE</bodyTitle>
          <sanspuceslist>
            <li id="uid193">
              <p noindent="true">Program: ANR TESCAN</p>
            </li>
            <li id="uid194">
              <p noindent="true">Project acronym: SafEE</p>
            </li>
            <li id="uid195">
              <p noindent="true">Project title: Safe &amp; Easy Environment for Alzheimer Disease and related disorders</p>
            </li>
            <li id="uid196">
              <p noindent="true">Duration: December 2013-May 2017</p>
            </li>
            <li id="uid197">
              <p noindent="true">Coordinator: CHU Nice</p>
            </li>
            <li id="uid198">
              <p noindent="true">Other partners: Nice Hospital(FR), Nice University (CobTeck FR), Inria Sophia-Antipolis (FR), Aromatherapeutics (FR), SolarGames(FR), Taichung Veterans General Hospital TVGH (TW), NCKU Hospital(TW), SMILE Lab at National Cheng Kung University NCKU (TW), BDE (TW)</p>
            </li>
            <li id="uid199">
              <p noindent="true">Abstract: SafEE project aims at investigating technologies for stimulation and intervention for Alzheimer patients. More precisely, the main goals are: (1) to focus on specific clinical targets in three domains behavior, motricity and cognition (2) to merge assessment and non pharmacological help/intervention and (3) to propose easy ICT device solutions for the end users. In this project, experimental studies will be conducted both in France (at Hospital and Nursery Home) and in Taiwan.</p>
            </li>
          </sanspuceslist>
        </subsection>
      </subsection>
      <subsection id="uid200" level="2">
        <bodyTitle>FUI</bodyTitle>
        <subsection id="uid201" level="3">
          <bodyTitle>Visionum</bodyTitle>
          <sanspuceslist>
            <li id="uid202">
              <p noindent="true">Program: FUI</p>
            </li>
            <li id="uid203">
              <p noindent="true">Project acronym: Visionum</p>
            </li>
            <li id="uid204">
              <p noindent="true">Project title: Visonium.</p>
            </li>
            <li id="uid205">
              <p noindent="true">Duration: January 2015- December 2018</p>
            </li>
            <li id="uid206">
              <p noindent="true">Coordinator: Groupe Genious</p>
            </li>
            <li id="uid207">
              <p noindent="true">Other partners: Inria(Stars), StreetLab, Fondation Ophtalmologique Rothschild, Fondation Hospitaliere Sainte-Marie.</p>
            </li>
            <li id="uid208">
              <p noindent="true">Abstract: This French project from Industry Minister aims at designing a platform to re-educate at home people with visual impairment.</p>
            </li>
          </sanspuceslist>
        </subsection>
      </subsection>
    </subsection>
    <subsection id="uid209" level="1">
      <bodyTitle>European Initiatives</bodyTitle>
      <subsection id="uid210" level="2">
        <bodyTitle>FP7 &amp; H2020 Projects</bodyTitle>
        <subsection id="uid211" level="3">
          <bodyTitle>CENTAUR</bodyTitle>
          <sanspuceslist>
            <li id="uid212">
              <p noindent="true">Title: Crowded ENvironments moniToring for Activity Understanding and Recognition</p>
            </li>
            <li id="uid213">
              <p noindent="true">Programm: FP7</p>
            </li>
            <li id="uid214">
              <p noindent="true">Duration: January 2013 - December 2016</p>
            </li>
            <li id="uid215">
              <p noindent="true">Coordinator: Honeywell</p>
            </li>
            <li id="uid216">
              <p noindent="true">Partners:</p>
              <sanspuceslist>
                <li id="uid217">
                  <p noindent="true">Ecole Polytechnique Federale de Lausanne (Switzerland)</p>
                </li>
                <li id="uid218">
                  <p noindent="true">"honeywell, Spol. S.R.O" (Czech Republic)</p>
                </li>
                <li id="uid219">
                  <p noindent="true">Neovision Sro (Czech Republic)</p>
                </li>
                <li id="uid220">
                  <p noindent="true">Queen Mary University of London (United Kingdom)</p>
                </li>
              </sanspuceslist>
            </li>
            <li id="uid221">
              <p noindent="true">Inria contact: François Bremond</p>
            </li>
            <li id="uid222">
              <p noindent="true">'We aim to develop a network of scientific excellence addressing research topics in computer vision and advancing the state of the art in video surveillance. The cross fertilization of ideas and technology between academia, research institutions and industry will lay the foundations to new methodologies and commercial solutions for monitoring crowded scenes. Research activities will be driven by specific sets of scenarios, requirements and datasets that reflect security operators’ needs for guaranteeing the safety of EU citizens. CENTAUR gives a unique opportunity to academia to be exposed to real life dataset, while enabling the validation of state-of-the-art video surveillance methodology developed at academia on data that illustrate real operational scenarios. The research agenda is motivated by ongoing advanced research activities in the participating entities. With Honeywell as a multi-industry partner, with security technologies developed and deployed in both its Automation and Control Solutions and Aerospace businesses, we have multiple global channels to exploit the developed technologies. With Neovison as a SME, we address small fast paced local markets, where the quick assimilation of new technologies is crucial. Three thrusts identified will enable the monitoring of crowded scenes, each led by an academic partner in collaboration with scientists from Honeywell: a) multi camera, multicoverage tracking of objects of interest, b) Anomaly detection and fusion of multimodal sensors, c) activity recognition and behavior analysis in crowded environments. We expect a long term impact on the field of video surveillance by: contributions to the state-of-the-art in the field, dissemination of results within the scientific and practitioners community, and establishing long term scientific exchanges between academia and industry, for a forum of scientific and industrial partners to collaborate on addressing technical challenges faced by scientists and the industry.'</p>
            </li>
          </sanspuceslist>
        </subsection>
      </subsection>
    </subsection>
    <subsection id="uid223" level="1">
      <bodyTitle>International Initiatives</bodyTitle>
      <subsection id="uid224" level="2">
        <bodyTitle>Inria International Labs</bodyTitle>
        <subsection id="uid225" level="3">
          <bodyTitle>Informal International Partners</bodyTitle>
          <simplelist>
            <li id="uid226">
              <p noindent="true"><b>Collaborations with Asia:</b>
Stars has been cooperating with the Multimedia Research Center in
Hanoi MICA on semantics extraction from multimedia data.
Stars also collaborates with the National Cheng Kung University in Taiwan and I2R in Singapore.</p>
            </li>
            <li id="uid227">
              <p noindent="true"><b>Collaboration with U.S.A.:</b>
Stars collaborates with the University of Southern California.</p>
            </li>
            <li id="uid228">
              <p noindent="true"><b>Collaboration with Europe:</b>
Stars collaborates with Multitel in Belgium, the University of Kingston upon Thames UK, and the University of Bergen in Norway.</p>
            </li>
          </simplelist>
        </subsection>
        <subsection id="uid229" level="3">
          <bodyTitle>Other IIL projects</bodyTitle>
          <p>The ANR SafEE (see section  <ref xlink:href="#uid192" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>) collaborates with international partners such as Taichung Veterans General Hospital TVGH (TW), NCKU Hospital(TW), SMILE Lab at National Cheng Kung University NCKU (TW) and BDE (TW).</p>
        </subsection>
      </subsection>
    </subsection>
    <subsection id="uid230" level="1">
      <bodyTitle>International Research Visitors</bodyTitle>
      <subsection id="uid231" level="2">
        <bodyTitle>Visits of International Scientists</bodyTitle>
        <p>This year, Stars has been visited by the following international scientists:</p>
        <simplelist>
          <li id="uid232">
            <p noindent="true">Salwa Baabou, Ecole Nationale d'Ingénieurs de Gabès, Tunisia;</p>
          </li>
          <li id="uid233">
            <p noindent="true">Siyuan Chen, University of New South Wales, Australia;</p>
          </li>
          <li id="uid234">
            <p noindent="true">Adlen Kerboua, University of Skikda, Algeria;</p>
          </li>
          <li id="uid235">
            <p noindent="true">Karel Krehnac, Neovision, Praha, Czech Republic;</p>
          </li>
          <li id="uid236">
            <p noindent="true">Jana Trojnova, Honeywell, Praha, Czech Republic;</p>
          </li>
          <li id="uid237">
            <p noindent="true">Luis Emiliano Sanchez, Rosario University, Argentina.</p>
          </li>
        </simplelist>
        <subsection id="uid238" level="3">
          <bodyTitle>Internships</bodyTitle>
          <sanspuceslist>
            <li id="uid239">
              <p noindent="true">Seongro Yoon</p>
              <sanspuceslist>
                <li id="uid240">
                  <p noindent="true">Date: Apr 2016-Dec 2016</p>
                </li>
                <li id="uid241">
                  <p noindent="true">Institution: Korea Advanced Institute of Science and Technology, Daejeon, Korea</p>
                </li>
                <li id="uid242">
                  <p noindent="true">Supervisor: François Brémond</p>
                </li>
              </sanspuceslist>
            </li>
          </sanspuceslist>
          <sanspuceslist>
            <li id="uid243">
              <p noindent="true">Yashas Annadani</p>
              <sanspuceslist>
                <li id="uid244">
                  <p noindent="true">Date: May2016-June 2016</p>
                </li>
                <li id="uid245">
                  <p noindent="true">Institution: National Institute Of Technology Karnataka, India</p>
                </li>
                <li id="uid246">
                  <p noindent="true">Supervisor: Carlos Fernando Crispim Junior</p>
                </li>
              </sanspuceslist>
            </li>
          </sanspuceslist>
          <sanspuceslist>
            <li id="uid247">
              <p noindent="true">Chandraja Dharmana</p>
              <sanspuceslist>
                <li id="uid248">
                  <p noindent="true">Date: May 2016-June 2016</p>
                </li>
                <li id="uid249">
                  <p noindent="true">Institution: Birla Institute of Technology and Science, Pilani, Hyderbrad</p>
                </li>
                <li id="uid250">
                  <p noindent="true">Supervisor: Carlos Fernando Crispim Junior</p>
                </li>
              </sanspuceslist>
            </li>
          </sanspuceslist>
          <sanspuceslist>
            <li id="uid251">
              <p noindent="true">Shanu Vashistha</p>
              <sanspuceslist>
                <li id="uid252">
                  <p noindent="true">Date: May 2016-June 2016</p>
                </li>
                <li id="uid253">
                  <p noindent="true">Institution: Indian Institute of Technology, Kanpur, India</p>
                </li>
                <li id="uid254">
                  <p noindent="true">Supervisor: Carlos Fernando Crispim Junior</p>
                </li>
              </sanspuceslist>
            </li>
          </sanspuceslist>
          <sanspuceslist>
            <li id="uid255">
              <p noindent="true">Nairouz Mrabah</p>
              <sanspuceslist>
                <li id="uid256">
                  <p noindent="true">Date: Apr 2016-Sep 2016</p>
                </li>
                <li id="uid257">
                  <p noindent="true">Institution: National School of Computer Science (ENSI), Tunisia</p>
                </li>
                <li id="uid258">
                  <p noindent="true">Supervisor: Inès Sarray</p>
                </li>
              </sanspuceslist>
            </li>
          </sanspuceslist>
          <sanspuceslist>
            <li id="uid259">
              <p noindent="true">Isabel Rayas</p>
              <sanspuceslist>
                <li id="uid260">
                  <p noindent="true">Date: June 2016-Dec 2016</p>
                </li>
                <li id="uid261">
                  <p noindent="true">Institution: Massachusetts Institute of Technology, USA</p>
                </li>
                <li id="uid262">
                  <p noindent="true">Supervisor: Farhood Negin</p>
                </li>
              </sanspuceslist>
            </li>
          </sanspuceslist>
        </subsection>
      </subsection>
    </subsection>
  </partenariat>
  <diffusion id="uid263">
    <bodyTitle>Dissemination</bodyTitle>
    <subsection id="uid264" level="1">
      <bodyTitle>Promoting Scientific Activities</bodyTitle>
      <subsection id="uid265" level="2">
        <bodyTitle>Scientific events organisation</bodyTitle>
        <subsection id="uid266" level="3">
          <bodyTitle>General chair, scientific chair</bodyTitle>
          <sanspuceslist>
            <li id="uid267">
              <p noindent="true">François Brémond was organizer of the ISG 2016, 10th World Conference of Gerontechnology, Nice,28th to 30th September 2016.</p>
            </li>
            <li id="uid268">
              <p noindent="true">François Brémond was editor of the Crowd Understanding workshop, part of ECCV, Amsterdam, October 2016.</p>
            </li>
          </sanspuceslist>
        </subsection>
        <subsection id="uid269" level="3">
          <bodyTitle>Member of the organizing committee</bodyTitle>
          <sanspuceslist>
            <li id="uid270">
              <p noindent="true">François Brémond was a member of the Management Committee and COST Action IC1307 in 2016.</p>
            </li>
          </sanspuceslist>
        </subsection>
      </subsection>
      <subsection id="uid271" level="2">
        <bodyTitle>Scientific events selection</bodyTitle>
        <subsection id="uid272" level="3">
          <bodyTitle>Member of the conference program committees</bodyTitle>
          <sanspuceslist>
            <li id="uid273">
              <p noindent="true">François Brémond was program committee member of the conferences and workshops: KSE 2016, PETS2016, MMM2017.</p>
            </li>
            <li id="uid274">
              <p noindent="true">François Brémond was ACM Multimedia Area Chair for Multimedia and Vision, Amsterdam, 2016.</p>
            </li>
            <li id="uid275">
              <p noindent="true">François Brémond was session chair of AVSS-16, Colorado Springs, USA, 2016.</p>
            </li>
            <li id="uid276">
              <p noindent="true">Jean-Paul Rigault is a member of the <i>Association Internationale pour les Technologies à Objets</i> (AITO) which organizes international conferences such as ECOOP.</p>
            </li>
            <li id="uid277">
              <p noindent="true">Antitza Dantcheva was program committee member of the conference International Conference on Biometrics (ICB 2016), the CVPR Workshop ChaLearn Looking at People 2016 and the Healthcare Conference Workshop
within the EAIInternational Conference on Pervasive Computing Technologies.</p>
            </li>
          </sanspuceslist>
        </subsection>
        <subsection id="uid278" level="3">
          <bodyTitle>Reviewer</bodyTitle>
          <sanspuceslist>
            <li id="uid279">
              <p noindent="true">François Brémond was reviewer for the conferences : CVPR2016-7, ECCV2016, VOT2016, MARMI2016, WACV 2017.</p>
            </li>
            <li id="uid280">
              <p noindent="true">Carlos Fernando Crispim Junior was reviewer for the conferences: International Conference on Intelligent Robot and Systems, IEEE International Conference on Robotics and Automaton,
Brazilian Conference in Biomadical Engineering, AMBIANT Conference, Computer on the Beach.</p>
            </li>
          </sanspuceslist>
        </subsection>
      </subsection>
      <subsection id="uid281" level="2">
        <bodyTitle>Journal</bodyTitle>
        <subsection id="uid282" level="3">
          <bodyTitle>Member of the editorial boards</bodyTitle>
          <sanspuceslist>
            <li id="uid283">
              <p noindent="true">François Brémond was handling editor of the international journal "SDECLAREMachine Vision and Application".</p>
            </li>
          </sanspuceslist>
        </subsection>
        <subsection id="uid284" level="3">
          <bodyTitle>Reviewer - Reviewing activities</bodyTitle>
          <sanspuceslist>
            <li id="uid285">
              <p noindent="true">François Brémond was reviewer for the journal r̎evue Retraite et sociétéänd Medical Engineering &amp; Physics.</p>
            </li>
            <li id="uid286">
              <p noindent="true">Carlos Fernando Crispim Junior was reviewer for the journals: Pattern Recognition, Neurocomputing, Computer Vision and Image Understanding Journal, Computers in Biology and Medecine Journal,
PLOS One Journal, Frontiers in Neuroscience, Sensors.</p>
            </li>
            <li id="uid287">
              <p noindent="true">Antitza Dantcheva reviewed for the journals: IEEE Transactions on Information Forensics and Security (TIFS), Information Processing Letters, The Computer Journal, IET Biometrics, Multimedia Systems,
International Journal for Information Management, Information Fusion (INFFUS), Sensors, Pattern Recognition.</p>
            </li>
          </sanspuceslist>
        </subsection>
      </subsection>
      <subsection id="uid288" level="2">
        <bodyTitle>Invited talks</bodyTitle>
        <sanspuceslist>
          <li id="uid289">
            <p noindent="true">François Brémond was invited by Prof. Ram Nevatia to give a talk on research initiatives and new directions in Video Understanding, USC, LA, USA 17 August 2016.</p>
          </li>
          <li id="uid290">
            <p noindent="true">François Brémond was invited by Prof. Jonathan Ventura to give a talk on People detection, at the SLDP 2016 workshop of AVSS, Colorado Springs, USA,23 August 2016.</p>
          </li>
          <li id="uid291">
            <p noindent="true">François Brémond was invited by Prof. William Robson Schwartz,Department of Computer Science, Federal University of Minas Gerais to give a talk on Video Analytic, at Video Surveillance workshop in
Belo Horizonte-Brazil, 03 October 2016.</p>
          </li>
          <li id="uid292">
            <p noindent="true">François Brémond was invited by Prof. William Robson Schwartz to give a talk on People Tracking, at SIBGRAPI 2016, Sao Paulo-Brazil, 05 October 2016.</p>
          </li>
          <li id="uid293">
            <p noindent="true">François Brémond was invited by Prof. Cosimo Distante, Consiglio Nazionale delle Ricerche to give a talk on Activity Recognition, at ACIVS 2016, Lecce, Italy, 26 October 2016.</p>
          </li>
          <li id="uid294">
            <p noindent="true">François Brémond was invited by Sebastien Ambellouis (IFSTTAR) to give a talk on Activity Monitoring, at IEEE IPAS 2016,Hammamet, Tunisia,5-7 November 2016.</p>
          </li>
          <li id="uid295">
            <p noindent="true">Carlos Fernando Crispim Junior was invited to give a talk at the 1st Inter-lalex seminar "Smart Systems", Besançon, France, November 23rd 2016.</p>
          </li>
          <li id="uid296">
            <p noindent="true">Carlos Fernando Crispim Junior was invited to make a presentation at PSI-VISICS seminar at ESAT department in KU Leuven University,October 24th 2016.</p>
          </li>
          <li id="uid297">
            <p noindent="true">Carlos Fernando Crispim Junior was invited to make a presentation at Machine Learning seminar at Computer Science department of KU Leuven University, October 17th 2016.</p>
          </li>
          <li id="uid298">
            <p noindent="true">Carlos Fernando Crispim Junior was invited to give a talk at ISG 2016 - Seminar European FP7 project Dem@care: Automatic Video Analysis for Diagnosis and Care, Nice, France, October 29th 2016.</p>
          </li>
          <li id="uid299">
            <p noindent="true">Carlos Fernando Crispim Junior was invited speaker at the internal seminar of LAAS-CNRS, Toulouse, France, July3rd-4th.</p>
          </li>
          <li id="uid300">
            <p noindent="true">Carlos Fernando Crispim Junior was invited speaker at CPUEX seminar, LABRI-CNRS, Bordeaux, France, February 2016.</p>
          </li>
        </sanspuceslist>
      </subsection>
      <subsection id="uid301" level="2">
        <bodyTitle>Scientific expertise</bodyTitle>
        <sanspuceslist>
          <li id="uid302">
            <p noindent="true">François Brémond was expert for EU European Reference Network for Critical Infrastructure Protection (ERNCIP) - Video Anlaytics and surveillance Group, at European Commission’s Joint Research Centre
in Brussels in July 2016.</p>
          </li>
          <li id="uid303">
            <p noindent="true">François Brémond was expert for the Foundation Médéric Alzheimer, for the doctoral fellowship selection, September 2016.</p>
          </li>
        </sanspuceslist>
      </subsection>
    </subsection>
    <subsection id="uid304" level="1">
      <bodyTitle>Teaching - Supervision - Juries</bodyTitle>
      <subsection id="uid305" level="2">
        <bodyTitle>Teaching</bodyTitle>
        <sanspuceslist>
          <li id="uid306">
            <p noindent="true">Master : Annie Ressouche, Safety in Middleware for Internet of Things, 10h, niveau (M2), Polytech Nice School of Nice University.</p>
          </li>
          <li id="uid307">
            <p noindent="true">Jean-Paul Rigault is Full Professor of Computer Science at Polytech'Nice (University of Nice): courses on C++ (beginners and advanced), C, System Programming, Software Modeling.</p>
          </li>
        </sanspuceslist>
      </subsection>
      <subsection id="uid308" level="2">
        <bodyTitle>Supervision</bodyTitle>
        <sanspuceslist>
          <li id="uid309">
            <p noindent="true">PhD in progress : Auriane Gros, Evaluation and Specific Management of Emotionnal Disturbances with Activity Recognition Systems for Alzheimer patient, Sept 2014, François Brémond.</p>
          </li>
          <li id="uid310">
            <p noindent="true">PhD in progress : Minh Khue Phan Tran, Man-machine interaction for older adults with dementia, May 2013, François Brémond.</p>
          </li>
          <li id="uid311">
            <p noindent="true">PhD in progress : Michal Koperski, Detecting critical human activities using RGB/RGBD cameras in home environment, François Brémond.</p>
          </li>
          <li id="uid312">
            <p noindent="true">PhD in progress : Thi Lan Anh Nguyen, Complex Activity Recognition from 3D sensors, Dec 2014, François Brémond.</p>
          </li>
          <li id="uid313">
            <p noindent="true">PhD in progress : Ines Sarray, Activity Recognition System Design, Oct 2015, Sabine Moisan.</p>
          </li>
          <li id="uid314">
            <p noindent="true">PhD in progress : Farhood Negin, People Detection for Activity Recognition using RGB-Depth Sensors, Jan 2015, François Brémond.</p>
          </li>
          <li id="uid315">
            <p noindent="true">PhD in progress : Ujjwal Ujjwal, Pedestrian Detection to Dynamically Populate the Map of a Crossroad, Sep 2016, François Brémond.</p>
          </li>
        </sanspuceslist>
      </subsection>
      <subsection id="uid316" level="2">
        <bodyTitle>Juries</bodyTitle>
        <p>François Brémond was jury member of the following PhD theses:</p>
        <sanspuceslist>
          <li id="uid317">
            <p noindent="true">PhD, Andrei Stoian, CNAM, Paris, 15 January 2016.</p>
          </li>
          <li id="uid318">
            <p noindent="true">PhD, Romain Endelin, University of Montpellier, 2nd June 2016.</p>
          </li>
          <li id="uid319">
            <p noindent="true">PhD, Jean-Charles Bricola, CMM, Mines ParisTech, Fontainebleau, 19 October 2016.</p>
          </li>
          <li id="uid320">
            <p noindent="true">PhD, Salma Moujtahid, LIRIS-Equipe IMAGINE-INSA Lyon, 3 November 2016.</p>
          </li>
          <li id="uid321">
            <p noindent="true">PhD, Marion Chevalier, Laboratoire d'Informatique de Paris 6, Thales Optronique S.A.S., Paris,2 December 2016.</p>
          </li>
        </sanspuceslist>
      </subsection>
    </subsection>
    <subsection id="uid322" level="1">
      <bodyTitle>Popularization</bodyTitle>
      <sanspuceslist>
        <li id="uid323">
          <p noindent="true">François Brémond was invited to give a talk at Conférence des métiers at International Lycée (CIV) in Sophia, January 2016.</p>
        </li>
        <li id="uid324">
          <p noindent="true">François Brémond was interviewed by the agence Citizen Press February 2016.</p>
        </li>
        <li id="uid325">
          <p noindent="true">François Brémond was invited to give a talk at the Artificial Intelligence Workshop Meeting Amadeus - Inria, June 2016.</p>
        </li>
        <li id="uid326">
          <p noindent="true">François Brémond was invited to give a talk at la rencontre Inria Industrie Ed-Tech, 1 December 2016.</p>
        </li>
        <li id="uid327">
          <p noindent="true">François Brémond has published an article in ERCIM news, December 2016.</p>
        </li>
      </sanspuceslist>
    </subsection>
  </diffusion>
  <biblio id="bibliography" html="bibliography" numero="10" titre="Bibliography">
    
    <biblStruct id="stars-2016-bid7" type="article" rend="refer" n="refercite:EURASIP2005">
      <analytic>
        <title level="a">Design and Assessment of an Intelligent Activity Monitoring Platform</title>
        <author>
          <persName>
            <foreName>A.</foreName>
            <surname>Avanzi</surname>
            <initial>A.</initial>
          </persName>
          <persName key="stars-2014-idp60040">
            <foreName>F.</foreName>
            <surname>Brémond</surname>
            <initial>F.</initial>
          </persName>
          <persName>
            <foreName>C.</foreName>
            <surname>Tornieri</surname>
            <initial>C.</initial>
          </persName>
          <persName key="stars-2014-idp66712">
            <foreName>M.</foreName>
            <surname>Thonnat</surname>
            <initial>M.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">EURASIP Journal on Applied Signal Processing, Special Issue on “Advances in Intelligent Vision Systems: Methods and Applications”</title>
        <imprint>
          <biblScope type="volume">2005:14</biblScope>
          <dateStruct>
            <month>August</month>
            <year>2005</year>
          </dateStruct>
          <biblScope type="pages">2359-2374</biblScope>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid95" type="inproceedings" rend="refer" n="refercite:Benhadda2007">
      <analytic>
        <title level="a">Data Mining on Large Video Recordings</title>
        <author>
          <persName>
            <foreName>H.</foreName>
            <surname>Benhadda</surname>
            <initial>H.</initial>
          </persName>
          <persName>
            <foreName>J.L.</foreName>
            <surname>Patino</surname>
            <initial>J.</initial>
          </persName>
          <persName key="stars-2014-idp116256">
            <foreName>E.</foreName>
            <surname>Corvee</surname>
            <initial>E.</initial>
          </persName>
          <persName key="stars-2014-idp60040">
            <foreName>F.</foreName>
            <surname>Bremond</surname>
            <initial>F.</initial>
          </persName>
          <persName key="stars-2014-idp66712">
            <foreName>M.</foreName>
            <surname>Thonnat</surname>
            <initial>M.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="m">5eme Colloque Veille Stratégique Scientifique et Technologique VSST 2007</title>
        <loc>Marrakech, Marrocco</loc>
        <imprint>
          <dateStruct>
            <month>21st - 25th October</month>
            <year>2007</year>
          </dateStruct>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid92" type="article" rend="refer" n="refercite:PRL2006">
      <analytic>
        <title level="a">Applying 3D Human Model in a Posture Recognition System</title>
        <author>
          <persName key="stars-2014-idp115016">
            <foreName>B.</foreName>
            <surname>Boulay</surname>
            <initial>B.</initial>
          </persName>
          <persName key="stars-2014-idp60040">
            <foreName>F.</foreName>
            <surname>Bremond</surname>
            <initial>F.</initial>
          </persName>
          <persName key="stars-2014-idp66712">
            <foreName>M.</foreName>
            <surname>Thonnat</surname>
            <initial>M.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">Pattern Recognition Letter</title>
        <imprint>
          <biblScope type="volume">27</biblScope>
          <biblScope type="number">15</biblScope>
          <dateStruct>
            <year>2006</year>
          </dateStruct>
          <biblScope type="pages">1785-1796</biblScope>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid88" type="article" rend="refer" n="refercite:IJHC98">
      <analytic>
        <title level="a">Issues of Representing Context Illustrated by Video-surveillance Applications</title>
        <author>
          <persName key="stars-2014-idp60040">
            <foreName>F.</foreName>
            <surname>Brémond</surname>
            <initial>F.</initial>
          </persName>
          <persName key="stars-2014-idp66712">
            <foreName>M.</foreName>
            <surname>Thonnat</surname>
            <initial>M.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">International Journal of Human-Computer Studies, Special Issue on Context</title>
        <imprint>
          <biblScope type="volume">48</biblScope>
          <dateStruct>
            <year>1998</year>
          </dateStruct>
          <biblScope type="pages">375-391</biblScope>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid100" type="inproceedings" rend="refer" n="refercite:charpiat09">
      <analytic>
        <title level="a">Learning Shape Metrics based on Deformations and Transport</title>
        <author>
          <persName key="stars-2014-idp61536">
            <foreName>Guillaume</foreName>
            <surname>Charpiat</surname>
            <initial>G.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="m">Proceedings of ICCV 2009 and its Workshops, Second Workshop on Non-Rigid Shape Analysis and Deformable Image Alignment (NORDIA)</title>
        <loc>Kyoto, Japan</loc>
        <imprint>
          <dateStruct>
            <month>September</month>
            <year>2009</year>
          </dateStruct>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid6" type="inbook" rend="refer" n="refercite:CBT98">
      <analytic>
        <author>
          <persName key="aoste-2014-idp78608">
            <foreName>N.</foreName>
            <surname>Chleq</surname>
            <initial>N.</initial>
          </persName>
          <persName key="stars-2014-idp60040">
            <foreName>F.</foreName>
            <surname>Brémond</surname>
            <initial>F.</initial>
          </persName>
          <persName key="stars-2014-idp66712">
            <foreName>M.</foreName>
            <surname>Thonnat</surname>
            <initial>M.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="m">Advanced Video-based Surveillance Systems</title>
        <imprint>
          <publisher>
            <orgName>Kluwer A.P. , Hangham, MA, USA</orgName>
          </publisher>
          <dateStruct>
            <month>November</month>
            <year>1998</year>
          </dateStruct>
          <biblScope type="pages">108-118</biblScope>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid84" type="inbook" rend="refer" n="refercite:cup02">
      <analytic>
        <author>
          <persName>
            <foreName>F.</foreName>
            <surname>Cupillard</surname>
            <initial>F.</initial>
          </persName>
          <persName key="stars-2014-idp60040">
            <foreName>F.</foreName>
            <surname>Brémond</surname>
            <initial>F.</initial>
          </persName>
          <persName key="stars-2014-idp66712">
            <foreName>M.</foreName>
            <surname>Thonnat</surname>
            <initial>M.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="m">Tracking Group of People for Video Surveillance</title>
        <title level="s">Video-Based Surveillance Systems</title>
        <imprint>
          <biblScope type="volume">The Kluwer International Series in Computer Vision and Distributed Processing</biblScope>
          <publisher>
            <orgName>Kluwer Academic Publishers</orgName>
          </publisher>
          <dateStruct>
            <year>2002</year>
          </dateStruct>
          <biblScope type="pages">89-100</biblScope>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid8" type="article" rend="refer" n="refercite:MVAa06">
      <analytic>
        <title level="a">Video Understanding for Complex Activity Recognition</title>
        <author>
          <persName>
            <foreName>Florent</foreName>
            <surname>Fusier</surname>
            <initial>F.</initial>
          </persName>
          <persName>
            <foreName>Valery</foreName>
            <surname>Valentin</surname>
            <initial>V.</initial>
          </persName>
          <persName key="stars-2014-idp60040">
            <foreName>François</foreName>
            <surname>Bremond</surname>
            <initial>F.</initial>
          </persName>
          <persName key="stars-2014-idp66712">
            <foreName>Monique</foreName>
            <surname>Thonnat</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>Mark</foreName>
            <surname>Borg</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>David</foreName>
            <surname>Thirde</surname>
            <initial>D.</initial>
          </persName>
          <persName>
            <foreName>James</foreName>
            <surname>Ferryman</surname>
            <initial>J.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">Machine Vision and Applications Journal</title>
        <imprint>
          <biblScope type="volume">18</biblScope>
          <dateStruct>
            <year>2007</year>
          </dateStruct>
          <biblScope type="pages">167-188</biblScope>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid97" type="article" rend="refer" n="refercite:MVAb06">
      <analytic>
        <title level="a">Real-Time Control of Video Surveillance Systems with Program Supervision Techniques</title>
        <author>
          <persName>
            <foreName>Benoit</foreName>
            <surname>Georis</surname>
            <initial>B.</initial>
          </persName>
          <persName key="stars-2014-idp60040">
            <foreName>François</foreName>
            <surname>Bremond</surname>
            <initial>F.</initial>
          </persName>
          <persName key="stars-2014-idp66712">
            <foreName>Monique</foreName>
            <surname>Thonnat</surname>
            <initial>M.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">Machine Vision and Applications Journal</title>
        <imprint>
          <biblScope type="volume">18</biblScope>
          <dateStruct>
            <year>2007</year>
          </dateStruct>
          <biblScope type="pages">189-205</biblScope>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid98" type="article" rend="refer" n="refercite:JHSN07">
      <analytic>
        <title level="a">Understanding of Human Behaviors from Videos in Nursing Care Monitoring Systems</title>
        <author>
          <persName key="in-situ-2014-idp87024">
            <foreName>C.</foreName>
            <surname>Liu</surname>
            <initial>C.</initial>
          </persName>
          <persName>
            <foreName>P.</foreName>
            <surname>Chung</surname>
            <initial>P.</initial>
          </persName>
          <persName>
            <foreName>Y</foreName>
            <surname>Chung</surname>
            <initial>Y.</initial>
          </persName>
          <persName key="stars-2014-idp66712">
            <foreName>M.</foreName>
            <surname>Thonnat</surname>
            <initial>M.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">Journal of High Speed Networks</title>
        <imprint>
          <biblScope type="volume">16</biblScope>
          <dateStruct>
            <year>2007</year>
          </dateStruct>
          <biblScope type="pages">91-103</biblScope>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid85" type="article" rend="refer" n="refercite:maillot04b">
      <analytic>
        <title level="a">Towards Ontology Based Cognitive Vision</title>
        <author>
          <persName>
            <foreName>N.</foreName>
            <surname>Maillot</surname>
            <initial>N.</initial>
          </persName>
          <persName key="stars-2014-idp66712">
            <foreName>M.</foreName>
            <surname>Thonnat</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>A.</foreName>
            <surname>Boucher</surname>
            <initial>A.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">Machine Vision and Applications (MVA)</title>
        <imprint>
          <biblScope type="volume">16</biblScope>
          <biblScope type="number">1</biblScope>
          <dateStruct>
            <month>December</month>
            <year>2004</year>
          </dateStruct>
          <biblScope type="pages">33-40</biblScope>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid102" type="article" rend="refer" n="refercite:martin:2010:inria-00499599:1">
      <analytic>
        <title level="a">Thermal Event Recognition Applied to Protection of Tokamak Plasma-Facing Components</title>
        <author>
          <persName key="reo-2014-idp106312">
            <foreName>Vincent</foreName>
            <surname>Martin</surname>
            <initial>V.</initial>
          </persName>
          <persName>
            <foreName>Jean-Marcel</foreName>
            <surname>Travere</surname>
            <initial>J.-M.</initial>
          </persName>
          <persName key="stars-2014-idp60040">
            <foreName>Francois</foreName>
            <surname>Bremond</surname>
            <initial>F.</initial>
          </persName>
          <persName>
            <foreName>Victor</foreName>
            <surname>Moncada</surname>
            <initial>V.</initial>
          </persName>
          <persName>
            <foreName>Gwenael</foreName>
            <surname>Dunand</surname>
            <initial>G.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-editorial-board="yes" x-international-audience="yes">
        <title level="j">IEEE Transactions on Instrumentation and Measurement</title>
        <imprint>
          <biblScope type="volume">59</biblScope>
          <biblScope type="number">5</biblScope>
          <dateStruct>
            <month>Apr</month>
            <year>2010</year>
          </dateStruct>
          <biblScope type="pages">1182-1191</biblScope>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid99" type="inproceedings" rend="refer" n="refercite:ECAI02-sabine">
      <analytic>
        <title level="a">Knowledge Representation for Program Reuse</title>
        <author>
          <persName key="stars-2014-idp64040">
            <foreName>S.</foreName>
            <surname>Moisan</surname>
            <initial>S.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="m">European Conference on Artificial Intelligence (ECAI)</title>
        <loc>Lyon, France</loc>
        <imprint>
          <dateStruct>
            <month>July</month>
            <year>2002</year>
          </dateStruct>
          <biblScope type="pages">240-244</biblScope>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid0" type="hdrthesis" rend="refer" n="refercite:Lama98">
      <monogr>
        <title level="m">Une plate-forme pour une programmation par composants de systèmes à base de connaissances</title>
        <author>
          <persName key="stars-2014-idp64040">
            <foreName>S.</foreName>
            <surname>Moisan</surname>
            <initial>S.</initial>
          </persName>
        </author>
        <imprint>
          <publisher>
            <orgName type="school">Université de Nice-Sophia Antipolis</orgName>
          </publisher>
          <dateStruct>
            <month>April</month>
            <year>1998</year>
          </dateStruct>
        </imprint>
      </monogr>
      <note type="typdoc">Habilitation à diriger les recherches</note>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid89" type="article" rend="refer" n="refercite:Informatica01">
      <analytic>
        <title level="a">Blocks, a Component Framework with Checking Facilities for Knowledge-Based Systems</title>
        <author>
          <persName key="stars-2014-idp64040">
            <foreName>S.</foreName>
            <surname>Moisan</surname>
            <initial>S.</initial>
          </persName>
          <persName key="stars-2014-idp65472">
            <foreName>A.</foreName>
            <surname>Ressouche</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>J-P.</foreName>
            <surname>Rigault</surname>
            <initial>J.-P.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">Informatica, Special Issue on Component Based Software Development</title>
        <imprint>
          <biblScope type="volume">25</biblScope>
          <biblScope type="number">4</biblScope>
          <dateStruct>
            <month>November</month>
            <year>2001</year>
          </dateStruct>
          <biblScope type="pages">501-507</biblScope>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid93" type="inproceedings" rend="refer" n="refercite:Patino2007a">
      <analytic>
        <title level="a">Video-Data Modelling and Discovery</title>
        <author>
          <persName>
            <foreName>J.L.</foreName>
            <surname>Patino</surname>
            <initial>J.</initial>
          </persName>
          <persName>
            <foreName>H.</foreName>
            <surname>Benhadda</surname>
            <initial>H.</initial>
          </persName>
          <persName key="stars-2014-idp116256">
            <foreName>E.</foreName>
            <surname>Corvee</surname>
            <initial>E.</initial>
          </persName>
          <persName key="stars-2014-idp60040">
            <foreName>F.</foreName>
            <surname>Bremond</surname>
            <initial>F.</initial>
          </persName>
          <persName key="stars-2014-idp66712">
            <foreName>M.</foreName>
            <surname>Thonnat</surname>
            <initial>M.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="m">4th IET International Conference on Visual Information Engineering VIE 2007</title>
        <loc>London, UK</loc>
        <imprint>
          <dateStruct>
            <month>25th - 27th July</month>
            <year>2007</year>
          </dateStruct>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid94" type="inproceedings" rend="refer" n="refercite:Patino2007b">
      <analytic>
        <title level="a">Management of Large Video Recordings</title>
        <author>
          <persName>
            <foreName>J.L.</foreName>
            <surname>Patino</surname>
            <initial>J.</initial>
          </persName>
          <persName key="stars-2014-idp116256">
            <foreName>E.</foreName>
            <surname>Corvee</surname>
            <initial>E.</initial>
          </persName>
          <persName key="stars-2014-idp60040">
            <foreName>F.</foreName>
            <surname>Bremond</surname>
            <initial>F.</initial>
          </persName>
          <persName key="stars-2014-idp66712">
            <foreName>M.</foreName>
            <surname>Thonnat</surname>
            <initial>M.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="m">2nd International Conference on Ambient Intelligence Developments AmI.d 2007</title>
        <loc>Sophia Antipolis, France</loc>
        <imprint>
          <dateStruct>
            <month>17th - 19th September</month>
            <year>2007</year>
          </dateStruct>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid101" type="incollection" rend="refer" n="refercite:clem-sera2008">
      <analytic>
        <title level="a">Modular Compilation of a Synchronous Language</title>
        <author>
          <persName key="stars-2014-idp65472">
            <foreName>Annie</foreName>
            <surname>Ressouche</surname>
            <initial>A.</initial>
          </persName>
          <persName key="stars-2014-idp62776">
            <foreName>Daniel</foreName>
            <surname>Gaffé</surname>
            <initial>D.</initial>
          </persName>
          <persName>
            <foreName>Valérie</foreName>
            <surname>Roy</surname>
            <initial>V.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <editor role="editor">
          <persName>
            <foreName>Roger</foreName>
            <surname>Lee</surname>
            <initial>R.</initial>
          </persName>
        </editor>
        <title level="m">Software Engineering Research, Management and Applications</title>
        <title level="s">Studies in Computational Intelligence</title>
        <imprint>
          <biblScope type="volume">150</biblScope>
          <publisher>
            <orgName>Springer</orgName>
          </publisher>
          <dateStruct>
            <year>2008</year>
          </dateStruct>
          <biblScope type="pages">157-171</biblScope>
        </imprint>
      </monogr>
      <note type="bnote">selected as one of the 17 best papers of SERA'08 conference</note>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid105" type="article" rend="refer" n="refercite:ressouche:2011:inria-00524499:1">
      <identifiant type="hal" value="inria-00524499"/>
      <analytic>
        <title level="a">Compilation Modulaire d'un Langage Synchrone</title>
        <author>
          <persName key="stars-2014-idp65472">
            <foreName>Annie</foreName>
            <surname>Ressouche</surname>
            <initial>A.</initial>
          </persName>
          <persName key="stars-2014-idp62776">
            <foreName>Daniel</foreName>
            <surname>Gaffé</surname>
            <initial>D.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-editorial-board="yes" x-international-audience="no">
        <title level="j">Revue des sciences et technologies de l'information, série Théorie et Science Informat ique</title>
        <imprint>
          <biblScope type="volume">4</biblScope>
          <biblScope type="number">30</biblScope>
          <dateStruct>
            <month>June</month>
            <year>2011</year>
          </dateStruct>
          <biblScope type="pages">441-471</biblScope>
          <ref xlink:href="http://hal.inria.fr/inria-00524499/en" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>inria-00524499/<allowbreak/>en</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid87" type="article" rend="refer" n="refercite:IEE00">
      <analytic>
        <title level="a">What Can Program Supervision Do for Software Re-use?</title>
        <author>
          <persName key="stars-2014-idp66712">
            <foreName>M.</foreName>
            <surname>Thonnat</surname>
            <initial>M.</initial>
          </persName>
          <persName key="stars-2014-idp64040">
            <foreName>S.</foreName>
            <surname>Moisan</surname>
            <initial>S.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">IEE Proceedings - Software Special Issue on Knowledge Modelling for Software Components Reuse</title>
        <imprint>
          <biblScope type="volume">147</biblScope>
          <biblScope type="number">5</biblScope>
          <dateStruct>
            <year>2000</year>
          </dateStruct>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid91" type="hdrthesis" rend="refer" n="refercite:MHDR03">
      <monogr>
        <title level="m">Vers une vision cognitive: mise en oeuvre de connaissances et de raisonnements pour l'analyse et l'interprétation d'images</title>
        <author>
          <persName key="stars-2014-idp66712">
            <foreName>M.</foreName>
            <surname>Thonnat</surname>
            <initial>M.</initial>
          </persName>
        </author>
        <imprint>
          <publisher>
            <orgName type="school">Université de Nice-Sophia Antipolis</orgName>
          </publisher>
          <dateStruct>
            <month>October</month>
            <year>2003</year>
          </dateStruct>
        </imprint>
      </monogr>
      <note type="typdoc">Habilitation à diriger les recherches</note>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid103" type="article" rend="refer" n="refercite:thonnat:2010:inria-00502843:1">
      <analytic>
        <title level="a">Special issue on Intelligent Vision Systems</title>
        <author>
          <persName key="stars-2014-idp66712">
            <foreName>Monique</foreName>
            <surname>Thonnat</surname>
            <initial>M.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-editorial-board="yes" x-international-audience="yes">
        <title level="j">Computer Vision and Image Understanding</title>
        <imprint>
          <biblScope type="volume">114</biblScope>
          <biblScope type="number">5</biblScope>
          <dateStruct>
            <month>May</month>
            <year>2010</year>
          </dateStruct>
          <biblScope type="pages">501-502</biblScope>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid96" type="inproceedings" rend="refer" n="refercite:Toshev2006">
      <analytic>
        <title level="a">An A priori-based Method for Frequent Composite Event Discovery in Videos</title>
        <author>
          <persName>
            <foreName>A.</foreName>
            <surname>Toshev</surname>
            <initial>A.</initial>
          </persName>
          <persName key="stars-2014-idp60040">
            <foreName>F.</foreName>
            <surname>Brémond</surname>
            <initial>F.</initial>
          </persName>
          <persName key="stars-2014-idp66712">
            <foreName>M.</foreName>
            <surname>Thonnat</surname>
            <initial>M.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="m">Proceedings of 2006 IEEE International Conference on Computer Vision Systems</title>
        <loc>New York USA</loc>
        <imprint>
          <dateStruct>
            <month>January</month>
            <year>2006</year>
          </dateStruct>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid90" type="inproceedings" rend="refer" n="refercite:ECAI2002">
      <analytic>
        <title level="a">Temporal Constraints for Video Interpretation</title>
        <author>
          <persName>
            <foreName>V.T.</foreName>
            <surname>Vu</surname>
            <initial>V.</initial>
          </persName>
          <persName key="stars-2014-idp60040">
            <foreName>F.</foreName>
            <surname>Brémond</surname>
            <initial>F.</initial>
          </persName>
          <persName key="stars-2014-idp66712">
            <foreName>M.</foreName>
            <surname>Thonnat</surname>
            <initial>M.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="m">Proc of the 15th European Conference on Artificial Intelligence</title>
        <loc>Lyon, France</loc>
        <imprint>
          <dateStruct>
            <year>2002</year>
          </dateStruct>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid86" type="inproceedings" rend="refer" n="refercite:IJCAI2003">
      <analytic>
        <title level="a">Automatic Video Interpretation: A Novel Algorithm based for Temporal Scenario Recognition</title>
        <author>
          <persName>
            <foreName>V.T.</foreName>
            <surname>Vu</surname>
            <initial>V.</initial>
          </persName>
          <persName key="stars-2014-idp60040">
            <foreName>F.</foreName>
            <surname>Brémond</surname>
            <initial>F.</initial>
          </persName>
          <persName key="stars-2014-idp66712">
            <foreName>M.</foreName>
            <surname>Thonnat</surname>
            <initial>M.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="m">The Eighteenth International Joint Conference on Artificial Intelligence (IJCAI'03)</title>
        <imprint>
          <dateStruct>
            <month>9-15 September</month>
            <year>2003</year>
          </dateStruct>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid104" type="article" rend="refer" n="refercite:zouba:2010:inria-00504703:1">
      <analytic>
        <title level="a">Monitoring elderly activities at home</title>
        <author>
          <persName>
            <foreName>Nadia</foreName>
            <surname>Zouba</surname>
            <initial>N.</initial>
          </persName>
          <persName key="stars-2014-idp60040">
            <foreName>Francois</foreName>
            <surname>Bremond</surname>
            <initial>F.</initial>
          </persName>
          <persName>
            <foreName>Alain</foreName>
            <surname>Anfosso</surname>
            <initial>A.</initial>
          </persName>
          <persName key="stars-2014-idp66712">
            <foreName>Monique</foreName>
            <surname>Thonnat</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>Eric</foreName>
            <surname>Pascual</surname>
            <initial>E.</initial>
          </persName>
          <persName>
            <foreName>Olivier</foreName>
            <surname>Guerin</surname>
            <initial>O.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-editorial-board="yes" x-international-audience="yes">
        <title level="j">Gerontechnology</title>
        <imprint>
          <biblScope type="volume">9</biblScope>
          <biblScope type="number">2</biblScope>
          <dateStruct>
            <month>May</month>
            <year>2010</year>
          </dateStruct>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid81" type="article" rend="year" n="cite:crispimjunior:hal-01399025">
      <identifiant type="doi" value="10.1109/TPAMI.2016.2537323"/>
      <identifiant type="hal" value="hal-01399025"/>
      <analytic>
        <title level="a">Semantic Event Fusion of Different Visual Modality Concepts for Activity Recognition</title>
        <author>
          <persName>
            <foreName>Carlos F</foreName>
            <surname>Crispim-Junior</surname>
            <initial>C. F.</initial>
          </persName>
          <persName>
            <foreName>Vincent</foreName>
            <surname>Buso</surname>
            <initial>V.</initial>
          </persName>
          <persName>
            <foreName>Konstantinos</foreName>
            <surname>Avgerinakis</surname>
            <initial>K.</initial>
          </persName>
          <persName>
            <foreName>Georgios</foreName>
            <surname>Meditskos</surname>
            <initial>G.</initial>
          </persName>
          <persName>
            <foreName>Alexia</foreName>
            <surname>Briassouli</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Jenny</foreName>
            <surname>Benois-Pineau</surname>
            <initial>J.</initial>
          </persName>
          <persName>
            <foreName>Yiannis</foreName>
            <surname>KOMPATSIARIS</surname>
            <initial>Y.</initial>
          </persName>
          <persName key="stars-2014-idp60040">
            <foreName>Francois</foreName>
            <surname>Bremond</surname>
            <initial>F.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid00747">
        <idno type="issn">0162-8828</idno>
        <title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
        <imprint>
          <biblScope type="volume">38</biblScope>
          <dateStruct>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">1598 - 1611</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01399025" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01399025</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid61" type="article" rend="year" n="cite:dantcheva:hal-01412408">
      <identifiant type="doi" value="10.1109/TIFS.2016.2632070"/>
      <identifiant type="hal" value="hal-01412408"/>
      <analytic>
        <title level="a">Gender estimation based on smile-dynamics</title>
        <author>
          <persName key="stars-2014-idp88472">
            <foreName>Antitza</foreName>
            <surname>Dantcheva</surname>
            <initial>A.</initial>
          </persName>
          <persName key="stars-2014-idp60040">
            <foreName>François</foreName>
            <surname>Brémond</surname>
            <initial>F.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid00732">
        <idno type="issn">1556-6013</idno>
        <title level="j">IEEE Transactions on Information Forensics and Security</title>
        <imprint>
          <dateStruct>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">11</biblScope>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01412408" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01412408</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid62" type="inproceedings" rend="year" n="cite:bilinski:hal-01387134">
      <identifiant type="hal" value="hal-01387134"/>
      <analytic>
        <title level="a">Can a smile reveal your gender?</title>
        <author>
          <persName key="stars-2014-idp113792">
            <foreName>Piotr</foreName>
            <surname>Bilinski</surname>
            <initial>P.</initial>
          </persName>
          <persName key="stars-2014-idp88472">
            <foreName>Antitza</foreName>
            <surname>Dantcheva</surname>
            <initial>A.</initial>
          </persName>
          <persName key="stars-2014-idp60040">
            <foreName>François</foreName>
            <surname>Brémond</surname>
            <initial>F.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="no" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">15th International Conference of the Biometrics Special Interest Group (BIOSIG 2016)</title>
        <loc>Darmstadt, Germany</loc>
        <imprint>
          <dateStruct>
            <month>September</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01387134" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01387134</ref>
        </imprint>
        <meeting id="cid38114">
          <title>Biometrics and Electronic Signatures Research and Applications Workshop</title>
          <num>15</num>
          <abbr type="sigle">BIOSIG</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid11" type="inproceedings" rend="year" n="cite:chen:hal-01414757">
      <identifiant type="hal" value="hal-01414757"/>
      <analytic>
        <title level="a">Exploring Depth Information for Head Detection with Depth Images</title>
        <author>
          <persName key="stars-2015-idp131904">
            <foreName>Siyuan</foreName>
            <surname>Chen</surname>
            <initial>S.</initial>
          </persName>
          <persName key="stars-2014-idp60040">
            <foreName>Francois</foreName>
            <surname>Bremond</surname>
            <initial>F.</initial>
          </persName>
          <persName key="galaad2-2014-idp68304">
            <foreName>Hung</foreName>
            <surname>Nguyen</surname>
            <initial>H.</initial>
          </persName>
          <persName key="stars-2016-idp228848">
            <foreName>Hugues</foreName>
            <surname>Thomas</surname>
            <initial>H.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="yes" x-editorial-board="yes">
        <title level="m">AVSS 2016 - 13th International Conference on Advanced Video and Signal-Based Surveillance</title>
        <loc>Colorado Springs, United States</loc>
        <imprint>
          <dateStruct>
            <month>August</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01414757" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01414757</ref>
        </imprint>
        <meeting id="cid80971">
          <title>IEEE International Conference on Advanced Video and Signal based Surveillance</title>
          <num>13</num>
          <abbr type="sigle">AVSS</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid67" type="inproceedings" rend="year" n="cite:fernandocrispimjunior:hal-01398958">
      <identifiant type="hal" value="hal-01398958"/>
      <analytic>
        <title level="a">Semi-supervised understanding of complex activities from temporal concepts</title>
        <author>
          <persName>
            <foreName>Carlos Fernando</foreName>
            <surname>Crispim-Junior</surname>
            <initial>C. F.</initial>
          </persName>
          <persName key="stars-2014-idp123800">
            <foreName>Michal</foreName>
            <surname>Koperski</surname>
            <initial>M.</initial>
          </persName>
          <persName key="stars-2014-idp87240">
            <foreName>Serhan</foreName>
            <surname>Cosar</surname>
            <initial>S.</initial>
          </persName>
          <persName key="stars-2014-idp60040">
            <foreName>Francois</foreName>
            <surname>Bremond</surname>
            <initial>F.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">13th International Conference on Advanced Video and Signal-Based Surveillance</title>
        <loc>Colorado Springs, United States</loc>
        <imprint>
          <dateStruct>
            <month>August</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01398958" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01398958</ref>
        </imprint>
        <meeting id="cid80971">
          <title>IEEE International Conference on Advanced Video and Signal based Surveillance</title>
          <num>13</num>
          <abbr type="sigle">AVSS</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid77" type="inproceedings" rend="year" n="cite:demaria:hal-01377288">
      <identifiant type="doi" value="10.1007/978-3-319-47151-8_7"/>
      <identifiant type="hal" value="hal-01377288"/>
      <analytic>
        <title level="a">Verification of Temporal Properties of Neuronal Archetypes Modeled as Synchronous Reactive Systems</title>
        <author>
          <persName>
            <foreName>Elisabetta</foreName>
            <surname>De Maria</surname>
            <initial>E.</initial>
          </persName>
          <persName>
            <foreName>Alexandre</foreName>
            <surname>Muzy</surname>
            <initial>A.</initial>
          </persName>
          <persName key="stars-2014-idp62776">
            <foreName>Daniel</foreName>
            <surname>Gaffé</surname>
            <initial>D.</initial>
          </persName>
          <persName key="stars-2014-idp65472">
            <foreName>Annie</foreName>
            <surname>Ressouche</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Franck</foreName>
            <surname>Grammont</surname>
            <initial>F.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">HSB 2016 - 5th International Workshop Hybrid Systems Biology</title>
        <loc>Grenoble, France</loc>
        <title level="s">Lecture Notes in Bioinformatics series</title>
        <imprint>
          <dateStruct>
            <month>October</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">15</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01377288" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01377288</ref>
        </imprint>
        <meeting id="cid624903">
          <title>International Workshop on Hybrid Systems Biology</title>
          <num>5</num>
          <abbr type="sigle"/>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid63" type="inproceedings" rend="year" n="cite:gonzalezsosa:hal-01384324">
      <identifiant type="hal" value="hal-01384324"/>
      <analytic>
        <title level="a">Image-based Gender Estimation from Body and Face across Distances</title>
        <author>
          <persName>
            <foreName>Ester</foreName>
            <surname>Gonzalez-Sosa</surname>
            <initial>E.</initial>
          </persName>
          <persName key="stars-2014-idp88472">
            <foreName>Antitza</foreName>
            <surname>Dantcheva</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Ruben</foreName>
            <surname>Vera-Rodriguez</surname>
            <initial>R.</initial>
          </persName>
          <persName>
            <foreName>Jean-Luc</foreName>
            <surname>Dugelay</surname>
            <initial>J.-L.</initial>
          </persName>
          <persName key="stars-2014-idp60040">
            <foreName>François</foreName>
            <surname>Brémond</surname>
            <initial>F.</initial>
          </persName>
          <persName>
            <foreName>Julian</foreName>
            <surname>Fierrez</surname>
            <initial>J.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="no" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">23rd International Conference on Pattern Recognition (ICPR 2016): "Image analysis and machine learning for scene understanding"</title>
        <loc>Cancun, Mexico</loc>
        <imprint>
          <dateStruct>
            <month>December</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01384324" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01384324</ref>
        </imprint>
        <meeting id="cid295950">
          <title>International Conference on Pattern Recognition</title>
          <num>23</num>
          <abbr type="sigle">ICPR</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid64" type="inproceedings" rend="year" n="cite:khan:hal-01400147">
      <identifiant type="doi" value="10.1109/AVSS.2016.7738058"/>
      <identifiant type="hal" value="hal-01400147"/>
      <analytic>
        <title level="a">Unsupervised data association for metric learning in the context of multi-shot person re-identification</title>
        <author>
          <persName key="stars-2015-idp112000">
            <foreName>Furqan M</foreName>
            <surname>Khan</surname>
            <initial>F. M.</initial>
          </persName>
          <persName key="stars-2014-idp60040">
            <foreName>Francois</foreName>
            <surname>Bremond</surname>
            <initial>F.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">Advance Video and Signal based Surveillance</title>
        <loc>Colorado Springs, United States</loc>
        <imprint>
          <dateStruct>
            <month>August</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01400147" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01400147</ref>
        </imprint>
        <meeting id="cid80971">
          <title>IEEE International Conference on Advanced Video and Signal based Surveillance</title>
          <num>13</num>
          <abbr type="sigle">AVSS</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid15" type="inproceedings" rend="year" n="cite:koperski:hal-01399037">
      <identifiant type="doi" value="10.1109/AVSS.2016.7738023"/>
      <identifiant type="hal" value="hal-01399037"/>
      <analytic>
        <title level="a">Modeling Spatial Layout of Features for Real World Scenario RGB-D Action Recognition</title>
        <author>
          <persName key="stars-2014-idp123800">
            <foreName>Michal</foreName>
            <surname>Koperski</surname>
            <initial>M.</initial>
          </persName>
          <persName key="stars-2014-idp60040">
            <foreName>Francois</foreName>
            <surname>Bremond</surname>
            <initial>F.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="no" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">AVSS 2016</title>
        <loc>Colorado Springs, United States</loc>
        <imprint>
          <dateStruct>
            <month>August</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">44 - 50</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01399037" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01399037</ref>
        </imprint>
        <meeting id="cid80971">
          <title>IEEE International Conference on Advanced Video and Signal based Surveillance</title>
          <num>13</num>
          <abbr type="sigle">AVSS</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid83" type="inproceedings" rend="year" n="cite:negin:hal-01416372">
      <identifiant type="hal" value="hal-01416372"/>
      <analytic>
        <title level="a">Praxis and Gesture Recognition</title>
        <author>
          <persName key="stars-2014-idp125016">
            <foreName>Farhood</foreName>
            <surname>Negin</surname>
            <initial>F.</initial>
          </persName>
          <persName>
            <foreName>Jérémy</foreName>
            <surname>Bourgeois</surname>
            <initial>J.</initial>
          </persName>
          <persName key="reves-2014-idp69088">
            <foreName>Emmanuelle</foreName>
            <surname>Chapoulie</surname>
            <initial>E.</initial>
          </persName>
          <persName key="rap-2014-idm40528">
            <foreName>Philippe</foreName>
            <surname>Robert</surname>
            <initial>P.</initial>
          </persName>
          <persName key="stars-2014-idp60040">
            <foreName>Francois</foreName>
            <surname>Bremond</surname>
            <initial>F.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">The 10th World Conference of Gerontechnology (ISG 2016)</title>
        <loc>Nice, France</loc>
        <imprint>
          <dateStruct>
            <month>September</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01416372" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01416372</ref>
        </imprint>
        <meeting id="cid45809">
          <title>Conference of the International Society for Gerontechnology</title>
          <num>10</num>
          <abbr type="sigle">ISG</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid72" type="inproceedings" rend="year" n="cite:negin:hal-01384710">
      <identifiant type="doi" value="10.1109/AVSS.2016.7738021"/>
      <identifiant type="hal" value="hal-01384710"/>
      <analytic>
        <title level="a">A hybrid framework for online recognition of activities of daily living in real-world settings</title>
        <author>
          <persName key="stars-2014-idp125016">
            <foreName>Farhood F</foreName>
            <surname>Negin</surname>
            <initial>F. F.</initial>
          </persName>
          <persName key="stars-2014-idp87240">
            <foreName>Serhan</foreName>
            <surname>Cosar</surname>
            <initial>S.</initial>
          </persName>
          <persName key="stars-2014-idp123800">
            <foreName>Michal F</foreName>
            <surname>Koperski</surname>
            <initial>M. F.</initial>
          </persName>
          <persName>
            <foreName>Carlos F</foreName>
            <surname>Crispim-Junior</surname>
            <initial>C. F.</initial>
          </persName>
          <persName>
            <foreName>Konstantinos</foreName>
            <surname>Avgerinakis</surname>
            <initial>K.</initial>
          </persName>
          <persName key="stars-2014-idp60040">
            <foreName>Francois F</foreName>
            <surname>Bremond</surname>
            <initial>F. F.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="yes" x-editorial-board="yes">
        <title level="m">13th IEEE International Conference on Advanced Video and Signal Based Surveillance - AVSS 2016</title>
        <loc>Colorado springs, United States</loc>
        <imprint>
          <publisher>
            <orgName>IEEE</orgName>
          </publisher>
          <dateStruct>
            <month>August</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01384710" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01384710</ref>
        </imprint>
        <meeting id="cid80971">
          <title>IEEE International Conference on Advanced Video and Signal based Surveillance</title>
          <num>13</num>
          <abbr type="sigle">AVSS</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid79" type="inproceedings" rend="year" n="cite:phantran:hal-01369878">
      <identifiant type="hal" value="hal-01369878"/>
      <analytic>
        <title level="a">A Virtual Agent for enhancing performance and engagement of older people with dementia in Serious Games</title>
        <author>
          <persName key="stars-2014-idp130088">
            <foreName>Minh Khue</foreName>
            <surname>Phan Tran</surname>
            <initial>M. K.</initial>
          </persName>
          <persName key="rap-2014-idm40528">
            <foreName>Philippe</foreName>
            <surname>Robert</surname>
            <initial>P.</initial>
          </persName>
          <persName key="stars-2014-idp60040">
            <foreName>François</foreName>
            <surname>Bremond</surname>
            <initial>F.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="no" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">Workshop Artificial Compagnon-Affect-Interaction 2016</title>
        <loc>Brest, France</loc>
        <imprint>
          <dateStruct>
            <month>June</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01369878" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01369878</ref>
        </imprint>
        <meeting id="cid624678">
          <title>Workshop Affect, Compagnon Artificiel, Interaction</title>
          <num>2016</num>
          <abbr type="sigle">WACAI</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid33" type="inproceedings" rend="year" n="cite:thilananh:hal-01383186">
      <identifiant type="hal" value="hal-01383186"/>
      <analytic>
        <title level="a">Multi-Object Tracking of Pedestrian Driven by Context</title>
        <author>
          <persName>
            <foreName>Nguyen</foreName>
            <surname>Thi Lan Anh</surname>
            <initial>N.</initial>
          </persName>
          <persName key="stars-2014-idp60040">
            <foreName>Francois</foreName>
            <surname>Bremond</surname>
            <initial>F.</initial>
          </persName>
          <persName key="stars-2016-idp171696">
            <foreName>Jana</foreName>
            <surname>Trojanova</surname>
            <initial>J.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="no" x-invited-conference="no" x-editorial-board="no">
        <title level="m">Advance Video and Signal-based Surveillance</title>
        <loc>Colorado Springs, United States</loc>
        <imprint>
          <publisher>
            <orgName type="organisation">IEEE</orgName>
          </publisher>
          <dateStruct>
            <month>August</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01383186" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01383186</ref>
        </imprint>
        <meeting id="cid80971">
          <title>IEEE International Conference on Advanced Video and Signal based Surveillance</title>
          <num>13</num>
          <abbr type="sigle">AVSS</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid80" type="techreport" rend="year" n="cite:demaria:hal-01349019">
      <identifiant type="hal" value="hal-01349019"/>
      <monogr>
        <title level="m">Verification of Temporal Properties of Neuronal Archetypes Using Synchronous Models</title>
        <author>
          <persName>
            <foreName>Elisabetta</foreName>
            <surname>De Maria</surname>
            <initial>E.</initial>
          </persName>
          <persName>
            <foreName>Alexandre</foreName>
            <surname>Muzy</surname>
            <initial>A.</initial>
          </persName>
          <persName key="stars-2014-idp62776">
            <foreName>Daniel</foreName>
            <surname>Gaffé</surname>
            <initial>D.</initial>
          </persName>
          <persName key="stars-2014-idp65472">
            <foreName>Annie</foreName>
            <surname>Ressouche</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Franck</foreName>
            <surname>Grammont</surname>
            <initial>F.</initial>
          </persName>
        </author>
        <imprint>
          <biblScope type="number">RR-8937</biblScope>
          <publisher>
            <orgName type="institution">UCA, Inria ; UCA, I3S ; UCA, LEAT ; UCA, LJAD</orgName>
          </publisher>
          <dateStruct>
            <month>July</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">21</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01349019" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01349019</ref>
        </imprint>
      </monogr>
      <note type="typdoc">Research Report</note>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid78" type="techreport" rend="year" n="cite:moisan:hal-01392796">
      <identifiant type="hal" value="hal-01392796"/>
      <monogr>
        <title level="m">Dynamic Reconfiguration of Feature Models: an Algorithm and its Evaluation</title>
        <author>
          <persName key="stars-2014-idp64040">
            <foreName>Sabine</foreName>
            <surname>Moisan</surname>
            <initial>S.</initial>
          </persName>
          <persName key="stars-2014-idp135032">
            <foreName>Jean-Paul</foreName>
            <surname>Rigault</surname>
            <initial>J.-P.</initial>
          </persName>
        </author>
        <imprint>
          <biblScope type="number">RR-8972</biblScope>
          <publisher>
            <orgName type="institution">Inria Sophia Antipolis</orgName>
          </publisher>
          <dateStruct>
            <month>November</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">16</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01392796" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01392796</ref>
        </imprint>
      </monogr>
      <note type="typdoc">Research Report</note>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid82" type="misc" rend="year" n="cite:crispimjunior:hal-01399259">
      <identifiant type="hal" value="hal-01399259"/>
      <monogr x-scientific-popularization="no">
        <title level="m">Automatic prediction of autonomy in activities of daily living of older adults</title>
        <author>
          <persName>
            <foreName>Carlos Fernando</foreName>
            <surname>Crispim-Junior</surname>
            <initial>C. F.</initial>
          </persName>
          <persName key="stars-2014-idp142632">
            <foreName>Alexandra</foreName>
            <surname>Konig</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Renaud</foreName>
            <surname>David</surname>
            <initial>R.</initial>
          </persName>
          <persName key="rap-2014-idm40528">
            <foreName>Philippe</foreName>
            <surname>Robert</surname>
            <initial>P.</initial>
          </persName>
          <persName key="stars-2014-idp60040">
            <foreName>Francois</foreName>
            <surname>Bremond</surname>
            <initial>F.</initial>
          </persName>
        </author>
        <imprint>
          <dateStruct>
            <month>November</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">74s</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01399259" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01399259</ref>
        </imprint>
      </monogr>
      <note type="bnote">Short-paper</note>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid65" type="unpublished" rend="year" n="cite:khan:hal-01399939">
      <identifiant type="hal" value="hal-01399939"/>
      <monogr>
        <title level="m">Person Re-identification for Real-world Surveillance Systems</title>
        <author>
          <persName key="stars-2015-idp112000">
            <foreName>Furqan M</foreName>
            <surname>Khan</surname>
            <initial>F. M.</initial>
          </persName>
          <persName key="stars-2014-idp60040">
            <foreName>François M</foreName>
            <surname>Brémond</surname>
            <initial>F. M.</initial>
          </persName>
        </author>
        <imprint>
          <dateStruct>
            <month>November</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01399939" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01399939</ref>
        </imprint>
      </monogr>
      <note type="bnote">working paper or preprint</note>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid3" type="inproceedings" rend="foot" n="footcite:MRT09">
      <identifiant type="hal" value="hal-00419990"/>
      <analytic>
        <title level="a">Modeling Context and Dynamic Adaptations with Feature Models</title>
        <author>
          <persName key="diverse-2014-idm5288">
            <foreName>M</foreName>
            <surname>Acher</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>P</foreName>
            <surname>Collet</surname>
            <initial>P.</initial>
          </persName>
          <persName>
            <foreName>F</foreName>
            <surname>Fleurey</surname>
            <initial>F.</initial>
          </persName>
          <persName>
            <foreName>P</foreName>
            <surname>Lahire</surname>
            <initial>P.</initial>
          </persName>
          <persName key="stars-2014-idp64040">
            <foreName>Sabine</foreName>
            <surname>Moisan</surname>
            <initial>S.</initial>
          </persName>
          <persName key="stars-2014-idp135032">
            <foreName>Jean-Paul</foreName>
            <surname>Rigault</surname>
            <initial>J.-P.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="m">Models@run.time Workshop</title>
        <loc>Denver, CO, USA</loc>
        <imprint>
          <dateStruct>
            <month>October</month>
            <year>2009</year>
          </dateStruct>
          <ref xlink:href="http://hal.inria.fr/hal-00419990/en" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-00419990/<allowbreak/>en</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid1" type="inproceedings" rend="foot" n="footcite:ICSE-MISE09">
      <identifiant type="hal" value="hal-00415770"/>
      <analytic>
        <title level="a">Tackling High Variability in Video Surveillance Systems through a Model Transformation Approach</title>
        <author>
          <persName key="diverse-2014-idm5288">
            <foreName>M</foreName>
            <surname>Acher</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>P</foreName>
            <surname>Lahire</surname>
            <initial>P.</initial>
          </persName>
          <persName key="stars-2014-idp64040">
            <foreName>Sabine</foreName>
            <surname>Moisan</surname>
            <initial>S.</initial>
          </persName>
          <persName key="stars-2014-idp135032">
            <foreName>Jean-Paul</foreName>
            <surname>Rigault</surname>
            <initial>J.-P.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="m">ICSE'2009 - MISE Workshop</title>
        <loc>Vancouver, Canada</loc>
        <imprint>
          <dateStruct>
            <month>May</month>
            <year>2009</year>
          </dateStruct>
          <ref xlink:href="http://hal.inria.fr/hal-00415770/en" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-00415770/<allowbreak/>en</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid55" type="article" rend="foot" n="footcite:alahi:2009">
      <analytic>
        <title level="a">Sparsity-driven people localization algorithm: Evaluation in crowded scenes environments</title>
        <author>
          <persName>
            <foreName>A.</foreName>
            <surname>Alahi</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>L.</foreName>
            <surname>Jacques</surname>
            <initial>L.</initial>
          </persName>
          <persName>
            <foreName>Y.</foreName>
            <surname>Boursier</surname>
            <initial>Y.</initial>
          </persName>
          <persName key="panama-2015-idp65528">
            <foreName>P.</foreName>
            <surname>Vandergheynst</surname>
            <initial>P.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">PETS workshop</title>
        <imprint>
          <dateStruct>
            <year>2009</year>
          </dateStruct>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid38" type="inproceedings" rend="foot" n="footcite:Schindler_2012">
      <identifiant type="doi" value="10.1109/CVPR.2011.5995311"/>
      <analytic>
        <title level="a">Multi-target tracking by continuous energy minimization</title>
        <author>
          <persName>
            <foreName>A.</foreName>
            <surname>Andriyenko</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>K.</foreName>
            <surname>Schindler</surname>
            <initial>K.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
        <imprint>
          <dateStruct>
            <month>June</month>
            <year>2011</year>
          </dateStruct>
          <biblScope type="pages">1265-1272</biblScope>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid54" type="article" rend="foot" n="footcite:arsic:2009">
      <analytic>
        <title level="a">Multi-camera person tracking applying a graph-cuts based foreground segmentation in a homography framework</title>
        <author>
          <persName>
            <foreName>D.</foreName>
            <surname>Arsic</surname>
            <initial>D.</initial>
          </persName>
          <persName>
            <foreName>A.</foreName>
            <surname>Lyutskanov</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>G.</foreName>
            <surname>Rigoll</surname>
            <initial>G.</initial>
          </persName>
          <persName>
            <foreName>B.</foreName>
            <surname>Kwolek</surname>
            <initial>B.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">PETS workshop</title>
        <imprint>
          <dateStruct>
            <year>2009</year>
          </dateStruct>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid74" type="inproceedings" rend="foot" n="footcite:greekCVIU20151">
      <analytic>
        <title level="a">Activity detection using sequential statistical boundary detection (ssbd)</title>
        <author>
          <persName>
            <foreName>Konstantinos</foreName>
            <surname>Avgerinakis</surname>
            <initial>K.</initial>
          </persName>
          <persName>
            <foreName>Alexia</foreName>
            <surname>Briassouli</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Ioannis</foreName>
            <surname>Kompatsiaris</surname>
            <initial>I.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="m">to appear in Computer Vision and Image Understanding</title>
        <imprint>
          <publisher>
            <orgName type="organisation">CVIU</orgName>
          </publisher>
          <dateStruct>
            <year>2015</year>
          </dateStruct>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid35" type="inproceedings" rend="foot" n="footcite:CVPR_2014">
      <analytic>
        <title level="a">Robust Online Multi-Object Tracking based on Tracklet Confidence and Online Discriminative Appearance Learning</title>
        <author>
          <persName>
            <foreName>Seung-Hwan</foreName>
            <surname>Bae</surname>
            <initial>S.-H.</initial>
          </persName>
          <persName>
            <foreName>Kuk-Jin</foreName>
            <surname>Yoon</surname>
            <initial>K.-J.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">CVPR</title>
        <loc>Columbus</loc>
        <imprint>
          <publisher>
            <orgName type="organisation">IEEE</orgName>
          </publisher>
          <dateStruct>
            <month>June</month>
            <year>2014</year>
          </dateStruct>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid47" type="article" rend="foot" n="footcite:bar-hillel:2010">
      <analytic>
        <title level="a">Part-based feature synthesis for human detection</title>
        <author>
          <persName>
            <foreName>A.</foreName>
            <surname>Bar-Hillel</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>D.</foreName>
            <surname>Levi</surname>
            <initial>D.</initial>
          </persName>
          <persName>
            <foreName>E.</foreName>
            <surname>Krupka</surname>
            <initial>E.</initial>
          </persName>
          <persName>
            <foreName>C.</foreName>
            <surname>Goldberg</surname>
            <initial>C.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">ECCV</title>
        <imprint>
          <dateStruct>
            <year>2010</year>
          </dateStruct>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid34" type="inproceedings" rend="foot" n="footcite:shitrit:2011">
      <analytic>
        <title level="a">Tracking multiple people under global appearance constraints</title>
        <author>
          <persName>
            <foreName>H.</foreName>
            <surname>Ben Shitrit</surname>
            <initial>H.</initial>
          </persName>
          <persName>
            <foreName>J.</foreName>
            <surname>Berclaz</surname>
            <initial>J.</initial>
          </persName>
          <persName>
            <foreName>F.</foreName>
            <surname>Fleuret</surname>
            <initial>F.</initial>
          </persName>
          <persName>
            <foreName>P.</foreName>
            <surname>Fua</surname>
            <initial>P.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
        <imprint>
          <dateStruct>
            <year>2011</year>
          </dateStruct>
          <biblScope type="pages">137-144</biblScope>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid52" type="article" rend="foot" n="footcite:beneson:2013">
      <analytic>
        <title level="a">Pedestrian detection at 100 frames per second</title>
        <author>
          <persName>
            <foreName>R.</foreName>
            <surname>Benenson</surname>
            <initial>R.</initial>
          </persName>
          <persName>
            <foreName>M.</foreName>
            <surname>Mathias</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>R.</foreName>
            <surname>Timofte</surname>
            <initial>R.</initial>
          </persName>
          <persName>
            <foreName>L. Van</foreName>
            <surname>Gool</surname>
            <initial>L. V.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">CVPR</title>
        <imprint>
          <dateStruct>
            <year>2013</year>
          </dateStruct>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid42" type="article" rend="foot" n="footcite:berkin:2010">
      <analytic>
        <title level="a">Fast Human Detection With Cascaded Ensembles On The GPU</title>
        <author>
          <persName>
            <foreName>B.</foreName>
            <surname>Berkin</surname>
            <initial>B.</initial>
          </persName>
          <persName>
            <foreName>B. K.P.</foreName>
            <surname>Horn</surname>
            <initial>B. K.</initial>
          </persName>
          <persName>
            <foreName>I.</foreName>
            <surname>Masaki</surname>
            <initial>I.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">IEEE Intelligent Vehicles Symposium</title>
        <imprint>
          <dateStruct>
            <year>2010</year>
          </dateStruct>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid57" type="article" rend="foot" n="footcite:breitenstein:2009">
      <analytic>
        <title level="a">Markovian tracking-by-detection from a single, uncalibrated camera</title>
        <author>
          <persName>
            <foreName>M. D.</foreName>
            <surname>Breitenstein</surname>
            <initial>M. D.</initial>
          </persName>
          <persName>
            <foreName>F.</foreName>
            <surname>Reichlin</surname>
            <initial>F.</initial>
          </persName>
          <persName>
            <foreName>B.</foreName>
            <surname>Leibe</surname>
            <initial>B.</initial>
          </persName>
          <persName>
            <foreName>E.</foreName>
            <surname>Koller-Meier</surname>
            <initial>E.</initial>
          </persName>
          <persName>
            <foreName>L.</foreName>
            <surname>van Gool</surname>
            <initial>L.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">PETS workshop</title>
        <imprint>
          <dateStruct>
            <year>2009</year>
          </dateStruct>
          <biblScope type="pages">71-78</biblScope>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid32" type="inproceedings" rend="foot" n="footcite:chau:hal-00846920">
      <identifiant type="hal" value="hal-00846920"/>
      <analytic>
        <title level="a">Online Tracking Parameter Adaptation based on Evaluation</title>
        <author>
          <persName key="stars-2014-idp71944">
            <foreName>Duc Phu</foreName>
            <surname>Chau</surname>
            <initial>D. P.</initial>
          </persName>
          <persName key="stars-2014-idp112576">
            <foreName>Julien</foreName>
            <surname>Badie</surname>
            <initial>J.</initial>
          </persName>
          <persName key="stars-2014-idp60040">
            <foreName>François</foreName>
            <surname>Bremond</surname>
            <initial>F.</initial>
          </persName>
          <persName key="stars-2014-idp66712">
            <foreName>Monique</foreName>
            <surname>Thonnat</surname>
            <initial>M.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="m">IEEE International Conference on Advanced Video and Signal-based Surveillance</title>
        <loc>Krakow, Poland</loc>
        <imprint>
          <dateStruct>
            <month>August</month>
            <year>2013</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-00846920" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-00846920</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid36" type="inproceedings" rend="foot" n="footcite:Phu_ICDP2009">
      <identifiant type="hal" value="inria-00486479"/>
      <analytic>
        <title level="a">Online evaluation of tracking algorithm performance</title>
        <author>
          <persName key="stars-2014-idp71944">
            <foreName>Duc Phu</foreName>
            <surname>Chau</surname>
            <initial>D. P.</initial>
          </persName>
          <persName key="stars-2014-idp60040">
            <foreName>Francois</foreName>
            <surname>Bremond</surname>
            <initial>F.</initial>
          </persName>
          <persName key="stars-2014-idp66712">
            <foreName>Monique</foreName>
            <surname>Thonnat</surname>
            <initial>M.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">The 3rd International Conference on Imaging for Crime Detection and Prevention (ICDP)</title>
        <loc>London,UK</loc>
        <imprint>
          <publisher>
            <orgName type="organisation"/>
          </publisher>
          <dateStruct>
            <month>December</month>
            <year>2009</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/inria-00486479" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>inria-00486479</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid37" type="article" rend="foot" n="footcite:chau:hal-00976594">
      <identifiant type="hal" value="hal-00976594"/>
      <analytic>
        <title level="a">Online Parameter Tuning for Object Tracking Algorithms</title>
        <author>
          <persName key="stars-2014-idp71944">
            <foreName>Duc Phu</foreName>
            <surname>Chau</surname>
            <initial>D. P.</initial>
          </persName>
          <persName key="stars-2014-idp66712">
            <foreName>Monique</foreName>
            <surname>Thonnat</surname>
            <initial>M.</initial>
          </persName>
          <persName key="stars-2014-idp60040">
            <foreName>François</foreName>
            <surname>Bremond</surname>
            <initial>F.</initial>
          </persName>
          <persName key="stars-2014-idp116256">
            <foreName>Etienne</foreName>
            <surname>Corvee</surname>
            <initial>E.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">Image and Vision Computing</title>
        <imprint>
          <biblScope type="volume">32</biblScope>
          <biblScope type="number">4</biblScope>
          <dateStruct>
            <month>February</month>
            <year>2014</year>
          </dateStruct>
          <biblScope type="pages">287-302</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-00976594" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-00976594</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid56" type="article" rend="foot" n="footcite:conte:2009">
      <analytic>
        <title level="a">Performance evaluation of a people tracking system on the pets video database</title>
        <author>
          <persName>
            <foreName>D.</foreName>
            <surname>Conte</surname>
            <initial>D.</initial>
          </persName>
          <persName>
            <foreName>P.</foreName>
            <surname>Foggia</surname>
            <initial>P.</initial>
          </persName>
          <persName>
            <foreName>G.</foreName>
            <surname>Percannella</surname>
            <initial>G.</initial>
          </persName>
          <persName>
            <foreName>M.</foreName>
            <surname>Vento</surname>
            <initial>M.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">PETS workshop</title>
        <imprint>
          <dateStruct>
            <year>2009</year>
          </dateStruct>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid53" type="article" rend="foot" n="footcite:costea:2014">
      <analytic>
        <title level="a">Word Channel Based Multiscale Pedestrian Detection without Image Resizing and Using Only One Classifier</title>
        <author>
          <persName>
            <foreName>A. D.</foreName>
            <surname>Costea</surname>
            <initial>A. D.</initial>
          </persName>
          <persName>
            <foreName>S.</foreName>
            <surname>Nedevschi</surname>
            <initial>S.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">CVPR</title>
        <imprint>
          <dateStruct>
            <year>2014</year>
          </dateStruct>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid14" type="inproceedings" rend="foot" n="footcite:dalal2005histograms">
      <analytic>
        <title level="a">Histograms of oriented gradients for human detection</title>
        <author>
          <persName>
            <foreName>Navneet</foreName>
            <surname>Dalal</surname>
            <initial>N.</initial>
          </persName>
          <persName>
            <foreName>Bill</foreName>
            <surname>Triggs</surname>
            <initial>B.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="m">Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on</title>
        <imprint>
          <biblScope type="volume">1</biblScope>
          <publisher>
            <orgName type="organisation">IEEE</orgName>
          </publisher>
          <dateStruct>
            <year>2005</year>
          </dateStruct>
          <biblScope type="pages">886–893</biblScope>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid13" type="inproceedings" rend="foot" n="footcite:dalal2006human">
      <analytic>
        <title level="a">Human detection using oriented histograms of flow and appearance</title>
        <author>
          <persName>
            <foreName>Navneet</foreName>
            <surname>Dalal</surname>
            <initial>N.</initial>
          </persName>
          <persName>
            <foreName>Bill</foreName>
            <surname>Triggs</surname>
            <initial>B.</initial>
          </persName>
          <persName key="lear-2014-idp61664">
            <foreName>Cordelia</foreName>
            <surname>Schmid</surname>
            <initial>C.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="m">European conference on computer vision</title>
        <imprint>
          <publisher>
            <orgName type="organisation">Springer</orgName>
          </publisher>
          <dateStruct>
            <year>2006</year>
          </dateStruct>
          <biblScope type="pages">428–441</biblScope>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid9" type="article" rend="foot" n="footcite:David2010">
      <analytic>
        <title level="a">Measurement of Neuropsychiatric Symptoms in Clinical Trials Targeting Alzheimer's Disease and Related Disorders</title>
        <author>
          <persName key="carte-2014-idp98232">
            <foreName>R.</foreName>
            <surname>David</surname>
            <initial>R.</initial>
          </persName>
          <persName>
            <foreName>E.</foreName>
            <surname>Mulin</surname>
            <initial>E.</initial>
          </persName>
          <persName>
            <foreName>P.</foreName>
            <surname>Mallea</surname>
            <initial>P.</initial>
          </persName>
          <persName>
            <foreName>P.H.</foreName>
            <surname>Robert</surname>
            <initial>P.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">Pharmaceuticals</title>
        <imprint>
          <biblScope type="volume">3</biblScope>
          <dateStruct>
            <year>2010</year>
          </dateStruct>
          <biblScope type="pages">2387-2397</biblScope>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid50" type="article" rend="foot" n="footcite:dollar:2010">
      <analytic>
        <title level="a">The Fastest Pedestrian Detector in the West</title>
        <author>
          <persName>
            <foreName>P.</foreName>
            <surname>Dollar</surname>
            <initial>P.</initial>
          </persName>
          <persName>
            <foreName>S.</foreName>
            <surname>Belongie</surname>
            <initial>S.</initial>
          </persName>
          <persName>
            <foreName>P.</foreName>
            <surname>Perona</surname>
            <initial>P.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">BMVC</title>
        <imprint>
          <dateStruct>
            <year>2010</year>
          </dateStruct>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid49" type="article" rend="foot" n="footcite:dollar:2009">
      <analytic>
        <title level="a">Integral channel features</title>
        <author>
          <persName>
            <foreName>P.</foreName>
            <surname>Dollar</surname>
            <initial>P.</initial>
          </persName>
          <persName>
            <foreName>Z.</foreName>
            <surname>Tu</surname>
            <initial>Z.</initial>
          </persName>
          <persName>
            <foreName>P.</foreName>
            <surname>Perona</surname>
            <initial>P.</initial>
          </persName>
          <persName>
            <foreName>S.</foreName>
            <surname>Belongie</surname>
            <initial>S.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">BMVC</title>
        <imprint>
          <dateStruct>
            <year>2009</year>
          </dateStruct>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid73" type="article" rend="foot" n="footcite:elloumi:hal-01123895">
      <identifiant type="hal" value="hal-01123895"/>
      <analytic>
        <title level="a">Unsupervised discovery of human activities from long-time videos</title>
        <author>
          <persName>
            <foreName>Salma</foreName>
            <surname>Elloumi</surname>
            <initial>S.</initial>
          </persName>
          <persName key="stars-2014-idp87240">
            <foreName>Serhan</foreName>
            <surname>Cosar</surname>
            <initial>S.</initial>
          </persName>
          <persName>
            <foreName>Guido</foreName>
            <surname>Pusiol</surname>
            <initial>G.</initial>
          </persName>
          <persName key="stars-2014-idp60040">
            <foreName>Francois</foreName>
            <surname>Bremond</surname>
            <initial>F.</initial>
          </persName>
          <persName key="stars-2014-idp66712">
            <foreName>Monique</foreName>
            <surname>Thonnat</surname>
            <initial>M.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">IET Computer Vision</title>
        <imprint>
          <dateStruct>
            <month>March</month>
            <year>2015</year>
          </dateStruct>
          <biblScope type="pages">1</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01123895" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01123895</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid51" type="article" rend="foot" n="footcite:felzenszwalb:2009">
      <analytic>
        <title level="a">Object Detection with Discriminatively Trained Part-Based Models</title>
        <author>
          <persName>
            <foreName>P.F.</foreName>
            <surname>Felzenszwalb</surname>
            <initial>P.</initial>
          </persName>
          <persName>
            <foreName>R.B.</foreName>
            <surname>Girshick</surname>
            <initial>R.</initial>
          </persName>
          <persName>
            <foreName>D.</foreName>
            <surname>McAllester</surname>
            <initial>D.</initial>
          </persName>
          <persName>
            <foreName>D.</foreName>
            <surname>Ramanan</surname>
            <initial>D.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">PAMI</title>
        <imprint>
          <biblScope type="volume">32</biblScope>
          <biblScope type="number">9</biblScope>
          <dateStruct>
            <year>2009</year>
          </dateStruct>
          <biblScope type="pages">1627–1645</biblScope>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid44" type="article" rend="foot" n="footcite:felzenszwalb:2008">
      <analytic>
        <title level="a">A discriminatively trained, multiscale, deformable part model</title>
        <author>
          <persName>
            <foreName>P.</foreName>
            <surname>Felzenszwalb</surname>
            <initial>P.</initial>
          </persName>
          <persName>
            <foreName>D.</foreName>
            <surname>McAllester</surname>
            <initial>D.</initial>
          </persName>
          <persName>
            <foreName>D.</foreName>
            <surname>Ramanan</surname>
            <initial>D.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">CVPR</title>
        <imprint>
          <dateStruct>
            <year>2008</year>
          </dateStruct>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid43" type="article" rend="foot" n="footcite:ferryman:2009">
      <analytic>
        <title level="a">An overview of the pets2009 challenge</title>
        <author>
          <persName>
            <foreName>J.</foreName>
            <surname>Ferryman</surname>
            <initial>J.</initial>
          </persName>
          <persName>
            <foreName>A.</foreName>
            <surname>Shahrokni</surname>
            <initial>A.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">PETS</title>
        <imprint>
          <dateStruct>
            <year>2009</year>
          </dateStruct>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid68" type="inproceedings" rend="foot" n="footcite:gao2013trajectory">
      <analytic>
        <title level="a">Trajectory-based human activity recognition with hierarchical Dirichlet process hidden Markov models</title>
        <author>
          <persName>
            <foreName>Qingbin</foreName>
            <surname>Gao</surname>
            <initial>Q.</initial>
          </persName>
          <persName>
            <foreName>Shiliang</foreName>
            <surname>Sun</surname>
            <initial>S.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="m">Proceedings of the 1st IEEE China Summit and International Conference on Signal and Information Processing</title>
        <imprint>
          <dateStruct>
            <year>2013</year>
          </dateStruct>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid70" type="article" rend="foot" n="footcite:hupami2006">
      <analytic>
        <title level="a">A system for learning statistical motion patterns</title>
        <author>
          <persName>
            <foreName>Weiming</foreName>
            <surname>Hu</surname>
            <initial>W.</initial>
          </persName>
          <persName>
            <foreName>Xuejuan</foreName>
            <surname>Xiao</surname>
            <initial>X.</initial>
          </persName>
          <persName>
            <foreName>Zhouyu</foreName>
            <surname>Fu</surname>
            <initial>Z.</initial>
          </persName>
          <persName>
            <foreName>Dan</foreName>
            <surname>Xie</surname>
            <initial>D.</initial>
          </persName>
          <persName>
            <foreName>Tieniu</foreName>
            <surname>Tan</surname>
            <initial>T.</initial>
          </persName>
          <persName>
            <foreName>Steve</foreName>
            <surname>Maybank</surname>
            <initial>S.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
        <imprint>
          <biblScope type="volume">28</biblScope>
          <biblScope type="number">9</biblScope>
          <dateStruct>
            <year>2006</year>
          </dateStruct>
          <biblScope type="pages">1450–1464</biblScope>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid27" type="inproceedings" rend="foot" n="footcite:hu_jointly_2015">
      <identifiant type="doi" value="10.1109/CVPR.2015.7299172"/>
      <analytic>
        <title level="a">Jointly learning heterogeneous features for RGB-D activity recognition</title>
        <author>
          <persName>
            <foreName>Jian-Fang</foreName>
            <surname>Hu</surname>
            <initial>J.-F.</initial>
          </persName>
          <persName>
            <foreName>Wei-Shi</foreName>
            <surname>Zheng</surname>
            <initial>W.-S.</initial>
          </persName>
          <persName>
            <foreName>Jianhuang</foreName>
            <surname>Lai</surname>
            <initial>J.</initial>
          </persName>
          <persName key="galaad2-2015-idp73912">
            <foreName>JianGuo</foreName>
            <surname>Zhang</surname>
            <initial>J.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="m">CVPR</title>
        <imprint>
          <dateStruct>
            <year>2015</year>
          </dateStruct>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid71" type="techreport" rend="foot" n="footcite:gaadrd">
      <monogr>
        <title level="m">The Dem@Care Experiments and Datasets: a Technical Report</title>
        <author>
          <persName>
            <foreName>A.</foreName>
            <surname>Karakostas</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>A.</foreName>
            <surname>Briassouli</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>K.</foreName>
            <surname>Avgerinakis</surname>
            <initial>K.</initial>
          </persName>
          <persName>
            <foreName>I.</foreName>
            <surname>Kompatsiaris</surname>
            <initial>I.</initial>
          </persName>
          <persName>
            <foreName>M.</foreName>
            <surname>Tsolaki</surname>
            <initial>M.</initial>
          </persName>
        </author>
        <imprint>
          <dateStruct>
            <year>2014</year>
          </dateStruct>
        </imprint>
      </monogr>
      <note type="typdoc">Technical report</note>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid24" type="inproceedings" rend="foot" n="footcite:kong_bilinear_2015">
      <analytic>
        <title level="a">Bilinear heterogeneous information machine for RGB-D action recognition</title>
        <author>
          <persName>
            <foreName>Yu</foreName>
            <surname>Kong</surname>
            <initial>Y.</initial>
          </persName>
          <persName>
            <foreName>Yun</foreName>
            <surname>Fu</surname>
            <initial>Y.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="m">CVPR</title>
        <imprint>
          <dateStruct>
            <year>2015</year>
          </dateStruct>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid26" type="article" rend="foot" n="footcite:Koppula:2013:LHA:2502923.2502928">
      <identifiant type="doi" value="10.1177/0278364913478446"/>
      <analytic>
        <title level="a">Learning Human Activities and Object Affordances from RGB-D Videos</title>
        <author>
          <persName>
            <foreName>Hema Swetha</foreName>
            <surname>Koppula</surname>
            <initial>H. S.</initial>
          </persName>
          <persName>
            <foreName>Rudhir</foreName>
            <surname>Gupta</surname>
            <initial>R.</initial>
          </persName>
          <persName>
            <foreName>Ashutosh</foreName>
            <surname>Saxena</surname>
            <initial>A.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">Int. J. Rob. Res.</title>
        <imprint>
          <biblScope type="volume">32</biblScope>
          <biblScope type="number">8</biblScope>
          <dateStruct>
            <month>July</month>
            <year>2013</year>
          </dateStruct>
          <biblScope type="pages">951–970</biblScope>
          <ref xlink:href="http://dx.doi.org/10.1177/0278364913478446" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>dx.<allowbreak/>doi.<allowbreak/>org/<allowbreak/>10.<allowbreak/>1177/<allowbreak/>0278364913478446</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid29" type="inproceedings" rend="foot" n="footcite:koppula2013learning">
      <analytic>
        <title level="a">Learning spatio-temporal structure from rgb-d videos for human activity detection and anticipation</title>
        <author>
          <persName>
            <foreName>Hema</foreName>
            <surname>Koppula</surname>
            <initial>H.</initial>
          </persName>
          <persName>
            <foreName>Ashutosh</foreName>
            <surname>Saxena</surname>
            <initial>A.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="m">ICML</title>
        <imprint>
          <dateStruct>
            <year>2013</year>
          </dateStruct>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid2" type="inproceedings" rend="foot" n="footcite:Batory09">
      <analytic>
        <title level="a">Guaranteeing Syntactic Correctness for All Product Line Variants: A Language-Independent Approach</title>
        <author>
          <persName>
            <foreName>C.</foreName>
            <surname>Kästner</surname>
            <initial>C.</initial>
          </persName>
          <persName>
            <foreName>S.</foreName>
            <surname>Apel</surname>
            <initial>S.</initial>
          </persName>
          <persName>
            <foreName>S.</foreName>
            <surname>Trujillo</surname>
            <initial>S.</initial>
          </persName>
          <persName>
            <foreName>M.</foreName>
            <surname>Kuhlemann</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>D.S.</foreName>
            <surname>Batory</surname>
            <initial>D.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="m">TOOLS (47)</title>
        <imprint>
          <dateStruct>
            <year>2009</year>
          </dateStruct>
          <biblScope type="pages">175-194</biblScope>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid60" type="article" rend="foot" n="footcite:Liu2015SSD:Detector">
      <identifiant type="doi" value="10.1016/j.nima.2015.05.028"/>
      <identifiant type="arXiv" value=""/>
      <analytic>
        <title level="a">SSD: Single Shot MultiBox Detector</title>
        <author>
          <persName key="ex-situ-2015-idp82496">
            <foreName>Wei</foreName>
            <surname>Liu</surname>
            <initial>W.</initial>
          </persName>
          <persName>
            <foreName>Dragomir</foreName>
            <surname>Anguelov</surname>
            <initial>D.</initial>
          </persName>
          <persName>
            <foreName>Dumitru</foreName>
            <surname>Erhan</surname>
            <initial>D.</initial>
          </persName>
          <persName>
            <foreName>Christian</foreName>
            <surname>Szegedy</surname>
            <initial>C.</initial>
          </persName>
          <persName>
            <foreName>Scott</foreName>
            <surname>Reed</surname>
            <initial>S.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">arXiv preprint</title>
        <imprint>
          <dateStruct>
            <year>2015</year>
          </dateStruct>
          <biblScope type="pages">1–15</biblScope>
          <ref xlink:href="http://arxiv.org/abs/1512.02325" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>arxiv.<allowbreak/>org/<allowbreak/>abs/<allowbreak/>1512.<allowbreak/>02325</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid22" type="inproceedings" rend="foot" n="footcite:Liu:2013:LDR:2540128.2540343">
      <analytic>
        <title level="a">Learning Discriminative Representations from RGB-D Video Data</title>
        <author>
          <persName>
            <foreName>Li</foreName>
            <surname>Liu</surname>
            <initial>L.</initial>
          </persName>
          <persName>
            <foreName>Ling</foreName>
            <surname>Shao</surname>
            <initial>L.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="m">IJCAI</title>
        <imprint>
          <dateStruct>
            <year>2013</year>
          </dateStruct>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid23" type="inproceedings" rend="foot" n="footcite:lu:CVPR:2014">
      <analytic>
        <title level="a">Range-Sample Depth Feature for Action Recognition</title>
        <author>
          <persName>
            <foreName>Cewu</foreName>
            <surname>Lu</surname>
            <initial>C.</initial>
          </persName>
          <persName>
            <foreName>Jiaya</foreName>
            <surname>Jia</surname>
            <initial>J.</initial>
          </persName>
          <persName>
            <foreName>Chi-Keung</foreName>
            <surname>Tang</surname>
            <initial>C.-K.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="m">CVPR</title>
        <imprint>
          <dateStruct>
            <year>2014</year>
          </dateStruct>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid39" type="article" rend="foot" n="footcite:milan:2015">
      <identifiant type="doi" value="10.1109/TPAMI.2015.2505309"/>
      <analytic>
        <title level="a">Multi-Target Tracking by Discrete-Continuous Energy Minimization</title>
        <author>
          <persName>
            <foreName>A.</foreName>
            <surname>Milan</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>K.</foreName>
            <surname>Schindler</surname>
            <initial>K.</initial>
          </persName>
          <persName>
            <foreName>S.</foreName>
            <surname>Roth</surname>
            <initial>S.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
        <imprint>
          <biblScope type="volume">PP</biblScope>
          <biblScope type="number">99</biblScope>
          <dateStruct>
            <year>2015</year>
          </dateStruct>
          <biblScope type="pages">1-1</biblScope>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid5" type="inproceedings" rend="foot" n="footcite:MOISAN:ICVS:2011">
      <identifiant type="hal" value="inria-00617279"/>
      <analytic>
        <title level="a">Run Time Adaptation of Video-Surveillance Systems: A software Modeling Approach</title>
        <author>
          <persName key="stars-2014-idp64040">
            <foreName>S.</foreName>
            <surname>Moisan</surname>
            <initial>S.</initial>
          </persName>
          <persName>
            <foreName>J-P.</foreName>
            <surname>Rigault</surname>
            <initial>J.-P.</initial>
          </persName>
          <persName key="diverse-2014-idm5288">
            <foreName>M.</foreName>
            <surname>Acher</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>P.</foreName>
            <surname>Collet</surname>
            <initial>P.</initial>
          </persName>
          <persName>
            <foreName>P.</foreName>
            <surname>Lahire</surname>
            <initial>P.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="m">ICVS, 8th International Conference on Computer Vision Systems</title>
        <loc>Sophia Antipolis, France</loc>
        <imprint>
          <dateStruct>
            <month>September</month>
            <year>2011</year>
          </dateStruct>
          <ref xlink:href="http://hal.inria.fr/inria-00617279/en" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>inria-00617279/<allowbreak/>en</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid69" type="article" rend="foot" n="footcite:morrispami2011">
      <identifiant type="doi" value="10.1109/TPAMI.2011.64"/>
      <analytic>
        <title level="a">Trajectory Learning for Activity Understanding: Unsupervised, Multilevel, and Long-Term Adaptive Approach</title>
        <author>
          <persName>
            <foreName>B.T.</foreName>
            <surname>Morris</surname>
            <initial>B.</initial>
          </persName>
          <persName>
            <foreName>M.M.</foreName>
            <surname>Trivedi</surname>
            <initial>M.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
        <imprint>
          <biblScope type="volume">33</biblScope>
          <biblScope type="number">11</biblScope>
          <dateStruct>
            <month>Nov</month>
            <year>2011</year>
          </dateStruct>
          <biblScope type="pages">2287-2301</biblScope>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid75" type="article" rend="foot" n="footcite:mozaz2006posture">
      <analytic>
        <title level="a">Posture recognition in Alzheimer’s disease</title>
        <author>
          <persName>
            <foreName>Maria</foreName>
            <surname>Mozaz</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>Maite</foreName>
            <surname>Garaigordobil</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>Leslie J Gonzalez</foreName>
            <surname>Rothi</surname>
            <initial>L. J. G.</initial>
          </persName>
          <persName>
            <foreName>Jeffrey</foreName>
            <surname>Anderson</surname>
            <initial>J.</initial>
          </persName>
          <persName>
            <foreName>Gregory P</foreName>
            <surname>Crucian</surname>
            <initial>G. P.</initial>
          </persName>
          <persName>
            <foreName>Kenneth M</foreName>
            <surname>Heilman</surname>
            <initial>K. M.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">Brain and cognition</title>
        <imprint>
          <biblScope type="volume">62</biblScope>
          <biblScope type="number">3</biblScope>
          <dateStruct>
            <year>2006</year>
          </dateStruct>
          <biblScope type="pages">241–245</biblScope>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid30" type="inproceedings" rend="foot" n="footcite:nguyen:hal-01185874">
      <identifiant type="hal" value="hal-01185874"/>
      <analytic>
        <title level="a">Robust Global Tracker based on an Online Estimation of Tracklet Descriptor Reliability</title>
        <author>
          <persName>
            <foreName>Thi Lan Anh</foreName>
            <surname>NGUYEN</surname>
            <initial>T. L. A.</initial>
          </persName>
          <persName>
            <foreName>Duc Phu</foreName>
            <surname>CHAU</surname>
            <initial>D. P.</initial>
          </persName>
          <persName key="stars-2014-idp60040">
            <foreName>Francois</foreName>
            <surname>Bremond</surname>
            <initial>F.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="yes" x-international-audience="yes" x-proceedings="no" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">Advanded Video and Signal-based Surveillance</title>
        <loc>Karlsruhe, Germany</loc>
        <imprint>
          <dateStruct>
            <month>August</month>
            <year>2015</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01185874" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01185874</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid25" type="inproceedings" rend="foot" n="footcite:Ni:2012:OSC:2403006.2403020">
      <analytic>
        <title level="a">Order-Preserving Sparse Coding for Sequence Classification</title>
        <author>
          <persName>
            <foreName>Bingbing</foreName>
            <surname>Ni</surname>
            <initial>B.</initial>
          </persName>
          <persName>
            <foreName>Pierre</foreName>
            <surname>Moulin</surname>
            <initial>P.</initial>
          </persName>
          <persName>
            <foreName>Shuicheng</foreName>
            <surname>Yan</surname>
            <initial>S.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="m">ECCV</title>
        <imprint>
          <dateStruct>
            <year>2012</year>
          </dateStruct>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid17" type="inproceedings" rend="foot" n="footcite:Oreifej13:histogram">
      <analytic>
        <title level="a">HON4D: Histogram of oriented 4D normals for activity recognition from depth sequences</title>
        <author>
          <persName>
            <foreName>Omar</foreName>
            <surname>Oreifej</surname>
            <initial>O.</initial>
          </persName>
          <persName>
            <foreName>Zicheng</foreName>
            <surname>Liu</surname>
            <initial>Z.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="m">CVPR</title>
        <imprint>
          <dateStruct>
            <year>2013</year>
          </dateStruct>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid59" type="article" rend="foot" n="footcite:Ren">
      <analytic>
        <title level="a">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title>
        <author>
          <persName>
            <foreName>Shaoqing</foreName>
            <surname>Ren</surname>
            <initial>S.</initial>
          </persName>
          <persName>
            <foreName>Kaiming</foreName>
            <surname>He</surname>
            <initial>K.</initial>
          </persName>
          <persName>
            <foreName>Ross</foreName>
            <surname>Girshick</surname>
            <initial>R.</initial>
          </persName>
          <persName key="willow-2014-idp111096">
            <foreName>Jian</foreName>
            <surname>Sun</surname>
            <initial>J.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <imprint>
          <dateStruct>
            <year>2015</year>
          </dateStruct>
          <ref xlink:href="https://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>papers.<allowbreak/>nips.<allowbreak/>cc/<allowbreak/>paper/<allowbreak/>5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.<allowbreak/>pdf</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid4" type="inproceedings" rend="foot" n="footcite:ROCHA:ICVS:2011">
      <identifiant type="hal" value="inria-00616642"/>
      <analytic>
        <title level="a">Girgit: A Dynamically Adaptive Vision System for Scene Understanding</title>
        <author>
          <persName>
            <foreName>Leonardo Manuel</foreName>
            <surname>Rocha</surname>
            <initial>L. M.</initial>
          </persName>
          <persName key="stars-2014-idp64040">
            <foreName>Sabine</foreName>
            <surname>Moisan</surname>
            <initial>S.</initial>
          </persName>
          <persName key="stars-2014-idp135032">
            <foreName>Jean-Paul</foreName>
            <surname>Rigault</surname>
            <initial>J.-P.</initial>
          </persName>
          <persName>
            <foreName>Sen</foreName>
            <surname>Sagar</surname>
            <initial>S.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="m">ICVS</title>
        <loc>Sophia Antipolis, France</loc>
        <imprint>
          <dateStruct>
            <month>September</month>
            <year>2011</year>
          </dateStruct>
          <ref xlink:href="http://hal.inria.fr/inria-00616642/en" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>inria-00616642/<allowbreak/>en</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid66" type="inproceedings" rend="foot" n="footcite:Rohrbach2012">
      <identifiant type="doi" value="10.1007/978-3-642-33718-5_11"/>
      <analytic>
        <title level="a">Script Data for Attribute-Based Recognition of Composite Activities</title>
        <author>
          <persName>
            <foreName>Marcus</foreName>
            <surname>Rohrbach</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>Michaela</foreName>
            <surname>Regneri</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>Mykhaylo</foreName>
            <surname>Andriluka</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>Sikandar</foreName>
            <surname>Amin</surname>
            <initial>S.</initial>
          </persName>
          <persName>
            <foreName>Manfred</foreName>
            <surname>Pinkal</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>Bernt</foreName>
            <surname>Schiele</surname>
            <initial>B.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="m">Computer Vision - ECCV 2012 - 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part I</title>
        <imprint>
          <dateStruct>
            <year>2012</year>
          </dateStruct>
          <biblScope type="pages">144–157</biblScope>
          <ref xlink:href="http://dx.doi.org/10.1007/978-3-642-33718-5_11" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>dx.<allowbreak/>doi.<allowbreak/>org/<allowbreak/>10.<allowbreak/>1007/<allowbreak/>978-3-642-33718-5_11</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid10" type="article" rend="foot" n="footcite:Romdhane:JNHA:2011">
      <identifiant type="hal" value="inria-00616747"/>
      <analytic>
        <title level="a">Automatic Video Monitoring system for assessment of Alzheimer's Disease symptoms</title>
        <author>
          <persName>
            <foreName>R.</foreName>
            <surname>Romdhane</surname>
            <initial>R.</initial>
          </persName>
          <persName>
            <foreName>E.</foreName>
            <surname>Mulin</surname>
            <initial>E.</initial>
          </persName>
          <persName>
            <foreName>A.</foreName>
            <surname>Derreumeaux</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>N.</foreName>
            <surname>Zouba</surname>
            <initial>N.</initial>
          </persName>
          <persName>
            <foreName>J.</foreName>
            <surname>Piano</surname>
            <initial>J.</initial>
          </persName>
          <persName>
            <foreName>L.</foreName>
            <surname>Lee</surname>
            <initial>L.</initial>
          </persName>
          <persName>
            <foreName>I.</foreName>
            <surname>Leroi</surname>
            <initial>I.</initial>
          </persName>
          <persName>
            <foreName>P.</foreName>
            <surname>Mallea</surname>
            <initial>P.</initial>
          </persName>
          <persName key="carte-2014-idp98232">
            <foreName>R.</foreName>
            <surname>David</surname>
            <initial>R.</initial>
          </persName>
          <persName key="stars-2014-idp66712">
            <foreName>M.</foreName>
            <surname>Thonnat</surname>
            <initial>M.</initial>
          </persName>
          <persName key="stars-2014-idp60040">
            <foreName>F.</foreName>
            <surname>Bremond</surname>
            <initial>F.</initial>
          </persName>
          <persName>
            <foreName>P.H.</foreName>
            <surname>Robert</surname>
            <initial>P.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">The Journal of Nutrition, Health and Aging Ms(JNHA)</title>
        <imprint>
          <biblScope type="volume">JNHA-D-11-00004R1</biblScope>
          <dateStruct>
            <year>2011</year>
          </dateStruct>
          <ref xlink:href="http://hal.inria.fr/inria-00616747/en" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>inria-00616747/<allowbreak/>en</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid28" type="inproceedings" rend="foot" n="footcite:RybokWACV">
      <analytic>
        <title level="a">Important stuff, everywhere! Activity recognition with salient proto-objects as context</title>
        <author>
          <persName>
            <foreName>L.</foreName>
            <surname>Rybok</surname>
            <initial>L.</initial>
          </persName>
          <persName>
            <foreName>B.</foreName>
            <surname>Schauerte</surname>
            <initial>B.</initial>
          </persName>
          <persName>
            <foreName>Z.</foreName>
            <surname>Al-Halah</surname>
            <initial>Z.</initial>
          </persName>
          <persName>
            <foreName>R.</foreName>
            <surname>Stiefelhagen</surname>
            <initial>R.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="m">WACV</title>
        <imprint>
          <dateStruct>
            <year>2014</year>
          </dateStruct>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid76" type="inproceedings" rend="foot" n="footcite:sarray:hal-01236976">
      <identifiant type="doi" value="10.1145/2836127.2836131"/>
      <identifiant type="hal" value="hal-01236976"/>
      <analytic>
        <title level="a">Safe Composition in Middleware for the Internet of Things</title>
        <author>
          <persName key="stars-2014-idp94784">
            <foreName>Ines</foreName>
            <surname>Sarray</surname>
            <initial>I.</initial>
          </persName>
          <persName key="stars-2014-idp65472">
            <foreName>Annie</foreName>
            <surname>Ressouche</surname>
            <initial>A.</initial>
          </persName>
          <persName key="stars-2014-idp62776">
            <foreName>Daniel</foreName>
            <surname>Gaffé</surname>
            <initial>D.</initial>
          </persName>
          <persName key="stars-2014-idp138816">
            <foreName>Jean-Yves</foreName>
            <surname>Tigli</surname>
            <initial>J.-Y.</initial>
          </persName>
          <persName>
            <foreName>Stéphane</foreName>
            <surname>Lavirotte</surname>
            <initial>S.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="no" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">Middleware for Context-aware Applications for Internet of thing (M4IoT)</title>
        <loc>Vancouver, Canada</loc>
        <imprint>
          <dateStruct>
            <month>December</month>
            <year>2015</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01236976" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01236976</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid16" type="inproceedings" rend="foot" n="footcite:seidenari:CVPRW:2013">
      <analytic>
        <title level="a">Recognizing Actions from Depth Cameras as Weakly Aligned Multi-part Bag-of-Poses</title>
        <author>
          <persName>
            <foreName>L.</foreName>
            <surname>Seidenari</surname>
            <initial>L.</initial>
          </persName>
          <persName>
            <foreName>V.</foreName>
            <surname>Varano</surname>
            <initial>V.</initial>
          </persName>
          <persName>
            <foreName>S.</foreName>
            <surname>Berretti</surname>
            <initial>S.</initial>
          </persName>
          <persName>
            <foreName>A.</foreName>
            <surname>Del Bimbo</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>P.</foreName>
            <surname>Pala</surname>
            <initial>P.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="m">CVPRW</title>
        <imprint>
          <dateStruct>
            <year>2013</year>
          </dateStruct>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid19" type="inproceedings" rend="foot" n="footcite:shahroudy:ISCCSP:2014">
      <analytic>
        <title level="a">Multi-modal feature fusion for action recognition in RGB-D sequences</title>
        <author>
          <persName>
            <foreName>A.</foreName>
            <surname>Shahroudy</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Gang</foreName>
            <surname>Wang</surname>
            <initial>G.</initial>
          </persName>
          <persName>
            <foreName>Tian-Tsong</foreName>
            <surname>Ng</surname>
            <initial>T.-T.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="m">ISCCSP</title>
        <imprint>
          <dateStruct>
            <year>2014</year>
          </dateStruct>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid40" type="inproceedings" rend="foot" n="footcite:ICCV2013">
      <analytic>
        <title level="a">Learning People Detectors for Tracking in Crowded Scenes</title>
        <author>
          <persName>
            <foreName>Siyu</foreName>
            <surname>Tang</surname>
            <initial>S.</initial>
          </persName>
          <persName>
            <foreName>Mykhaylo</foreName>
            <surname>Andriluka</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>Anton</foreName>
            <surname>Milan</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Konrad</foreName>
            <surname>Schindler</surname>
            <initial>K.</initial>
          </persName>
          <persName>
            <foreName>Stefan</foreName>
            <surname>Roth</surname>
            <initial>S.</initial>
          </persName>
          <persName>
            <foreName>Bernt</foreName>
            <surname>Schiele</surname>
            <initial>B.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
        <imprint>
          <dateStruct>
            <month>December</month>
            <year>2013</year>
          </dateStruct>
          <ref xlink:href="http://www.cv-foundation.org/openaccess/content_iccv_2013/html/Tang_Learning_People_Detectors_2013_ICCV_paper.html" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>www.<allowbreak/>cv-foundation.<allowbreak/>org/<allowbreak/>openaccess/<allowbreak/>content_iccv_2013/<allowbreak/>html/<allowbreak/>Tang_Learning_People_Detectors_2013_ICCV_paper.<allowbreak/>html</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid48" type="article" rend="foot" n="footcite:walk:2010">
      <analytic>
        <title level="a">New features and insights for pedestrian detection</title>
        <author>
          <persName>
            <foreName>S.</foreName>
            <surname>Walk</surname>
            <initial>S.</initial>
          </persName>
          <persName>
            <foreName>N.</foreName>
            <surname>Majer</surname>
            <initial>N.</initial>
          </persName>
          <persName>
            <foreName>K.</foreName>
            <surname>Schindler</surname>
            <initial>K.</initial>
          </persName>
          <persName>
            <foreName>B.</foreName>
            <surname>Schiele</surname>
            <initial>B.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">CVPR</title>
        <imprint>
          <dateStruct>
            <year>2010</year>
          </dateStruct>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid45" type="article" rend="foot" n="footcite:wang:2009">
      <analytic>
        <title level="a">An hog-lbp human detector with partial occlusion handling</title>
        <author>
          <persName key="crypt-2014-idm29872">
            <foreName>X.</foreName>
            <surname>Wang</surname>
            <initial>X.</initial>
          </persName>
          <persName>
            <foreName>T. X.</foreName>
            <surname>Han</surname>
            <initial>T. X.</initial>
          </persName>
          <persName>
            <foreName>S.</foreName>
            <surname>Yan</surname>
            <initial>S.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">ICCV</title>
        <imprint>
          <dateStruct>
            <year>2009</year>
          </dateStruct>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid12" type="inproceedings" rend="foot" n="footcite:wang:2011:inria-00583818:1">
      <identifiant type="hal" value="inria-00583818"/>
      <analytic>
        <title level="a">Action Recognition by Dense Trajectories</title>
        <author>
          <persName key="lear-2014-idp80408">
            <foreName>Heng</foreName>
            <surname>Wang</surname>
            <initial>H.</initial>
          </persName>
          <persName>
            <foreName>Alexander</foreName>
            <surname>Kläser</surname>
            <initial>A.</initial>
          </persName>
          <persName key="lear-2014-idp61664">
            <foreName>Cordelia</foreName>
            <surname>Schmid</surname>
            <initial>C.</initial>
          </persName>
          <persName>
            <foreName>Cheng-Lin</foreName>
            <surname>Liu</surname>
            <initial>C.-L.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="m">IEEE Conference on Computer Vision &amp; Pattern Recognition</title>
        <loc>Colorado Springs, United States</loc>
        <imprint>
          <dateStruct>
            <month>June</month>
            <year>2011</year>
          </dateStruct>
          <biblScope type="pages">3169-3176</biblScope>
          <ref xlink:href="http://hal.inria.fr/inria-00583818/en" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>inria-00583818/<allowbreak/>en</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid46" type="article" rend="foot" n="footcite:wojeck:2008">
      <analytic>
        <title level="a">A performance evaluation of single and multi-feature people detection</title>
        <author>
          <persName>
            <foreName>C.</foreName>
            <surname>Wojek</surname>
            <initial>C.</initial>
          </persName>
          <persName>
            <foreName>B.</foreName>
            <surname>Schiele</surname>
            <initial>B.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">DAGM Symposium Pattern Recognition</title>
        <imprint>
          <dateStruct>
            <year>2008</year>
          </dateStruct>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid21" type="inproceedings" rend="foot" n="footcite:Wu:2012:MAE:2354409.2354966">
      <analytic>
        <title level="a">Mining Actionlet Ensemble for Action Recognition with Depth Cameras</title>
        <author>
          <persName>
            <foreName>Ying</foreName>
            <surname>Wu</surname>
            <initial>Y.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="m">Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
        <loc>Washington, DC, USA</loc>
        <title level="s">CVPR '12</title>
        <imprint>
          <publisher>
            <orgName>IEEE Computer Society</orgName>
          </publisher>
          <dateStruct>
            <year>2012</year>
          </dateStruct>
          <biblScope type="pages">1290–1297</biblScope>
          <ref xlink:href="http://dl.acm.org/citation.cfm?id=2354409.2354966" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>dl.<allowbreak/>acm.<allowbreak/>org/<allowbreak/>citation.<allowbreak/>cfm?id=2354409.<allowbreak/>2354966</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid20" type="inproceedings" rend="foot" n="footcite:xia:CVPR:2013">
      <analytic>
        <title level="a">Spatio-temporal Depth Cuboid Similarity Feature for Activity Recognition Using Depth Camera</title>
        <author>
          <persName key="prosecco-2015-idp109488">
            <foreName>Lu</foreName>
            <surname>Xia</surname>
            <initial>L.</initial>
          </persName>
          <persName>
            <foreName>J.K.</foreName>
            <surname>Aggarwal</surname>
            <initial>J.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="m">CVPR</title>
        <imprint>
          <dateStruct>
            <year>2013</year>
          </dateStruct>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid58" type="article" rend="foot" n="footcite:yang:2009">
      <analytic>
        <title level="a">Probabilistic multiple people tracking through complex situations</title>
        <author>
          <persName key="lifeware-2014-idp79896">
            <foreName>J.</foreName>
            <surname>Yang</surname>
            <initial>J.</initial>
          </persName>
          <persName>
            <foreName>Z.</foreName>
            <surname>Shi</surname>
            <initial>Z.</initial>
          </persName>
          <persName>
            <foreName>P.</foreName>
            <surname>Vela</surname>
            <initial>P.</initial>
          </persName>
          <persName>
            <foreName>J.</foreName>
            <surname>Teizer</surname>
            <initial>J.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">PETS workshop</title>
        <imprint>
          <dateStruct>
            <year>2009</year>
          </dateStruct>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid31" type="inproceedings" rend="foot" n="footcite:zhang:2008">
      <identifiant type="doi" value="10.1109/CVPR.2008.4587584"/>
      <analytic>
        <title level="a">Global data association for multi-object tracking using network flows</title>
        <author>
          <persName key="i4s-2014-idp77312">
            <foreName>Li</foreName>
            <surname>Zhang</surname>
            <initial>L.</initial>
          </persName>
          <persName>
            <foreName>Yuan</foreName>
            <surname>Li</surname>
            <initial>Y.</initial>
          </persName>
          <persName>
            <foreName>R.</foreName>
            <surname>Nevatia</surname>
            <initial>R.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="m">Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on</title>
        <imprint>
          <dateStruct>
            <month>June</month>
            <year>2008</year>
          </dateStruct>
          <biblScope type="pages">1-8</biblScope>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid41" type="article" rend="foot" n="footcite:zhu:2006">
      <analytic>
        <title level="a">Fast Human Detection using a Cascade of Histograms of Oriented Gradients</title>
        <author>
          <persName>
            <foreName>Q.</foreName>
            <surname>Zhu</surname>
            <initial>Q.</initial>
          </persName>
          <persName>
            <foreName>S.</foreName>
            <surname>Avidan</surname>
            <initial>S.</initial>
          </persName>
          <persName>
            <foreName>M.</foreName>
            <surname>Yeh</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>K.</foreName>
            <surname>Cheng</surname>
            <initial>K.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">CVPR</title>
        <imprint>
          <dateStruct>
            <year>2006</year>
          </dateStruct>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="stars-2016-bid18" type="article" rend="foot" n="footcite:zhu:2014">
      <identifiant type="doi" value="10.1016/j.imavis.2014.04.005"/>
      <analytic>
        <title level="a">Evaluating spatiotemporal interest point features for depth-based action recognition</title>
        <author>
          <persName>
            <foreName>Yu</foreName>
            <surname>Zhu</surname>
            <initial>Y.</initial>
          </persName>
          <persName>
            <foreName>Wenbin</foreName>
            <surname>Chen</surname>
            <initial>W.</initial>
          </persName>
          <persName>
            <foreName>Guodong</foreName>
            <surname>Guo</surname>
            <initial>G.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">Image and Vision Computing</title>
        <imprint>
          <biblScope type="volume">32</biblScope>
          <biblScope type="number">8</biblScope>
          <dateStruct>
            <year>2014</year>
          </dateStruct>
          <biblScope type="pages">453 - 464</biblScope>
          <ref xlink:href="http://www.sciencedirect.com/science/article/pii/S0262885614000651" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>www.<allowbreak/>sciencedirect.<allowbreak/>com/<allowbreak/>science/<allowbreak/>article/<allowbreak/>pii/<allowbreak/>S0262885614000651</ref>
        </imprint>
      </monogr>
    </biblStruct>
  </biblio>
</raweb>
