<?xml version="1.0" encoding="utf-8"?>
<raweb xmlns:xlink="http://www.w3.org/1999/xlink" xml:lang="en" year="2016">
  <identification id="multispeech" isproject="true">
    <shortname>MULTISPEECH</shortname>
    <projectName>Speech Modeling for Facilitating Oral-Based Communication</projectName>
    <theme-de-recherche>Language, Speech and Audio</theme-de-recherche>
    <domaine-de-recherche>Perception, Cognition and Interaction</domaine-de-recherche>
    <urlTeam>https://team.inria.fr/multispeech/</urlTeam>
    <structure_exterieure type="Labs">
      <libelle>Laboratoire lorrain de recherche en informatique et ses applications (LORIA)</libelle>
    </structure_exterieure>
    <structure_exterieure type="Organism">
      <libelle>CNRS</libelle>
    </structure_exterieure>
    <structure_exterieure type="Organism">
      <libelle>Université de Lorraine</libelle>
    </structure_exterieure>
    <header_dates_team>Creation of the Team: 2014 July 01, updated into Project-Team: 2015 July 01</header_dates_team>
    <LeTypeProjet>Project-Team</LeTypeProjet>
    <keywordsSdN>
      <term>3.1.4. - Uncertain data</term>
      <term>3.4.6. - Neural networks</term>
      <term>3.4.8. - Deep learning</term>
      <term>5.1.7. - Multimodal interfaces</term>
      <term>5.7.2. - Music</term>
      <term>5.7.3. - Speech</term>
      <term>5.7.4. - Analysis</term>
      <term>5.7.5. - Synthesis</term>
      <term>5.8. - Natural language processing</term>
      <term>5.9.1. - Sampling, acquisition</term>
      <term>5.9.2. - Estimation, modeling</term>
      <term>5.9.3. - Reconstruction, enhancement</term>
      <term>5.9.5. - Sparsity-aware processing</term>
      <term>5.10.2. - Perception</term>
      <term>5.11.2. - Home/building control and interaction</term>
      <term>6.2.4. - Statistical methods</term>
      <term>6.3.1. - Inverse problems</term>
      <term>6.3.5. - Uncertainty Quantification</term>
      <term>8.2. - Machine learning</term>
      <term>8.3. - Signal analysis</term>
    </keywordsSdN>
    <keywordsSecteurs>
      <term>4.3.3. - Wind energy</term>
      <term>8.1.2. - Sensor networks for smart buildings</term>
      <term>8.4. - Security and personal assistance</term>
      <term>9.1.1. - E-learning, MOOC</term>
      <term>9.2.1. - Music, sound</term>
      <term>9.2.2. - Cinema, Television</term>
      <term>9.4.1. - Computer science</term>
      <term>9.4.2. - Mathematics</term>
      <term>9.4.5. - Data science</term>
      <term>9.5.8. - Linguistics</term>
      <term>9.5.10. - Digital humanities</term>
    </keywordsSecteurs>
    <UR name="Nancy"/>
  </identification>
  <team id="uid1">
    <person key="multispeech-2014-idp62112">
      <firstname>Denis</firstname>
      <lastname>Jouvet</lastname>
      <categoryPro>Chercheur</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>Team leader, Inria, Senior Researcher</moreinfo>
      <hdr>oui</hdr>
    </person>
    <person key="multispeech-2014-idp63592">
      <firstname>Anne</firstname>
      <lastname>Bonneau</lastname>
      <categoryPro>Chercheur</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>CNRS, Researcher</moreinfo>
    </person>
    <person key="multispeech-2014-idp64832">
      <firstname>Dominique</firstname>
      <lastname>Fohr</lastname>
      <categoryPro>Chercheur</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>CNRS, Researcher</moreinfo>
    </person>
    <person key="multispeech-2014-idp66072">
      <firstname>Yves</firstname>
      <lastname>Laprie</lastname>
      <categoryPro>Chercheur</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>CNRS, Senior Researcher</moreinfo>
      <hdr>oui</hdr>
    </person>
    <person key="multispeech-2014-idp67512">
      <firstname>Antoine</firstname>
      <lastname>Liutkus</lastname>
      <categoryPro>Chercheur</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>Inria, Researcher</moreinfo>
    </person>
    <person key="multispeech-2014-idp68752">
      <firstname>Emmanuel</firstname>
      <lastname>Vincent</lastname>
      <categoryPro>Chercheur</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>Inria, Senior Researcher</moreinfo>
      <hdr>oui</hdr>
    </person>
    <person key="multispeech-2014-idp70184">
      <firstname>Vincent</firstname>
      <lastname>Colotte</lastname>
      <categoryPro>Enseignant</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>Univ. Lorraine, Associate Professor</moreinfo>
    </person>
    <person key="multispeech-2016-idp128576">
      <firstname>Irène</firstname>
      <lastname>Illina</lastname>
      <categoryPro>Enseignant</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>Univ. Lorraine, Associate Professor</moreinfo>
      <hdr>oui</hdr>
    </person>
    <person key="multispeech-2014-idp74144">
      <firstname>Odile</firstname>
      <lastname>Mella</lastname>
      <categoryPro>Enseignant</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>Univ. Lorraine, Associate Professor</moreinfo>
    </person>
    <person key="multispeech-2014-idp75400">
      <firstname>Slim</firstname>
      <lastname>Ouni</lastname>
      <categoryPro>Enseignant</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>Univ. Lorraine, Associate Professor</moreinfo>
      <hdr>oui</hdr>
    </person>
    <person key="multispeech-2014-idp76848">
      <firstname>Agnès</firstname>
      <lastname>Piquard-Kipffer</lastname>
      <categoryPro>Enseignant</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>ESPE, Univ. Lorraine, Associate Professor</moreinfo>
    </person>
    <person key="multispeech-2016-idp139280">
      <firstname>Romain</firstname>
      <lastname>Serizel</lastname>
      <categoryPro>Enseignant</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>Univ. Lorraine, Associate Professor, from Sep 2016</moreinfo>
    </person>
    <person key="multispeech-2016-idp141776">
      <firstname>Ismaël</firstname>
      <lastname>Bada</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>CNRS, from Aug 2016</moreinfo>
    </person>
    <person key="multispeech-2015-idp120032">
      <firstname>Sara</firstname>
      <lastname>Dahmani</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="multispeech-2015-idp121264">
      <firstname>Valérian</firstname>
      <lastname>Girard</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>Univ. Lorraine, until Jun 2016; then CNRS</moreinfo>
    </person>
    <person key="multispeech-2016-idp149200">
      <firstname>Karan</firstname>
      <lastname>Nathwani</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>Inria, from Jun 2016</moreinfo>
    </person>
    <person key="multispeech-2014-idp83080">
      <firstname>Aghilas</firstname>
      <lastname>Sini</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>CNRS, until Nov 2016</moreinfo>
    </person>
    <person key="multispeech-2015-idp123760">
      <firstname>Sunit</firstname>
      <lastname>Sivasankaran</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="multispeech-2015-idp125016">
      <firstname>Ken</firstname>
      <lastname>Deguernel</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="multispeech-2014-idp84320">
      <firstname>Baldwin</firstname>
      <lastname>Dumortier</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="multispeech-2016-idp161472">
      <firstname>Mathieu</firstname>
      <lastname>Fontaine</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>Inria, from May 2016</moreinfo>
    </person>
    <person key="multispeech-2015-idp127472">
      <firstname>Amal</firstname>
      <lastname>Houidhek</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>École Nationale d’Ingénieurs de Tunis, Tunisie</moreinfo>
    </person>
    <person key="multispeech-2016-idp166368">
      <firstname>Yang</firstname>
      <lastname>Liu</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>Univ. Lorraine, from Oct 2016</moreinfo>
    </person>
    <person key="multispeech-2015-idp130000">
      <firstname>Aditya</firstname>
      <lastname>Nugraha</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="multispeech-2014-idp88000">
      <firstname>Luiza</firstname>
      <lastname>Orosanu</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>Inria, until Feb 2016</moreinfo>
    </person>
    <person key="multispeech-2014-idp89216">
      <firstname>Imran</firstname>
      <lastname>Sheikh</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>Univ. Lorraine</moreinfo>
    </person>
    <person key="multispeech-2016-idp176080">
      <firstname>Anastasiia</firstname>
      <lastname>Tsukanova</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>Univ. Lorraine, from May 2016</moreinfo>
    </person>
    <person key="multispeech-2015-idp136160">
      <firstname>Imene</firstname>
      <lastname>Zangar</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>École Nationale d’Ingénieurs de Tunis, Tunisie</moreinfo>
    </person>
    <person key="multispeech-2015-idp137432">
      <firstname>Mohamed</firstname>
      <lastname>Bouallegue</lastname>
      <categoryPro>PostDoc</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>Univ. Lorraine, until Sep 2016</moreinfo>
    </person>
    <person key="multispeech-2014-idp92912">
      <firstname>Benjamin</firstname>
      <lastname>Elie</lastname>
      <categoryPro>PostDoc</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>CNRS</moreinfo>
    </person>
    <person key="multispeech-2015-idp141192">
      <firstname>Sucheta</firstname>
      <lastname>Ghosh</lastname>
      <categoryPro>PostDoc</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>CNRS</moreinfo>
    </person>
    <person key="multispeech-2015-idp143688">
      <firstname>Juan Andres</firstname>
      <lastname>Morales Cordovilla</lastname>
      <categoryPro>PostDoc</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>Inria, until Mar 2016</moreinfo>
    </person>
    <person key="multispeech-2016-idp190864">
      <firstname>Sebastian</firstname>
      <lastname>Gonzalez Mora</lastname>
      <categoryPro>Visiteur</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>Univ. Chile, Jan 2016</moreinfo>
    </person>
    <person key="multispeech-2016-idp193328">
      <firstname>Benjamin</firstname>
      <lastname>Martinez Elizalde</lastname>
      <categoryPro>Visiteur</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>Carnegie Mellon University, from May 2016 until Aug 2016</moreinfo>
    </person>
    <person key="multispeech-2014-idp99128">
      <firstname>Dayana</firstname>
      <lastname>Ribas</lastname>
      <categoryPro>Visiteur</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>CENATAV, from Sep 2016 until Dec 2016</moreinfo>
    </person>
    <person key="multispeech-2016-idp198320">
      <firstname>Ziteng</firstname>
      <lastname>Wang</lastname>
      <categoryPro>Visiteur</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>Institute of Acoustics, Chinese Academy of Sciences, from Sep 2016</moreinfo>
    </person>
    <person key="semagramme-2014-idp69240">
      <firstname>Antoinette</firstname>
      <lastname>Courrier</lastname>
      <categoryPro>Assistant</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>CNRS</moreinfo>
    </person>
    <person key="semagramme-2014-idp70472">
      <firstname>Sylvie</firstname>
      <lastname>Musilli</lastname>
      <categoryPro>Assistant</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>Univ. Lorraine</moreinfo>
    </person>
    <person key="vegas-2014-idp75096">
      <firstname>Hélène</firstname>
      <lastname>Zganic</lastname>
      <categoryPro>Assistant</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="multispeech-2016-idp208224">
      <firstname>Lucas</firstname>
      <lastname>Antonelli</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>Inria, Intern, from Apr 2016 until Jun 2016</moreinfo>
    </person>
    <person key="multispeech-2015-idp155096">
      <firstname>Imen</firstname>
      <lastname>Ben Othmane</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>ESTI, Univ. de Carthage, Tunisia, Visiting PhD Student, until Jun 2016</moreinfo>
    </person>
    <person key="multispeech-2016-idp213232">
      <firstname>Clement</firstname>
      <lastname>Bordes</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>Inria, Intern, from Jun 2016 until Sep 2016</moreinfo>
    </person>
    <person key="multispeech-2015-idp162752">
      <firstname>Freha</firstname>
      <lastname>Boumazza</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>Uuiv. Hassiba Benbouali de Chlef, Algérie, Visiting PhD Student, Mar 2016</moreinfo>
    </person>
    <person key="multispeech-2016-idp218320">
      <firstname>Anna</firstname>
      <lastname>Currey</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>Univ. Lorraine, Intern, from Feb 2016 until Jun 2016</moreinfo>
    </person>
    <person key="multispeech-2015-idp164120">
      <firstname>Siddharth</firstname>
      <lastname>Dalmia</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>Univ. Lorraine, Intern, until Jul 2016</moreinfo>
    </person>
    <person key="multispeech-2016-idp223296">
      <firstname>Narjes</firstname>
      <lastname>Daoud</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>ESTI, Univ. de Carthage, Tunisia, Intern, from Mar 2016 until Aug 2016</moreinfo>
    </person>
    <person key="multispeech-2016-idp225808">
      <firstname>Boyuan</firstname>
      <lastname>Deng</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>Univ. Lorraine, Intern, from Feb 2016 until Jul 2016</moreinfo>
    </person>
    <person key="multispeech-2016-idp228304">
      <firstname>Diego</firstname>
      <lastname>Di Carlo</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>Inria, Intern, from Nov 2016</moreinfo>
    </person>
    <person key="multispeech-2016-idp230784">
      <firstname>Bertrand</firstname>
      <lastname>Muller</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Nancy</research-centre>
      <moreinfo>Inria, Intern, from Apr 2016 until Jun 2016</moreinfo>
    </person>
  </team>
  <presentation id="uid2">
    <bodyTitle>Overall Objectives</bodyTitle>
    <subsection id="uid3" level="1">
      <bodyTitle>Overall Objectives</bodyTitle>
      <p>MULTISPEECH is a joint project between Inria, CNRS and University of Lorraine, hosted in the LORIA laboratory (UMR 7503).
The goal of the project is the modeling of speech for facilitating oral-based communication.
The name MULTISPEECH comes from the following aspects that are particularly considered:</p>
      <simplelist>
        <li id="uid4">
          <p noindent="true"><b>Multisource aspects</b> - which means dealing with speech signals originating from several sources, such as speaker plus noise, or overlapping speech signals resulting from multiple speakers; sounds captured from several microphones are also considered.</p>
        </li>
        <li id="uid5">
          <p noindent="true"><b>Multilingual aspects</b> - which means dealing with speech in a multilingual context, as for example for computer assisted language learning, where the pronunciations of words in a foreign language (i.e., non-native speech) is strongly influenced by the mother tongue.</p>
        </li>
        <li id="uid6">
          <p noindent="true"><b>Multimodal aspects</b> - which means considering simultaneously the various modalities of speech signals, acoustic and visual, in particular for the expressive synthesis of audio-visual speech.</p>
        </li>
      </simplelist>
      <p spacebefore="6.0pt"/>
      <p noindent="true">The project is organized along the three following scientific challenges:</p>
      <simplelist>
        <li id="uid7">
          <p noindent="true"><b>The explicit modeling of speech.</b> - Speech signals result from the movements of articulators. A good knowledge of their position with respect to sounds is essential to improve, on the one hand, articulatory speech synthesis, and on the other hand, the relevance of the diagnosis and of the associated feedback in computer assisted language learning. Production and perception processes are interrelated, so a better understanding of how humans perceive speech will lead to more relevant diagnoses in language learning as well as pointing out critical parameters for expressive speech synthesis. Also, as the expressivity translates into both visual and acoustic effects that must be considered simultaneously, the multimodal components of expressivity, which are both on the voice and on the face, will be addressed to produce expressive multimodal speech.</p>
        </li>
        <li id="uid8">
          <p noindent="true"><b>The statistical modeling of speech.</b> - Statistical approaches are common for processing speech and they achieve performance that makes possible their use in actual applications. However, speech recognition systems still have limited capabilities (for example, even if large, the vocabulary is limited) and their performance drops significantly when dealing with degraded speech, such as noisy signals, distant microphone recording and spontaneous speech. Source separation based approaches are investigated as a way of making speech recognition systems more robust to noise. Handling new proper names is an example of critical aspect that is tackled, along with the use of statistical models for speech-text automatic alignment and for speech production.</p>
        </li>
        <li id="uid9">
          <p noindent="true"><b>The estimation and the exploitation of uncertainty in speech processing.</b> - Speech signals are highly variable and often disturbed with noise or other spurious signals (such as music or undesired extra speech). In addition, the output of speech enhancement and of source separation techniques is not exactly the accurate "clean" original signal, and estimation errors have to be taken into account in further processing. This is the goal of computing and handling the uncertainty of the reconstructed signal provided by source separation approaches.
Finally, MULTISPEECH also aims at estimating the reliability of phonetic segment boundaries and prosodic parameters for which no such information is yet available.</p>
        </li>
      </simplelist>
      <p>Although being interdependent, each of these three scientific challenges constitutes a founding research direction for the MULTISPEECH project. Consequently, the research program is organized along three research directions, each one matching a scientific challenge.
A large part of the research is conducted on French speech data; English and German languages are also considered in speech recognition experiments and language learning.
Adaptation to other languages of the machine learning based approaches is possible, depending on the availability of corresponding speech corpora.</p>
    </subsection>
  </presentation>
  <fondements id="uid10">
    <bodyTitle>Research Program</bodyTitle>
    <subsection id="uid11" level="1">
      <bodyTitle>Explicit Modeling of Speech Production and Perception</bodyTitle>
      <p>Speech signals are the consequence of the deformation of the vocal tract under the effect of the movements of the articulators (jaw, lips, tongue, ...) to modulate the excitation signal produced by the vocal cords or air turbulence. These deformations are visible on the face (lips, cheeks, jaw) through the coordination of different orofacial muscles and skin deformation induced by the latter.
These deformations may also express different emotions. We should note that human speech expresses more than just phonetic content, to be able to communicate effectively.
In this project, we address the different aspects related to speech production from the modeling of the vocal tract up to the production of expressive audiovisual speech.
Phonetic contrasts used by the phonological system of any language result from constraints imposed by the nature of the human speech production apparatus. For a given language these contrasts are organized so as to guarantee that human listeners can identify (categorize) sounds robustly.
The study of the categorization of sounds and prosody thus provides a complementary view on speech signals by focusing on the discrimination of sounds by humans, particularly in the context of language learning.</p>
      <subsection id="uid12" level="2">
        <bodyTitle>Articulatory modeling</bodyTitle>
        <p>Modeling speech production is a major issue in speech sciences. Acoustic simulation makes the link between articulatory and acoustic domains. Unfortunately this link cannot be fully exploited because there is almost always an acoustic mismatch between natural and synthetic speech generated with an articulatory model approximating the vocal tract. However, the respective effects of the geometric approximation, of the fact of neglecting some cavities in the simulation, of the imprecision of some physical constants and of the dimensionality of the acoustic simulation are still unknown. Hence, the first objective is to investigate the origin of the acoustic mismatch by designing more precise articulatory models, developing new methods to acquire tridimensional Magnetic Resonance Imaging (MRI) data of the entire vocal tract together with denoised speech signals, and evaluating several approaches of acoustic simulation.
The articulatory data acquisition relies on a head-neck antenna at Nancy Hospital to acquire MRI of the vocal tract, and on the articulograph Carstens AG501 available in the laboratory.</p>
        <p>Up to now, acoustic-to-articulatory inversion has been addressed as an instantaneous problem, articulatory gestures being recovered by concatenating local solutions.
The second objective is thus to investigate how more elaborated strategies (a syllabus of primitive gestures, articulatory targets…) can be incorporated in the acoustic-to-articulatory inversion algorithms to take into account dynamic aspects.</p>
      </subsection>
      <subsection id="uid13" level="2">
        <bodyTitle>Expressive acoustic-visual synthesis</bodyTitle>
        <p>Speech is considered as a bimodal communication means; the first modality is audio, provided by acoustic speech signals and the second one is visual, provided by the face of the speaker.
In our approach, the Acoustic-Visual Text-To-Speech synthesis (AV-TTS) is performed simultaneously with respect to its acoustic and visible components, by considering a bimodal signal comprising both acoustic and visual channels.
A first AV-TTS system has been developed resulting in a talking head;
the system relied on 3D-visual data and on an extension of our acoustic-unit concatenation text-to-speech synthesis system (SoJA).
An important goal is to provide an audiovisual synthesis that is intelligible, both acoustically and visually. Thus, we continue working on adding visible components of the head through a tongue model and a lip model.
We will also improve the TTS engine to increase the accuracy of the unit selection simultaneously into the acoustic and visual domains.
To acquire the facial data, we consider using a marker-less motion capture system using a kinect-like system with a face tracking software, which constitutes a relatively low-cost alternative to the Vicon system.</p>
        <p>Another challenging research goal is to add expressivity in the AV-TTS. The expressivity comes through the acoustic signal (prosody aspects) and also through head and eyebrow movements. One objective is to add a prosodic component in the TTS engine in order to take into account some prosodic entities such as emphasis (to highlight some important key words).
One intended approach will be to explore an expressivity measure at sound, syllable and/or sentence levels that describes the degree of perception or realization of an expression/emotion (audio and 3D domain). Such measures will be used as criteria in the selection process of the synthesis system.
To tackle the expressivity issue we will also investigate Hidden Markov Model (HMM) based synthesis which allows for easy adaptation of the system to available data and to various conditions.</p>
      </subsection>
      <subsection id="uid14" level="2">
        <bodyTitle>Categorization of sounds and prosody for native and non-native speech</bodyTitle>
        <p>Discriminating speech sounds and prosodic patterns is the keystone of language learning whether in the mother tongue or in a second language. This issue is associated with the emergence of phonetic categories, i.e., classes of sounds related to phonemes and prosodic patterns. The study of categorization is concerned not only with acoustic modeling but also with speech perception and phonology. Foreign language learning raises the issue of categorizing phonemes of the second language given the phonetic categories of the mother tongue. Thus, studies on the emergence of new categories, whether in the mother tongue (for people with language deficiencies) or in a second language, must rely upon studies on native and non-native acoustic realizations of speech sounds and prosody, and on perceptual experiments.
Concerning prosody, studies are focused on native and non-native realizations of modalities (e.g., question, affirmation, command, ...), as well as non-native realizations of lexical accents and focus (emphasis).</p>
        <p>For language learning, the analysis of the prosody and of the acoustic realization of the sounds aims at providing automatic feedback to language learners with respect to acquisition of prosody as well as acquisition of a correct pronunciation of the sounds of the foreign language.
Concerning the mother tongue we are interested in the monitoring of the process of sound categorization in the long term (mainly at primary school) and its relation with the learning of reading and writing skills <ref xlink:href="#multispeech-2016-bid0" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, especially for children with language deficiencies.</p>
      </subsection>
    </subsection>
    <subsection id="uid15" level="1">
      <bodyTitle>Statistical Modeling of Speech</bodyTitle>
      <p>Whereas the first research direction deals with the physical aspects of speech and its explicit modeling, this second research direction investigates statistical models for speech data.
Acoustic models are used to represent the pronunciation of the sounds or other acoustic events such as noise. Whether they are used for source separation, for speech recognition, for speech transcription, or for speech synthesis, the achieved performance strongly depends on the accuracy of these models. At the linguistic level, MULTISPEECH investigates models for handling the context (beyond the few preceding words currently handled by the <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>n</mi></math></formula>-gram models) and evolutive lexicons necessary when dealing with diachronic audio documents. Statistical approaches are also useful for generating speech signals. Along this direction, MULTISPEECH considers voice transformation techniques, with their application to pathological voices, and statistical speech synthesis applied to expressive multimodal speech synthesis.</p>
      <subsection id="uid16" level="2">
        <bodyTitle>Source separation</bodyTitle>
        <p>Acoustic modeling is a key issue for automatic speech recognition. Despite the progress made for many years, current speech recognition applications rely on strong constraints (close-talk microphone, limited vocabulary, or restricted syntax) to achieve acceptable performance. The quality of the input speech signals is particularly important and performance degrades quickly with noisy signals. Accurate signal enhancement techniques are therefore essential to increase the robustness of both automatic speech recognition and speech-text alignment systems to noise and non-speech events.</p>
        <p>In MULTISPEECH, focus is set on source separation techniques using multiple microphones and/or models of non-speech events. Some of the challenges include getting the most of the new modeling frameworks based on alpha-stable distributions and deep neural networks, combining them with established spatial filtering approaches, modeling more complex properties of speech and audio sources (phase, inter-frame and inter-frequency properties), and exploiting large data sets of speech, noise, and acoustic impulse responses to automatically discover new models. Beyond the definition of such models, the difficulty will be to design scalable estimation algorithms robust to overfitting, integrate them into the recently developed FASST <ref xlink:href="#multispeech-2016-bid1" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> and KAM software frameworks if relevant, and develop new software frameworks otherwise.</p>
      </subsection>
      <subsection id="uid17" level="2">
        <bodyTitle>Linguistic modeling</bodyTitle>
        <p>MULTISPEECH investigates lexical and language models in speech recognition with a focus on improving the processing of proper names and of spontaneous speech.
Proper names are relevant keys in information indexing, but are a real problem in transcribing many diachronic spoken documents which refer to data, especially proper names, that evolve over time. This leads to the challenge of dynamically adjusting lexicons and language models through the use of the context of the documents or of some relevant external information.
We also investigate language models defined on a continuous space (through neural network based approaches) in order to achieve a better generalization on unseen data, and to model long-term dependencies. We also want to introduce into these models additional relevant information such as linguistic features, semantic relation, topic or user-dependent information.</p>
        <p>Other topics are spontaneous speech and pronunciation lexicons.
Spontaneous speech utterances are often ill-formed and frequently contain disfluencies (hesitations, repetitions, ...) that degrade speech recognition performance. Hence the objective of improving the modeling of disfluencies and of spontaneous speech pronunciation variants.
Attention will also be set on pronunciation lexicons with respect to non-native speech and foreign names. Non-native pronunciation variants have to take into account frequent mis-pronunciations due to differences between mother tongue and target language phoneme inventories. Proper name pronunciation variants are a similar problem where difficulties are mainly observed for names of foreign origin that can be pronounced either in a French way or kept close to foreign origin native pronunciation.</p>
      </subsection>
      <subsection id="uid18" level="2">
        <bodyTitle>Speech generation by statistical methods</bodyTitle>
        <p>Over the last few years statistical speech synthesis has emerged as an alternative to corpus-based speech synthesis.
The announced advantages of the statistical speech synthesis are the possibility to deal with small amounts of speech resources and the flexibility for adapting models (for new emotions or new speakers), however, the quality is not as good as that of the concatenation-based speech synthesis.
MULTISPEECH will focus on a hybrid approach, combining corpus-based synthesis, for its high-quality speech signal output, and HMM-based speech synthesis for its flexibility to drive selection, and the main challenge will be on its application to producing expressive audio-visual speech.</p>
        <p>Moreover, in the context of acoustic feedback in foreign language learning, voice modification approaches are investigated to modify the learner’s (or teacher’s) voice in order to emphasize the difference between the learner’s acoustic realization and the expected realization.</p>
      </subsection>
    </subsection>
    <subsection id="uid19" level="1">
      <bodyTitle>Uncertainty Estimation and Exploitation in Speech Processing</bodyTitle>
      <p>This axis focuses on the uncertainty associated with some processing steps.
Uncertainty stems from the high variability of speech signals and from imperfect models. For example, enhanced speech signals resulting from source separation are not exactly the clean original speech signals. Words or phonemes resulting from automatic speech recognition contain errors, and the phone boundaries resulting from an automatic speech-text alignment are not always correct, especially in acoustically degraded conditions. Hence it is important to know the reliability of the results and/or to estimate the uncertainty of the results.</p>
      <subsection id="uid20" level="2">
        <bodyTitle>Uncertainty and acoustic modeling</bodyTitle>
        <p>Because small distortions in the separated source signals can translate into large distortions in the cepstral features used for speech recognition, this limits the recognition performance on noisy data. One way to address this issue is to estimate the uncertainty of the separated sources in the form of their posterior distribution and to propagate this distribution, instead of a point estimate, through the subsequent feature extraction and speech decoding stages. Although major improvements have been demonstrated in proof-of-concept experiments using knowledge of the true uncertainty, accurate uncertainty estimation and propagation remains an open issue.</p>
        <p>MULTISPEECH seeks to provide more accurate estimates of the posterior distribution of the separated source signals accounting for, e.g., posterior correlations over time and frequency which have not been considered so far.
The framework of variational Bayesian (VB) inference appears to be a promising direction.
Mappings learned on training data and fusion of multiple uncertainty estimators are also explored.
The estimated uncertainties are then exploited for acoustic modeling in speech recognition and, in the future, also for speech-text alignment.
This approach may later be extended to the estimation of the resulting uncertainty of the acoustic model parameters and of the acoustic scores themselves.</p>
      </subsection>
      <subsection id="uid21" level="2">
        <bodyTitle>Uncertainty and phonetic segmentation</bodyTitle>
        <p>The accuracy of the phonetic segmentation is important in several cases, as for example for the computation of prosodic features, for avoiding incorrect feedback to the learner in computer assisted foreign language learning, or for the post-synchronization of speech with face/lip images. Currently the phonetic boundaries obtained are quite correct on good quality speech, but the precision degrades significantly on noisy and non-native speech. Phonetic segmentation aspects will be investigated, both in speech recognition (i.e., spoken text unknown) and in forced alignment (i.e., when the spoken text is known).</p>
        <p>In the same way that combining several speech recognition outputs leads to improved speech recognition performance, MULTISPEECH will investigate the combination of several speech-text alignments as a way of improving the quality of speech-text alignment and of determining which phonetic boundaries are reliable and which ones are not, and also for estimating the uncertainty of the boundaries. Knowing the reliability of the boundaries will also be useful when segmenting speech corpora; this will help deciding which parts of the corpora need to be manually checked and corrected without an exhaustive checking of the whole corpus.</p>
      </subsection>
      <subsection id="uid22" level="2">
        <bodyTitle>Uncertainty and prosody</bodyTitle>
        <p>Prosody information is also investigated as a means for structuring speech data (determining sentence boundaries, punctuation…) possibly in addition to syntactic dependencies. Structuring automatic transcription output is important for further exploitation of the transcription results such as easier reading after the addition of punctuation, or exploitation of full sentences in automatic translation. Prosody information is also necessary for determining the modality of the utterance (question or not), as well as determining accented words.</p>
        <p>Prosody information comes from the fundamental frequency, the duration of the sounds and their energy. Any error in estimating these parameters may lead to a wrong decision. MULTISPEECH will investigate estimating the uncertainty of the duration of the phones (see uncertainty of phonetic boundaries above) and on the fundamental frequency, as well as how this uncertainty shall be propagated in the detection of prosodic phenomena such as accented words, utterance modality, or determination of the structure of the utterance.</p>
      </subsection>
    </subsection>
  </fondements>
  <domaine id="uid23">
    <bodyTitle>Application Domains</bodyTitle>
    <subsection id="uid24" level="1">
      <bodyTitle>Introduction</bodyTitle>
      <p>Approaches and models developed in the MULTISPEECH project are intended to be used for facilitating
oral communication in various situations through enhancements of the communication channels, either
directly via automatic speech recognition or speech production technologies, or indirectly, thanks to computer
assisted language learning. Applications also include the usage of speech technologies for helping people in
handicapped situations or for improving their autonomy. Foreseen application domains are related to computer
assisted learning, health and autonomy (more precisely aided communication and monitoring), annotation and
processing of spoken documents, and multimodal computer interaction.</p>
    </subsection>
    <subsection id="uid25" level="1">
      <bodyTitle>Computer Assisted Learning</bodyTitle>
      <p>Although speaking seems quite natural, learning foreign languages, or learning the mother tongue for people
with language deficiencies, represents critical cognitive stages. Hence, many scientific activities have been
devoted to these issues either from a production or a perception point of view. The general guiding principle
with respect to computer assisted mother or foreign language learning is to combine modalities or to augment
speech to make learning easier. Based upon a comparison of the learner’s production to a reference, automatic
diagnoses of the learner’s production can be considered, as well as perceptual feedback relying on an automatic
transformation of the learner’s voice. The diagnosis step strongly relies on the studies on categorization of
sounds and prosody in the mother tongue and in the second language. Furthermore, reliable diagnosis on each
individual utterance is still a challenge, and elaboration of advanced automatic feedback requires a temporally
accurate segmentation of speech utterances into phones and this explains why accurate segmentation of native
and non-native speech is an important topic in the field of acoustic speech modeling.</p>
    </subsection>
    <subsection id="uid26" level="1">
      <bodyTitle>Aided Communication and Monitoring</bodyTitle>
      <p>A foreseen application aims at improving the autonomy of elderly or disabled people, and fit with smartroom applications. In
a first step, source separation techniques could be tuned and should help for locating and monitoring people
through the detection of sound events inside apartments. In a longer perspective, adapting speech recognition
technologies to the voice of elderly people should also be useful for such applications, but this requires
the recording of adequate databases. Sound monitoring in other application fields (security, environmental
monitoring) could also be envisaged.</p>
    </subsection>
    <subsection id="uid27" level="1">
      <bodyTitle>Annotation and Processing of Spoken Documents and Audio Archives</bodyTitle>
      <p>A first type of annotation consists in transcribing a spoken document in order to get the corresponding
sequences of words, with possibly some complementary information, such as the structure (punctuation) or
the modality (affirmation/question) of the utterances to make the reading and understanding easier. Typical
applications of the automatic transcription of radio or TV shows, or of any other spoken document, include
making possible their access by deaf people, as well as by text-based indexing tools.</p>
      <p>A second type of annotation is related to speech-text alignment, which aims at determining the starting and
ending times of the words, and possibly of the sounds (phonemes). This is of interest in several cases as
for example, for annotating speech corpora for linguistic studies, and for synchronizing lip movements with
speech sounds, for example for avatar-based communications. Although good results are currently achieved on
clean data, automatic speech-text alignment needs to be improved for properly processing noisy spontaneous
speech data and needs to be extended to handle overlapping speech.</p>
      <p>Large audio archives are important for some communities of users, e.g., linguists, ethnologists or researchers
in digital humanities in general. In France, a notorious example is the "Archives du CNRS — Musée de
l’homme", gathering about 50,000 recordings dating back to the early 1900s. When dealing with very old
recordings, the practitioner is often faced with the problem of noise. This stems from the fact that a lot of
interesting material from a scientific point of view is very old or has been recorded in very adverse noisy
conditions, so that the resulting audio is poor. The work on source separation can lead to the design of semi-automatic denoising and enhancement features, that would allow these researchers to significantly enhance
their investigation capabilities, even without expert knowledge in sound engineering.</p>
      <p>Finally, there is also a need for speech signal processing techniques in the field of multimedia content
creation and rendering. Relevant techniques include speech and music separation, speech equalization, prosody
modification, and speaker conversion.</p>
    </subsection>
    <subsection id="uid28" level="1">
      <bodyTitle>Multimodal Computer Interactions</bodyTitle>
      <p>Speech synthesis has tremendous applications in facilitating communication in a human-machine interaction
context to make machines more accessible. For example, it started to be widely common to use acoustic speech
synthesis in smartphones to make possible the uttering of all the information. This is valuable in particular in
the case of handicap, as for blind people. Audiovisual speech synthesis, when used in an application such as
a talking head, i.e., virtual 3D animated face synchronized with acoustic speech, is beneficial in particular
for hard-of-hearing individuals. This requires an audiovisual synthesis that is intelligible, both acoustically
and visually. A talking head could be an intermediate between two persons communicating remotely when
their video information is not available, and can also be used in language learning applications as vocabulary
tutoring or pronunciation training tool. Expressive acoustic synthesis is of interest for the reading of a story,
such as audiobook, to facilitate the access to literature (for instance for blind people or illiterate people).</p>
    </subsection>
  </domaine>
  <highlights id="uid29">
    <bodyTitle>Highlights of the Year</bodyTitle>
    <subsection id="uid30" level="1">
      <bodyTitle>Highlights of the Year</bodyTitle>
      <p>We ranked 1st ex aqueo for the ”Professionally produced music recordings” task of the 2016 Signal Separation Evaluation Campaign (SiSEC) <ref xlink:href="#multispeech-2016-bid2" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
    </subsection>
  </highlights>
  <logiciels id="uid31">
    <bodyTitle>New Software and Platforms</bodyTitle>
    <subsection id="uid32" level="1">
      <bodyTitle>ASTALI</bodyTitle>
      <p>Automatic Speech-Text Alignment Software</p>
      <p noindent="true"><span class="smallcap" align="left">Keyword:</span> Speech-text alignment</p>
      <p noindent="true">
        <span class="smallcap" align="left">Functional Description</span>
      </p>
      <p>ASTALI is a software for aligning a speech signal with its corresponding orthographic transcription (given in simple text file for short audio signals or in .trs files as generated by transcriber for longer speech signals). Using a phonetic lexicon and automatic grapheme-to-phoneme converters, all the possible sequences of phones corresponding to the text are generated. Then, using acoustic models, the tool finds the best phone sequence and provides the boundaries at the phone and at the word levels.
ASTALI is available through a web application, which makes the service easy to use, without requiring any software downloading.
This year, the integration of the web application on the ORTOLANG platform has been finalized.</p>
      <simplelist>
        <li id="uid33">
          <p noindent="true">Participants: Dominique Fohr, Odile Mella, Antoine Chemardin, Valérian Girard and Denis Jouvet</p>
        </li>
        <li id="uid34">
          <p noindent="true">Contact: Dominique Fohr</p>
        </li>
        <li id="uid35">
          <p noindent="true">URLs: <ref xlink:href="https://www.ortolang.fr/market/tools/astali" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>www.<allowbreak/>ortolang.<allowbreak/>fr/<allowbreak/>market/<allowbreak/>tools/<allowbreak/>astali</ref>; <ref xlink:href="http://astali.loria.fr/" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>astali.<allowbreak/>loria.<allowbreak/>fr/</ref>; and <ref xlink:href="http://ortolang108.inist.fr/astali/" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>ortolang108.<allowbreak/>inist.<allowbreak/>fr/<allowbreak/>astali/</ref></p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid36" level="1">
      <bodyTitle>dnnsep</bodyTitle>
      <p>Multichannel audio source separation with deep neural networks</p>
      <p noindent="true"><span class="smallcap" align="left">Keywords:</span> Audio - Source Separation - Deep learning</p>
      <p noindent="true">
        <span class="smallcap" align="left">Scientific Description</span>
      </p>
      <p>dnnsep is the only source separation software relying on multichannel Wiener filtering based on deep learning. Deep neural networks are used to initialize and reestimate the power spectrum of the sources at every iteration of an expectation-maximization (EM) algorithm. This results in state-of-the-art separation quality for both speech and music.</p>
      <p noindent="true">
        <span class="smallcap" align="left">Functional Description</span>
      </p>
      <p>dnnsep is a new software that combines deep neural networks and multichannel signal processing for speech enhancement and separation of musical recordings.</p>
      <simplelist>
        <li id="uid37">
          <p noindent="true">Participants: Aditya Nugraha, Antoine Liutkus and Emmanuel Vincent</p>
        </li>
        <li id="uid38">
          <p noindent="true">Contact: Emmanuel Vincent</p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid39" level="1">
      <bodyTitle>JSnoori</bodyTitle>
      <p>
        <span class="smallcap" align="left">Functional Description</span>
      </p>
      <p>JSnoori is written in Java and uses signal processing algorithms developed within the WinSnoori software with the double objective of being a platform independent signal visualization and manipulation tool, and also for designing exercises for learning the prosody of a foreign language.
JSnoori can be used directly or via scripts written in Jython. This year, several approaches for computing the fundamental frequency have been added; and, JSnoori is now available through the ORTOLANG platform.</p>
      <simplelist>
        <li id="uid40">
          <p noindent="true">Participants: Yves Laprie, Slim Ouni, Aghilas Sini and Ilef Ben Farhat</p>
        </li>
        <li id="uid41">
          <p noindent="true">Contact: Yves Laprie</p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid42" level="1">
      <bodyTitle>KATS</bodyTitle>
      <p>Kaldi-based Automatic Transcription System</p>
      <p noindent="true"><span class="smallcap" align="left">Keyword:</span> Speech recognition</p>
      <p noindent="true">
        <span class="smallcap" align="left">Functional Description</span>
      </p>
      <p>KATS is a multipass system for transcribing audio data, and in particular radio or TV shows. The audio stream is first split into homogeneous segments that are decoded using the most adequate acoustic model with a large vocabulary continuous speech recognition engine. In this new software, the recognition engine is based on the Kaldi toolkit, and uses Deep Neural Network - DNN - based acoustic models. An extra processing pass is run in order to rescore the <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>n</mi></math></formula>-best hypotheses with a higher order language model.</p>
      <simplelist>
        <li id="uid43">
          <p noindent="true">Participants: Odile Mella, Dominique Fohr and Denis Jouvet</p>
        </li>
        <li id="uid44">
          <p noindent="true">Contact: Dominique Fohr</p>
        </li>
        <li id="uid45">
          <p noindent="true">URL: Available online on the A||go platform: <ref xlink:href="https://allgo.inria.fr/app/loriasts_kaldi" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>allgo.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>app/<allowbreak/>loriasts_kaldi</ref></p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid46" level="1">
      <bodyTitle>PLAVIS</bodyTitle>
      <p>Sofware for audio-visual and multimodal data acquisition and processing</p>
      <p noindent="true">
        <span class="smallcap" align="left">Functional Description</span>
      </p>
      <p>Within the ADT PLAVIS (cf. <ref xlink:href="#uid208" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>), we have developed a software for 3D audiovisual data acquisition and synthesis. The system incorporates an animation module of the talking head to reconstruct the animated face along with audio. The acquisition software handles one or several acquisition systems: motion-capture (Kinect-like), Vicon or EMA systems. The various acquisition channels are synchronized. The animation technique can exploit multimodal data to define blendshapes that controls the face; the advantage of using blendshapes is to be able to transfer the animation from one 3D human model to another. A semi-automatic acoustic boundary correction process is integrated in the corpus building process. The text-to-speech processing is driven by the Soja software.</p>
      <simplelist>
        <li id="uid47">
          <p noindent="true">Participants: Vincent Colotte, Slim Ouni, Sara Dahmani</p>
        </li>
        <li id="uid48">
          <p noindent="true">Contact: Vincent Colotte</p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid49" level="1">
      <bodyTitle>SOJA</bodyTitle>
      <p>Speech Synthesis platform in JAva</p>
      <p noindent="true">
        <span class="smallcap" align="left">Functional Description</span>
      </p>
      <p>SOJA is a software for Text-To-Speech synthesis (TTS) which relies on a non uniform unit selection algorithm. It performs all steps from text input to speech signal output. A set of associated tools is available for elaborating a corpus for a TTS system (transcription, alignment. . . ). Currently, the corpus contains about 3 hours of speech recorded by a female speaker. Most of the modules are in Java, some are in C. The SOJA software runs under Windows and Linux. It can be launched with a graphical user interface or directly integrated in a Java code or by following the client-server paradigm. During 2016, the part of code in C was reduced to go to a full-Java software in the future. The natural language processing can now be restarted from any step. This functionality is useful for instance during corpus processing when using semi-automatic boundaries correction.</p>
      <simplelist>
        <li id="uid50">
          <p noindent="true">Participants: Vincent Colotte and Alexandre Lafosse</p>
        </li>
        <li id="uid51">
          <p noindent="true">Contact: Vincent Colotte</p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid52" level="1">
      <bodyTitle>VisArtico</bodyTitle>
      <p>Visualization of EMA Articulatory data</p>
      <p noindent="true">
        <span class="smallcap" align="left">Functional Description</span>
      </p>
      <p>VisArtico is a user-friendly software which allows visualizing EMA data acquired by an articulograph (AG500, AG501 or NDI Wave). This visualization software has been designed so that it can directly use the data provided by the articulograph to display the articulatory coil trajectories, synchronized with the corresponding acoustic recordings. Moreover, VisArtico not only allows viewing the coil trajectories but also enriches the visual information by indicating clearly and graphically the data for the tongue, lips and jaw. In addition, it is possible to insert images (MRI or X-Ray, for instance) to compare the EMA data with data obtained through other acquisition techniques. It is possible to generate a movie for any articulatory-acoustic sequence.
During 2016, we have made a new version of VisArtico where the 3D view is now based on OpenGL. This allows a better quality rendering. It is possible to make measurement between sensors to compute the distance. Finally, we added the possibility to display the fundamental frequency on the spectrogram.</p>
      <simplelist>
        <li id="uid53">
          <p noindent="true">Participants: Slim Ouni, Loic Mangeonjean, Ilef Ben Farhat and Bertrand Muller</p>
        </li>
        <li id="uid54">
          <p noindent="true">Contact: Slim Ouni</p>
        </li>
        <li id="uid55">
          <p noindent="true">URL: <ref xlink:href="http://visartico.loria.fr" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>visartico.<allowbreak/>loria.<allowbreak/>fr</ref></p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid56" level="1">
      <bodyTitle>Xarticulators</bodyTitle>
      <p><span class="smallcap" align="left">Keyword:</span> Medical imaging</p>
      <p noindent="true">
        <span class="smallcap" align="left">Functional Description</span>
      </p>
      <p>The Xarticulators software is intended to delineate contours of speech articulators in X-ray images, construct articulatory models and synthesize speech from X-ray films. This software provides tools to track contours automatically, semi-automatically or by hand, to make the visibility of contours easier, to add anatomical landmarks to speech articulators and to synchronize images with the sound. In addition we also added the possibility of processing digitized manual delineation results made on sheets of papers when no software is available. Xarticulators also enables the construction of adaptable linear articulatory models from the X-ray images and incorporates acoustic simulation tools to synthesize speech signals from the vocal tract shape. Recent work was on the possibility of synthesizing speech from X-ray or 2D-MRI films.</p>
      <p noindent="true">During 2016, we developed a new version of the articulatory model which incorporates a more realistic model of the epiglottis and lips.</p>
      <simplelist>
        <li id="uid57">
          <p noindent="true">Contact: Yves Laprie</p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid58" level="1">
      <bodyTitle>Platforms</bodyTitle>
      <subsection id="uid59" level="2">
        <bodyTitle>Platform MultiMod : Multimodal Acquisition Data Platform</bodyTitle>
        <p>
          <span class="smallcap" align="left">Functional Description</span>
        </p>
        <p>Within a LORIA exploratory project (cf. <ref xlink:href="#uid214" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>), we have set up an acquisition hardware platform to acquire multimodal data in speech communication context. The system is composed of the articulograph Carstens AG501 (which was acquired as part of the EQUIPEX ORTOLANG - cf. <ref xlink:href="#uid131" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>), 4 Vicon cameras (a motion capture system), an Intel RealSense which is a depth camera (acquired as part of the project CORExp - cf. <ref xlink:href="#uid106" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>), a video camera and a microphone. With such heterogeneous hardware the synchronization is essential; this is achieved through a trigger device. All the data processing is performed with the PLAVIS software.
This year, the system has been used to acquire multimodal data for the MCC project (cf. <ref xlink:href="#uid239" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>) and a first exploratory expressive multimodal corpus <ref xlink:href="#multispeech-2016-bid3" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
        <simplelist>
          <li id="uid60">
            <p noindent="true">Participants: Slim Ouni, Vincent Colotte, Valerian Girard, Sara Dahmani</p>
          </li>
          <li id="uid61">
            <p noindent="true">Contact: Slim Ouni</p>
          </li>
        </simplelist>
      </subsection>
    </subsection>
  </logiciels>
  <resultats id="uid62">
    <bodyTitle>New Results</bodyTitle>
    <subsection id="uid63" level="1">
      <bodyTitle>Explicit Modeling of Speech Production and Perception</bodyTitle>
      <participants>
        <person key="multispeech-2014-idp66072">
          <firstname>Yves</firstname>
          <lastname>Laprie</lastname>
        </person>
        <person key="multispeech-2014-idp75400">
          <firstname>Slim</firstname>
          <lastname>Ouni</lastname>
        </person>
        <person key="multispeech-2014-idp70184">
          <firstname>Vincent</firstname>
          <lastname>Colotte</lastname>
        </person>
        <person key="multispeech-2014-idp63592">
          <firstname>Anne</firstname>
          <lastname>Bonneau</lastname>
        </person>
        <person key="multispeech-2014-idp76848">
          <firstname>Agnès</firstname>
          <lastname>Piquard-Kipffer</lastname>
        </person>
        <person key="multispeech-2014-idp62112">
          <firstname>Denis</firstname>
          <lastname>Jouvet</lastname>
        </person>
        <person key="multispeech-2014-idp74144">
          <firstname>Odile</firstname>
          <lastname>Mella</lastname>
        </person>
        <person key="multispeech-2014-idp64832">
          <firstname>Dominique</firstname>
          <lastname>Fohr</lastname>
        </person>
        <person key="multispeech-2014-idp92912">
          <firstname>Benjamin</firstname>
          <lastname>Elie</lastname>
        </person>
        <person key="multispeech-2015-idp141192">
          <firstname>Sucheta</firstname>
          <lastname>Ghosh</lastname>
        </person>
        <person key="multispeech-2016-idp176080">
          <firstname>Anastasiia</firstname>
          <lastname>Tsukanova</lastname>
        </person>
        <person key="multispeech-2016-idp166368">
          <firstname>Yang</firstname>
          <lastname>Liu</lastname>
        </person>
        <person key="multispeech-2015-idp120032">
          <firstname>Sara</firstname>
          <lastname>Dahmani</lastname>
        </person>
        <person key="multispeech-2015-idp121264">
          <firstname>Valérian</firstname>
          <lastname>Girard</lastname>
        </person>
        <person key="multispeech-2014-idp83080">
          <firstname>Aghilas</firstname>
          <lastname>Sini</lastname>
        </person>
      </participants>
      <subsection id="uid64" level="2">
        <bodyTitle>Articulatory modeling</bodyTitle>
        <subsection id="uid65" level="3">
          <bodyTitle>Acoustic simulations</bodyTitle>
          <p>The acoustic simulations play a central role in articulatory synthesis and should enable the production of all classes of sounds in a realistic manner.
The production of voiced fricatives relies on a partial closure of the glottis which simultaneously creates an airflow which generates turbulence downwards from the constriction and the vibration of the vocal folds. Our acoustic simulation framework <ref xlink:href="#multispeech-2016-bid4" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> has been extended to incorporate a glottal chink <ref xlink:href="#multispeech-2016-bid5" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> in a self-oscillating vocal fold model. The glottis is then made up of two main separated components: a self-oscillating part and a constantly open chink. This feature allows the simulation of voiced fricatives, thanks to a self-oscillating model of the vocal folds to generate the voiced source, and the glottal opening that is necessary to generate the frication noise.</p>
          <p>The acoustic propagation paradigm is appropriately chosen so that it can deal with complex geometries and a time-varying length of the vocal tract. Temporal scenarios for the dynamic shapes of the vocal tract and the glottal configurations were derived from the simultaneous acquisition of X-ray or MRI images and audio recording. Copy synthesis of a few French sentences <ref xlink:href="#multispeech-2016-bid6" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, <ref xlink:href="#multispeech-2016-bid7" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, <ref xlink:href="#multispeech-2016-bid8" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> shows the accuracy of the simulation framework to reproduce acoustic cues of phrase-level utterances containing most of French phone (sound) classes while considering the real geometric shape of the speaker. For this purpose the articulatory model has been extended to offer a better precision of the epiglottis and of lips.</p>
        </subsection>
        <subsection id="uid66" level="3">
          <bodyTitle>Acquisition of articulatory data</bodyTitle>
          <p>The acquisition of dynamic data is a key objective since speech production gestures involve the anticipation of the articulatory targets of the coming sounds. Cine-MRI represents an invaluable tool since it can image the whole vocal tract. However, speech requires a sampling frequency above 30 Hz to capture interesting information. Compressive sampling relies on partially collecting data in the Fourier space of the images acquired via MRI. The combination of compressed sensing technique, along with homodyne reconstruction, enables the missing data to be recovered <ref xlink:href="#multispeech-2016-bid9" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. The good reconstruction is guaranteed by an appropriate design of the sampling pattern. It is based on a pseudo-random Cartesian scheme, where each line is partially acquired for use of the homodyne reconstruction, and where the lines are pseudo-randomly sampled: central lines are constantly acquired and the sampling density decreases as the lines are far from the center.</p>
        </subsection>
        <subsection id="uid67" level="3">
          <bodyTitle>Markerless articulatory acquisition techniques</bodyTitle>
          <p>With the spread of depth cameras (kinect-like systems), many researchers consider using these systems to track the movement of some speech articulators as lips and jaw. We are considering using this kind of system if it is suitable for speech production studies. For this reason, we have assessed the precision of markerless acquisition techniques when used to acquire articulatory data for speech production studies <ref xlink:href="#multispeech-2016-bid10" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. Two different markerless systems have been evaluated and compared to a marker-based one. The main finding is that both markerless systems provide reasonable results during normal speech and the quality is uneven during fast articulated speech. The quality of the data is dependent on the temporal resolution of the markerless system.</p>
        </subsection>
      </subsection>
      <subsection id="uid68" level="2">
        <bodyTitle>Expressive acoustic-visual synthesis</bodyTitle>
        <subsection id="uid69" level="3">
          <bodyTitle>Expressive speech</bodyTitle>
          <p>A comparison between emotional and neutral speech was conducted using a small database containing utterances recorded in six emotional types (anger, fear, sadness, disgust, surprise and joy) as well as in a neutral pronunciation. The prosodic analysis focused on the main prosodic parameters such as vowel duration, energy and fundamental frequency (F0) level, and pause occurrences. The values of prosodic parameters were compared among the various emotional styles, as well as between emotional style and neutral style utterances. Moreover, the structuration of the sentences, in the various emotional styles, was particularly studied through a detailed analysis of pause occurrences and their length, and of the length of prosodic groups <ref xlink:href="#multispeech-2016-bid11" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
        </subsection>
        <subsection id="uid70" level="3">
          <bodyTitle>Expressive acoustic and visual speech</bodyTitle>
          <p>Concerning expressive audiovisual speech synthesis, a case study of a semi-professional actor who uttered a set of sentences for 6 different emotions in addition to neutral speech was conducted. Our purpose is to identify the main characteristics of audiovisual expressions that need to be integrated during synthesis to provide believable emotions to the virtual 3D talking head. We have recorded concurrently audio and motion capture data. The acoustic and the visual data have been analyzed. The main finding is that although some expressions are not well identified, some expressions were well characterized and tied in both acoustic and visual space <ref xlink:href="#multispeech-2016-bid3" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. The acquisition of the corpus was done with the platform software PLAVIS (cf. <ref xlink:href="#uid208" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>).</p>
        </subsection>
      </subsection>
      <subsection id="uid71" level="2">
        <bodyTitle>Categorization of sounds and prosody for native and non-native speech</bodyTitle>
        <subsection id="uid72" level="3">
          <bodyTitle>Categorization of sounds for native speech</bodyTitle>
          <p>We examined the schooling experiences of 166 young people with disabilities, aged from 6 to 20 years old. These children and teenagers had specific language impairment : SLI (severe language impairment), dyslexia, dysorthographia. The phonemic discrimination, phonological and phonemic analysis difficulties faced in their childhoods had raised reading difficulties which constituted a major obstacle, which the pupils did not overcome. Consequently, this led them to repeat one or more grades. This rate is 18 times higher than the French average. The importance of this cycle of learning can be better understood through this data, which could also enable, if not overcoming the handicap, to at least improving their learning possibilities <ref xlink:href="#multispeech-2016-bid12" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
        </subsection>
        <subsection id="uid73" level="3">
          <bodyTitle>Digital books for language impaired children</bodyTitle>
          <p>Three digital albums for language impaired children were designed within the Handicom (ADT funded by Inria). These three prototypes focus on the importance of multimodal speech combining written words and visual clues: a 3D avatar telling the stories and coding oral language in LPC (french cued speech) for hearing impaired children. Eight speech and language therapists used one of these albums (the digital prototype <i>Nina fête son anniversaire</i> !) with 8 children who are aged 5 years: 4 hearing impaired children, 2 children with SLI and 2 children with autism. The training they experienced with these children showed that the use of the digital book can foster some capacities involved in language learning <ref xlink:href="#multispeech-2016-bid13" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
        </subsection>
        <subsection id="uid74" level="3">
          <bodyTitle>Analysis of non-native pronunciations</bodyTitle>
          <p>The IFCASL corpus is a French-German bilingual phonetic learner corpus designed, recorded and annotated in the IFCASL project (cf. <ref xlink:href="#uid167" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>).
It incorporates data for a language pair in both directions, i.e. in our case French learners of German, and German learners of French.
In addition, the corpus is complemented by two sub-corpora of native speech by the same speakers.
The corpus has been finalized, and provides spoken data by about 100 speakers with comparable productions, annotated and segmented at the word and phone levels, with more than 50% of manually checked and corrected data <ref xlink:href="#multispeech-2016-bid14" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
          <p>We investigated the correct placement of lexical (German) or post-lexical (French) accents <ref xlink:href="#multispeech-2016-bid15" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. French and German differ with respect to the representation and implementation of prominence. French can be assumed to have no prominence represented in the mental lexicon and accents are regularly assigned post-lexically on the last full vowel of an accentual group. In German, prominence is considered to be represented lexically. This difference may give rise to interferences when German speakers learn French and French speakers learn German.
Results of a judgment task (conducted with 3 trained phoneticians) of native and nonnative productions of French learners of German and German learners of French, all of them beginners, show that both groups have not completely acquired the correct suprasegmental structures in the respective L2 <footnote id="uid75" id-text="1">L2 indicates the non-native language, whereas L1 indicates the native language</footnote>, since both groups are worse concerning the correct placement of prominence than the native speakers. Furthermore, the results suggest that the native pattern is one of the most important factors for wrong prominence placements in the foreign language,
e.g., if the prominence placement of L1 and L2 coincide, speakers produce the smallest amount of errors. Finally, results indicate that visual display of accented syllables increases the likelihood of a correct accent placement.</p>
        </subsection>
        <subsection id="uid76" level="3">
          <bodyTitle>Implementation of acoustic feedback for devoicing of final fricatives</bodyTitle>
          <p>In view of implementing acoustic feedback in foreign language learning we analyzed acoustic cues which could explain that final fricatives are perceived as voiced or unvoiced. The ratio of unvoiced frames in the consonantal segment and also the ratio between consonantal duration and vowel duration were measured. As expected, we found that beginners face more difficulties to produce voiced fricatives than advanced learners. Also, the production becomes easier for the learners, especially for beginners, if they practice repetition after a native speaker. We use these findings to design and develop feedback via speech analysis/synthesis technique TD-PSOLA using the learner’s own voice and voiced fricatives uttered by French speakers <ref xlink:href="#multispeech-2016-bid16" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. We selected fully voiced exemplars and evaluated whether the presence of an additional schwa fosters the perception of voicing by native French speakers.</p>
        </subsection>
      </subsection>
    </subsection>
    <subsection id="uid77" level="1">
      <bodyTitle>Statistical Modeling of Speech</bodyTitle>
      <participants>
        <person key="multispeech-2014-idp67512">
          <firstname>Antoine</firstname>
          <lastname>Liutkus</lastname>
        </person>
        <person key="multispeech-2014-idp68752">
          <firstname>Emmanuel</firstname>
          <lastname>Vincent</lastname>
        </person>
        <person key="multispeech-2016-idp128576">
          <firstname>Irène</firstname>
          <lastname>Illina</lastname>
        </person>
        <person key="multispeech-2014-idp64832">
          <firstname>Dominique</firstname>
          <lastname>Fohr</lastname>
        </person>
        <person key="multispeech-2014-idp62112">
          <firstname>Denis</firstname>
          <lastname>Jouvet</lastname>
        </person>
        <person key="multispeech-2014-idp70184">
          <firstname>Vincent</firstname>
          <lastname>Colotte</lastname>
        </person>
        <person key="multispeech-2015-idp125016">
          <firstname>Ken</firstname>
          <lastname>Deguernel</lastname>
        </person>
        <person key="multispeech-2016-idp161472">
          <firstname>Mathieu</firstname>
          <lastname>Fontaine</lastname>
        </person>
        <person key="multispeech-2015-idp127472">
          <firstname>Amal</firstname>
          <lastname>Houidhek</lastname>
        </person>
        <person key="multispeech-2015-idp130000">
          <firstname>Aditya</firstname>
          <lastname>Nugraha</lastname>
        </person>
        <person key="multispeech-2014-idp89216">
          <firstname>Imran</firstname>
          <lastname>Sheikh</lastname>
        </person>
        <person key="multispeech-2015-idp136160">
          <firstname>Imene</firstname>
          <lastname>Zangar</lastname>
        </person>
        <person key="multispeech-2015-idp137432">
          <firstname>Mohamed</firstname>
          <lastname>Bouallegue</lastname>
        </person>
        <person key="multispeech-2015-idp123760">
          <firstname>Sunit</firstname>
          <lastname>Sivasankaran</lastname>
        </person>
      </participants>
      <subsection id="uid78" level="2">
        <bodyTitle>Source separation</bodyTitle>
        <subsection id="uid79" level="3">
          <bodyTitle>Deep neural models for source separation</bodyTitle>
          <p>We pursued our research on the use of deep learning for multichannel source separation <ref xlink:href="#multispeech-2016-bid17" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.
Our technique exploits both the spatial properties of the sources
as modeled by their spatial covariance matrices and their spectral properties as modeled by a deep neural
network. The model parameters are alternately estimated in an expectation-maximization (EM) fashion.
We used this technique for music separation in the context of the 2016 Signal
Separation Evaluation Campaign (SiSEC) <ref xlink:href="#multispeech-2016-bid2" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. We also used deep learning to address the fusion of multiple source separation techniques
and found it to perform much better than the variational Bayesian model averaging techniques previously
investigated <ref xlink:href="#multispeech-2016-bid18" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
          <p>We wrote an article about music source separation for the general public <ref xlink:href="#multispeech-2016-bid19" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
        </subsection>
        <subsection id="uid80" level="3">
          <bodyTitle><formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>α</mi></math></formula>-stable modeling of audio signals</bodyTitle>
          <p>The alpha-harmonizable model has recently been proposed by A. Liutkus et al. <ref xlink:href="#multispeech-2016-bid20" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> as the only available probabilistic framework to account for signal processing methods manipulating fractional spectrograms instead of more traditional power spectrograms. Indeed, they generalize the classical Gaussian formulation and permit to handle large uncertainties or signal dynamics, which are both common in audio.</p>
          <p>Our work on this topic this year has notably focused on its extension to the multichannel setting, which is important for music processing and source localization. Since inference in multivariate alpha-stable distribution is a very intricate issue, the approach undertaken has focused on analysing the multichannel signals through the joint analysis of multiple scalar projections on the real line. This results in an original algorithm called PROJET that combines computational tractability with the inherent robustness of alpha-stable models <ref xlink:href="#multispeech-2016-bid21" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, <ref xlink:href="#multispeech-2016-bid22" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
        </subsection>
      </subsection>
      <subsection id="uid81" level="2">
        <bodyTitle>Acoustic modeling</bodyTitle>
        <subsection id="uid82" level="3">
          <bodyTitle>Noise-robust acoustic modeling</bodyTitle>
          <p>In many real-world conditions, the target speech signal is reverberated and noisy.
In order to motivate further work by the community, we created an international
evaluation campaign on that topic in 2011: the CHiME Speech Separation and Recognition Challenge. After
three successful editions <ref xlink:href="#multispeech-2016-bid23" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, <ref xlink:href="#multispeech-2016-bid24" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, we organized the fourth edition in 2016.
We also summarized the speech distortion conditions in real scenarios for speech processing applications <ref xlink:href="#multispeech-2016-bid25" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>
and collected a French corpus for distant-microphone speech processing in real homes <ref xlink:href="#multispeech-2016-bid26" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
          <p>Speech enhancement and automatic speech recognition (ASR) are most often evaluated in matched (or multi-condition) settings where the acoustic conditions
of the training data match (or cover) those of the test data. We conducted a systematic assessment of the impact of acoustic mismatches
(noise environment, microphone response, data simulation) between training and
test data on the performance of recent DNN-based speech enhancement and ASR techniques <ref xlink:href="#multispeech-2016-bid27" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.
The results show that most algorithms perform consistently on real and simulated data and are barely affected by training on different noise environments.
This suggests that DNNs generalize more easily than previously thought.</p>
        </subsection>
        <subsection id="uid83" level="3">
          <bodyTitle>Environmental sounds</bodyTitle>
          <p>We explored acoustic modeling for the classification of environmental sound events and sound scenes and submitted our system to the DCASE 2016 Challenge <ref xlink:href="#multispeech-2016-bid28" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
        </subsection>
      </subsection>
      <subsection id="uid84" level="2">
        <bodyTitle>Linguistic modeling</bodyTitle>
        <subsection id="uid85" level="3">
          <bodyTitle>Out-of-vocabulary proper name retrieval</bodyTitle>
          <p>The diachronic nature of broadcast news causes frequent variations in the linguistic
content and vocabulary, leading to the problem of Out-Of-Vocabulary (OOV)
words in automatic speech recognition. Most of the OOV words are found to
be proper names whereas proper names are important for automatic indexing
of audio-video content as well as for obtaining reliable automatic transcriptions.
New proper names missed by the speech recognition system can be recovered by a
dynamic vocabulary multi-pass recognition approach in which new proper names
are added to the speech recognition vocabulary based on the context of the spoken
content <ref xlink:href="#multispeech-2016-bid29" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. The goal of this work is to model the semantic and
topical context of new proper names in order to retrieve OOV words which are relevant
to the spoken content in the audio document. Probabilistic topic models <ref xlink:href="#multispeech-2016-bid30" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> and word embeddings
from neural network models are explored for the task of retrieval of relevant
proper names. Neural network context models trained with an objective to maximise the retrieval performance
are proposed. A Neural Bag-of-Words (NBOW) model trained to learn
context vector representations at a document level is shown to outperform the
generic representations. The proposed Neural Bag-of-Weighted-Words (NBOW2)
model learns to assign a degree of importance to input words and has the ability
to capture task specific key-words <ref xlink:href="#multispeech-2016-bid31" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> <ref xlink:href="#multispeech-2016-bid32" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. Experiments on automatic speech recognition
of French broadcast news videos demonstrate the effectiveness of the proposed
models. Further evaluation of the NBOW2 model on standard text classification
tasks, including movie review sentiment classification and newsgroup topic classification, shows that it learns interesting information about the task and gives
the best classification accuracies among the bag-of-words models.</p>
        </subsection>
        <subsection id="uid86" level="3">
          <bodyTitle>Adding words in a language model</bodyTitle>
          <p>Out-of-vocabulary (OOV) words can pose a particular problem
for automatic speech recognition of broadcast
news. The language models (LMs) of ASR systems are
typically trained on static corpora, whereas new words (particularly
new proper nouns) are continually introduced in
the media. Additionally, such OOVs are often content-rich
proper nouns that are vital to understanding the topic. We explore methods for dynamically adding OOVs to
language models by adapting the n-gram language model
used in our ASR system. We propose two strategies: the
first one relies on finding in-vocabulary (IV) words similar to the
OOVs, where word embeddings are used to define similarity.
Our second strategy leverages a small contemporary corpus
to estimate OOV probabilities. The models we propose yield
improvements in perplexity over the baseline; in addition,
the corpus-based approach leads to a significant decrease
in proper noun error rate over the baseline in recognition
experiments <ref xlink:href="#multispeech-2016-bid33" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
        </subsection>
        <subsection id="uid87" level="3">
          <bodyTitle>Music language modeling</bodyTitle>
          <p>Similarly to speech, music involves several levels of information, from the acoustic signal up to cognitive
quantities such as composer style or key, through mid-level quantities such as a musical score or a sequence
of chords. The dependencies between mid-level and lower- or higher-level information can be represented
through acoustic models and language models, respectively. We published two articles that summarize our work
on the System &amp; Contrast model for the characterization of the mid-term and long-term structure of music <ref xlink:href="#multispeech-2016-bid34" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>
and on the structural segmentation of popular music pieces using a regularity constraint that naturally
stems from this model <ref xlink:href="#multispeech-2016-bid35" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, <ref xlink:href="#multispeech-2016-bid36" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. We also proposed a new model for automatic music
improvisation that combines a multi-dimensional probabilistic model encoding the musical experience of the
system and a factor oracle encoding the local context of the improvisation <ref xlink:href="#multispeech-2016-bid37" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
        </subsection>
      </subsection>
      <subsection id="uid88" level="2">
        <bodyTitle>Speech generation by statistical methods</bodyTitle>
        <p>Work on HMM-based Arabic speech synthesis was carried out within a CMCU PHC project with ENIT (Engineer school at Tunis-Tunisia; cf. <ref xlink:href="#uid247" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>). A first version of the system, based on the HTS toolkit (HMM-based Speech Synthesis System), is now working; and the study of the impact of some parameters is ongoing.</p>
        <p>In parallel, the HTS system is also applied to the French language.</p>
      </subsection>
    </subsection>
    <subsection id="uid89" level="1">
      <bodyTitle>Uncertainty Estimation and Exploitation in Speech Processing</bodyTitle>
      <participants>
        <person key="multispeech-2014-idp68752">
          <firstname>Emmanuel</firstname>
          <lastname>Vincent</lastname>
        </person>
        <person key="multispeech-2014-idp74144">
          <firstname>Odile</firstname>
          <lastname>Mella</lastname>
        </person>
        <person key="multispeech-2014-idp64832">
          <firstname>Dominique</firstname>
          <lastname>Fohr</lastname>
        </person>
        <person key="multispeech-2014-idp62112">
          <firstname>Denis</firstname>
          <lastname>Jouvet</lastname>
        </person>
        <person key="multispeech-2014-idp84320">
          <firstname>Baldwin</firstname>
          <lastname>Dumortier</lastname>
        </person>
        <person key="multispeech-2015-idp143688">
          <firstname>Juan Andres</firstname>
          <lastname>Morales Cordovilla</lastname>
        </person>
        <person key="multispeech-2016-idp149200">
          <firstname>Karan</firstname>
          <lastname>Nathwani</lastname>
        </person>
        <person key="multispeech-2016-idp141776">
          <firstname>Ismaël</firstname>
          <lastname>Bada</lastname>
        </person>
      </participants>
      <subsection id="uid90" level="2">
        <bodyTitle>Uncertainty and acoustic modeling</bodyTitle>
        <subsection id="uid91" level="3">
          <bodyTitle>Uncertainty in noise-robust speech and speaker recognition</bodyTitle>
          <p>In many real-world conditions, the target speech signal overlaps with noise and some distortion remains
after speech enhancement.
The framework of uncertainty decoding assumes that this distortion has a Gaussian distribution and seeks
to estimate its covariance matrix in order to exploit it for subsequent feature extraction and decoding. A
number of uncertainty estimators have been proposed in the literature, which are typically based on fixed
mathematical approximations or heuristics. We finalized our work on a principled variational Bayesian
approach to uncertainty estimation and showed its benefit w.r.t. other estimators for speech and speaker
recognition <ref xlink:href="#multispeech-2016-bid38" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. We also pursued our work on the propagation of uncertainty in deep
neural network acoustic models.</p>
        </subsection>
        <subsection id="uid92" level="3">
          <bodyTitle>Uncertainty in other applications</bodyTitle>
          <p>Besides the above applications, we pursued our exploration of uncertainty modeling for robot audition
and wind turbine control. In the first context, uncertainty arises about the location of acoustic
sources and the robot is controlled to locate the sources as quickly as possible <ref xlink:href="#multispeech-2016-bid39" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. In the second context,
uncertainty arises about the noise intensity of each wind turbine and the turbines are controlled to maximize
electrical production under a maximum noise threshold <ref xlink:href="#multispeech-2016-bid40" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
        </subsection>
      </subsection>
      <subsection id="uid93" level="2">
        <bodyTitle>Uncertainty and phonetic segmentation</bodyTitle>
        <subsection id="uid94" level="3">
          <bodyTitle>Speech-text alignment</bodyTitle>
          <p>We have continued our work on determining more accurate phonetic boundaries with two new approaches based on DNN. The first approach proposes to find phonetic boundaries directly from the parameterized speech signal using an LSTM (Long Short-Term Memory) neural network. The aim of the second approach is twofold: provide confidence measures for evaluating speech-text alignment outputs and refine these outputs. One of these studies was done with the Synalp team of LORIA in the framework of the project ORFEO (cf. <ref xlink:href="#uid159" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>). The achieved confidence measure outperforms a confidence score (based on acoustic posterior probability) derived from a state-of-the-art text-to-speech aligner <ref xlink:href="#multispeech-2016-bid41" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
          <p>Within the IFCASL project (cf. <ref xlink:href="#uid167" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>), we have also developed a speech-text alignment system for German which will be integrated into the ASTALI software.</p>
        </subsection>
      </subsection>
      <subsection id="uid95" level="2">
        <bodyTitle>Uncertainty and prosody</bodyTitle>
        <p>The study of discourse particles that was initiated last year, has continued in the framework of the CPER LCHN (cf. <ref xlink:href="#uid113" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>). A larger set of words and expressions that can be used either as normal lexical words or as discourse particles (as for example <i>quoi</i> (what), <i>voilà</i> (there it is), ...) has been considered. For each of these words/expressions and for each speech corpus that was aligned in the ORFEO project (cf. <ref xlink:href="#uid159" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>), a subset of about one hundred occurrences were selected. Thanks to the CPER LCHN support, a part of these occurrences have been annotated as "discourse particle" or "non discourse particle". Detailed analysis is in progress, with respect to the function (discourse particle or not), the type of speech corpus, and the associated prosodic features.</p>
        <p>The fundamental frequency is one of the prosodic features. Numerous approaches exist for the computation of F0. Most of them lead to good performance on good quality speech. The performance degradation with respect to noise level has been studied on reference databases, for several (about ten) F0 detection approaches. It was observed that for each algorithm, a large part of the errors are due to incorrect voiced/unvoiced decision.
Studies have also been initiated for computing a confidence measure on the estimated F0 values through the use of neural network approaches.</p>
      </subsection>
    </subsection>
  </resultats>
  <contrats id="uid96">
    <bodyTitle>Bilateral Contracts and Grants with Industry</bodyTitle>
    <subsection id="uid97" level="1">
      <bodyTitle>Bilateral Contracts with Industry</bodyTitle>
      <subsection id="uid98" level="2">
        <bodyTitle>Venathec</bodyTitle>
        <sanspuceslist>
          <li id="uid99">
            <p noindent="true">Company: <ref xlink:href="http://www.venathec.com/" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">Venathec SAS</ref></p>
          </li>
          <li id="uid100">
            <p noindent="true">Other partners: <ref xlink:href="http://www.acoemgroup.fr/" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">ACOEM Group</ref>, <ref xlink:href="http://www.ge-ip.com/fr/" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">GE Intelligent Platforms</ref> (contracted directly with Venathec)</p>
          </li>
          <li id="uid101">
            <p noindent="true">Duration: June 2014 - August 2017</p>
          </li>
          <li id="uid102">
            <p noindent="true">Supported by: Bpifrance</p>
          </li>
          <li id="uid103">
            <p noindent="true">Abstract: The project aims to design a real-time control system for wind farms that will maximize energy production while limiting sound nuisance. This will leverage our know-how on audio source separation and uncertainty modeling and propagation.</p>
          </li>
        </sanspuceslist>
      </subsection>
    </subsection>
  </contrats>
  <partenariat id="uid104">
    <bodyTitle>Partnerships and Cooperations</bodyTitle>
    <subsection id="uid105" level="1">
      <bodyTitle>Regional Initiatives</bodyTitle>
      <subsection id="uid106" level="2">
        <bodyTitle>CORExp</bodyTitle>
        <sanspuceslist>
          <li id="uid107">
            <p noindent="true">Project acronym: CORExp</p>
          </li>
          <li id="uid108">
            <p noindent="true">Project title: Acquisition, Processing and Analysis of a Corpus for the Synthesis of Expressive Audiovisual Speech</p>
          </li>
          <li id="uid109">
            <p noindent="true">Duration: December 2014 - December 2016</p>
          </li>
          <li id="uid110">
            <p noindent="true">Coordinator: Slim Ouni</p>
          </li>
          <li id="uid111">
            <p noindent="true">Cofunded by Inria and Région Lorraine</p>
          </li>
          <li id="uid112">
            <p noindent="true">Abstract: The main objective of this project was the acquisition of a bimodal corpus of a considerable size (several thousand sentences) to study the expressiveness and emotions during speech (for example, how to decode facial expressions that are merged with speech signals). The main purpose was to acquire, process and analyze the corpus and to study the expressiveness; the results will be used for the expressive audiovisual speech synthesis system.</p>
          </li>
        </sanspuceslist>
      </subsection>
      <subsection id="uid113" level="2">
        <bodyTitle>CPER LCHN</bodyTitle>
        <sanspuceslist>
          <li id="uid114">
            <p noindent="true">Project acronym: CPER LCHN</p>
          </li>
          <li id="uid115">
            <p noindent="true">Project title: CPER "Langues, Connaissances et Humanités Numériques"</p>
          </li>
          <li id="uid116">
            <p noindent="true">Duration: 2015-2020</p>
          </li>
          <li id="uid117">
            <p noindent="true">Coordinator: Bruno Guillaume (LORIA) &amp; Alain Polguère (ATILF)</p>
          </li>
          <li id="uid118">
            <p noindent="true">Abstract: The main goal of the project is related to experimental platforms for supporting research activities in the domain of languages, knowledge and numeric humanities engineering.</p>
          </li>
        </sanspuceslist>
        <p>MULTISPEECH contributes to automatic speech recognition, speech-text alignment and prosody aspects.</p>
      </subsection>
      <subsection id="uid119" level="2">
        <bodyTitle>CPER IT2MP</bodyTitle>
        <sanspuceslist>
          <li id="uid120">
            <p noindent="true">Project acronym: CPER IT2MP</p>
          </li>
          <li id="uid121">
            <p noindent="true">Project title: CPER "Innovation Technologique Modélisation et Médecine Personalisée"</p>
          </li>
          <li id="uid122">
            <p noindent="true">Duration: 2015-2020</p>
          </li>
          <li id="uid123">
            <p noindent="true">Coordinator: Faiez Zannad (Inserm-CHU-UL)</p>
          </li>
          <li id="uid124">
            <p noindent="true">Abstract: The goal of the project is to develop innovative technologies for health, and tools and strategies for personalized medicine.</p>
          </li>
        </sanspuceslist>
        <p>MULTISPEECH will investigate acoustic monitoring using an array of microphones.</p>
      </subsection>
      <subsection id="uid125" level="2">
        <bodyTitle>SATT Dynalips</bodyTitle>
        <sanspuceslist>
          <li id="uid126">
            <p noindent="true">Project title: Control of the movements of the lips in the context of facial animation for an intelligible lipsync.</p>
          </li>
          <li id="uid127">
            <p noindent="true">Duration: May 2016 - December 2017</p>
          </li>
          <li id="uid128">
            <p noindent="true">Coordinator: Slim Ouni</p>
          </li>
          <li id="uid129">
            <p noindent="true">Abstract: We propose in this project the development of tools of lipsync which from recorded speech will provide realistic mechanisms of animating the lips. These tools will be available to be integrated into existing 3D animation software and existing game engines. One objective is that these lipsync tools fit easily into the production pipeline in the field of 3D animation and video games. The goal of this maturation is to propose a product ready to be exploited in the industry whether by the creation of a start-up or by the distribution of licenses.</p>
          </li>
        </sanspuceslist>
      </subsection>
    </subsection>
    <subsection id="uid130" level="1">
      <bodyTitle>National Initiatives</bodyTitle>
      <subsection id="uid131" level="2">
        <bodyTitle>EQUIPEX ORTOLANG</bodyTitle>
        <sanspuceslist>
          <li id="uid132">
            <p noindent="true">Project acronym: ORTOLANG <footnote id="uid133" id-text="2"><ref xlink:href="http://www.ortolang.fr" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>www.<allowbreak/>ortolang.<allowbreak/>fr</ref></footnote></p>
          </li>
          <li id="uid134">
            <p noindent="true">Project title: Open Resources and TOols for LANGuage</p>
          </li>
          <li id="uid135">
            <p noindent="true">Duration: September 2012 - December 2016 (phase I)</p>
          </li>
          <li id="uid136">
            <p noindent="true">Coordinator: Jean-Marie Pierrel, ATILF (Nancy)</p>
          </li>
          <li id="uid137">
            <p noindent="true">Other partners: LPL (Aix en Provence), LORIA (Nancy), Modyco (Paris), LLL (Orléans), INIST (Nancy)</p>
          </li>
          <li id="uid138">
            <p noindent="true">Abstract: The aim of ORTOLANG was to propose a network infrastructure offering a repository of language data (corpora, lexicons, dictionaries, etc.) and tools and their treatment that are readily available and well-documented. This will enable a real mutualization of analysis research, of modeling and automatic treatment of the French language. This will also facilitate the use and transfer of resources and tools set up within public laboratories towards industrial partners, in particular towards SME which often cannot develop such resources and tools for language treatment due to the costs of their realization. Moreover, this will promote the French language and local languages of France by sharing knowledge which has been acquired by public laboratories.</p>
          </li>
        </sanspuceslist>
        <p>Several teams of the LORIA laboratory contribute to this Equipex, mainly with respect to providing tools for speech and language processing. MULTISPEECH contributes with text-speech alignment and speech visualization tools.</p>
      </subsection>
      <subsection id="uid139" level="2">
        <bodyTitle>E-FRAN METAL</bodyTitle>
        <sanspuceslist>
          <li id="uid140">
            <p noindent="true">Project acronym: E-FRAN METAL</p>
          </li>
          <li id="uid141">
            <p noindent="true">Project title: Modèles Et Traces au service de l’Apprentissage des Langues</p>
          </li>
          <li id="uid142">
            <p noindent="true">Duration: October 2016 - September 2020</p>
          </li>
          <li id="uid143">
            <p noindent="true">Coordinator: Anne Boyer (LORIA)</p>
          </li>
          <li id="uid144">
            <p noindent="true">Other partners: Interpsy, LISEC, ESPE de Lorraine, D@NTE (Univ. Versailles Saint Quentin), Sailendra SAS, ITOP Education, Rectorat.</p>
          </li>
          <li id="uid145">
            <p noindent="true">Abstract: METAL aims at improving the learning of languages (both written and oral components) through the development of new tools and the analysis of numeric traces associated with students' learning, in order to adapt to the needs and rythm of each learner.</p>
          </li>
        </sanspuceslist>
        <p>Multispeech is concerned by oral language learning aspects.</p>
      </subsection>
      <subsection id="uid146" level="2">
        <bodyTitle>PIA2 ISITE LUE</bodyTitle>
        <sanspuceslist>
          <li id="uid147">
            <p noindent="true">Project acronym: ISITE LUE</p>
          </li>
          <li id="uid148">
            <p noindent="true">Project title: Lorraine Université d’Excellence</p>
          </li>
          <li id="uid149">
            <p noindent="true">Duration: starting in 2016</p>
          </li>
          <li id="uid150">
            <p noindent="true">Coordinator: Univ. Lorraine</p>
          </li>
          <li id="uid151">
            <p noindent="true">Abstract: The initiative aims at developing and densifying the initial perimeter of excellence, within the scope of the social and economic challenges, so as to build an original model for a leading global engineering university, with a strong emphasis on technological research and education through research. For this, we have designed LUE as an “engine” for the development of excellence, by stimulating an original dialogue between knowledge fields.</p>
          </li>
        </sanspuceslist>
        <p>MULTISPEECH is mainly concerned with challenge number 6: "Knowledge engineering", i.e., engineering applied to the field of knowledge and language, which represent our immaterial wealth while being a critical factor for the consistency of future choices. In 2016, this project has funded a new PhD thesis.</p>
      </subsection>
      <subsection id="uid152" level="2">
        <bodyTitle>ANR ContNomina</bodyTitle>
        <sanspuceslist>
          <li id="uid153">
            <p noindent="true">Project acronym: ContNomina</p>
          </li>
          <li id="uid154">
            <p noindent="true">Project title: Exploitation of context for proper names recognition in diachronic audio documents</p>
          </li>
          <li id="uid155">
            <p noindent="true">Duration: February 2013 - March 2017</p>
          </li>
          <li id="uid156">
            <p noindent="true">Coordinator: Irina Illina</p>
          </li>
          <li id="uid157">
            <p noindent="true">Other partners: LIA, Synalp</p>
          </li>
          <li id="uid158">
            <p noindent="true">Abstract: The ContNomina project focuses on the problem of proper names in automatic audio processing systems by exploiting in the most efficient way the context of the processed documents. To do this, the project addresses the statistical modeling of contexts and of relationships between contexts and proper names; the contextualization of the recognition module (through the dynamic adjustment of the lexicon and of the language model in order to make them more accurate and certainly more relevant in terms of lexical coverage, particularly with respect to proper names); and the detection of proper names (on the one hand, in text documents for building lists of proper names, and on the other hand, in the output of the recognition system to identify spoken proper names in the audio/video data).</p>
          </li>
        </sanspuceslist>
      </subsection>
      <subsection id="uid159" level="2">
        <bodyTitle>ANR ORFEO</bodyTitle>
        <sanspuceslist>
          <li id="uid160">
            <p noindent="true">Project acronym: ORFEO <footnote id="uid161" id-text="3"><ref xlink:href="http://www.agence-nationale-recherche.fr/en/anr-funded-project/?tx_lwmsuivibilan_pi2[CODE]=ANR-12-CORP-0005" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>www.<allowbreak/>agence-nationale-recherche.<allowbreak/>fr/<allowbreak/>en/<allowbreak/>anr-funded-project/<allowbreak/>?tx_lwmsuivibilan_pi2[CODE]=ANR-12-CORP-0005</ref></footnote></p>
          </li>
          <li id="uid162">
            <p noindent="true">Project title: Outils et Ressources pour le Français Écrit et Oral</p>
          </li>
          <li id="uid163">
            <p noindent="true">Duration: February 2013 - February 2016</p>
          </li>
          <li id="uid164">
            <p noindent="true">Coordinator: Jeanne-Marie DEBAISIEUX (Université Paris 3)</p>
          </li>
          <li id="uid165">
            <p noindent="true">Other partners: ATILF, CLLE-ERSS, ICAR, LIF, LORIA, LATTICE, MoDyCo</p>
          </li>
          <li id="uid166">
            <p noindent="true">Abstract: The main objective of the ORFEO project is the constitution of a corpus for the study of contemporary French.</p>
          </li>
        </sanspuceslist>
        <p>In this project, we were concerned by the automatic speech-text alignment at the word and phoneme levels for audio files from several corpora gathered by the project. These corpora orthographically transcribed with Transcriber contain mainly spontaneous speech, recorded under various conditions with a large SNR range and a lot of overlapping speech and anonymised speech segments. For the forced speech-text alignment phase, we applied our 2-step methodology (the first step uses a detailed acoustic model for finding the pronunciation variants; then, in the second step a more compact model is used to provide more temporally accurate boundaries).</p>
      </subsection>
      <subsection id="uid167" level="2">
        <bodyTitle>ANR-DFG IFCASL</bodyTitle>
        <sanspuceslist>
          <li id="uid168">
            <p noindent="true">Project acronym: IFCASL</p>
          </li>
          <li id="uid169">
            <p noindent="true">Project title: Individualized feedback in computer-assisted spoken language learning</p>
          </li>
          <li id="uid170">
            <p noindent="true">Duration: March 2013 - December 2016</p>
          </li>
          <li id="uid171">
            <p noindent="true">Coordinator: Jürgen Trouvain (Saarland University)</p>
          </li>
          <li id="uid172">
            <p noindent="true">Other partners: Saarland University (COLI department)</p>
          </li>
          <li id="uid173">
            <p noindent="true">Abstract: The main objective of IFCASL is to investigate learning of oral French by German speakers, and oral German by French speakers at the phonetic level.</p>
          </li>
        </sanspuceslist>
        <p>A French-German learner corpus was designed and recorded. French speakers were recorded in Nancy, whereas German speakers were recorded in Saarbrücken. An automatic speech-text alignment process was applied on all the data. Then, the French speech data (native and non-native) were manually checked and annotated in France, and the German speech data (native and non-native) were manually checked and annotated in Germany.
The corpora are currently used for analyzing non-native pronunciations, and studying feedback procedures.</p>
      </subsection>
      <subsection id="uid174" level="2">
        <bodyTitle>ANR DYCI2</bodyTitle>
        <sanspuceslist>
          <li id="uid175">
            <p noindent="true">Project acronym: DYCI2 <footnote id="uid176" id-text="4"><ref xlink:href="http://repmus.ircam.fr/dyci2/" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>repmus.<allowbreak/>ircam.<allowbreak/>fr/<allowbreak/>dyci2/</ref></footnote></p>
          </li>
          <li id="uid177">
            <p noindent="true">Project title: Creative Dynamics of Improvised Interaction</p>
          </li>
          <li id="uid178">
            <p noindent="true">Duration: March 2015 - February 2018</p>
          </li>
          <li id="uid179">
            <p noindent="true">Coordinator: Ircam (Paris)</p>
          </li>
          <li id="uid180">
            <p noindent="true">Other partners: Inria (Nancy), University of La Rochelle</p>
          </li>
          <li id="uid181">
            <p noindent="true">Abstract: The goal of this project is to design a music improvisation system which will be able to listen to the other musicians, improvise in their style, and modify its improvisation according to their feedback in real time.</p>
          </li>
        </sanspuceslist>
      </subsection>
      <subsection id="uid182" level="2">
        <bodyTitle>ANR JCJC KAMoulox</bodyTitle>
        <sanspuceslist>
          <li id="uid183">
            <p noindent="true">Project acronym: KAMoulox</p>
          </li>
          <li id="uid184">
            <p noindent="true">Project title: Kernel additive modelling for the unmixing of large audio archives</p>
          </li>
          <li id="uid185">
            <p noindent="true">Duration: January 2016 - January 2019</p>
          </li>
          <li id="uid186">
            <p noindent="true">Coordinator: Antoine Liutkus</p>
          </li>
          <li id="uid187">
            <p noindent="true">Abstract: Develop the theoretical and applied tools required to embed audio denoising and separation tools in web-based audio archives. The applicative scenario is to deal with large audio archives, and more precisely with the notorious "Archives du CNRS — Musée de l'homme", gathering about 50,000 recordings dating back to the early 1900s.</p>
          </li>
        </sanspuceslist>
      </subsection>
      <subsection id="uid188" level="2">
        <bodyTitle>ANR ArtSpeech</bodyTitle>
        <sanspuceslist>
          <li id="uid189">
            <p noindent="true">Project acronym: ArtSpeech</p>
          </li>
          <li id="uid190">
            <p noindent="true">Project title: Synthèse articulatoire phonétique</p>
          </li>
          <li id="uid191">
            <p noindent="true">Duration: October 2015 - March 2019</p>
          </li>
          <li id="uid192">
            <p noindent="true">Coordinator: Yves Laprie</p>
          </li>
          <li id="uid193">
            <p noindent="true">Other partners: Gipsa-Lab (Grenoble), IADI (Nancy), LPP (Paris)</p>
          </li>
          <li id="uid194">
            <p noindent="true">Abstract: The objective is to synthesize speech from text via the numerical simulation of the human speech production processes, i.e. the articulatory, aerodynamic and acoustic aspects.
Corpus based approaches have taken a hegemonic place in text to speech synthesis. They exploit very good acoustic quality speech databases while covering a high number of expressions and of phonetic contexts. This is sufficient to produce intelligible speech. However, these approaches face almost insurmountable obstacles as soon as parameters intimately related to the physical process of speech production have to be modified. On the contrary, an approach which rests on the simulation of the physical speech production process makes explicitly use of source parameters, anatomy and geometry of the vocal tract, and of a temporal supervision strategy. It thus offers direct control on the nature of the synthetic speech.</p>
            <p>Measurements of glottis opening during the production of fricatives via EPGG (ElectroPhotoGlottoGraphy), the design of acoustic experiments with a replica of the vocal tract and the design of dynamic acquisition with MRI were the main activities of this first year.</p>
          </li>
        </sanspuceslist>
      </subsection>
      <subsection id="uid195" level="2">
        <bodyTitle>FUI RAPSODIE</bodyTitle>
        <sanspuceslist>
          <li id="uid196">
            <p noindent="true">Project acronym: RAPSODIE</p>
          </li>
          <li id="uid197">
            <p noindent="true">Project title: Automatic Speech Recognition for Hard of Hearing or Handicapped People</p>
          </li>
          <li id="uid198">
            <p noindent="true">Duration: March 2012 - February 2016</p>
          </li>
          <li id="uid199">
            <p noindent="true">Coordinator: eRocca (Mieussy, Haute-Savoie)</p>
          </li>
          <li id="uid200">
            <p noindent="true">Other partners: CEA (Grenoble), Inria (Nancy), CASTORAMA (France)</p>
          </li>
          <li id="uid201">
            <p noindent="true">Abstract: The goal of the project was to realize a portable device to help a hard-of-hearing person to communicate with other people. To achieve this goal the portable device needs to access a speech recognition system, adapted to this task. Another application of the device is environment vocal control for handicapped persons.</p>
          </li>
        </sanspuceslist>
        <p>In this project, MULTISPEECH was involved in optimizing the speech recognition models for the envisaged task, and in finding the best way of presenting the speech recognition results in order to maximize the communication efficiency between the hard-of-hearing person and the speaking person.</p>
      </subsection>
      <subsection id="uid202" level="2">
        <bodyTitle>FUI VoiceHome</bodyTitle>
        <sanspuceslist>
          <li id="uid203">
            <p noindent="true">Project acronym: VoiceHome</p>
          </li>
          <li id="uid204">
            <p noindent="true">Duration: February 2015 - July 2017</p>
          </li>
          <li id="uid205">
            <p noindent="true">Coordinator: onMobile</p>
          </li>
          <li id="uid206">
            <p noindent="true">Other partners: Orange, Delta Dore, Technicolor Connected Home, eSoftThings, Inria (Nancy), IRISA, LOUSTIC</p>
          </li>
          <li id="uid207">
            <p noindent="true">Abstract: The goal of this project is to design a robust voice control system for smart home and multimedia applications. We are responsible for the robust automatic speech recognition brick.</p>
          </li>
        </sanspuceslist>
      </subsection>
      <subsection id="uid208" level="2">
        <bodyTitle>ADT Plavis</bodyTitle>
        <sanspuceslist>
          <li id="uid209">
            <p noindent="true">Project acronym: Plavis</p>
          </li>
          <li id="uid210">
            <p noindent="true">Project title: Platform for acquisition and audiovisual speech synthesis</p>
          </li>
          <li id="uid211">
            <p noindent="true">Duration: January 2015 - December 2016</p>
          </li>
          <li id="uid212">
            <p noindent="true">Coordinator: Vincent Colotte</p>
          </li>
          <li id="uid213">
            <p noindent="true">Abstract: The objective of this project was to develop a platform acquisition and audiovisual synthesis system (3D animation of the face synchronously with audio). The main purpose was to build a comprehensive platform for acquisition and processing of audiovisual corpus (selection, acquisition and acoustic processing, 3D visual processing and linguistic processing). The acquisition was performed using a motion-capture system (Kinect-like), a Vicon system, and an electromagnetic articulography (EMA) system.</p>
          </li>
        </sanspuceslist>
      </subsection>
      <subsection id="uid214" level="2">
        <bodyTitle>LORIA exploratory project</bodyTitle>
        <sanspuceslist>
          <li id="uid215">
            <p noindent="true">Project title: Acquisition and processing of multimodal corpus in the context of interactive human communication</p>
          </li>
          <li id="uid216">
            <p noindent="true">Duration: June 2015 - May 2016</p>
          </li>
          <li id="uid217">
            <p noindent="true">Coordinator: Slim Ouni</p>
          </li>
          <li id="uid218">
            <p noindent="true">Abstract: The aim of this project was the study of the various mechanisms involved in multimodal human communication that can be oral, visual, gestural and tactile. This project focused on the identification and acquisition of a very large corpus of multimodal data from multiple information sources and acquired in the context of interaction and communication between two people or more.</p>
          </li>
        </sanspuceslist>
      </subsection>
      <subsection id="uid219" level="2">
        <bodyTitle>SYNABE</bodyTitle>
        <sanspuceslist>
          <li id="uid220">
            <p noindent="true">Project acronym: SYNABE</p>
          </li>
          <li id="uid221">
            <p noindent="true">Project title: Articulatory data synchronization for studying stuttering</p>
          </li>
          <li id="uid222">
            <p noindent="true">Duration: January 2016 - December 2016</p>
          </li>
          <li id="uid223">
            <p noindent="true">Coordinator: Fabrice Hirsch (Praxiling, UMR 5267, Montpellier)</p>
          </li>
          <li id="uid224">
            <p noindent="true">Other partners: S. Ouni</p>
          </li>
          <li id="uid225">
            <p noindent="true">Funding: CNRS DEFI Instrumentation aux limites</p>
          </li>
          <li id="uid226">
            <p noindent="true">Abstract: The objective of this project is to use simultaneously three hardware allowing having information on the subglottic (respiratory belt), glottic (electroglottograph) and supraglottic (articulograph) levels during the production of the speech in order to know the timing of the gestures during speech. This system will be used to study the motor coordination between the three levels mentioned in the stuttering and normo-fluent words. We will propose a new typology of normal and pathological disfluencies.</p>
          </li>
        </sanspuceslist>
        <p>Our main contribution concerned the articulatory data acquisition using the articulograph AG501.</p>
      </subsection>
    </subsection>
    <subsection id="uid227" level="1">
      <bodyTitle>European Initiatives</bodyTitle>
      <subsection id="uid228" level="2">
        <bodyTitle>Collaborations with Major European Organizations</bodyTitle>
        <sanspuceslist>
          <li id="uid229">
            <p noindent="true">Jon Barker: University of Sheffield (UK)</p>
          </li>
          <li id="uid230">
            <p noindent="true">Robust speech recognition <ref xlink:href="#multispeech-2016-bid23" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, <ref xlink:href="#multispeech-2016-bid24" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
          </li>
        </sanspuceslist>
      </subsection>
    </subsection>
    <subsection id="uid231" level="1">
      <bodyTitle>International Initiatives</bodyTitle>
      <subsection id="uid232" level="2">
        <bodyTitle>Inria International Partners</bodyTitle>
        <subsection id="uid233" level="3">
          <bodyTitle>Informal International Partners</bodyTitle>
          <sanspuceslist>
            <li id="uid234">
              <p noindent="true">Jonathan Le Roux, Shinji Watanabe, John R. Hershey: Mitsubishi Electric Research Labs (MERL, Boston, USA)</p>
            </li>
            <li id="uid235">
              <p noindent="true">Robust speech recognition <ref xlink:href="#multispeech-2016-bid23" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, <ref xlink:href="#multispeech-2016-bid24" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
            </li>
          </sanspuceslist>
          <sanspuceslist>
            <li id="uid236">
              <p noindent="true">Dayana Ribas Gonzalez, Ramón J. Calvo: CENATAV (Habana, Cuba)</p>
            </li>
            <li id="uid237">
              <p noindent="true">Robust speaker recognition <ref xlink:href="#multispeech-2016-bid25" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
            </li>
          </sanspuceslist>
        </subsection>
      </subsection>
      <subsection id="uid238" level="2">
        <bodyTitle>Participation in Other International Programs</bodyTitle>
        <subsection id="uid239" level="3">
          <bodyTitle>STIC-AmSud - multimodal communication corpus</bodyTitle>
          <sanspuceslist>
            <li id="uid240">
              <p noindent="true">STIC-AmSud: MCC - Multimodal Communication Corpus. A collaboration: Argentina, Chile and France (01/2015-12/2016)</p>
            </li>
            <li id="uid241">
              <p noindent="true">Project acronym: MCC</p>
            </li>
            <li id="uid242">
              <p noindent="true">Project title: Multimodal Communication Corpus</p>
            </li>
            <li id="uid243">
              <p noindent="true">Duration: January 2015 - December 2016</p>
            </li>
            <li id="uid244">
              <p noindent="true">International Coordinator: S. Ouni</p>
            </li>
            <li id="uid245">
              <p noindent="true">National Coordinators: Nancy Hitschfeld (Depto. de Ciencias de la Computación (DCC), Universidad de Chile) - Chile; and, Juan Carlos Gomez (Centro Internacional Franco Argentino de Ciencias de la Información y de Sistemas (CIFASIS), UNR, CONICET) - Argentina</p>
            </li>
            <li id="uid246">
              <p noindent="true">Abstract: The project aims to collect a multimodal speech corpus containing synchronized audio-visual data recorded from talking individuals. The corpus will incorporate several communication modes which appear in the communication among humans, such as the acoustic signal, facial movements and body gestures during speech.
During 2016, a complete corpus of 8 speakers (4 French and 4 Spanish) has been acquired and processed. The corpus will be distributed using the Ortolang platform.</p>
            </li>
          </sanspuceslist>
        </subsection>
        <subsection id="uid247" level="3">
          <bodyTitle>PHC UTIQUE - HMM-based Arabic speech synthesis</bodyTitle>
          <sanspuceslist>
            <li id="uid248">
              <p noindent="true">PHC UTIQUE - HMM-based Arabic speech synthesis, with ENIT (Engineer school at Tunis-Tunisia)</p>
            </li>
            <li id="uid249">
              <p noindent="true">Duration: 2015 - 2018.</p>
            </li>
            <li id="uid250">
              <p noindent="true">Coordinators: Vincent Colotte (France) and Noureddine Ellouze (Tunisia).</p>
            </li>
            <li id="uid251">
              <p noindent="true">Abstract: Development of an HMM-based speech synthesis system for the Arabic language. This includes the development of an Arabic speech corpus, the selection of linguistic features relevant to Arabic HMM-based speech synthesis, as well as improving the quality of the speech signal generated by the system.</p>
            </li>
          </sanspuceslist>
        </subsection>
      </subsection>
    </subsection>
    <subsection id="uid252" level="1">
      <bodyTitle>International Research Visitors</bodyTitle>
      <subsection id="uid253" level="2">
        <bodyTitle>Visits of International Scientists</bodyTitle>
        <sanspuceslist>
          <li id="uid254">
            <p noindent="true">Sebastian Gonzales Mora</p>
            <sanspuceslist>
              <li id="uid255">
                <p noindent="true">Date: Jan 2016</p>
              </li>
              <li id="uid256">
                <p noindent="true">Faculty de Cs. Físicas y Matemáticas, University of Chile</p>
              </li>
            </sanspuceslist>
          </li>
        </sanspuceslist>
        <sanspuceslist>
          <li id="uid257">
            <p noindent="true">Benjamin Martinez Elizalde</p>
            <sanspuceslist>
              <li id="uid258">
                <p noindent="true">Date: May 2016 - Aug 2016</p>
              </li>
              <li id="uid259">
                <p noindent="true">Institution: Carnegie Mellon University (USA)</p>
              </li>
            </sanspuceslist>
          </li>
        </sanspuceslist>
        <sanspuceslist>
          <li id="uid260">
            <p noindent="true">Dayana Ribas Gonzalez</p>
            <sanspuceslist>
              <li id="uid261">
                <p noindent="true">Date: Sep 2016 - Dec 2016</p>
              </li>
              <li id="uid262">
                <p noindent="true">Institution: CENATAV (Cuba)</p>
              </li>
            </sanspuceslist>
          </li>
        </sanspuceslist>
        <sanspuceslist>
          <li id="uid263">
            <p noindent="true">Ziteng Wang</p>
            <sanspuceslist>
              <li id="uid264">
                <p noindent="true">Date: Sep 2016 - Sep 2017</p>
              </li>
              <li id="uid265">
                <p noindent="true">Institution: Institute of Acoustics, Chinese Academy of Sciences (China)</p>
              </li>
            </sanspuceslist>
          </li>
        </sanspuceslist>
      </subsection>
    </subsection>
  </partenariat>
  <diffusion id="uid266">
    <bodyTitle>Dissemination</bodyTitle>
    <subsection id="uid267" level="1">
      <bodyTitle>Promoting Scientific Activities</bodyTitle>
      <subsection id="uid268" level="2">
        <bodyTitle>Scientific Events Organisation</bodyTitle>
        <subsection id="uid269" level="3">
          <bodyTitle>General Chair, Scientific Chair</bodyTitle>
          <sanspuceslist>
            <li id="uid270">
              <p noindent="true">General co-chair, 4th CHiME Speech Separation and Recognition Challenge (E. Vincent)</p>
            </li>
            <li id="uid271">
              <p noindent="true">General co-chair, 4th International Workshop on Speech Processing in Everyday Environments, San Francisco, USA, September 2016 (E. Vincent)</p>
            </li>
            <li id="uid272">
              <p noindent="true">General co-chair, 14th International Conference on Auditory-Visual Speech Processing, Stockholm, Sweden August 2017 (S. Ouni)</p>
            </li>
            <li id="uid273">
              <p noindent="true">Chair, SiSEC 2016, Signal Separation Evaluation Challenge (A. Liutkus)</p>
            </li>
            <li id="uid274">
              <p noindent="true">Elected chair, Steering Committee of the Latent Variable Analysis and Signal Separation (LVA/ICA) conference series (E. Vincent)</p>
            </li>
            <li id="uid275">
              <p noindent="true">Chair, Challenges Subcommittee, IEEE Technical Committee on Audio and Acoustic Signal Processing (E. Vincent)</p>
            </li>
          </sanspuceslist>
        </subsection>
        <subsection id="uid276" level="3">
          <bodyTitle>Member of the Organizing Committees</bodyTitle>
          <sanspuceslist>
            <li id="uid277">
              <p noindent="true">Member of the organizing committee, 2017 IEEE Automatic Speech Recognition and Understanding Workshop, Okinawa, Japan, December 2017 (E. Vincent)</p>
            </li>
            <li id="uid278">
              <p noindent="true">Member of the steering committee, Detection and Classification of Acoustic Scenes and Events (DCASE) challenge series (E. Vincent)</p>
            </li>
          </sanspuceslist>
        </subsection>
        <subsection id="uid279" level="3">
          <bodyTitle>Member of the Conference Program Committees</bodyTitle>
          <sanspuceslist>
            <li id="uid280">
              <p noindent="true">Area chair for Analysis of Speech and Audio Signal, INTERSPEECH'2016 (D. Jouvet)</p>
            </li>
            <li id="uid281">
              <p noindent="true">Area chair for Audio and Speech Source Separation, IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (E. Vincent)</p>
            </li>
          </sanspuceslist>
        </subsection>
        <subsection id="uid282" level="3">
          <bodyTitle>Reviewer</bodyTitle>
          <sanspuceslist>
            <li id="uid283">
              <p noindent="true">CHiME 2016 - Speech Separation and Recognition Challenge (E. Vincent)</p>
            </li>
            <li id="uid284">
              <p noindent="true">EUSIPCO 2016 - European Signal Processing Conference (D. Jouvet)</p>
            </li>
            <li id="uid285">
              <p noindent="true">ICECS 2016 - Conference on Environmental and Computer Science (R. Serizel)</p>
            </li>
            <li id="uid286">
              <p noindent="true">INTERSPEECH 2016 (A. Bonneau, S.Ouni, E. Vincent, I. Illina, Y. Laprie)</p>
            </li>
            <li id="uid287">
              <p noindent="true">IROS 2016 - International Conference on Intelligent Robots and Systems (E. Vincent)</p>
            </li>
            <li id="uid288">
              <p noindent="true">IVA 2016 - International Conference on Intelligent Virtual Agents (S. Ouni)</p>
            </li>
            <li id="uid289">
              <p noindent="true">JEP 2016 - Journées d'Etudes sur la Parole (D. Jouvet, A. Bonneau, E. Vincent, D. Fohr, I. Illina, O. Mella, Y. Laprie)</p>
            </li>
            <li id="uid290">
              <p noindent="true">SLT 2016 - IEEE Spoken Language Technology Workshop (E. Vincent)</p>
            </li>
          </sanspuceslist>
        </subsection>
      </subsection>
      <subsection id="uid291" level="2">
        <bodyTitle>Journal</bodyTitle>
        <subsection id="uid292" level="3">
          <bodyTitle>Member of the Editorial Boards</bodyTitle>
          <sanspuceslist>
            <li id="uid293">
              <p noindent="true">Computer Speech and Language, special issue on Multi-Microphone Speech Recognition in Everyday Environments (E. Vincent)</p>
            </li>
            <li id="uid294">
              <p noindent="true">Speech Communication (D. Jouvet)</p>
            </li>
            <li id="uid295">
              <p noindent="true">Traitement du signal (E. Vincent)</p>
            </li>
            <li id="uid296">
              <p noindent="true">International Journal of Learner Corpus Research, special issue on "Investigating segmental, prosodic and fluency features in spoken learner corpora" (A. Bonneau, Guest Editor)</p>
            </li>
            <li id="uid297">
              <p noindent="true">Speech Communication, special issue on Realism in Robust Speech and Language Processing (E. Vincent)</p>
            </li>
          </sanspuceslist>
        </subsection>
        <subsection id="uid298" level="3">
          <bodyTitle>Reviewer - Reviewing Activities</bodyTitle>
          <sanspuceslist>
            <li id="uid299">
              <p noindent="true">Computer Speech and Language (D. Jouvet, S. Ouni, E. Vincent)</p>
            </li>
            <li id="uid300">
              <p noindent="true">Digital Signal Processing (E. Vincent)</p>
            </li>
            <li id="uid301">
              <p noindent="true">IEEE Transactions on Audio, Speech and Language Processing (A. Liutkus, S. Ouni, R. Serizel)</p>
            </li>
            <li id="uid302">
              <p noindent="true">IEEE Transactions on Signal Processing (A. Liutkus)</p>
            </li>
            <li id="uid303">
              <p noindent="true">IEEE Signal Processing Letters (A. Liutkus, R. Serizel)</p>
            </li>
            <li id="uid304">
              <p noindent="true">Journal of the Acoustical Society of America (Y. Laprie)</p>
            </li>
            <li id="uid305">
              <p noindent="true">JASA Express Letters (Y. Laprie)</p>
            </li>
            <li id="uid306">
              <p noindent="true">Speech Communication (E. Vincent, Y. Laprie)</p>
            </li>
          </sanspuceslist>
        </subsection>
      </subsection>
      <subsection id="uid307" level="2">
        <bodyTitle>Invited Talks</bodyTitle>
        <sanspuceslist>
          <li id="uid308">
            <p noindent="true">Séparation de sources: quand l'acoustique rencontre le machine learning, keynote talk, 13e Congrès Français d'Acoustique (E. Vincent) <ref xlink:href="#multispeech-2016-bid42" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
          </li>
          <li id="uid309">
            <p noindent="true">Speech recognition, Ecole Nationale d’Ingénieurs de Tunis, May 2016 (D. Jouvet)</p>
          </li>
          <li id="uid310">
            <p noindent="true">Les corpus acoustiques et langagiers pour la reconnaissance de la parole, Seminar about Big Data, LIA-LINOS (Laboratoire International Associé), May 2016 (O. Mella)</p>
          </li>
          <li id="uid311">
            <p noindent="true">Parole audiovisuelle : pour faciliter la communication parlée, Praxiling, Université de Montpellier 3, October 2016 (S. Ouni)</p>
          </li>
        </sanspuceslist>
      </subsection>
      <subsection id="uid312" level="2">
        <bodyTitle>Leadership within the Scientific Community</bodyTitle>
        <sanspuceslist>
          <li id="uid313">
            <p noindent="true">Elected chair, ISCA Special Interest Group on Robust Speech Processing (E. Vincent)</p>
          </li>
          <li id="uid314">
            <p noindent="true">Secretary/Treasurer, executive member of AVISA (Auditory-VIsual Speech Association), an ISCA Special Interest Group (S. Ouni)</p>
          </li>
          <li id="uid315">
            <p noindent="true">Member IEEE Technical Committee on Audio and Acoustic Signal Processing (A. Liutkus)</p>
          </li>
        </sanspuceslist>
      </subsection>
      <subsection id="uid316" level="2">
        <bodyTitle>Scientific Expertise</bodyTitle>
        <sanspuceslist>
          <li id="uid317">
            <p noindent="true">Expertise of an ANR project proposal (D. Jouvet, Y. Laprie, E. Vincent)</p>
          </li>
          <li id="uid318">
            <p noindent="true">Expertise of a project for the Research Foundation Flanders – FWO (S. Ouni)</p>
          </li>
        </sanspuceslist>
      </subsection>
      <subsection id="uid319" level="2">
        <bodyTitle>Research Administration</bodyTitle>
        <sanspuceslist>
          <li id="uid320">
            <p noindent="true">Elected Member of the board of the AM2I Scientific Pole - Université de Lorraine (Y. Laprie)</p>
          </li>
          <li id="uid321">
            <p noindent="true">Member of the HCERES visiting committee for LIUM (D. Jouvet)</p>
          </li>
          <li id="uid322">
            <p noindent="true">Chairman of selection committee for the position of Assistant Professor (ESSTIN 0762, May 2016), Y. Laprie.</p>
          </li>
          <li id="uid323">
            <p noindent="true">Member of a selection committee (Université d'Avignon, May 2016), E. Vincent</p>
          </li>
          <li id="uid324">
            <p noindent="true">Member of a selection committee (Université du Maine, May 2016), D. Jouvet</p>
          </li>
          <li id="uid325">
            <p noindent="true">Member of a selection committee (Télécom ParisTech, June 2016), E. Vincent</p>
          </li>
          <li id="uid326">
            <p noindent="true">Member of the "Commission de développement technologique" (A. Bonneau)</p>
          </li>
        </sanspuceslist>
      </subsection>
    </subsection>
    <subsection id="uid327" level="1">
      <bodyTitle>Teaching - Supervision - Juries</bodyTitle>
      <subsection id="uid328" level="2">
        <bodyTitle>Teaching</bodyTitle>
        <sanspuceslist>
          <li id="uid329">
            <p noindent="true">DUT: I. Illina, Programming in Java, 150 hours, L1, University of Lorraine, France</p>
          </li>
          <li id="uid330">
            <p noindent="true">DUT: I. Illina, Linux System, 65 hours, L1, University of Lorraine, France</p>
          </li>
          <li id="uid331">
            <p noindent="true">DUT: I. Illina, Supervision of student projects and stages, 50 hours, L2, University of Lorraine, France</p>
          </li>
          <li id="uid332">
            <p noindent="true">DUT: S. Ouni, Programming in Java, 24 hours, L1, University of Lorraine, France</p>
          </li>
          <li id="uid333">
            <p noindent="true">DUT: S. Ouni, Web Programming, 24 hours, L1, University of Lorraine, France</p>
          </li>
          <li id="uid334">
            <p noindent="true">DUT: S. Ouni, Graphical User Interface, 96 hours, L1, University of Lorraine, France</p>
          </li>
          <li id="uid335">
            <p noindent="true">DUT: S. Ouni, Advanced Algorihms, 24 hours, L2, University of Lorraine, France</p>
          </li>
          <li id="uid336">
            <p noindent="true">DUT: R. Serizel, Introduction to computer tools, 108h, L1, University of Lorraine – IUT Nancy Charlemagne, France</p>
          </li>
          <li id="uid337">
            <p noindent="true">Licence: V. Colotte, C2i - Certificat Informatique et Internet, 50h, L1, University of Lorraine, France</p>
          </li>
          <li id="uid338">
            <p noindent="true">Licence: V. Colotte, System, 115h, L3, University of Lorraine, France</p>
          </li>
          <li id="uid339">
            <p noindent="true">Licence: O. Mella, C2i - Certificat Informatique et Internet, 28h, L1, University of Lorraine, France</p>
          </li>
          <li id="uid340">
            <p noindent="true">Licence: O. Mella, Introduction to Web Programming, 30h, L1, University of Lorraine, France</p>
          </li>
          <li id="uid341">
            <p noindent="true">Licence: O. Mella, Computer Networking, 80h, L2-L3, University of Lorraine, France</p>
          </li>
          <li id="uid342">
            <p noindent="true">Licence: A. Piquard-Kipffer, Education Sciences, 36h, L1, France</p>
          </li>
          <li id="uid343">
            <p noindent="true">Licence: A. Piquard-Kipffer, Reading and Writing, 27h, L2, Département Orthophonie, University of Lorraine, France</p>
          </li>
          <li id="uid344">
            <p noindent="true">Licence: A. Piquard-Kipffer, Psycholinguistics, 6 hours, L2 Département Orthophonie, University Pierre et Marie Curie-Paris, France</p>
          </li>
          <li id="uid345">
            <p noindent="true">Licence: A. Piquard-Kipffer, Reading and Writing assessment, 10h, L3, Département Orthophonie, University of Lorraine, France</p>
          </li>
          <li id="uid346">
            <p noindent="true">Master: V. Colotte, Introduction to Speech Analysis and Recognition, 18h, M1, University of Lorraine, France</p>
          </li>
          <li id="uid347">
            <p noindent="true">Master: Y. Laprie, Analyse, perception et reconnaissance de la parole, 32 hours, M1, University of Lorraine, France</p>
          </li>
          <li id="uid348">
            <p noindent="true">Master: O. Mella, Computer Networking, 74h, M1, University of Lorraine, France</p>
          </li>
          <li id="uid349">
            <p noindent="true">Master: O. Mella, Introduction to Speech Analysis and Recognition, 12h, M1, University of Lorraine, France</p>
          </li>
          <li id="uid350">
            <p noindent="true">Master: S. Ouni, Multimedia in Distributed Information Systems, 31 hours, M2, University of Lorraine, France</p>
          </li>
          <li id="uid351">
            <p noindent="true">Master: A. Piquard-Kipffer, Dyslexia, 25 hours, M1, Département Orthophonie, University of Lorraine, France</p>
          </li>
          <li id="uid352">
            <p noindent="true">Master: A. Piquard-Kipffer, Reading and writing, 6 hours, M1, Département Orthophonie, University Pierre et Marie Curie-Paris, France</p>
          </li>
          <li id="uid353">
            <p noindent="true">Master: A. Piquard-Kipffer, Deaf People and Reading, 15 hours, M2 Département Orthophonie, University of Lorraine, France</p>
          </li>
          <li id="uid354">
            <p noindent="true">Master: A. Piquard-Kipffer, Psychology, 40 hours, M2, ESPE, University of Lorraine, France</p>
          </li>
          <li id="uid355">
            <p noindent="true">Master: A. Piquard-Kipffer, French Language Didactics, 80 hours, M2, ESPE, University of Lorraine, France</p>
          </li>
          <li id="uid356">
            <p noindent="true">Engineer school: V. Colotte, Conception and developpement in XML, 20h, Bac+3, Telecom Nancy, France</p>
          </li>
          <li id="uid357">
            <p noindent="true">Ecole d'audioprothèse : A. Bonneau, Phonetics, 16 h, University of Lorraine</p>
          </li>
          <li id="uid358">
            <p noindent="true">Doctorat: A. Piquard-Kipffer, Language Pathology - speech and language screening, 15 hours, EHESP, University of Sorbonne- Paris Cité, France</p>
          </li>
          <li id="uid359">
            <p noindent="true">Adults: O. Mella, Computer science courses for seconday school teachers (Informatique et Sciences du Numérique courses) (21h), ESPE of Academy Nancy-Metz, University of Lorraine, France</p>
          </li>
          <li id="uid360">
            <p noindent="true">Other: V. Colotte, Responsible for "Certificat Informatique et Internet" for the University of Lorraine, France (50000 students, 30 departments)</p>
          </li>
          <li id="uid361">
            <p noindent="true">Other: S. Ouni, Responsible of Année Spéciale DUT, University of Lorraine, France</p>
          </li>
        </sanspuceslist>
      </subsection>
      <subsection id="uid362" level="2">
        <bodyTitle>Supervision</bodyTitle>
        <sanspuceslist>
          <li id="uid363">
            <p noindent="true">PhD: Imran Sheikh, "Exploiting Semantic and Topic Context to Improve Recognition of Proper Names in Diachronic Audio Documents", November 2016, Irina Illina, Dominique Fohr and Georges Linares.</p>
          </li>
          <li id="uid364">
            <p noindent="true">PhD in progress: Baldwin Dumortier, "Contrôle acoustique d'un parc éolien", September 2014, Emmanuel Vincent and Madalina Deaconu.</p>
          </li>
          <li id="uid365">
            <p noindent="true">PhD in progress: Quan Nguyen, "Mapping of a sound environment by a mobile robot", November 2014, Francis Colas and Emmanuel Vincent.</p>
          </li>
          <li id="uid366">
            <p noindent="true">PhD in progress: Aditya Nugraha, "Deep neural networks for source separation and noise-robust speech recognition", January 2015, Antoine Liutkus and Emmanuel Vincent.</p>
          </li>
          <li id="uid367">
            <p noindent="true">PhD in progress: Ken Deguernel, "Apprentissage de structures musicales en situation d’improvisation", March 2015, Emmanuel Vincent and Gérard Assayag.</p>
          </li>
          <li id="uid368">
            <p noindent="true">PhD in progress: Amal Houidhek, "Élaboration et analyse d’une base de parole arabe pour la synthèse vocale", December 2015, Denis Jouvet and Vincent Colotte (France) and Zied Mnasri (Tunisia).</p>
          </li>
          <li id="uid369">
            <p noindent="true">PhD in progress: Imène Zangar, "Amélioration de la qualité de synthèse vocale par HMM pour la parole arabe", December 2015, Denis Jouvet and Vincent Colotte (France) and Zied Mnasri (Tunisia).</p>
          </li>
          <li id="uid370">
            <p noindent="true">PhD in progress: Mathieu Fontaine, "Processus alpha-stable pour le traitement du signal", May 2016, Antoine Liutkus and Roland Badeau (Télécom ParisTech).</p>
          </li>
          <li id="uid371">
            <p noindent="true">PhD in progress: Amine Menacer, "Traduction automatique de vidéos", May 2016, Kamel Smaïli and Denis Jouvet.</p>
          </li>
          <li id="uid372">
            <p noindent="true">PhD in progress: Anastasiia Tsukanova, "Coarticulation modeling in articulatory synthesis", May 2016, Yves Laprie.</p>
          </li>
          <li id="uid373">
            <p noindent="true">PhD in progress: Nathan Libermann, "Deep learning for musical structure analysis and generation", October 2016, Frédéric Bimbot and Emmanuel Vincent.</p>
          </li>
          <li id="uid374">
            <p noindent="true">PhD in progress: Yang Liu, "Merging acquisition and processing of cineMRI of the vocal tract", October 2016, Pierre-André Vuissoz and Yves Laprie.</p>
          </li>
        </sanspuceslist>
      </subsection>
      <subsection id="uid375" level="2">
        <bodyTitle>Participation in HDR and PhD juries</bodyTitle>
        <sanspuceslist>
          <li id="uid376">
            <p noindent="true">Participation in PhD thesis Jury for David Guennec (Université Rennes 1, September 2016), Y. Laprie.</p>
          </li>
          <li id="uid377">
            <p noindent="true">Participation in PhD thesis Jury for Ugo Marchand (Université Paris 6, November 2016), E. Vincent, reviewer.</p>
          </li>
          <li id="uid378">
            <p noindent="true">Participation in PhD thesis Jury for Joachim Flocon-Cholet (Université Rennes 1, June 2016), E. Vincent, reviewer.</p>
          </li>
          <li id="uid379">
            <p noindent="true">Participation in PhD thesis Jury for Aly Magassouba (Université Rennes 1, December 2016), E. Vincent, reviewer.</p>
          </li>
          <li id="uid380">
            <p noindent="true">Participation in PhD thesis Jury for Diandra Fabre (Université Grenoble Alpes, December 2016), S. Ouni, reviewer.</p>
          </li>
          <li id="uid381">
            <p noindent="true">Participation in PhD thesis Jury for Ivana Didirková (Université Montpellier 3, December 2016), Y. Laprie, reviewer.</p>
          </li>
        </sanspuceslist>
      </subsection>
      <subsection id="uid382" level="2">
        <bodyTitle>Participation in other juries</bodyTitle>
        <sanspuceslist>
          <li id="uid383">
            <p noindent="true">Chairman of Scientific « Baccalauréat », specialty Earth Sciences (Académie de Nancy-Metz and Université de Lorraine, July 2016), A. Piquard-Kipffer.</p>
          </li>
          <li id="uid384">
            <p noindent="true">Participation in the Competitive Entrance Examination into Speech-Language Pathology Departement (University of Lorraine, June 2016), A. Piquard-Kipffer.</p>
          </li>
        </sanspuceslist>
      </subsection>
    </subsection>
    <subsection id="uid385" level="1">
      <bodyTitle>Popularization</bodyTitle>
      <sanspuceslist>
        <li id="uid386">
          <p noindent="true">Demonstration at Village Sciences LORIA, March 2016 (K. Deguernel, E. Vincent, S. Ouni).</p>
        </li>
        <li id="uid387">
          <p noindent="true">Demonstration at Forum des métiers, Collège Peguy, Le Chesnay, March 2016 (A. Piquard-Kipffer).</p>
        </li>
        <li id="uid388">
          <p noindent="true">Demonstration at EHESP-University of Sorbonne- Paris Cité, March 2016 (A. Piquard-Kipffer).</p>
        </li>
        <li id="uid389">
          <p noindent="true">Demonstration at LORIA's 40th Anniversary, June 2016 (K. Deguernel, E. Vincent).</p>
        </li>
        <li id="uid390">
          <p noindent="true">"Démixer la musique", Interstices, January 2016 (A. Liutkus and E. Vincent).</p>
        </li>
        <li id="uid391">
          <p noindent="true">Intervention lors d'une action pour la Maison pour la Science en Lorraine au service des professeurs (A. Bonneau)</p>
        </li>
        <li id="uid392">
          <p noindent="true">Démonstration lors de la journée Rencontre Inria Industrie sur le thème « Ed-Techs au service de e-Education », December 2016 (D. Jouvet)</p>
        </li>
      </sanspuceslist>
    </subsection>
  </diffusion>
  <biblio id="bibliography" html="bibliography" numero="10" titre="Bibliography">
    
    <biblStruct id="multispeech-2016-bid61" type="article" rend="refer" n="refercite:bahja:hal-00831660">
      <identifiant type="doi" value="10.1007/s11760-013-0488-4"/>
      <identifiant type="hal" value="hal-00831660"/>
      <analytic>
        <title level="a">An overview of the CATE algorithms for real-time pitch determination</title>
        <author>
          <persName>
            <foreName>Fadoua</foreName>
            <surname>Bahja</surname>
            <initial>F.</initial>
          </persName>
          <persName key="multispeech-2014-idp71440">
            <foreName>Joseph</foreName>
            <surname>Di Martino</surname>
            <initial>J.</initial>
          </persName>
          <persName>
            <foreName>El Hassan</foreName>
            <surname>Ibn Elhaj</surname>
            <initial>E. H.</initial>
          </persName>
          <persName key="geostat-2014-idp71600">
            <foreName>Driss</foreName>
            <surname>Aboutajdine</surname>
            <initial>D.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">Signal, Image and Video Processing</title>
        <imprint>
          <dateStruct>
            <year>2013</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-00831660" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-00831660</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid63" type="article" rend="refer" n="refercite:barker:hal-00743529">
      <identifiant type="doi" value="10.1016/j.csl.2012.10.004"/>
      <identifiant type="hal" value="hal-00743529"/>
      <analytic>
        <title level="a">The PASCAL CHiME Speech Separation and Recognition Challenge</title>
        <author>
          <persName>
            <foreName>Jon</foreName>
            <surname>Barker</surname>
            <initial>J.</initial>
          </persName>
          <persName key="multispeech-2014-idp68752">
            <foreName>Emmanuel</foreName>
            <surname>Vincent</surname>
            <initial>E.</initial>
          </persName>
          <persName>
            <foreName>Ning</foreName>
            <surname>Ma</surname>
            <initial>N.</initial>
          </persName>
          <persName>
            <foreName>Heidi</foreName>
            <surname>Christensen</surname>
            <initial>H.</initial>
          </persName>
          <persName>
            <foreName>Phil</foreName>
            <surname>Green</surname>
            <initial>P.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">Computer Speech and Language</title>
        <imprint>
          <biblScope type="volume">27</biblScope>
          <biblScope type="number">3</biblScope>
          <dateStruct>
            <month>February</month>
            <year>2013</year>
          </dateStruct>
          <biblScope type="pages">621-633</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-00743529" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-00743529</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid65" type="article" rend="refer" n="refercite:bonneau:hal-00834278">
      <identifiant type="hal" value="hal-00834278"/>
      <analytic>
        <title level="a">Gestion d'erreurs pour la fiabilisation des retours automatiques en apprentissage de la prosodie d'une langue seconde</title>
        <author>
          <persName key="multispeech-2014-idp63592">
            <foreName>Anne</foreName>
            <surname>Bonneau</surname>
            <initial>A.</initial>
          </persName>
          <persName key="multispeech-2014-idp64832">
            <foreName>Dominique</foreName>
            <surname>Fohr</surname>
            <initial>D.</initial>
          </persName>
          <persName key="multispeech-2014-idp72696">
            <foreName>Irina</foreName>
            <surname>Illina</surname>
            <initial>I.</initial>
          </persName>
          <persName key="multispeech-2014-idp62112">
            <foreName>Denis</foreName>
            <surname>Jouvet</surname>
            <initial>D.</initial>
          </persName>
          <persName key="multispeech-2014-idp74144">
            <foreName>Odile</foreName>
            <surname>Mella</surname>
            <initial>O.</initial>
          </persName>
          <persName>
            <foreName>Larbi</foreName>
            <surname>Mesbahi</surname>
            <initial>L.</initial>
          </persName>
          <persName key="multispeech-2014-idp88000">
            <foreName>Luiza</foreName>
            <surname>Orosanu</surname>
            <initial>L.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">Traitement Automatique des Langues</title>
        <imprint>
          <biblScope type="volume">53</biblScope>
          <biblScope type="number">3</biblScope>
          <dateStruct>
            <year>2013</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-00834278" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-00834278</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid64" type="inproceedings" rend="refer" n="refercite:jouvet:hal-00834282">
      <identifiant type="hal" value="hal-00834282"/>
      <analytic>
        <title level="a">Combining Forward-based and Backward-based Decoders for Improved Speech Recognition Performance</title>
        <author>
          <persName key="multispeech-2014-idp62112">
            <foreName>Denis</foreName>
            <surname>Jouvet</surname>
            <initial>D.</initial>
          </persName>
          <persName key="multispeech-2014-idp64832">
            <foreName>Dominique</foreName>
            <surname>Fohr</surname>
            <initial>D.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="m">InterSpeech - 14th Annual Conference of the International Speech Communication Association - 2013</title>
        <loc>Lyon, France</loc>
        <imprint>
          <dateStruct>
            <month>August</month>
            <year>2013</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-00834282" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-00834282</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid62" type="article" rend="refer" n="refercite:ozerov:hal-00717992">
      <identifiant type="doi" value="10.1016/j.csl.2012.07.002"/>
      <identifiant type="hal" value="hal-00717992"/>
      <analytic>
        <title level="a">Uncertainty-based learning of acoustic models from noisy data</title>
        <author>
          <persName>
            <foreName>Alexey</foreName>
            <surname>Ozerov</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Mathieu</foreName>
            <surname>Lagrange</surname>
            <initial>M.</initial>
          </persName>
          <persName key="multispeech-2014-idp68752">
            <foreName>Emmanuel</foreName>
            <surname>Vincent</surname>
            <initial>E.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">Computer Speech and Language</title>
        <imprint>
          <biblScope type="volume">27</biblScope>
          <biblScope type="number">3</biblScope>
          <dateStruct>
            <month>February</month>
            <year>2013</year>
          </dateStruct>
          <biblScope type="pages">874-894</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-00717992" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-00717992</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid1" type="article" rend="refer" n="refercite:ozerov:hal-00626962">
      <identifiant type="hal" value="hal-00626962"/>
      <analytic>
        <title level="a">A General Flexible Framework for the Handling of Prior Information in Audio Source Separation</title>
        <author>
          <persName>
            <foreName>Alexey</foreName>
            <surname>Ozerov</surname>
            <initial>A.</initial>
          </persName>
          <persName key="multispeech-2014-idp68752">
            <foreName>Emmanuel</foreName>
            <surname>Vincent</surname>
            <initial>E.</initial>
          </persName>
          <persName key="panama-2014-idp79976">
            <foreName>Frédéric</foreName>
            <surname>Bimbot</surname>
            <initial>F.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">IEEE Transactions on Audio, Speech and Language Processing</title>
        <imprint>
          <biblScope type="volume">20</biblScope>
          <biblScope type="number">4</biblScope>
          <dateStruct>
            <month>May</month>
            <year>2012</year>
          </dateStruct>
          <biblScope type="pages">1118 - 1133</biblScope>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-00626962" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-00626962</ref>
        </imprint>
      </monogr>
      <note type="bnote">16</note>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid0" type="article" rend="refer" n="refercite:piquardkipffer:hal-00833951">
      <identifiant type="hal" value="hal-00833951"/>
      <analytic>
        <title level="a">Predicting reading level at the end of Grade 2 from skills assessed in kindergarten: contribution of phonemic discrimination (Follow-up of 85 French-speaking children from 4 to 8 years old)</title>
        <author>
          <persName key="multispeech-2014-idp76848">
            <foreName>Agnès</foreName>
            <surname>Piquard-Kipffer</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Liliane</foreName>
            <surname>Sprenger-Charolles</surname>
            <initial>L.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">Topics in Cognitive Psychology</title>
        <imprint>
          <dateStruct>
            <year>2013</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-00833951" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-00833951</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid56" type="phdthesis" rend="year" n="cite:sheikh:tel-01400694">
      <identifiant type="hal" value="tel-01400694"/>
      <monogr>
        <title level="m"> Exploiting Semantic and Topic Context to Improve Recognition of Proper Names in Diachronic Audio Documents</title>
        <author>
          <persName key="multispeech-2014-idp89216">
            <foreName>Imran</foreName>
            <surname>Sheikh</surname>
            <initial>I.</initial>
          </persName>
        </author>
        <imprint>
          <publisher>
            <orgName type="school">Université de Lorraine</orgName>
          </publisher>
          <dateStruct>
            <month>November</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/tel-01400694" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>tel-01400694</ref>
        </imprint>
      </monogr>
      <note type="typdoc">Theses</note>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid38" type="article" rend="year" n="cite:adilolu:hal-00726146">
      <identifiant type="doi" value="10.1109/TASLP.2016.2583794"/>
      <identifiant type="hal" value="hal-00726146"/>
      <analytic>
        <title level="a">Variational Bayesian Inference for Source Separation and Robust Feature Extraction</title>
        <author>
          <persName>
            <foreName>Kamil</foreName>
            <surname>Adiloğlu</surname>
            <initial>K.</initial>
          </persName>
          <persName key="multispeech-2014-idp68752">
            <foreName>Emmanuel</foreName>
            <surname>Vincent</surname>
            <initial>E.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid00706">
        <idno type="issn">1558-7916</idno>
        <title level="j">IEEE Transactions on Audio Speech and Language Processing</title>
        <imprint>
          <dateStruct>
            <month>June</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-00726146" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-00726146</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid43" type="article" rend="year" n="cite:aron:hal-01269578">
      <identifiant type="doi" value="10.1121/1.4940666"/>
      <identifiant type="hal" value="hal-01269578"/>
      <analytic>
        <title level="a">Multimodal acquisition of articulatory data: Geometrical and temporal registration</title>
        <author>
          <persName>
            <foreName>Michaël</foreName>
            <surname>Aron</surname>
            <initial>M.</initial>
          </persName>
          <persName key="magrit-2014-idm29264">
            <foreName>Marie-Odile</foreName>
            <surname>Berger</surname>
            <initial>M.-O.</initial>
          </persName>
          <persName key="magrit-2014-idm27784">
            <foreName>Erwan</foreName>
            <surname>Kerrien</surname>
            <initial>E.</initial>
          </persName>
          <persName key="magrit-2014-idp67840">
            <foreName>Brigitte</foreName>
            <surname>Wrobel-Dautcourt</surname>
            <initial>B.</initial>
          </persName>
          <persName>
            <foreName>Blaise</foreName>
            <surname>Potard</surname>
            <initial>B.</initial>
          </persName>
          <persName key="multispeech-2014-idp66072">
            <foreName>Yves</foreName>
            <surname>Laprie</surname>
            <initial>Y.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid01300">
        <idno type="issn">0001-4966</idno>
        <title level="j">Journal of the Acoustical Society of America</title>
        <imprint>
          <biblScope type="volume">139</biblScope>
          <biblScope type="number">2</biblScope>
          <dateStruct>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">13</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01269578" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01269578</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid24" type="incollection" rend="year" n="cite:barker:hal-01383263">
      <identifiant type="hal" value="hal-01383263"/>
      <analytic>
        <title level="a">The CHiME challenges: Robust speech recognition in everyday environments</title>
        <author>
          <persName>
            <foreName>Jon</foreName>
            <surname>Barker</surname>
            <initial>J.</initial>
          </persName>
          <persName>
            <foreName>Ricard</foreName>
            <surname>Marxer</surname>
            <initial>R.</initial>
          </persName>
          <persName key="multispeech-2014-idp68752">
            <foreName>Emmanuel</foreName>
            <surname>Vincent</surname>
            <initial>E.</initial>
          </persName>
          <persName>
            <foreName>Shinji</foreName>
            <surname>Watanabe</surname>
            <initial>S.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no">
        <title level="m">New era for robust speech recognition - Exploiting deep learning</title>
        <imprint>
          <publisher>
            <orgName>Springer</orgName>
          </publisher>
          <dateStruct>
            <month>October</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01383263" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01383263</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid23" type="article" rend="year" n="cite:barker:hal-01382108">
      <identifiant type="hal" value="hal-01382108"/>
      <analytic>
        <title level="a">The third 'CHIME' speech separation and recognition challenge: Analysis and outcomes</title>
        <author>
          <persName>
            <foreName>Jon</foreName>
            <surname>Barker</surname>
            <initial>J.</initial>
          </persName>
          <persName>
            <foreName>Ricard</foreName>
            <surname>Marxer</surname>
            <initial>R.</initial>
          </persName>
          <persName key="multispeech-2014-idp68752">
            <foreName>Emmanuel</foreName>
            <surname>Vincent</surname>
            <initial>E.</initial>
          </persName>
          <persName>
            <foreName>Shinji</foreName>
            <surname>Watanabe</surname>
            <initial>S.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid00406">
        <idno type="issn">0885-2308</idno>
        <title level="j">Computer Speech and Language</title>
        <imprint>
          <dateStruct>
            <month>October</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01382108" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01382108</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid34" type="article" rend="year" n="cite:bimbot:hal-01188244">
      <identifiant type="hal" value="hal-01188244"/>
      <analytic>
        <title level="a">System &amp; Contrast : A Polymorphous Model of the Inner Organization of Structural Segments within Music Pieces</title>
        <author>
          <persName key="panama-2014-idp79976">
            <foreName>Frédéric</foreName>
            <surname>Bimbot</surname>
            <initial>F.</initial>
          </persName>
          <persName>
            <foreName>Emmanuel</foreName>
            <surname>Deruty</surname>
            <initial>E.</initial>
          </persName>
          <persName key="linkmedia-2016-idp137088">
            <foreName>Gabriel</foreName>
            <surname>Sargent</surname>
            <initial>G.</initial>
          </persName>
          <persName key="multispeech-2014-idp68752">
            <foreName>Emmanuel</foreName>
            <surname>Vincent</surname>
            <initial>E.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid02935">
        <idno type="issn">0730-7829</idno>
        <title level="j">Music Perception</title>
        <imprint>
          <dateStruct>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">41</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01188244" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01188244</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid60" type="incollection" rend="year" n="cite:cadot:hal-01398229">
      <identifiant type="hal" value="hal-01398229"/>
      <analytic>
        <title level="a">Recoder les variables pour obtenir un modèle implicatif optimal</title>
        <author>
          <persName>
            <foreName>Martine</foreName>
            <surname>Cadot</surname>
            <initial>M.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no">
        <editor role="editor">
          <persName>
            <foreName>Régis</foreName>
            <surname>Gras</surname>
            <initial>R.</initial>
          </persName>
        </editor>
        <title level="m">L'Analyse Statisqtique Implicative</title>
        <imprint>
          <publisher>
            <orgName>Cépaduès</orgName>
          </publisher>
          <dateStruct>
            <month>December</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01398229" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01398229</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid58" type="article" rend="year" n="cite:cadot:hal-01346987">
      <identifiant type="hal" value="hal-01346987"/>
      <analytic>
        <title level="a">Extraction d’un modèle articulatoire à partir d’une analyse tri-directionnelle de cinéradiographies d’un locuteur</title>
        <author>
          <persName>
            <foreName>Martine</foreName>
            <surname>Cadot</surname>
            <initial>M.</initial>
          </persName>
          <persName key="multispeech-2014-idp66072">
            <foreName>Yves</foreName>
            <surname>Laprie</surname>
            <initial>Y.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid01716">
        <idno type="issn">1764-1667</idno>
        <title level="j">Revue des Nouvelles Technologies de l'Information</title>
        <imprint>
          <biblScope type="volume">Fouille de Données Complexes</biblScope>
          <biblScope type="number">RNTI-E-31</biblScope>
          <dateStruct>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">73-92</biblScope>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01346987" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01346987</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid4" type="article" rend="year" n="cite:elie:hal-01199792">
      <identifiant type="doi" value="10.1016/j.specom.2016.06.002"/>
      <identifiant type="hal" value="hal-01199792"/>
      <analytic>
        <title level="a">Extension of the single-matrix formulation of the vocal tract: consideration of bilateral channels and connection of self-oscillating models of the vocal folds with a glottal chink</title>
        <author>
          <persName key="multispeech-2014-idp92912">
            <foreName>Benjamin</foreName>
            <surname>Elie</surname>
            <initial>B.</initial>
          </persName>
          <persName key="multispeech-2014-idp66072">
            <foreName>Yves</foreName>
            <surname>Laprie</surname>
            <initial>Y.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid01790">
        <idno type="issn">0167-6393</idno>
        <title level="j">Speech Communication</title>
        <imprint>
          <biblScope type="volume">82</biblScope>
          <dateStruct>
            <month>September</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">85-96</biblScope>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01199792" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01199792</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid21" type="article" rend="year" n="cite:fitzgerald:hal-01260588">
      <identifiant type="hal" value="hal-01260588"/>
      <analytic>
        <title level="a">Projection-based demixing of spatial audio</title>
        <author>
          <persName>
            <foreName>Derry</foreName>
            <surname>Fitzgerald</surname>
            <initial>D.</initial>
          </persName>
          <persName key="multispeech-2014-idp67512">
            <foreName>Antoine</foreName>
            <surname>Liutkus</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Roland</foreName>
            <surname>Badeau</surname>
            <initial>R.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid00706">
        <idno type="issn">1558-7916</idno>
        <title level="j">IEEE Transactions on Audio, Speech and Language Processing</title>
        <imprint>
          <dateStruct>
            <month>May</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01260588" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01260588</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid57" type="article" rend="year" n="cite:gannot:hal-01414179">
      <identifiant type="hal" value="hal-01414179"/>
      <analytic>
        <title level="a">A consolidated perspective on multi-microphone speech enhancement and source separation</title>
        <author>
          <persName key="perception-2014-idp74264">
            <foreName>Sharon</foreName>
            <surname>Gannot</surname>
            <initial>S.</initial>
          </persName>
          <persName key="multispeech-2014-idp68752">
            <foreName>Emmanuel</foreName>
            <surname>Vincent</surname>
            <initial>E.</initial>
          </persName>
          <persName>
            <foreName>Shmulik</foreName>
            <surname>Markovich-Golan</surname>
            <initial>S.</initial>
          </persName>
          <persName>
            <foreName>Alexey</foreName>
            <surname>Ozerov</surname>
            <initial>A.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid00706">
        <idno type="issn">1558-7916</idno>
        <title level="j">IEEE/ACM Transactions on Audio, Speech and Language Processing</title>
        <imprint>
          <dateStruct>
            <month>December</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01414179" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01414179</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid18" type="article" rend="year" n="cite:jaureguiberry:hal-01120685">
      <identifiant type="hal" value="hal-01120685"/>
      <analytic>
        <title level="a">Fusion methods for speech enhancement and audio source separation</title>
        <author>
          <persName key="multispeech-2014-idp86768">
            <foreName>Xabier</foreName>
            <surname>Jaureguiberry</surname>
            <initial>X.</initial>
          </persName>
          <persName key="multispeech-2014-idp68752">
            <foreName>Emmanuel</foreName>
            <surname>Vincent</surname>
            <initial>E.</initial>
          </persName>
          <persName>
            <foreName>Gaël</foreName>
            <surname>Richard</surname>
            <initial>G.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid00706">
        <idno type="issn">1558-7916</idno>
        <title level="j">IEEE Transactions on Audio, Speech and Language Processing</title>
        <imprint>
          <dateStruct>
            <month>April</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01120685" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01120685</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid19" type="article" rend="year" n="cite:liutkus:hal-01350450">
      <identifiant type="hal" value="hal-01350450"/>
      <analytic>
        <title level="a">Démixer la musique</title>
        <author>
          <persName key="multispeech-2014-idp67512">
            <foreName>Antoine</foreName>
            <surname>Liutkus</surname>
            <initial>A.</initial>
          </persName>
          <persName key="multispeech-2014-idp68752">
            <foreName>Emmanuel</foreName>
            <surname>Vincent</surname>
            <initial>E.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="yes" x-editorial-board="yes" x-international-audience="yes" id="rid01010">
        <title level="j">Interstices</title>
        <imprint>
          <dateStruct>
            <month>January</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01350450" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01350450</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid17" type="article" rend="year" n="cite:nugraha:hal-01163369">
      <identifiant type="doi" value="10.1109/TASLP.2016.2580946"/>
      <identifiant type="hal" value="hal-01163369"/>
      <analytic>
        <title level="a">Multichannel audio source separation with deep neural networks</title>
        <author>
          <persName key="multispeech-2015-idp130000">
            <foreName>Aditya Arie</foreName>
            <surname>Nugraha</surname>
            <initial>A. A.</initial>
          </persName>
          <persName key="multispeech-2014-idp67512">
            <foreName>Antoine</foreName>
            <surname>Liutkus</surname>
            <initial>A.</initial>
          </persName>
          <persName key="multispeech-2014-idp68752">
            <foreName>Emmanuel</foreName>
            <surname>Vincent</surname>
            <initial>E.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid00706">
        <idno type="issn">1558-7916</idno>
        <title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
        <imprint>
          <biblScope type="volume">24</biblScope>
          <biblScope type="number">10</biblScope>
          <dateStruct>
            <month>June</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">1652-1664</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01163369" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01163369</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid10" type="article" rend="year" n="cite:ouni:hal-01315579">
      <identifiant type="hal" value="hal-01315579"/>
      <analytic>
        <title level="a">Is markerless acquisition of speech production accurate ?</title>
        <author>
          <persName key="multispeech-2014-idp75400">
            <foreName>Slim</foreName>
            <surname>Ouni</surname>
            <initial>S.</initial>
          </persName>
          <persName key="multispeech-2015-idp120032">
            <foreName>Sara</foreName>
            <surname>Dahmani</surname>
            <initial>S.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid01300">
        <idno type="issn">0001-4966</idno>
        <title level="j">Journal of the Acoustical Society of America</title>
        <imprint>
          <biblScope type="volume">139</biblScope>
          <biblScope type="number">6</biblScope>
          <dateStruct>
            <month>May</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01315579" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01315579</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid51" type="article" rend="year" n="cite:piquardkipffer:hal-01191878">
      <identifiant type="hal" value="hal-01191878"/>
      <analytic>
        <title level="a">Faire voir une histoire : Louis et son incroyable chien Noisette</title>
        <author>
          <persName key="multispeech-2014-idp76848">
            <foreName>Agnès</foreName>
            <surname>Piquard-Kipffer</surname>
            <initial>A.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="yes" x-editorial-board="yes" x-international-audience="yes" id="rid02934">
        <idno type="issn">2268-7874</idno>
        <title level="j">Les Cahiers Pédagogiques</title>
        <imprint>
          <biblScope type="volume">Hors série numérique N°42</biblScope>
          <dateStruct>
            <month>February</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">7</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01191878" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01191878</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid35" type="article" rend="year" n="cite:sargent:hal-01403210">
      <identifiant type="hal" value="hal-01403210"/>
      <analytic>
        <title level="a">Estimating the structural segmentation of popular music pieces under regularity constraints</title>
        <author>
          <persName key="linkmedia-2016-idp137088">
            <foreName>Gabriel</foreName>
            <surname>Sargent</surname>
            <initial>G.</initial>
          </persName>
          <persName key="panama-2014-idp79976">
            <foreName>Frédéric</foreName>
            <surname>Bimbot</surname>
            <initial>F.</initial>
          </persName>
          <persName key="multispeech-2014-idp68752">
            <foreName>Emmanuel</foreName>
            <surname>Vincent</surname>
            <initial>E.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid00706">
        <idno type="issn">1558-7916</idno>
        <title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
        <imprint>
          <dateStruct>
            <year>2017</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01403210" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01403210</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid27" type="article" rend="year" n="cite:vincent:hal-01399180">
      <identifiant type="hal" value="hal-01399180"/>
      <analytic>
        <title level="a">An analysis of environment, microphone and data simulation mismatches in robust speech recognition</title>
        <author>
          <persName key="multispeech-2014-idp68752">
            <foreName>Emmanuel</foreName>
            <surname>Vincent</surname>
            <initial>E.</initial>
          </persName>
          <persName>
            <foreName>Shinji</foreName>
            <surname>Watanabe</surname>
            <initial>S.</initial>
          </persName>
          <persName key="multispeech-2015-idp130000">
            <foreName>Aditya Arie</foreName>
            <surname>Nugraha</surname>
            <initial>A. A.</initial>
          </persName>
          <persName>
            <foreName>Jon</foreName>
            <surname>Barker</surname>
            <initial>J.</initial>
          </persName>
          <persName>
            <foreName>Ricard</foreName>
            <surname>Marxer</surname>
            <initial>R.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid00406">
        <idno type="issn">0885-2308</idno>
        <title level="j">Computer Speech and Language</title>
        <imprint>
          <dateStruct>
            <month>November</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01399180" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01399180</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid11" type="inproceedings" rend="year" n="cite:bartkova:hal-01293516">
      <identifiant type="hal" value="hal-01293516"/>
      <analytic>
        <title level="a">Prosodic Parameters and Prosodic Structures of French Emotional Data</title>
        <author>
          <persName>
            <foreName>Katarina</foreName>
            <surname>Bartkova</surname>
            <initial>K.</initial>
          </persName>
          <persName key="multispeech-2014-idp62112">
            <foreName>Denis</foreName>
            <surname>Jouvet</surname>
            <initial>D.</initial>
          </persName>
          <persName>
            <foreName>Elisabeth</foreName>
            <surname>Delais-Roussarie</surname>
            <initial>E.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">Speech Prosody 2016</title>
        <loc>Boston, United States</loc>
        <title level="s">Speech Prosody 2016</title>
        <imprint>
          <dateStruct>
            <month>May</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01293516" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01293516</ref>
        </imprint>
        <meeting id="cid623717">
          <title>International Conference on Speech Prosody</title>
          <num>2016</num>
          <abbr type="sigle"/>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid26" type="inproceedings" rend="year" n="cite:bertin:hal-01343060">
      <identifiant type="hal" value="hal-01343060"/>
      <analytic>
        <title level="a">A French corpus for distant-microphone speech processing in real homes</title>
        <author>
          <persName key="panama-2014-idm11584">
            <foreName>Nancy</foreName>
            <surname>Bertin</surname>
            <initial>N.</initial>
          </persName>
          <persName key="panama-2014-idp83952">
            <foreName>Ewen</foreName>
            <surname>Camberlein</surname>
            <initial>E.</initial>
          </persName>
          <persName key="multispeech-2014-idp68752">
            <foreName>Emmanuel</foreName>
            <surname>Vincent</surname>
            <initial>E.</initial>
          </persName>
          <persName key="panama-2014-idp86568">
            <foreName>Romain</foreName>
            <surname>Lebarbenchon</surname>
            <initial>R.</initial>
          </persName>
          <persName>
            <foreName>Stéphane</foreName>
            <surname>Peillon</surname>
            <initial>S.</initial>
          </persName>
          <persName>
            <foreName>Éric</foreName>
            <surname>Lamandé</surname>
            <initial>É.</initial>
          </persName>
          <persName key="multispeech-2015-idp123760">
            <foreName>Sunit</foreName>
            <surname>Sivasankaran</surname>
            <initial>S.</initial>
          </persName>
          <persName key="panama-2014-idp79976">
            <foreName>Frédéric</foreName>
            <surname>Bimbot</surname>
            <initial>F.</initial>
          </persName>
          <persName key="multispeech-2014-idp72696">
            <foreName>Irina</foreName>
            <surname>Illina</surname>
            <initial>I.</initial>
          </persName>
          <persName>
            <foreName>Ariane</foreName>
            <surname>Tom</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Sylvain</foreName>
            <surname>Fleury</surname>
            <initial>S.</initial>
          </persName>
          <persName>
            <foreName>Eric</foreName>
            <surname>Jamet</surname>
            <initial>E.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">Interspeech 2016</title>
        <loc>San Francisco, United States</loc>
        <imprint>
          <dateStruct>
            <month>September</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01343060" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01343060</ref>
        </imprint>
        <meeting id="cid29182">
          <title>Annual Conference of the International Speech Communication Association</title>
          <num>17</num>
          <abbr type="sigle">INTERSPEECH</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid44" type="inproceedings" rend="year" n="cite:cadot:hal-01292121">
      <identifiant type="hal" value="hal-01292121"/>
      <analytic>
        <title level="a">Du fichier audio à l’intonation en Français :Graphes pour l’apprentissage de 3 classes intonatives</title>
        <author>
          <persName>
            <foreName>Martine</foreName>
            <surname>Cadot</surname>
            <initial>M.</initial>
          </persName>
          <persName key="multispeech-2014-idp63592">
            <foreName>Anne</foreName>
            <surname>Bonneau</surname>
            <initial>A.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">Fouille de données complexes (FDC@EGC2016)</title>
        <loc>Reims, France</loc>
        <title level="s">Proceedings of FDC@EGC2016</title>
        <imprint>
          <dateStruct>
            <month>January</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01292121" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01292121</ref>
        </imprint>
        <meeting id="cid624430">
          <title>Atelier sur la Fouille de Données Complexes</title>
          <num>13</num>
          <abbr type="sigle"/>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid33" type="inproceedings" rend="year" n="cite:currey:hal-01384365">
      <identifiant type="hal" value="hal-01384365"/>
      <analytic>
        <title level="a">Dynamic adjustment of language models for automatic speech recognition using word similarity </title>
        <author>
          <persName key="multispeech-2016-idp218320">
            <foreName>Anna</foreName>
            <surname>Currey</surname>
            <initial>A.</initial>
          </persName>
          <persName key="multispeech-2014-idp72696">
            <foreName>Irina</foreName>
            <surname>Illina</surname>
            <initial>I.</initial>
          </persName>
          <persName key="multispeech-2014-idp64832">
            <foreName>Dominique</foreName>
            <surname>Fohr</surname>
            <initial>D.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">IEEE Workshop on Spoken Language Technology (SLT 2016)</title>
        <loc>San Diego, CA, United States</loc>
        <title level="s">proceeding of IEEE Workshop on Spoken Language Technology</title>
        <imprint>
          <dateStruct>
            <month>December</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01384365" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01384365</ref>
        </imprint>
        <meeting id="cid391795">
          <title>Spoken Language Technologies Workshop</title>
          <num>2016</num>
          <abbr type="sigle">SLT</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid37" type="inproceedings" rend="year" n="cite:deguernel:hal-01346797">
      <identifiant type="hal" value="hal-01346797"/>
      <analytic>
        <title level="a">Using Multidimensional Sequences For Improvisation In The OMax Paradigm</title>
        <author>
          <persName key="multispeech-2015-idp125016">
            <foreName>Ken</foreName>
            <surname>Déguernel</surname>
            <initial>K.</initial>
          </persName>
          <persName key="multispeech-2014-idp68752">
            <foreName>Emmanuel</foreName>
            <surname>Vincent</surname>
            <initial>E.</initial>
          </persName>
          <persName>
            <foreName>Gérard</foreName>
            <surname>Assayag</surname>
            <initial>G.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">13th Sound and Music Computing Conference</title>
        <loc>Hamburg, Germany</loc>
        <imprint>
          <dateStruct>
            <month>August</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01346797" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01346797</ref>
        </imprint>
        <meeting id="cid623605">
          <title>Sound and Music Computing</title>
          <num>13</num>
          <abbr type="sigle">SMC</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid52" type="inproceedings" rend="year" n="cite:elie:hal-01372313">
      <identifiant type="hal" value="hal-01372313"/>
      <analytic>
        <title level="a">Robust tonal and noise separation in presence of colored noise, and application to voiced fricatives</title>
        <author>
          <persName key="multispeech-2014-idp92912">
            <foreName>Benjamin</foreName>
            <surname>Elie</surname>
            <initial>B.</initial>
          </persName>
          <persName>
            <foreName>Gilles</foreName>
            <surname>Chardon</surname>
            <initial>G.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="no">
        <title level="m">22nd International Congress on Acoustics (ICA)</title>
        <loc>Buenos Aires, Argentina</loc>
        <imprint>
          <dateStruct>
            <month>September</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01372313" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01372313</ref>
        </imprint>
        <meeting id="cid305749">
          <title>International Congress on Acoustics</title>
          <num>22</num>
          <abbr type="sigle">ICA</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid5" type="inproceedings" rend="year" n="cite:elie:hal-01314308">
      <identifiant type="hal" value="hal-01314308"/>
      <analytic>
        <title level="a">A glottal chink model for the synthesis of voiced fricatives</title>
        <author>
          <persName key="multispeech-2014-idp92912">
            <foreName>Benjamin</foreName>
            <surname>Elie</surname>
            <initial>B.</initial>
          </persName>
          <persName key="multispeech-2014-idp66072">
            <foreName>Yves</foreName>
            <surname>Laprie</surname>
            <initial>Y.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
        <loc>Shanghai, China</loc>
        <imprint>
          <publisher>
            <orgName type="organisation">IEEE</orgName>
          </publisher>
          <dateStruct>
            <month>March</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01314308" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01314308</ref>
        </imprint>
        <meeting id="cid80145">
          <title>IEEE International Conference on Acoustics, Speech and Signal Processing</title>
          <num>2011</num>
          <abbr type="sigle">ICASSP</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid6" type="inproceedings" rend="year" n="cite:elie:hal-01278462">
      <identifiant type="hal" value="hal-01278462"/>
      <analytic>
        <title level="a">Copy synthesis of phrase-level utterances</title>
        <author>
          <persName key="multispeech-2014-idp92912">
            <foreName>Benjamin</foreName>
            <surname>Elie</surname>
            <initial>B.</initial>
          </persName>
          <persName key="multispeech-2014-idp66072">
            <foreName>Yves</foreName>
            <surname>Laprie</surname>
            <initial>Y.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">EUSIPCO2016</title>
        <loc>Budapest, Hungary</loc>
        <imprint>
          <dateStruct>
            <month>August</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01278462" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01278462</ref>
        </imprint>
        <meeting id="cid70310">
          <title>European Signal Processing Conference</title>
          <num>24</num>
          <abbr type="sigle">EUSIPCO</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid7" type="inproceedings" rend="year" n="cite:elie:hal-01372310">
      <identifiant type="hal" value="hal-01372310"/>
      <analytic>
        <title level="a">Copy synthesis of running speech based on vocal tract imaging and audio recording</title>
        <author>
          <persName key="multispeech-2014-idp92912">
            <foreName>Benjamin</foreName>
            <surname>Elie</surname>
            <initial>B.</initial>
          </persName>
          <persName key="multispeech-2014-idp66072">
            <foreName>Yves</foreName>
            <surname>Laprie</surname>
            <initial>Y.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="no">
        <title level="m">22nd International Congress on Acoustics (ICA)</title>
        <loc>Buenos Aires, Argentina</loc>
        <imprint>
          <dateStruct>
            <month>September</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01372310" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01372310</ref>
        </imprint>
        <meeting id="cid305749">
          <title>International Congress on Acoustics</title>
          <num>22</num>
          <abbr type="sigle">ICA</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid8" type="inproceedings" rend="year" n="cite:elie:hal-01314313">
      <identifiant type="hal" value="hal-01314313"/>
      <analytic>
        <title level="a">Acquisition temps-réel de données articulatoires par IRM : application à la synthèse par copie</title>
        <author>
          <persName key="multispeech-2014-idp92912">
            <foreName>Benjamin</foreName>
            <surname>Elie</surname>
            <initial>B.</initial>
          </persName>
          <persName key="multispeech-2014-idp66072">
            <foreName>Yves</foreName>
            <surname>Laprie</surname>
            <initial>Y.</initial>
          </persName>
          <persName>
            <foreName>Pierre-André</foreName>
            <surname>Vuissoz</surname>
            <initial>P.-A.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="no" x-proceedings="yes" x-invited-conference="no" x-editorial-board="no">
        <title level="m">13ème Congrès Français d'Acoustique (CFA 2016)</title>
        <loc>Le Mans, France</loc>
        <imprint>
          <publisher>
            <orgName type="organisation">SFA</orgName>
          </publisher>
          <dateStruct>
            <month>April</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01314313" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01314313</ref>
        </imprint>
        <meeting id="cid394719">
          <title>Congrès Français d'Acoustique</title>
          <num>13</num>
          <abbr type="sigle">CFA</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid9" type="inproceedings" rend="year" n="cite:elie:hal-01372320">
      <identifiant type="hal" value="hal-01372320"/>
      <analytic>
        <title level="a">High spatiotemporal cineMRI films using compressed sensing for acquiring articulatory data</title>
        <author>
          <persName key="multispeech-2014-idp92912">
            <foreName>Benjamin</foreName>
            <surname>Elie</surname>
            <initial>B.</initial>
          </persName>
          <persName key="multispeech-2014-idp66072">
            <foreName>Yves</foreName>
            <surname>Laprie</surname>
            <initial>Y.</initial>
          </persName>
          <persName>
            <foreName>Pierre-André</foreName>
            <surname>Vuissoz</surname>
            <initial>P.-A.</initial>
          </persName>
          <persName>
            <foreName>Freddy</foreName>
            <surname>Odille</surname>
            <initial>F.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">EUSIPCO2016</title>
        <loc>Budapest, Hungary</loc>
        <imprint>
          <dateStruct>
            <month>August</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01372320" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01372320</ref>
        </imprint>
        <meeting id="cid70310">
          <title>European Signal Processing Conference</title>
          <num>24</num>
          <abbr type="sigle">EUSIPCO</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid28" type="inproceedings" rend="year" n="cite:elizalde:hal-01354007">
      <identifiant type="hal" value="hal-01354007"/>
      <analytic>
        <title level="a">Experiments on the DCASE Challenge 2016: Acoustic scene classification and sound event detection in real life recording</title>
        <author>
          <persName>
            <foreName>Benjamin</foreName>
            <surname>Elizalde</surname>
            <initial>B.</initial>
          </persName>
          <persName>
            <foreName>Anurag</foreName>
            <surname>Kumar</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Ankit</foreName>
            <surname>Shah</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Rohan</foreName>
            <surname>Badlani</surname>
            <initial>R.</initial>
          </persName>
          <persName key="multispeech-2014-idp68752">
            <foreName>Emmanuel</foreName>
            <surname>Vincent</surname>
            <initial>E.</initial>
          </persName>
          <persName>
            <foreName>Bhiksha</foreName>
            <surname>Raj</surname>
            <initial>B.</initial>
          </persName>
          <persName>
            <foreName>Ian</foreName>
            <surname>Lane</surname>
            <initial>I.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">DCASE2016 Workshop on Detection and Classification of Acoustic Scenes and Events</title>
        <loc>Budapest, Hungary</loc>
        <imprint>
          <dateStruct>
            <month>September</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01354007" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01354007</ref>
        </imprint>
        <meeting id="cid625544">
          <title>Workshop on Detection and Classification of Acoustic Scenes and Events</title>
          <num>2016</num>
          <abbr type="sigle">DCASE</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid22" type="inproceedings" rend="year" n="cite:fitzgerald:hal-01248014">
      <identifiant type="hal" value="hal-01248014"/>
      <analytic>
        <title level="a">PROJET - Spatial Audio Separation Using Projections</title>
        <author>
          <persName>
            <foreName>Derry</foreName>
            <surname>Fitzgerald</surname>
            <initial>D.</initial>
          </persName>
          <persName key="multispeech-2014-idp67512">
            <foreName>Antoine</foreName>
            <surname>Liutkus</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Roland</foreName>
            <surname>Badeau</surname>
            <initial>R.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">41st International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
        <loc>Shanghai, China</loc>
        <imprint>
          <publisher>
            <orgName>IEEE</orgName>
          </publisher>
          <dateStruct>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01248014" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01248014</ref>
        </imprint>
        <meeting id="cid80145">
          <title>IEEE International Conference on Acoustics, Speech and Signal Processing</title>
          <num>41</num>
          <abbr type="sigle">ICASSP</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid48" type="inproceedings" rend="year" n="cite:fontaine:hal-01401988">
      <identifiant type="hal" value="hal-01401988"/>
      <analytic>
        <title level="a">Sketching for nearfield acoustic imaging of heavy-tailed sources</title>
        <author>
          <persName key="multispeech-2016-idp161472">
            <foreName>Mathieu</foreName>
            <surname>Fontaine</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>Charles</foreName>
            <surname>Vanwynsberghe</surname>
            <initial>C.</initial>
          </persName>
          <persName key="multispeech-2014-idp67512">
            <foreName>Antoine</foreName>
            <surname>Liutkus</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Roland</foreName>
            <surname>Badeau</surname>
            <initial>R.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">13th International Conference on Latent Variable Analysis and Signal Separation (LVA/ICA 2017)</title>
        <loc>Grenoble, France</loc>
        <title level="s">Proc. 13th International Conference on Latent Variable Analysis and Signal Separation (LVA/ICA 2017)</title>
        <imprint>
          <dateStruct>
            <month>February</month>
            <year>2017</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01401988" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01401988</ref>
        </imprint>
        <meeting id="cid402484">
          <title>International Conference on Latent Variable Analysis and Signal Separation</title>
          <num>13</num>
          <abbr type="sigle"/>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid16" type="inproceedings" rend="year" n="cite:ghosh:hal-01397176">
      <identifiant type="doi" value="10.21437/Interspeech.2016-954"/>
      <identifiant type="hal" value="hal-01397176"/>
      <analytic>
        <title level="a">L1-L2 Interference: The case of final devoicing of French voiced fricatives in final position by German learners</title>
        <author>
          <persName key="multispeech-2015-idp141192">
            <foreName>Sucheta</foreName>
            <surname>Ghosh</surname>
            <initial>S.</initial>
          </persName>
          <persName key="multispeech-2014-idp94144">
            <foreName>Camille</foreName>
            <surname>Fauth</surname>
            <initial>C.</initial>
          </persName>
          <persName key="multispeech-2014-idp83080">
            <foreName>Aghilas</foreName>
            <surname>Sini</surname>
            <initial>A.</initial>
          </persName>
          <persName key="multispeech-2014-idp66072">
            <foreName>Yves</foreName>
            <surname>Laprie</surname>
            <initial>Y.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">Interspeech 2016</title>
        <loc>San Francisco, United States</loc>
        <imprint>
          <biblScope type="volume">2016</biblScope>
          <dateStruct>
            <month>September</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">3156 - 3160</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01397176" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01397176</ref>
        </imprint>
        <meeting id="cid29182">
          <title>Annual Conference of the International Speech Communication Association</title>
          <num>17</num>
          <abbr type="sigle">INTERSPEECH</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid54" type="inproceedings" rend="year" n="cite:leglaive:hal-01416366">
      <identifiant type="hal" value="hal-01416366"/>
      <analytic>
        <title level="a">Alpha-Stable Multichannel Audio Source Separation</title>
        <author>
          <persName>
            <foreName>Simon</foreName>
            <surname>Leglaive</surname>
            <initial>S.</initial>
          </persName>
          <persName>
            <foreName>Umut</foreName>
            <surname>Simsekli</surname>
            <initial>U.</initial>
          </persName>
          <persName key="multispeech-2014-idp67512">
            <foreName>Antoine</foreName>
            <surname>Liutkus</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Roland</foreName>
            <surname>Badeau</surname>
            <initial>R.</initial>
          </persName>
          <persName>
            <foreName>Gaël</foreName>
            <surname>Richard</surname>
            <initial>G.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">42nd International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
        <loc>New Orleans, United States</loc>
        <title level="s">Proc. 42nd International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
        <imprint>
          <publisher>
            <orgName type="organisation">IEEE</orgName>
          </publisher>
          <dateStruct>
            <month>March</month>
            <year>2017</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01416366" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01416366</ref>
        </imprint>
        <meeting id="cid80145">
          <title>IEEE International Conference on Acoustics, Speech and Signal Processing</title>
          <num>42</num>
          <abbr type="sigle">ICASSP</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid39" type="inproceedings" rend="year" n="cite:nguyen:hal-01354006">
      <identifiant type="hal" value="hal-01354006"/>
      <analytic>
        <title level="a">Localizing an Intermittent and Moving Sound Source Using a Mobile Robot</title>
        <author>
          <persName key="maia-2014-idp141704">
            <foreName>Van Quan</foreName>
            <surname>Nguyen</surname>
            <initial>V. Q.</initial>
          </persName>
          <persName key="maia-2014-idp79752">
            <foreName>Francis</foreName>
            <surname>Colas</surname>
            <initial>F.</initial>
          </persName>
          <persName key="multispeech-2014-idp68752">
            <foreName>Emmanuel</foreName>
            <surname>Vincent</surname>
            <initial>E.</initial>
          </persName>
          <persName key="maia-2014-idp77024">
            <foreName>François</foreName>
            <surname>Charpillet</surname>
            <initial>F.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">International Conference on Intelligent Robots and Systems (IROS)</title>
        <loc>Deajeon, South Korea</loc>
        <imprint>
          <dateStruct>
            <month>October</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01354006" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01354006</ref>
        </imprint>
        <meeting id="cid93437">
          <title>IEEE RSJ International Conference on Intelligent Robots and Systems</title>
          <num>2010</num>
          <abbr type="sigle">IROS</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid2" type="inproceedings" rend="year" n="cite:nugraha:hal-01334614">
      <identifiant type="hal" value="hal-01334614"/>
      <analytic>
        <title level="a">Multichannel music separation with deep neural networks</title>
        <author>
          <persName key="multispeech-2015-idp130000">
            <foreName>Aditya Arie</foreName>
            <surname>Nugraha</surname>
            <initial>A. A.</initial>
          </persName>
          <persName key="multispeech-2014-idp67512">
            <foreName>Antoine</foreName>
            <surname>Liutkus</surname>
            <initial>A.</initial>
          </persName>
          <persName key="multispeech-2014-idp68752">
            <foreName>Emmanuel</foreName>
            <surname>Vincent</surname>
            <initial>E.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">European Signal Processing Conference (EUSIPCO)</title>
        <loc>Budapest, Hungary</loc>
        <title level="s">Proceedings of the 24th European Signal Processing Conference (EUSIPCO)</title>
        <imprint>
          <dateStruct>
            <month>August</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">1748-1752</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01334614" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01334614</ref>
        </imprint>
        <meeting id="cid70310">
          <title>European Signal Processing Conference</title>
          <num>18</num>
          <abbr type="sigle">EUSIPCO</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid3" type="inproceedings" rend="year" n="cite:ouni:hal-01398528">
      <identifiant type="doi" value="10.21437/Interspeech.2016-730"/>
      <identifiant type="hal" value="hal-01398528"/>
      <analytic>
        <title level="a">Acoustic and Visual Analysis of Expressive Speech: A Case Study of French Acted Speech</title>
        <author>
          <persName key="multispeech-2014-idp75400">
            <foreName>Slim</foreName>
            <surname>Ouni</surname>
            <initial>S.</initial>
          </persName>
          <persName key="multispeech-2014-idp70184">
            <foreName>Vincent</foreName>
            <surname>Colotte</surname>
            <initial>V.</initial>
          </persName>
          <persName key="multispeech-2015-idp120032">
            <foreName>Sara</foreName>
            <surname>Dahmani</surname>
            <initial>S.</initial>
          </persName>
          <persName key="multispeech-2015-idp152528">
            <foreName>Soumaya</foreName>
            <surname>Azzi</surname>
            <initial>S.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">Interspeech 2016</title>
        <loc>San Francisco, United States</loc>
        <imprint>
          <biblScope type="volume">2016</biblScope>
          <publisher>
            <orgName type="organisation">ISCA</orgName>
          </publisher>
          <dateStruct>
            <month>November</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">580 - 584</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01398528" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01398528</ref>
        </imprint>
        <meeting id="cid29182">
          <title>Annual Conference of the International Speech Communication Association</title>
          <num>17</num>
          <abbr type="sigle">INTERSPEECH</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid13" type="inproceedings" rend="year" n="cite:piquardkipffer:hal-01403204">
      <identifiant type="hal" value="hal-01403204"/>
      <analytic>
        <title level="a">Storytelling with a digital album that use an avatar as narrator</title>
        <author>
          <persName key="multispeech-2014-idp76848">
            <foreName>Agnès</foreName>
            <surname>Piquard-Kipffer</surname>
            <initial>A.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">XVIèmes rencontres internationales en orthophonie - Orthophonie et technologies innovantes</title>
        <loc>PARIS, France</loc>
        <title level="s">XVIèmes rencontres internationales en orthophonie - Orthophonie et technologies innovantes</title>
        <imprint>
          <dateStruct>
            <month>December</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01403204" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01403204</ref>
        </imprint>
        <meeting id="cid625549">
          <title>Rencontres Internationales d’Orthophonie</title>
          <num>16</num>
          <abbr type="sigle"/>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid25" type="inproceedings" rend="year" n="cite:ribas:hal-01377638">
      <identifiant type="hal" value="hal-01377638"/>
      <analytic>
        <title level="a">A study of speech distortion conditions in real scenarios for speech processing applications</title>
        <author>
          <persName key="multispeech-2014-idp99128">
            <foreName>Dayana</foreName>
            <surname>Ribas</surname>
            <initial>D.</initial>
          </persName>
          <persName key="multispeech-2014-idp68752">
            <foreName>Emmanuel</foreName>
            <surname>Vincent</surname>
            <initial>E.</initial>
          </persName>
          <persName>
            <foreName>José Ramón</foreName>
            <surname>Calvo</surname>
            <initial>J. R.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">2016 IEEE Workshop on Spoken Language Technology</title>
        <loc>San Diego, United States</loc>
        <imprint>
          <dateStruct>
            <month>December</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01377638" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01377638</ref>
        </imprint>
        <meeting id="cid391795">
          <title>Spoken Language Technologies Workshop</title>
          <num>2016</num>
          <abbr type="sigle">SLT</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid41" type="inproceedings" rend="year" n="cite:serriere:hal-01378355">
      <identifiant type="hal" value="hal-01378355"/>
      <analytic>
        <title level="a">Weakly-supervised text-to-speech alignment confidence measure</title>
        <author>
          <persName key="neurosys-2014-idp68256">
            <foreName>Guillaume</foreName>
            <surname>Serrière</surname>
            <initial>G.</initial>
          </persName>
          <persName>
            <foreName>Christophe</foreName>
            <surname>Cerisara</surname>
            <initial>C.</initial>
          </persName>
          <persName key="multispeech-2014-idp64832">
            <foreName>Dominique</foreName>
            <surname>Fohr</surname>
            <initial>D.</initial>
          </persName>
          <persName key="multispeech-2014-idp74144">
            <foreName>Odile</foreName>
            <surname>Mella</surname>
            <initial>O.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">International Conference on Computational Linguistics (COLING)</title>
        <loc>Osaka, Japan</loc>
        <title level="s">Proceedings of the 26th International Conference on Computational Linguistics (COLING)</title>
        <imprint>
          <dateStruct>
            <month>December</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01378355" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01378355</ref>
        </imprint>
        <meeting id="cid115519">
          <title>International Conference on Computational Linguistics</title>
          <num>25</num>
          <abbr type="sigle">COLING</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid30" type="inproceedings" rend="year" n="cite:sheikh:hal-01331716">
      <identifiant type="doi" value="10.1109/ICASSP.2016.7472839"/>
      <identifiant type="hal" value="hal-01331716"/>
      <analytic>
        <title level="a">Document Level Semantic Context for Retrieving OOV Proper Names</title>
        <author>
          <persName key="multispeech-2014-idp89216">
            <foreName>Imran</foreName>
            <surname>Sheikh</surname>
            <initial>I.</initial>
          </persName>
          <persName key="multispeech-2014-idp72696">
            <foreName>Irina</foreName>
            <surname>Illina</surname>
            <initial>I.</initial>
          </persName>
          <persName key="multispeech-2014-idp64832">
            <foreName>Dominique</foreName>
            <surname>Fohr</surname>
            <initial>D.</initial>
          </persName>
          <persName>
            <foreName>Georges</foreName>
            <surname>Linares</surname>
            <initial>G.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
        <loc>shanghai, China</loc>
        <title level="s">Proceeding of IEEE ICASSP 2016</title>
        <imprint>
          <publisher>
            <orgName>IEEE</orgName>
          </publisher>
          <dateStruct>
            <month>March</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">6050-6054</biblScope>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01331716" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01331716</ref>
        </imprint>
        <meeting id="cid80145">
          <title>IEEE International Conference on Acoustics, Speech and Signal Processing</title>
          <num>41</num>
          <abbr type="sigle">ICASSP</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid32" type="inproceedings" rend="year" n="cite:sheikh:hal-01384488">
      <identifiant type="doi" value="10.21437/Interspeech.2016-1219"/>
      <identifiant type="hal" value="hal-01384488"/>
      <analytic>
        <title level="a">Improved Neural Bag-of-Words Model to Retrieve Out-of-Vocabulary Words in Speech Recognition</title>
        <author>
          <persName key="multispeech-2014-idp89216">
            <foreName>Imran</foreName>
            <surname>Sheikh</surname>
            <initial>I.</initial>
          </persName>
          <persName key="multispeech-2014-idp72696">
            <foreName>Irina</foreName>
            <surname>Illina</surname>
            <initial>I.</initial>
          </persName>
          <persName key="multispeech-2014-idp64832">
            <foreName>Dominique</foreName>
            <surname>Fohr</surname>
            <initial>D.</initial>
          </persName>
          <persName>
            <foreName>Georges</foreName>
            <surname>Linares</surname>
            <initial>G.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">INTERSPEECH 2016</title>
        <loc>San Francisco, United States</loc>
        <title level="s">Proceedings of INTERSPEECH 2016</title>
        <imprint>
          <biblScope type="volume">2016</biblScope>
          <dateStruct>
            <month>September</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01384488" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01384488</ref>
        </imprint>
        <meeting id="cid29182">
          <title>Annual Conference of the International Speech Communication Association</title>
          <num>17</num>
          <abbr type="sigle">INTERSPEECH</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid31" type="inproceedings" rend="year" n="cite:sheikh:hal-01331720">
      <identifiant type="hal" value="hal-01331720"/>
      <analytic>
        <title level="a">Learning Word Importance with the Neural Bag-of-Words Model</title>
        <author>
          <persName key="multispeech-2014-idp89216">
            <foreName>Imran</foreName>
            <surname>Sheikh</surname>
            <initial>I.</initial>
          </persName>
          <persName key="multispeech-2014-idp72696">
            <foreName>Irina</foreName>
            <surname>Illina</surname>
            <initial>I.</initial>
          </persName>
          <persName key="multispeech-2014-idp64832">
            <foreName>Dominique</foreName>
            <surname>Fohr</surname>
            <initial>D.</initial>
          </persName>
          <persName>
            <foreName>Georges</foreName>
            <surname>Linares</surname>
            <initial>G.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">ACL, Representation Learning for NLP (Repl4NLP) workshop</title>
        <loc>Berlin, Germany</loc>
        <title level="s">Proceedings of ACL 2016</title>
        <imprint>
          <dateStruct>
            <month>August</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01331720" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01331720</ref>
        </imprint>
        <meeting id="cid625540">
          <title>Workshop on Representation Learning for NLP</title>
          <num>1</num>
          <abbr type="sigle">RepL4NLP</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid29" type="inproceedings" rend="year" n="cite:sheikh:hal-01331714">
      <identifiant type="hal" value="hal-01331714"/>
      <analytic>
        <title level="a">How Diachronic Text Corpora Affect Context based Retrieval of OOV Proper Names for Audio News</title>
        <author>
          <persName key="multispeech-2014-idp89216">
            <foreName>Imran</foreName>
            <surname>Sheikh</surname>
            <initial>I.</initial>
          </persName>
          <persName key="multispeech-2014-idp72696">
            <foreName>Irina</foreName>
            <surname>Illina</surname>
            <initial>I.</initial>
          </persName>
          <persName key="multispeech-2014-idp64832">
            <foreName>Dominique</foreName>
            <surname>Fohr</surname>
            <initial>D.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">LREC 2016</title>
        <loc>Portoroz, Slovenia</loc>
        <title level="s">proceedings of LREC 2016</title>
        <imprint>
          <dateStruct>
            <month>May</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01331714" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01331714</ref>
        </imprint>
        <meeting id="cid289398">
          <title>International Conference on Language Resources and Evaluation</title>
          <num>10</num>
          <abbr type="sigle">LREC</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid53" type="inproceedings" rend="year" n="cite:simpson:hal-01410176">
      <identifiant type="hal" value="hal-01410176"/>
      <analytic>
        <title level="a">Evaluation of Audio Source Separation Models Using Hypothesis-Driven Non-Parametric Statistical Methods</title>
        <author>
          <persName>
            <foreName>Andrew J R</foreName>
            <surname>Simpson</surname>
            <initial>A. J. R.</initial>
          </persName>
          <persName>
            <foreName>Gerard</foreName>
            <surname>Roma</surname>
            <initial>G.</initial>
          </persName>
          <persName>
            <foreName>Emad M</foreName>
            <surname>Grais</surname>
            <initial>E. M.</initial>
          </persName>
          <persName>
            <foreName>Russell D</foreName>
            <surname>Mason</surname>
            <initial>R. D.</initial>
          </persName>
          <persName>
            <foreName>Chris</foreName>
            <surname>Hummersone</surname>
            <initial>C.</initial>
          </persName>
          <persName key="multispeech-2014-idp67512">
            <foreName>Antoine</foreName>
            <surname>Liutkus</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Mark D</foreName>
            <surname>Plumbley</surname>
            <initial>M. D.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">European Signal Processing Conference</title>
        <loc>Budapest, Hungary</loc>
        <imprint>
          <publisher>
            <orgName type="organisation">EURASIP</orgName>
          </publisher>
          <dateStruct>
            <month>August</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01410176" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01410176</ref>
        </imprint>
        <meeting id="cid70310">
          <title>European Signal Processing Conference</title>
          <num>21</num>
          <abbr type="sigle">EUSIPCO</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid55" type="inproceedings" rend="year" n="cite:sivasankaran:hal-01415759">
      <identifiant type="hal" value="hal-01415759"/>
      <analytic>
        <title level="a">Discriminative importance weighting of augmented training data for acoustic model training</title>
        <author>
          <persName key="multispeech-2015-idp123760">
            <foreName>Sunit</foreName>
            <surname>Sivasankaran</surname>
            <initial>S.</initial>
          </persName>
          <persName key="multispeech-2014-idp68752">
            <foreName>Emmanuel</foreName>
            <surname>Vincent</surname>
            <initial>E.</initial>
          </persName>
          <persName key="multispeech-2014-idp72696">
            <foreName>Irina</foreName>
            <surname>Illina</surname>
            <initial>I.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">42th International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2017)</title>
        <loc>New Orleans, United States</loc>
        <imprint>
          <dateStruct>
            <month>March</month>
            <year>2017</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01415759" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01415759</ref>
        </imprint>
        <meeting id="cid80145">
          <title>IEEE International Conference on Acoustics, Speech and Signal Processing</title>
          <num>42</num>
          <abbr type="sigle">ICASSP</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid47" type="inproceedings" rend="year" n="cite:stoter:hal-01248012">
      <identifiant type="hal" value="hal-01248012"/>
      <analytic>
        <title level="a">Common Fate Model for Unison source Separation</title>
        <author>
          <persName>
            <foreName>Fabian-Robert</foreName>
            <surname>Stöter</surname>
            <initial>F.-R.</initial>
          </persName>
          <persName key="multispeech-2014-idp67512">
            <foreName>Antoine</foreName>
            <surname>Liutkus</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Roland</foreName>
            <surname>Badeau</surname>
            <initial>R.</initial>
          </persName>
          <persName>
            <foreName>Bernd</foreName>
            <surname>Edler</surname>
            <initial>B.</initial>
          </persName>
          <persName>
            <foreName>Paul</foreName>
            <surname>Magron</surname>
            <initial>P.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">41st International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
        <loc>Shanghai, China</loc>
        <title level="s">Proceedings of the 41st International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
        <imprint>
          <publisher>
            <orgName>IEEE</orgName>
          </publisher>
          <dateStruct>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01248012" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01248012</ref>
        </imprint>
        <meeting id="cid80145">
          <title>IEEE International Conference on Acoustics, Speech and Signal Processing</title>
          <num>41</num>
          <abbr type="sigle">ICASSP</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid14" type="inproceedings" rend="year" n="cite:trouvain:hal-01293935">
      <identifiant type="hal" value="hal-01293935"/>
      <analytic>
        <title level="a">The IFCASL Corpus of French and German Non-native and Native Read Speech</title>
        <author>
          <persName>
            <foreName>Jürgen</foreName>
            <surname>Trouvain</surname>
            <initial>J.</initial>
          </persName>
          <persName key="multispeech-2014-idp63592">
            <foreName>Anne</foreName>
            <surname>Bonneau</surname>
            <initial>A.</initial>
          </persName>
          <persName key="multispeech-2014-idp70184">
            <foreName>Vincent</foreName>
            <surname>Colotte</surname>
            <initial>V.</initial>
          </persName>
          <persName key="multispeech-2014-idp94144">
            <foreName>Camille</foreName>
            <surname>Fauth</surname>
            <initial>C.</initial>
          </persName>
          <persName key="multispeech-2014-idp64832">
            <foreName>Dominique</foreName>
            <surname>Fohr</surname>
            <initial>D.</initial>
          </persName>
          <persName key="multispeech-2014-idp62112">
            <foreName>Denis</foreName>
            <surname>Jouvet</surname>
            <initial>D.</initial>
          </persName>
          <persName>
            <foreName>Jeanin</foreName>
            <surname>Jügler</surname>
            <initial>J.</initial>
          </persName>
          <persName key="multispeech-2014-idp66072">
            <foreName>Yves</foreName>
            <surname>Laprie</surname>
            <initial>Y.</initial>
          </persName>
          <persName key="multispeech-2014-idp74144">
            <foreName>Odile</foreName>
            <surname>Mella</surname>
            <initial>O.</initial>
          </persName>
          <persName>
            <foreName>Bernd</foreName>
            <surname>Möbius</surname>
            <initial>B.</initial>
          </persName>
          <persName>
            <foreName>Frank</foreName>
            <surname>Zimmerer</surname>
            <initial>F.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">LREC'2016, 10th edition of the Language Resources and Evaluation Conference</title>
        <loc>Portorož, Slovenia</loc>
        <title level="s">Proceedings LREC'2016</title>
        <imprint>
          <dateStruct>
            <month>May</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01293935" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01293935</ref>
        </imprint>
        <meeting id="cid289398">
          <title>International Conference on Language Resources and Evaluation</title>
          <num>10</num>
          <abbr type="sigle">LREC</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid42" type="inproceedings" rend="year" n="cite:vincent:hal-01398720">
      <identifiant type="hal" value="hal-01398720"/>
      <analytic>
        <title level="a">Séparation de sources: quand l'acoustique rencontre le machine learning</title>
        <author>
          <persName key="multispeech-2014-idp68752">
            <foreName>Emmanuel</foreName>
            <surname>Vincent</surname>
            <initial>E.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="no" x-invited-conference="yes" x-editorial-board="no">
        <title level="m">13e Congrès Français d'Acoustique</title>
        <loc>Le Mans, France</loc>
        <imprint>
          <dateStruct>
            <month>April</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01398720" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01398720</ref>
        </imprint>
        <meeting id="cid394719">
          <title>Congrès Français d'Acoustique</title>
          <num>13</num>
          <abbr type="sigle">CFA</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid15" type="inproceedings" rend="year" n="cite:zimmerer:hal-01399974">
      <identifiant type="doi" value="10.21437/SpeechProsody.2016-76"/>
      <identifiant type="hal" value="hal-01399974"/>
      <analytic>
        <title level="a">Influence of L1 prominence on L2 production: French and German speakers</title>
        <author>
          <persName>
            <foreName>Frank</foreName>
            <surname>Zimmerer</surname>
            <initial>F.</initial>
          </persName>
          <persName key="multispeech-2014-idp63592">
            <foreName>Anne</foreName>
            <surname>Bonneau</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Bistra</foreName>
            <surname>Andreeva</surname>
            <initial>B.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">Speech Prosody 2016</title>
        <loc>Boston, United States</loc>
        <imprint>
          <biblScope type="volume">2016</biblScope>
          <dateStruct>
            <month>May</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">370 - 374</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01399974" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01399974</ref>
        </imprint>
        <meeting id="cid623717">
          <title>International Conference on Speech Prosody</title>
          <num>2016</num>
          <abbr type="sigle"/>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid59" type="inproceedings" rend="year" n="cite:zimmerer:hal-01400005">
      <identifiant type="hal" value="hal-01400005"/>
      <analytic>
        <title level="a">Methods of investigating vowel interferences of French learners of German</title>
        <author>
          <persName>
            <foreName>Frank</foreName>
            <surname>Zimmerer</surname>
            <initial>F.</initial>
          </persName>
          <persName>
            <foreName>Jürgen</foreName>
            <surname>Trouvain</surname>
            <initial>J.</initial>
          </persName>
          <persName key="multispeech-2014-idp63592">
            <foreName>Anne</foreName>
            <surname>Bonneau</surname>
            <initial>A.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="no" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">New Sounds 2016</title>
        <loc>Aarhus, Denmark</loc>
        <imprint>
          <dateStruct>
            <month>June</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01400005" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01400005</ref>
        </imprint>
        <meeting id="cid625548">
          <title>International Symposium on the Acquisition of Second Language Speech</title>
          <num>8</num>
          <abbr type="sigle">New Sounds</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid49" type="techreport" rend="year" n="cite:magron:hal-01340797">
      <identifiant type="hal" value="hal-01340797"/>
      <monogr>
        <title level="m">Generalized Wiener filtering for positive alpha-stable random variables</title>
        <author>
          <persName>
            <foreName>Paul</foreName>
            <surname>Magron</surname>
            <initial>P.</initial>
          </persName>
          <persName>
            <foreName>Roland</foreName>
            <surname>Badeau</surname>
            <initial>R.</initial>
          </persName>
          <persName key="multispeech-2014-idp67512">
            <foreName>Antoine</foreName>
            <surname>Liutkus</surname>
            <initial>A.</initial>
          </persName>
        </author>
        <imprint>
          <publisher>
            <orgName type="institution">Télécom ParisTech</orgName>
          </publisher>
          <dateStruct>
            <month>June</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01340797" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01340797</ref>
        </imprint>
      </monogr>
      <note type="typdoc">Research Report</note>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid36" type="techreport" rend="year" n="cite:sargent:hal-01368683">
      <identifiant type="hal" value="hal-01368683"/>
      <monogr>
        <title level="m">Supplementary material to the article: Estimating the structural segmentation of popular music pieces under regularity constraints</title>
        <author>
          <persName key="linkmedia-2016-idp137088">
            <foreName>Gabriel</foreName>
            <surname>Sargent</surname>
            <initial>G.</initial>
          </persName>
          <persName key="panama-2014-idp79976">
            <foreName>Frédéric</foreName>
            <surname>Bimbot</surname>
            <initial>F.</initial>
          </persName>
          <persName key="multispeech-2014-idp68752">
            <foreName>Emmanuel</foreName>
            <surname>Vincent</surname>
            <initial>E.</initial>
          </persName>
        </author>
        <imprint>
          <publisher>
            <orgName type="institution">IRISA-Inria, Campus de Beaulieu, 35042 Rennes cedex ; Inria Nancy, équipe Multispeech</orgName>
          </publisher>
          <dateStruct>
            <month>September</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01368683" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01368683</ref>
        </imprint>
      </monogr>
      <note type="typdoc">Research Report</note>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid40" type="unpublished" rend="year" n="cite:dumortier:hal-01393125">
      <identifiant type="hal" value="hal-01393125"/>
      <monogr>
        <title level="m">Efficient optimisation of wind power under acoustic constraints</title>
        <author>
          <persName key="multispeech-2014-idp84320">
            <foreName>Baldwin</foreName>
            <surname>Dumortier</surname>
            <initial>B.</initial>
          </persName>
          <persName key="multispeech-2014-idp68752">
            <foreName>Emmanuel</foreName>
            <surname>Vincent</surname>
            <initial>E.</initial>
          </persName>
          <persName key="tosca-2014-idm28208">
            <foreName>Madalina</foreName>
            <surname>Deaconu</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>Patrice</foreName>
            <surname>Cornu</surname>
            <initial>P.</initial>
          </persName>
        </author>
        <imprint>
          <dateStruct>
            <month>November</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01393125" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01393125</ref>
        </imprint>
      </monogr>
      <note type="bnote">working paper or preprint</note>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid46" type="unpublished" rend="year" n="cite:elie:hal-01423206">
      <identifiant type="hal" value="hal-01423206"/>
      <monogr>
        <title level="m">Acoustic impact of the glottal chink on the production of fricatives: A numerical study</title>
        <author>
          <persName key="multispeech-2014-idp92912">
            <foreName>Benjamin</foreName>
            <surname>Elie</surname>
            <initial>B.</initial>
          </persName>
          <persName key="multispeech-2014-idp66072">
            <foreName>Yves</foreName>
            <surname>Laprie</surname>
            <initial>Y.</initial>
          </persName>
        </author>
        <imprint>
          <dateStruct>
            <month>December</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01423206" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01423206</ref>
        </imprint>
      </monogr>
      <note type="bnote">working paper or preprint</note>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid45" type="patent" rend="year" n="cite:ouni:hal-01294028">
      <identifiant type="hal" value="hal-01294028"/>
      <monogr>
        <title level="m">Dispositif de traitement d’image</title>
        <author>
          <persName key="multispeech-2014-idp75400">
            <foreName>Slim</foreName>
            <surname>Ouni</surname>
            <initial>S.</initial>
          </persName>
          <persName key="multispeech-2014-idp104128">
            <foreName>Guillaume</foreName>
            <surname>Gris</surname>
            <initial>G.</initial>
          </persName>
        </author>
        <imprint>
          <biblScope type="number">15 52058</biblScope>
          <dateStruct>
            <month>January</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01294028" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01294028</ref>
        </imprint>
      </monogr>
      <note type="bnote">Le rapport de recherche reconnait la brevetabilité</note>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid12" type="unpublished" rend="year" n="cite:piquardkipffer:hal-01402986">
      <identifiant type="hal" value="hal-01402986"/>
      <monogr>
        <title level="m">Parcours scolaire de 166 dysphasiques et/ou dyslexiques-dysorthographiques âgés de 6 à 20 ans en situation de handicap : Schooling experiences of 166 dysphasic or dyslexics-dysorthographic children, aged from 6 to 20 in a handicap situation</title>
        <author>
          <persName key="multispeech-2014-idp76848">
            <foreName>Agnès</foreName>
            <surname>Piquard-Kipffer</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Tamara</foreName>
            <surname>Léonova</surname>
            <initial>T.</initial>
          </persName>
        </author>
        <imprint>
          <dateStruct>
            <month>July</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01402986" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01402986</ref>
        </imprint>
      </monogr>
      <note type="bnote">working paper or preprint</note>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid50" type="misc" rend="year" n="cite:piquardkipffer:hal-01239910">
      <identifiant type="hal" value="hal-01239910"/>
      <monogr x-scientific-popularization="no">
        <title level="m">Terminal portable de communication et affichage de la reconnaissance vocale. Enjeux et rapports à l'écrit. Étude préliminaire auprès d'adultes déficients auditifs</title>
        <author>
          <persName key="multispeech-2014-idp76848">
            <foreName>Agnès</foreName>
            <surname>Piquard-Kipffer</surname>
            <initial>A.</initial>
          </persName>
          <persName key="multispeech-2014-idp74144">
            <foreName>Odile</foreName>
            <surname>Mella</surname>
            <initial>O.</initial>
          </persName>
          <persName>
            <foreName>Jérémy</foreName>
            <surname>Miranda</surname>
            <initial>J.</initial>
          </persName>
          <persName key="multispeech-2014-idp62112">
            <foreName>Denis</foreName>
            <surname>Jouvet</surname>
            <initial>D.</initial>
          </persName>
          <persName key="multispeech-2014-idp88000">
            <foreName>Luiza</foreName>
            <surname>Orosanu</surname>
            <initial>L.</initial>
          </persName>
        </author>
        <imprint>
          <dateStruct>
            <month>March</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">15p.</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01239910" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01239910</ref>
        </imprint>
      </monogr>
      <note type="bnote">In M.Frisch (Eds) Le réseau Idéki : Didactiques, métiers de l'humain et Intelligence collective. Nouveaux espaces et dispositifs en question. Nouveaux horizons en éducation, formation et en recherche. L'harmattan, Collection I.D</note>
    </biblStruct>
    
    <biblStruct id="multispeech-2016-bid20" type="inproceedings" rend="foot" n="footcite:liutkus2015generalized">
      <analytic>
        <title level="a">Generalized Wiener filtering with fractional power spectrograms</title>
        <author>
          <persName key="multispeech-2014-idp67512">
            <foreName>Antoine</foreName>
            <surname>Liutkus</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Roland</foreName>
            <surname>Badeau</surname>
            <initial>R.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
        <imprint>
          <publisher>
            <orgName type="organisation">IEEE</orgName>
          </publisher>
          <dateStruct>
            <year>2015</year>
          </dateStruct>
          <biblScope type="pages">266–270</biblScope>
        </imprint>
      </monogr>
    </biblStruct>
  </biblio>
</raweb>
