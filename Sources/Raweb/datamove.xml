<?xml version="1.0" encoding="utf-8"?>
<raweb xmlns:xlink="http://www.w3.org/1999/xlink" xml:lang="en" year="2016">
  <identification id="datamove" isproject="true">
    <shortname>DATAMOVE</shortname>
    <projectName>Data Aware Large Scale Computing</projectName>
    <theme-de-recherche>Distributed and High Performance Computing</theme-de-recherche>
    <domaine-de-recherche>Networks, Systems and Services, Distributed Computing</domaine-de-recherche>
    <urlTeam>https://team.inria.fr/datamove/</urlTeam>
    <header_dates_team>Creation of the Team: 2016 January 01</header_dates_team>
    <LeTypeProjet>Team</LeTypeProjet>
    <keywordsSdN>
      <term>1.1.4. - High performance computing</term>
      <term>1.1.5. - Exascale</term>
      <term>2.1.10. - Domain-specific languages</term>
      <term>2.6.2. - Middleware</term>
      <term>7.1. - Parallel and distributed algorithms</term>
    </keywordsSdN>
    <keywordsSecteurs>
      <term>1.1.2. - Molecular biology</term>
      <term>5.5. - Materials</term>
    </keywordsSecteurs>
    <DescriptionTeam>Inria teams are typically groups of researchers working on the definition of a common project, and objectives, with the goal to arrive at the creation of a project-team. Such project-teams may include other partners (universities or research institutions).</DescriptionTeam>
    <UR name="Grenoble"/>
    <moreinfo>
      <p>The DataMove team is localized in the Imag building on the Campus of Univ. Grenoble Alpes.</p>
    </moreinfo>
  </identification>
  <team id="uid1">
    <person key="moais-2014-idp86304">
      <firstname>Bruno</firstname>
      <lastname>Raffin</lastname>
      <categoryPro>Chercheur</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Team leader, Inria, Senior Researcher, Research Scientist</moreinfo>
      <hdr>oui</hdr>
    </person>
    <person key="mescal-2014-idp67136">
      <firstname>Yves</firstname>
      <lastname>Denneulin</lastname>
      <categoryPro>Enseignant</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Grenoble-INP, Faculty Member, Professor</moreinfo>
    </person>
    <person key="datamove-2016-idp113312">
      <firstname>Pierre Francois</firstname>
      <lastname>Dutot</lastname>
      <categoryPro>Enseignant</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Univ. Grenoble-Alpes, Faculty Member, Associate Professor</moreinfo>
    </person>
    <person key="moais-2014-idp92840">
      <firstname>Gregory</firstname>
      <lastname>Mounie</lastname>
      <categoryPro>Enseignant</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Grenoble-INP, Faculty Member, Associate Professor</moreinfo>
    </person>
    <person key="mescal-2014-idp71280">
      <firstname>Olivier</firstname>
      <lastname>Richard</lastname>
      <categoryPro>Enseignant</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Univ. Grenoble-Alpes, Faculty Member, Associate Professor</moreinfo>
    </person>
    <person key="moais-2014-idp95616">
      <firstname>Denis</firstname>
      <lastname>Trystram</lastname>
      <categoryPro>Enseignant</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Grenoble-INP, Faculty Member, Professor</moreinfo>
      <hdr>oui</hdr>
    </person>
    <person key="moais-2014-idp97056">
      <firstname>Frederic</firstname>
      <lastname>Wagner</lastname>
      <categoryPro>Enseignant</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Grenoble-INP, Faculty Member, Associate Professor</moreinfo>
    </person>
    <person key="mescal-2014-idp75040">
      <firstname>Romain</firstname>
      <lastname>Cavagna</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Univ. Grenoble-Alpes, until Aug 2016</moreinfo>
    </person>
    <person key="datamove-2016-idp128688">
      <firstname>Ivan</firstname>
      <lastname>Cores Gonzalez</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="moais-2015-idp79272">
      <firstname>Tristan</firstname>
      <lastname>Ezequel</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="datamove-2016-idp133616">
      <firstname>Nicolas</firstname>
      <lastname>Michon</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Inria, from Oct 2016</moreinfo>
    </person>
    <person key="mescal-2014-idp81232">
      <firstname>Pierre</firstname>
      <lastname>Neyron</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>CNRS</moreinfo>
    </person>
    <person key="mescal-2015-idp104808">
      <firstname>Baptiste</firstname>
      <lastname>Pichot</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="hiepacs-2015-idp118504">
      <firstname>Theophile</firstname>
      <lastname>Terraz</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="mescal-2014-idp109040">
      <firstname>Bruno</firstname>
      <lastname>Bzeznik</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Univ. Grenoble-Alpes</moreinfo>
    </person>
    <person key="mescal-2015-idp106056">
      <firstname>Christian</firstname>
      <lastname>Seguy</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>CNRS</moreinfo>
    </person>
    <person key="moais-2015-idp81768">
      <firstname>Marcos</firstname>
      <lastname>Amaris Gonzalez</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>USP, until Oct 2016</moreinfo>
    </person>
    <person key="moais-2014-idp118488">
      <firstname>Raphaël</firstname>
      <lastname>Bleuse</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Univ. Grenoble-Alpes</moreinfo>
    </person>
    <person key="moais-2015-idp85496">
      <firstname>Estelle</firstname>
      <lastname>Dirand</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>CEA</moreinfo>
    </person>
    <person key="moais-2015-idp89232">
      <firstname>Mohammed</firstname>
      <lastname>Khatiri</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>partly Inria</moreinfo>
    </person>
    <person key="moais-2014-idp109624">
      <firstname>Alessandro</firstname>
      <lastname>Kraemer</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Federal Technological University of Paraná</moreinfo>
    </person>
    <person key="moais-2014-idp129688">
      <firstname>Fernando</firstname>
      <lastname>Machado Mendonca</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>USP</moreinfo>
    </person>
    <person key="mescal-2014-idp79992">
      <firstname>Michael</firstname>
      <lastname>Mercier</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>ATOS/BULL, granted by CIFRE</moreinfo>
    </person>
    <person key="moais-2014-idp133408">
      <firstname>Millian</firstname>
      <lastname>Poquet</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Univ. Grenoble-Alpes</moreinfo>
    </person>
    <person key="moais-2015-idp96616">
      <firstname>Valentin</firstname>
      <lastname>Reis</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Grenoble-Alpes</moreinfo>
    </person>
    <person key="moais-2014-idp137136">
      <firstname>Marwa</firstname>
      <lastname>Sridi</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>CEA, until Apr 2016</moreinfo>
    </person>
    <person key="moais-2014-idp138376">
      <firstname>Abhinav</firstname>
      <lastname>Srivastav</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Univ. Grenoble-Alpes, until Jul 2016</moreinfo>
    </person>
    <person key="moais-2014-idp140888">
      <firstname>Julio</firstname>
      <lastname>Toss</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>UFRGS</moreinfo>
    </person>
    <person key="moais-2014-idp124696">
      <firstname>David</firstname>
      <lastname>Glesser</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>ATOS/BULL, granted by CIFRE,until Oct 2016</moreinfo>
    </person>
    <person key="moais-2014-idp105840">
      <firstname>Giorgio</firstname>
      <lastname>Lucarelli</lastname>
      <categoryPro>PostDoc</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Inria, until Oct 2017</moreinfo>
    </person>
    <person key="datamove-2016-idp182800">
      <firstname>Daniel</firstname>
      <lastname>Cordeiro</lastname>
      <categoryPro>Visiteur</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>USP, until Mar 2016</moreinfo>
    </person>
    <person key="datamove-2016-idp185280">
      <firstname>Sirine</firstname>
      <lastname>Marakchi</lastname>
      <categoryPro>Visiteur</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>ISBS SFAX, Jan 2016</moreinfo>
    </person>
    <person key="moais-2015-idp107696">
      <firstname>Wafa</firstname>
      <lastname>Nafti</lastname>
      <categoryPro>Visiteur</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>ESSTT, until May 2016</moreinfo>
    </person>
    <person key="datamove-2016-idp189488">
      <firstname>Ioannis</firstname>
      <lastname>Milis</lastname>
      <categoryPro>Visiteur</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Athens UEB, June 2016</moreinfo>
    </person>
    <person key="datamove-2016-idp190256">
      <firstname>Katrin</firstname>
      <lastname>Scharnowski</lastname>
      <categoryPro>Visiteur</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>University of Stuttgart, from May 2016 until Aug 2016</moreinfo>
    </person>
    <person key="mescal-2014-idp95048">
      <firstname>Annie</firstname>
      <lastname>Simon</lastname>
      <categoryPro>Assistant</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="datamove-2016-idp195152">
      <firstname>Luis Omar</firstname>
      <lastname>Alvarez Mures</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Inria, Student, from March 2016 until Mai 2016</moreinfo>
    </person>
    <person key="datamove-2016-idp197648">
      <firstname>Clement</firstname>
      <lastname>Mommessin</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Inria, Student, from Feb 2016 until Dec 2016</moreinfo>
    </person>
    <person key="datamove-2016-idp200144">
      <firstname>Waqas</firstname>
      <lastname>Imtiaz</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Inria,Student, from Feb 2016 until Jun 2016</moreinfo>
    </person>
    <person key="datamove-2016-idp202640">
      <firstname>Matthias</firstname>
      <lastname>Kohl</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Inria, Student, from Feb 2016 until Jun 2016</moreinfo>
    </person>
    <person key="datamove-2016-idp205136">
      <firstname>Thomas</firstname>
      <lastname>Lavocat</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Inria, Student, from Feb 2016 until Aug 2016</moreinfo>
    </person>
    <person key="datamove-2016-idp207632">
      <firstname>Piat</firstname>
      <lastname>Wegener</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Inria, Student, from May 2016 until Aug 2016</moreinfo>
    </person>
    <person key="datamove-2016-idp210128">
      <firstname>Mohamed</firstname>
      <lastname>Dyab</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Inria, Student, from Feb 2016 until Sep 2016</moreinfo>
    </person>
    <person key="datamove-2016-idp212624">
      <firstname>Jordan</firstname>
      <lastname>Ellapin</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Inria, Student, from May 2016 until Aug 2016</moreinfo>
    </person>
    <person key="datamove-2016-idp215120">
      <firstname>Lucas</firstname>
      <lastname>Barallon</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Inria, Student, from Feb 2016 until Aug 2016</moreinfo>
    </person>
    <person key="datamove-2016-idp217616">
      <firstname>Jad</firstname>
      <lastname>Darrous</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Grenoble</research-centre>
      <moreinfo>Inria, Student, from Feb 2016 until June 2016</moreinfo>
    </person>
  </team>
  <presentation id="uid2">
    <bodyTitle>Overall Objectives</bodyTitle>
    <subsection id="uid3" level="1">
      <bodyTitle>Overall Objectives</bodyTitle>
      <p>Moving data on large supercomputers is becoming a major performance bottleneck, and the situation is expected to worsen even more at exascale and beyond. Data transfer capabilities are growing at a slower rate than processing power ones. The profusion of flops available will be difficult to use efficiently due to constrained communication capabilities. Moving data is also an important source of power consumption. The DataMove team focuses on <b>data aware large scale computing</b>, investigating approaches to reduce data movements on large scale HPC machines. We will investigate data aware scheduling algorithms for job management systems. The growing cost of data movements requires adapted scheduling policies able to take into account the influence of intra-application communications, IOs as well as contention caused by data traffic generated by other concurrent applications.
At the same time experimenting new scheduling policies on real platforms is unfeasible.
Simulation tools are required to probe novel scheduling policies.
Our goal is to investigate how to extract information from actual compute
centers traces in order to replay job allocations and executions
with new scheduling policies.
Schedulers need information about the jobs behavior on the target machine
to actually make efficient allocation decisions.
We will research approaches relying on learning techniques applied
to execution traces to extract data and forecast job behaviors.
In addition to traditional computation intensive numerical simulations,
HPC platforms also need to execute more and more often data intensive
processing tasks like data analysis.
In particular, the ever growing amount of data generated by numerical
simulation calls for a tighter integration between the simulation
and the data analysis.
The goal is to reduce the data traffic and to speed-up result analysis by processing results in situ, i.e. as closely as possible to the locus and time of data generation. Our goal is here to investigate how to program and schedule such analysis workflows in the HPC context, requiring the development of adapted resource sharing strategies, data structures and parallel analytics schemes. To tackle these issues, we will intertwine theoretical research and practical developments to elaborate solutions generic and effective enough to be of practical interest. Algorithms with performance guarantees will be designed and experimented on large scale platforms with realistic usage scenarios developed with partner scientists or based on logs of the biggest available computing platforms. Conversely, our strong experimental expertise will enable to feed theoretical models with sound hypotheses, to twist proven algorithms with practical heuristics that could be further retro-feeded into adequate theoretical models.
</p>
    </subsection>
  </presentation>
  <fondements id="uid4">
    <bodyTitle>Research Program</bodyTitle>
    <subsection id="uid5" level="1">
      <bodyTitle>Motivation</bodyTitle>
      <p>Today's largest supercomputers <footnote id="uid6" id-text="1">Top500 Ranking, <ref xlink:href="http://www.top500.org" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>www.<allowbreak/>top500.<allowbreak/>org</ref></footnote> are composed of few millions of cores, with performances almost reaching 100 PetaFlops <footnote id="uid7" id-text="2"><formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msup><mn>10</mn><mn>15</mn></msup></math></formula> floating point operations per second</footnote> for the largest machine. Moving data in such large supercomputers is becoming a major performance bottleneck, and the situation is expected to worsen even more at exascale and beyond. The data transfer capabilities are growing at a slower rate than processing power ones. The profusion of available flops will very likely be underused due to constrained communication capabilities. It is commonly admitted that data movements account for 50% to 70% of the global power consumption <footnote id="uid8" id-text="3">SciDAC Review, 2010, <ref xlink:href="http://www.scidacreview.org/1001/pdf/hardware.pdf" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>www.<allowbreak/>scidacreview.<allowbreak/>org/<allowbreak/>1001/<allowbreak/>pdf/<allowbreak/>hardware.<allowbreak/>pdf</ref></footnote>. Thus, data movements are potentially one of the most important source of savings for enabling supercomputers to stay in the commonly adopted energy barrier of 20 MegaWatts. In the mid to long term, non volatile memory (NVRAM) is expected to deeply change the machine I/Os. Data distribution will shift from disk arrays with an access time often considered as uniform, towards permanent storage capabilities at each node of the machine, making data locality an even more prevalent paradigm.</p>
      <p>The DataMove team works on <b>optimizing data movements for large scale computing</b> mainly at two related levels:</p>
      <simplelist>
        <li id="uid9">
          <p noindent="true">Resource allocation</p>
        </li>
        <li id="uid10">
          <p noindent="true">Integration of numerical simulation and data analysis</p>
        </li>
      </simplelist>
      <p>The resource and job management system (also called batch scheduler or RJMS) is in charge of allocating resources upon user requests for executing their parallel applications.
The growing cost of data movements requires adapted scheduling policies able to take into account the influence of intra-application communications, I/Os as well as contention caused by data traffic generated by other concurrent applications.
Modelling the application behavior to anticipate its actual resource usage on such architecture is known to be challenging, but it becomes
critical for improving performances (execution time, energy, or any other relevant objective). The job management system also needs to handle new types of workloads: high performance platforms now need to execute more and more often data intensive processing tasks like data analysis in addition to traditional computation intensive numerical simulations. In particular, the ever growing amount of data generated by numerical simulation calls for a tighter integration between the simulation and the data analysis. The challenge here is to reduce data traffic and to speed-up result analysis by performing result processing (compression, indexation, analysis, visualization, etc.) as closely as possible to the locus and time of data generation. This emerging trend called <i>in situ analytics</i> requires to revisit the traditional workflow (loop of batch processing followed by postmortem analysis). The application becomes a whole including the simulation, in situ processing and I/Os. This motivates the development of new well-adapted resource sharing strategies, data structures and parallel analytics schemes to efficiently interleave the different components of the application and globally improve the performance.
</p>
    </subsection>
    <subsection id="uid11" level="1">
      <bodyTitle>Strategy</bodyTitle>
      <p>DataMove targets HPC (High Performance Computing) at Exascale. But such machines and the associated applications are expected to be available only in 5 to 10 years.
Meanwhile, we expect to see a growing number of petaflop machines to answer the needs for advanced numerical simulations. A sustainable exploitation of these petaflop machines is a real and hard challenge that we address. We may also see in the coming years a convergence between HPC and Big Data, HPC platforms becoming more elastic and supporting Big Data jobs, or HPC applications being more commonly executed on cloud like architectures. This is the second top objective of the 2015 US Strategic Computing Initiative  <footnote id="uid12" id-text="4"><ref xlink:href="https://www.whitehouse.gov/the-press-office/2015/07/29/executive-order-creating-national-strategic-computing-initiative" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>www.<allowbreak/>whitehouse.<allowbreak/>gov/<allowbreak/>the-press-office/<allowbreak/>2015/<allowbreak/>07/<allowbreak/>29/<allowbreak/>executive-order-creating-national-strategic-computing-initiative</ref></footnote>: <i>Increasing coherence between the technology base used for modelling and simulation and that used for data analytic computing</i>. We contribute to that convergence at our level, considering more dynamic and versatile target platforms and types of workloads.</p>
      <p>Our approaches should entail minimal modifications on the code of numerical simulations. Often large scale numerical simulations are complex domain specific codes with a long life span. We assume these codes as being sufficiently optimized. We influence the behavior of numerical simulations through resource allocation at the job management system level or when interleaving them with analytics code.</p>
      <p>To tackle these issues, we propose to intertwine theoretical research and practical developments in an agile mode. Algorithms with performance guarantees are designed and experimented on large scale platforms with realistic usage scenarios developed with partner scientists or based on logs of the biggest available computing platforms (national supercomputers like Curie, or the BlueWaters machine accessible through our collaboration with Argonne National Lab). Conversely, a strong experimental expertise enables to feed theoretical models with sound hypotheses, to twist proven algorithms with practical heuristics that could be further retro-feeded into adequate theoretical models.</p>
      <p>A central scientific question is to make the relevant choices for optimizing performance (in a broad sense) in a reasonable time. HPC architectures and applications are increasingly complex systems (heterogeneity, dynamicity, uncertainties), which leads to consider the <b>optimization of resource allocation based on multiple objectives</b>, often contradictory (like energy and run-time for instance). Focusing on the optimization of one particular objective usually leads to worsen the others. The historical positioning of some members of the team who are specialists in multi-objective optimization is to generate a (limited) set of trade-off configurations, called <i>Pareto points</i>, and choose when required the most suitable trade-off between all the objectives. This methodology differs from the classical approaches, which simplify the problem into a single objective one (focus on a particular objective, combining the various objectives or agglomerate them). The real challenge is thus to combine algorithmic techniques to account for this diversity while guaranteeing a target efficiency for all the various objectives.</p>
      <p>The DataMove team aims to elaborate generic and effective solutions of practical interest. We make our new algorithms accessible through the team flagship software tools, <b>the OAR batch scheduler and the in situ processing framework FlowVR</b>. We maintain and enforce strong links with teams closely connected with large architecture design and operation, as well as scientists of other disciplines, in particular computational biologists, with whom we elaborate and validate new usage scenarios.
</p>
    </subsection>
    <subsection id="uid13" level="1">
      <bodyTitle>Research Directions</bodyTitle>
      <p>DataMove targets HPC (High Performance Computing) at Exascale. But such machines and the associated applications are expected to be available only in 5 to 10 years.
Meanwhile, we expect to see a growing number of petaflop machines to answer the needs for advanced numerical simulations. A sustainable exploitation of these petaflop machines is a real and hard challenge that we address. We may also see in the coming years a convergence between HPC and Big Data, HPC platforms becoming more elastic and supporting Big Data jobs, or HPC applications being more commonly executed on cloud like architectures. This is the second top objective of the 2015 US Strategic Computing Initiative  <footnote id="uid14" id-text="5"><ref xlink:href="https://www.whitehouse.gov/the-press-office/2015/07/29/executive-order-creating-national-strategic-computing-initiative" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>www.<allowbreak/>whitehouse.<allowbreak/>gov/<allowbreak/>the-press-office/<allowbreak/>2015/<allowbreak/>07/<allowbreak/>29/<allowbreak/>executive-order-creating-national-strategic-computing-initiative</ref></footnote>: <i>Increasing coherence between the technology base used for modelling and simulation and that used for data analytic computing</i>. We contribute to that convergence at our level, considering more dynamic and versatile target platforms and types of workloads.</p>
      <p>Our approaches should entail minimal modifications on the code of numerical simulations. Often large scale numerical simulations are complex domain specific codes with a long life span. We assume these codes as being sufficiently optimized. We influence the behavior of numerical simulations through resource allocation at the job management system level or when interleaving them with analytics code.</p>
      <p>To tackle these issues, we propose to intertwine theoretical research and practical developments in an agile mode. Algorithms with performance guarantees are designed and experimented on large scale platforms with realistic usage scenarios developed with partner scientists or based on logs of the biggest available computing platforms (national supercomputers like Curie, or the BlueWaters machine accessible through our collaboration with Argonne National Lab). Conversely, a strong experimental expertise enables to feed theoretical models with sound hypotheses, to twist proven algorithms with practical heuristics that could be further retro-feeded into adequate theoretical models.</p>
      <p>A central scientific question is to make the relevant choices for optimizing performance (in a broad sense) in a reasonable time. HPC architectures and applications are increasingly complex systems (heterogeneity, dynamicity, uncertainties), which leads to consider the <b>optimization of resource allocation based on multiple objectives</b>, often contradictory (like energy and run-time for instance). Focusing on the optimization of one particular objective usually leads to worsen the others. The historical positioning of some members of the team who are specialists in multi-objective optimization is to generate a (limited) set of trade-off configurations, called <i>Pareto points</i>, and choose when required the most suitable trade-off between all the objectives. This methodology differs from the classical approaches, which simplify the problem into a single objective one (focus on a particular objective, combining the various objectives or agglomerate them). The real challenge is thus to combine algorithmic techniques to account for this diversity while guaranteeing a target efficiency for all the various objectives.</p>
      <p>The DataMove team aims to elaborate generic and effective solutions of practical interest. We make our new algorithms accessible through the team flagship software tools, <b>the OAR batch scheduler and the in situ processing framework FlowVR</b>. We maintain and enforce strong links with teams closely connected with large architecture design and operation, as well as scientists of other disciplines, in particular computational biologists, with whom we elaborate and validate new usage scenarios.
</p>
    </subsection>
  </fondements>
  <domaine id="uid15">
    <bodyTitle>Application Domains</bodyTitle>
    <subsection id="uid16" level="1">
      <bodyTitle>Data Aware Batch Scheduling</bodyTitle>
      <p>Large scale high performance computing platforms are becoming
increasingly complex. Determining efficient allocation and
scheduling strategies that can adapt to technological evolutions is a
strategic and difficult challenge. We are interested in
scheduling jobs in hierarchical and heterogeneous large scale
platforms. On such platforms, application developers typically submit their jobs in
centralized waiting queues. The job management system aims at
determining a suitable allocation for the jobs, which all compete
against each other for the available computing resources.
Performances are measured using different classical metrics like maximum
completion time or slowdown. Current systems make use of very simple (but fast)
algorithms that however rely on simplistic platform and execution models, and thus,
have limited performances.</p>
      <p>For all target scheduling problems we aim to provide both theoretical analysis and
complementary analysis through simulations. Achieving meaningful results will require strong improvements on existing models
(on power for example) and the design of new approximation algorithms with various objectives such as stretch, reliability,
throughput or energy consumption, while keeping in focus the need for a low-degree polynomial complexity.</p>
      <subsection id="uid17" level="2">
        <bodyTitle>Status of Current Algorithms</bodyTitle>
        <p>The most common batch scheduling policy is to consider the jobs according to the First Come First Served order
(FCFS) with backfilling (BF). BF is the most widely used policy due to its easy and robust implementation and known benefits such as high system utilization. It is well-known that this strategy does not optimize any sophisticated function, but it is simple to implement and it guarantees that there is no starvation (i.e. every job will be scheduled at some moment).</p>
        <p>More advanced algorithms are seldom used on production platforms due to both the gap
between theoretical models and practical systems and speed constraints. When looking at
theoretical scheduling problems, the generally accepted goal is to
provide polynomial algorithms (in the number of submitted jobs and the number of involved computing units).
However, with millions of processing cores where every process and data transfer have to be individually
scheduled, polynomial algorithms are prohibitive as soon as the
polynomial degree is too large. The model of <i>parallel tasks</i> simplifies this
problem by bundling many threads and communications into single boxes,
either rigid, rectangular or malleable.
Especially malleable tasks capture the dynamicity of the execution. Yet these models are
ill-adapted to heterogeneous platforms, as the running time depends on
more than simply the number of allotted resources, and some of the
common underlying assumptions on the speed-up functions (such as
monotony or concavity) are most often only partially verified.</p>
        <p>In practice, the job execution times depend on their allocation (due to
communication interferences and heterogeneity in both computation and
communication), while theoretical models of parallel jobs usually
consider jobs as black boxes with a fixed (maximum) execution time. Though
interesting and powerful, the classical models (namely, synchronous PRAM model, delay,
LogP) and their variants (such as hierarchical delay), are
not well-suited to large scale parallelism on platforms where the cost of
moving data is significant, non uniform and may change over time. Recent studies are still
refining such models in order to take into account communication contentions
more accurately while remaining tractable enough to provide a useful tool
for algorithm design.</p>
        <p>Today, all algorithms in use in production systems are oblivious
to communications. One of our main goals is to <b>design a new generation
of scheduling algorithms fitting more closely job schedules according
to platform topologies</b>.</p>
      </subsection>
      <subsection id="uid18" level="2">
        <bodyTitle>Locality Aware Allocations</bodyTitle>
        <p>Recently, we developed modifications of the standard back-filling algorithm taking into account platform
topologies.
The proposed algorithms take into account locality and contiguity
in order to hide communication patterns within parallel tasks.
The main result here is to establish good lower bounds and small approximation ratios for policies respecting the locality constraints.
The algorithms work in an online fashion, improving the global behavior of the system while still keeping a low running time.
These improvements rely mainly on our past experience in designing approximation algorithms.
Instead of relying on complex networking models and communication patterns for
estimating execution times, the communications are disconnected from the execution time.
Then, the scheduling problem leads to a trade-off: optimizing locality
of communications on one side and a performance objective (like the makespan or stretch) on the other side.</p>
        <p>In the perspective of taking care of locality, other ongoing works include the study of schedulers for platforms whose interconnection network
is a static structured topology (like the 3D-torus of the BlueWaters platform we work on in collaboration with the Argonne National Laboratory).
One main characteristic of this 3D-torus platform is to provide I/O nodes at
specific locations in the topology. Applications generate and access specific data
and are thus bounded to specific I/O nodes.
Resource allocations are constrained in a strong and unusual way. This problem is close for actual hierarchical platforms.
The scheduler needs to compute a schedule such that I/O nodes requirements are
filled for each application while at the same time
avoiding communication interferences.
Moreover, extra constraints can arise for applications requiring accelerators
that are gathered on the nodes at the edge of the network topology.</p>
        <p>While current results are encouraging, they are however limited in performance by
the low amount of information available to the scheduler. We look forward to
extend ongoing work by progressively increasing application and network knowledge (by technical mechanisms like profiling or monitoring or by more sophisticated methods like learning).
It is also important to anticipate on application resource usage in terms of compute units, memory as well as network and I/Os to
efficiently schedule a mix of applications with different profiles.
For instance, a simple solution is to partition the jobs as "communication intensive" or
"low communications". Such a tag could be achieved by the users them selves or obtained by learning techniques.
We could then schedule low communications jobs using leftover spaces while taking care of high communication jobs.
More sophisticated options are possible, for instance those that use more detailed communication patterns and networking models.
Such options would leverage the work proposed in Section <ref xlink:href="#uid22" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> for gathering application traces.</p>
      </subsection>
      <subsection id="uid19" level="2">
        <bodyTitle>Data-Centric Processing</bodyTitle>
        <p>Exascale computing is shifting away from the traditional compute-centric models to a more data-centric one.
This is driven by the evolving nature of large scale distributed computing, no longer dominated by pure computations
but also by the need to handle and analyze large volumes of data. These data can be large databases of results, data streamed from
a running application or another scientific instrument (collider for instance). These new workloads call for specific resource allocation strategies.</p>
        <p>Data movements and storage are expected to be a major energy and performance bottleneck on next generation platforms.
Storage architectures are also evolving, the standard centralized parallel file system being complemented with local persistent storage (Burst Buffers, NVRAM).
Thus, one data producer can stage data on some nodes' local storage, requiring to schedule close by the associated analytics tasks to limit data movements.
This kind of configuration, often referred as <i>in situ analytics</i>, is expected to become common as it enables to switch from the traditional I/O intensive workflow (batch-processing followed by <i>post mortem</i> analysis and visualization) to a more storage conscious approach where data are processed as closely as possible to where and when they are produced (in situ processing is addressed in details in section <ref xlink:href="#uid28" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>).
By reducing data movements and scheduling the extra processing on resources not fully exploited yet, in situ processing is expected to have also a significant positive energetic impact. Analytics codes can be executed in the same nodes than the application, often on dedicated cores commonly called helper cores, or on dedicated nodes called stagging nodes. The results are either forwarded to the users for visualization or saved to disk through I/O nodes. In situ analytics can also take benefit of node local disks or burst buffers to reduce data movements.
Future job scheduling strategies should take into account in situ processes in addition to the job allocation to optimize both energy consumption and execution time.
On the one hand, this problem can be reduced to an allocation problem of extra asynchronous tasks to idle computing units. But on the other hand, embedding analytics in applications brings extra difficulties by making the application more heterogeneous and imposing more constraints (data affinity) on the required resources.
Thus, the main point here is to develop efficient algorithms for dealing with heterogeneity without increasing the global computational cost.</p>
      </subsection>
      <subsection id="uid20" level="2">
        <bodyTitle>Learning</bodyTitle>
        <p>Another important issue is to adapt the job management system to deal with the bad effects of uncertainties, which may be catastrophic in large scale heterogeneous HPC platforms
(jobs delayed arbitrarly far or jobs killed).
A natural question is then: <i>is it possible to have a good estimation of the job and platform parameters in order to be able to obtain a better scheduling ?</i>
Many important parameters (like the number or type of required resources or the estimated running time of the jobs) are asked to the users when they submit their jobs. However, some of these values are not accurate and in many cases, they are not even provided by the end-users.
In DataMove, we propose to study new methods for a better prediction of the characteristics of the jobs and their execution in order to improve the optimization process. In particular, the methods well-studied in the field of big data (in supervised Machine Learning, like classical regression methods, Support Vector Methods, random forests, learning to rank techniques or deep learning) could and must be used to improve job scheduling in large scale HPC platforms. This topic received a great attention recently in the field of parallel and distributed processing.
A preliminary study has been done recently by our team with the target of predicting the job running times (called wall times).
We succeeded to improve significantly in average the reference EASY Back Filling algorithm by estimating the wall time of the jobs, however,
this method leads to big delay for the stretch of few jobs.
Even if we succeed in determining more precisely hidden parameters, like the wall time of the jobs, this is not enough to determine an optimized solution.
The shift is not only to learn on dedicated parameters but also on the scheduling policy. The data collected from the accounting and profiling of jobs can be used to better understand the needs of the jobs and through learning to propose adaptations for future submissions. The goal is to propose extensions to further improve the job scheduling and improve the performance and energy efficiency of the application. For instance preference learning may enable to compute on-line new priorities to back-fill the ready jobs.</p>
      </subsection>
      <subsection id="uid21" level="2">
        <bodyTitle>Multi-objective Optimization</bodyTitle>
        <p>Several optimization questions that arise in allocation and scheduling problems lead to the study of several objectives at the same time.
The goal is then not a single optimal solution, but a more complicated mathematical object that captures the notion of trade-off.
In broader terms, the goal of multi-objective optimization is not to externally arbitrate on disputes between entities with different
goals, but rather to explore the possible solutions to highlight the whole range of interesting compromises.
A classical tool for studying such multi-objective optimization problems is to use <i>Pareto curves</i>.
However, the full description of the Pareto curve can be very hard because of both the number of solutions and the hardness of computing each point.
Addressing this problem will opens new methodologies for the analysis of algorithms.</p>
        <p>To further illustrate this point here are three possible case studies with emphasis on conflicting interests measured with
different objectives. While these cases are good representatives of our HPC context, there are other pertinent trade-offs we may
investigate depending on the technology evolution in the coming years. This enumeration is certainly not limitative.</p>
        <p><b>Energy versus Performance</b>.
The classical scheduling algorithms designed for the purpose of performance can no longer be used because performance and energy are contradictory objectives to some extent.
The scheduling problem with energy becomes a multi-objective problem in nature since the energy consumption should be considered as equally important as performance at exascale.
A global constraint on energy could be a first idea for determining trade-offs but
the knowledge of the Pareto set (or an approximation of it) is also very useful.</p>
        <p><b>Administrators versus application developers</b>.
Both are naturally interested in different objectives:
In current algorithms, the performance is mainly computed from the point of view of administrators, but the users should be in the loop since they can give useful information and help to the construction of better schedules.
Hence, we face again a multi-objective problem where, as in the above case, the approximation of the Pareto set provides the trade-off between the administrator view and user demands.
Moreover, the objectives are usually of the same nature.
For example, <i>max stretch</i> and <i>average stretch</i> are two objectives based on the slowdown factor that can interest administrators and users, respectively.
In this case the study of the norm of stretch can be also used to describe the trade-off
(recall that the <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msub><mi>L</mi><mn>1</mn></msub></math></formula>-norm corresponds to the average objective while the <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msub><mi>L</mi><mi>∞</mi></msub></math></formula>-norm to the max objective).
Ideally, we would like to design an algorithm that gives good approximate solutions at the same time for all norms.
The <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msub><mi>L</mi><mn>2</mn></msub></math></formula> or <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msub><mi>L</mi><mn>3</mn></msub></math></formula>-norm are useful since they describe the performance of the whole schedule from the administrator point of view as well as they provide a fairness indication to the users.
The hard point here is to derive theoretical analysis for such complicated tools.</p>
        <p><b>Resource Augmentation</b>.
The classical resource augmentation models, i.e. speed and machine augmentation, are not sufficient to get good results when the execution of jobs cannot be frequently interrupted.
However, based on a resource augmentation model recently introduced, where the algorithm may reject a small number of jobs,
some members of our team have given the first interesting results in the non-preemptive direction.
In general, resource augmentation can explain the intuitive good behavior of some greedy algorithms while, more interestingly, it can give ideas for new algorithms. For example, in the rejection context we could dedicate a small number of nodes for the usually problematic rejected jobs.
Some initial experiments show that this can lead to a schedule for the remaining jobs that is very close to the optimal one.</p>
      </subsection>
    </subsection>
    <subsection id="uid22" level="1">
      <bodyTitle>Empirical Studies of Large Scale Platforms</bodyTitle>
      <p>Experiments or realistic simulations are required to take into account
the impact of allocations and assess the real behavior of scheduling algorithms.
While theoretical models still have their interest to lay the
groundwork for algorithmic designs, the models are necessarily
reflecting a purified view of the reality. As transferring our
algorithm in a more practical setting is an important part of our
creed, we need to ensure that the theoretical results found using
simplified models can really be transposed to real situations.
On the way to exascale computing, large scale systems become harder
to study, to develop or to calibrate because of the costs in both time
and energy of such processes. It is often impossible to convince
managers to use a production cluster for several hours simply to test
modifications in the RJMS. Moreover, as the existing RJMS production
systems need to be highly reliable, each evolution requires several
real scale test iterations. The consequence is that scheduling
algorithms used in production systems are mostly outdated and not
customized correctly. To circumvent this pitfall, we need to develop tools and methodologies for alternative empirical studies, from analysis of workload traces, to job models, simulation and emulation with reproducibility concerns.</p>
      <subsection id="uid23" level="2">
        <bodyTitle>Workload Traces with Resource Consumption</bodyTitle>
        <p>Workload traces are the base element to capture the behavior of complete systems composed of submitted jobs, running applications, and operating tools. These traces must be obtained on production platforms to provide relevant and representative data. To get a better understanding of the use of such systems, we need to look at both, how the jobs interact with the job management system, and how they use the allocated resources. We propose a general workload trace format that adds jobs resource consumption to the commonly used SWF <footnote id="uid24" id-text="6">Standard Workload Format: http://www.cs.huji.ac.il/labs/parallel/workload/swf.html</footnote> workload trace format. This requires to instrument the platforms, in particular to trace resource consumptions like CPU, data movements at memory, network and I/O levels, with an acceptable performance impact. In a previous work we studied and proposed a dedicated job monitoring tool whose impact on the system has been measured as lightweight (0.35<formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mo>%</mo></math></formula> speed-down) with a 1 minute sampling rate. Other tools also explore job monitoring, like TACC Stats. A unique feature from our tool is its ability to monitor distinctly jobs sharing common nodes.</p>
        <p>Collected workload traces with jobs resource consumption will be publicly released and serve to provide data for works presented in Section <ref xlink:href="#uid16" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. The trace analysis is expected to give valuable insights to define models encompassing complex behaviours like network topology sensitivity, network congestion and resource interferences.</p>
        <p>We expect to join efforts with partners for collecting quality traces (ATOS/Bull, Ciment meso center, Joint Laboratory on Extreme Scale Computing) and will collaborate with the Inria team POLARIS for their analysis.</p>
      </subsection>
      <subsection id="uid25" level="2">
        <bodyTitle>Simulation</bodyTitle>
        <p>Simulations of large scale systems are faster by multiple orders of
magnitude than real experiments. Unfortunately, replacing experiments
with simulations is not as easy as it may sound, as it brings a host
of new problems to address in order to ensure that the simulations are
closely approximating the execution of typical workloads on real
production clusters. Most of these problems are actually not directly
related to scheduling algorithms assessment, in the sense that the
workload and platform models should be defined independently from
the algorithm evaluations, in order to ensure a fair assessment of
the algorithms' strengths and weaknesses. These research
topics (namely platform modeling, job models and simulator calibration)
are addressed in the other subsections.</p>
        <p>We developed an open source platform simulator within
DataMove (in conjunction with the OAR development
team) to provide a widely distributable test
bed for reproducible scheduling algorithm evaluation. Our simulator,
named Batsim, allows to simulate the behavior of a computational
platform executing a workload scheduled by any given scheduling
algorithm. To obtain sound simulation results and to broaden the scope
of the experiments that can be done thanks to Batsim, we did not chose
to create a (necessarily limited) simulator from scratch, but instead
to build on top of the SimGrid simulation framework.</p>
        <p>To be open to as many batch schedulers as possible, Batsim
decouples the platform simulation and the scheduling decisions in two
clearly-separated software components communicating through a complete
and documented protocol. The Batsim component is in charge of
simulating the computational resources behaviour whereas the scheduler
component is in charge of taking scheduling decisions. The scheduler
component may be both a resource and a job management system. For
jobs, scheduling decisions can be to execute a job, to delay its
execution or simply to reject it. For resources, other decisions can
be taken, for example to change the power state of a machine i.e. to
change its speed (in order to lower its energy consumption) or to
switch it on or off. This separation of concerns also enables
interfacing with potentially any commercial RJMS, as long as the
communication protocol with Batsim is implemented. A proof of concept
is already available with the OAR RJMS.</p>
        <p>Using this test bed opens new research perspectives. It allows to test a
large range of platforms and workloads to better understand the real
behavior of our algorithms in a production setting. In turn, this
opens the possibility to tailor algorithms for a particular platform
or application, and to precisely identify the possible shortcomings of
the theoretical models used.</p>
      </subsection>
      <subsection id="uid26" level="2">
        <bodyTitle>Job and Platform Models</bodyTitle>
        <p>The central purpose of the Batsim simulator is to simulate job behaviors on a given target platform under a given resource allocation policy. Depending on the workload, a significant number of jobs are parallel applications with communications and file system accesses. It is not conceivable to simulate individually all these operations for each job on large plaforms with their associated workload due to implied simulation complexity. The challenge is to define a coarse grain job model accurate enough to reproduce parallel application behavior according to the target platform characteristics. We will explore models similar to the BSP (Bulk Synchronous Program) approach that decomposes an application in local computation supersteps ended by global communications and a global synchronization. The model parameters will be established by means of trace analysis as discussed previously, but also by instrumenting some parallel applications to capture communication patterns.
This instrumentation will have a significant impact on the concerned application performance, restricting its use to a few applications only. There are a lot of recurrent applications executed on HPC platform, this fact will help to reduce the required number of instrumentations and captures.
To assign each job a model, we are considering to adapt the concept of application signatures as proposed in.
Platform models and their calibration are also required. Large parts of these models, like those related to network, are provided by Simgrid. Other parts as the filesystem and energy models are comparatively recent and will need to be enhanced or reworked to reflect the HPC platform evolutions. These models are then generally calibrated by running suitable benchmarks.</p>
      </subsection>
      <subsection id="uid27" level="2">
        <bodyTitle>Emulation and Reproducibility</bodyTitle>
        <p>The use of coarse models in simulation implies to set aside some details. This simplification may hide system behaviors that could impact significantly and negatively the metrics we try to enhance. This issue is particularly relevant when large scale platforms are considered due to the impossibility to run tests at nominal scale on these real platforms. A common approach to circumvent this issue is the use of emulation techniques to reproduce, under certain conditions, the behavior of large platforms on smaller ones.
Emulation represents a natural complement to simulation by allowing to execute directly large parts of the actual evaluated software and system, but at the price of larger compute times and a need for more resources.
The emulation approach was chosen in to compare two job management systems from workload traces of the CURIE supercomputer (80000 cores). The challenge is to design methods and tools to emulate with sufficient accuracy the platform and the workload (data movement, I/O transfers, communication, applications interference). We will also intend to leverage emulation tools like Distem from the MADYNES team. It is also important to note that the Batsim simulator also uses emulation techniques to support the core scheduling module from actual RJMS. But the integration level is not the same when considering emulation for larger parts of the system (RJMS, compute node, network and filesystem).</p>
        <p>Replaying traces implies to prepare and manage complex software stacks including the OS, the resource management system, the distributed filesystem and the applications as well as the tools required to conduct experiments. Preparing these stacks generate specific issues, one of the major one being the support for reproducibility. We propose to further develop the concept of reconstructability to improve experiment reproducibility by capturing the build process of the complete software stack. This approach ensures reproducibility over time better than other ways
by keeping all data (original packages, build recipe and Kameleon engine) needed to build the software stack.</p>
        <p>In this context, the Grid'5000 (see Sec. <ref xlink:href="#uid42" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>) experimentation infrastructure that gives users the control on the complete software stack is a crucial tool for our research goals. We will pursue our strong implication in this infrastructure.
</p>
      </subsection>
    </subsection>
    <subsection id="uid28" level="1">
      <bodyTitle>Integration of High Performance Computing and Data Analytics</bodyTitle>
      <p>Data produced by large simulations are traditionally handled by an I/O layer that moves them from
the compute cores to the file system. Analysis of these data are performed after reading them back
from files, using some domain specific codes or some scientific visualisation libraries like VTK.
But writing and then reading back these data generates a lot of data movements and puts under pressure
the file system. To reduce these data movements, <b>the in situ analytics
paradigm proposes to process the data as closely as possible to where and when the data are
produced</b>. Some early solutions emerged either as extensions of visualisation
tools or of I/O libraries like ADIOS. But significant
progresses are still required to provide efficient and flexible high performance scientific data analysis
tools. Integrating data analytics in the HPC context will have an impact on resource allocation
strategies, analysis algorithms, data storage and access, as well as computer architectures and
software infrastructures. But this paradigm shift imposed by the machine performance also
sets the basis for a deep change on the way users work with numerical simulations.
The traditional workflow needs to be reinvented to make HPC more user-centric, more interactive
and turn HPC into a commodity tool for scientific discovery and engineering developments.
In this context DataMove aims at investigating programming environments for in situ analytics
with a specific focus on task scheduling in particular, to ensure an efficient sharing of resources with the simulation.</p>
      <subsection id="uid29" level="2">
        <bodyTitle>Programming Model and Software Architecture</bodyTitle>
        <p>In situ creates a tighter loop between the scientist and her/his simulation. As such, an in situ
framework needs to be flexible to let the user define and deploy its own set of analysis. A
manageable flexibility requires to favor simplicity and understandability, while still enabling an
efficient use of parallel resources. Visualization libraries like VTK
or Visit, as well as domain specific environments like VMD have initially been developed for traditional post-mortem
data analysis. They have been extended to support in situ processing with some simple resource
allocation strategies
but the level of performance, flexibility and
ease of use that is expected requires to rethink new environments. There is a need to develop a middleware and programming
environment taking into account in its fundations this specific context of high performance scientific
analytics.</p>
        <p>Similar needs for new data processing architectures occurred for the
emerging area of Big Data Analytics, mainly targeted to web data on
cloud-based infrastructures. Google Map/Reduce and its successors like Spark or
Stratosphere/Flink have been designed to match the specific context of
efficient analytics for large volumes of data produced on the web, on
social networks, or generated by business applications. These systems
have mainly been developed for cloud infrastructures based on commodity
architectures. They do not leverage the specifics of HPC
infrastructures. Some preliminary adaptations have been proposed for
handling scientific data in a HPC
context. However, these approaches do not support
in situ processing.</p>
        <p>Following the initial development of FlowVR, our middleware for in situ processing,
we will pursue our effort to develop a programming environment and software architecture for high
performance scientific data analytics. Like FlowVR, the map/reduce tools, as well as the machine
learning frameworks like TensorFlow, adopted a dataflow graph for
expressing analytics pipe-lines. We are convinced that this dataflow approach is both easy to
understand and yet expresses enough concurrency to enable efficient executions. The graph description
can be compiled towards lower level representations, a mechanism that is intensively used by
Stratosphere/Flink for instance. Existing in situ frameworks, including FlowVR, inherit from the
HPC way of programming with a thiner software stack and a programming model close to the machine.
Though this approach enables to program high performance applications, this is usually too low level
to enable the scientist to write its analysis pipe-line in a short amount of time. The data model, i.e. the data semantics level accessible at the framework level
for error check and optimizations, is also a fundamental aspect of such environments. The key/value store has been adopted by all map/reduce
tools. Except in some situations, it cannot be adopted as such for
scientific data. Results from numerical simulations are often more structured than web data,
associated with acceleration data structures to be processed
efficiently. We will investigate data models for scientific data building on existing approaches like Adios or DataSpaces.</p>
      </subsection>
      <subsection id="uid30" level="2">
        <bodyTitle>Resource Sharing</bodyTitle>
        <p>To alleviate the I/O bottleneck, the in situ paradigm proposes to start processing data as soon as
made available by the simulation, while still residing in the memory of the compute node. In situ
processings include data compression, indexing, computation of various types of descriptors (1D, 2D,
images, etc.). Per se, reducing data output to limit I/O related performance drops or keep the
output data size manageable is not new. Scientists have relied on solutions as simple as decreasing
the frequency of result savings. In situ processing proposes to move one step further, by providing a full
fledged processing framework enabling scientists to more easily and thoroughly manage the available
I/O budget.</p>
        <p>The most direct way to perform in situ analytics is to inline computations directly in the
simulation code. In this case, in situ processing is executed in sequence with the simulation that
is suspended meanwhile. Though this approach is direct to implement and does not require complex framework
environments, it does not enable to overlap analytics related computations and data movements with the simulation execution,
preventing to efficiently use the available resources. Instead of relying on this simple time sharing approach, several works propose to
rely on space sharing where one or several cores per node, called <i>helper cores</i>, are dedicated
to analytics. The simulation responsibility is simply to handle a copy of the relevant data to the
node-local in situ processes, both codes being executed concurrently. This approach often lead to significantly beter performance than in-simulation analytics.</p>
        <p>For a better isolation of the simulation and in situ processes, one solution consists in offloading in situ tasks from the simulation nodes towards
extra dedicated nodes, usually called <i>staging nodes</i>. These computations are said to be
performed <i>in-transit</i>. But this approach may not always be beneficial compared to processing on simulation nodes due to the costs of
moving the data from the simulation nodes to the staging nodes.</p>
        <p>FlowVR enables to mix these different resources allocation strategies for the different stages of
an analytics pile-line. Based on a component model, the scientist designs analytics workflows
by first developing processing components that are next assembled in a dataflow graph through a
Python script. At runtime the graph is instantiated according to the execution context, FlowVR
taking care of deploying the application on the target architecture, and of coordinating the
analytics workflows with the simulation execution.</p>
        <p>But today the choice of the resource allocation strategy is mostly ad-hoc and defined by the
programmer. We will investigate solutions that enable a cooperative use of the resource between the
analytics and the simulation with minimal hints from the programmer. In situ processings inherit
from the parallelization scale and data distribution adopted by the simulation, and must execute
with minimal perturbations on the simulation execution (whose actual resource usage is difficult to
know a priori). We need to develop adapted scheduling strategies that operate at compile and run
time. Because analysis are often data intensive, such solutions must take into consideration data
movements, a point that classical scheduling strategies designed first for compute intensive
applications often overlook. We expect to develop new scheduling strategies relying on the
methodologies developed in Section <ref xlink:href="#uid21" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. Simulations as well as analysis are iterative processes exposing a strong spatial and temporal coherency that we can take benefit of to anticipate their behavior and then take
more relevant resources allocation strategies, possibly based on advanced learning algorithms or as developed in Section <ref xlink:href="#uid16" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
        <p>In situ analytics represent a specific workload that needs to be scheduled very closely to the simulation, but not necessarily active during the full extent of the simulation execution and that may also require to access data from previous runs (stored in the file system or on specific burst-buffers). Several users may also need to run concurrent analytics pipe-lines on shared data. This departs significantly from the traditional batch scheduling model, motivating the need for a more elastic approach to resource provisioning. These issues will be conjointly addressed with research on batch scheduling policies (Section <ref xlink:href="#uid16" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>).</p>
      </subsection>
      <subsection id="uid31" level="2">
        <bodyTitle>Co-Design with Data Scientists</bodyTitle>
        <p>Given the importance of users in this context, it is of primary importance
that in situ tools be co-designed with advanced users, even if such multidisciplinary collaborations are challenging and require constant long term
investments to learn and understand the specific practices and expectations of the other domain.</p>
        <p>We will tightly collaborate with scientists of some application domains, like molecular dynamics or fluid simulation,
to design, develop, deploy and assess in situ analytics scenarios, as already done
with Marc Baaden, a computational biologist from LBT.</p>
        <p>We recently extended our collaboration network. We started in 2015 a PhD
co-advised with CEA DAM to investigate in situ analytics scenarios in the context of atomistic material simulations.
CEA DAM is a French energy lab hosting one of the largest european supercomputer.
They gather physicists, numerical scientists as well as high performance computer engineers,
making it a very interesting partner for developing new scientific data analysis solutions.
We also got a national grant (2015-2018) to compute in situ statistics for multi-parametric
parallel studies with the research department of French power company EDF.
In this context we collaborate with statisticians and fluid simulation experts to define in situ
scenarios, revisit the statistic operators to be amenable to in situ processing, and define an
adapted in situ framework.</p>
      </subsection>
    </subsection>
  </domaine>
  <logiciels id="uid32">
    <bodyTitle>New Software and Platforms</bodyTitle>
    <subsection id="uid33" level="1">
      <bodyTitle>OAR</bodyTitle>
      <p><span class="smallcap" align="left">Keywords:</span> HPC - Cloud - Clusters - Resource manager - Light grid</p>
      <p noindent="true"><span class="smallcap" align="left">Scientific Description</span>
This batch system is based on a database (PostgreSQL (preferred) or MySQL), a script language (Perl) and an optional scalable administrative tool (e.g. Taktuk). It is composed of modules which interact mainly via the database and are executed as independent programs. Therefore, formally, there is no API, the system interaction is completely defined by the database schema. This approach eases the development of specific modules. Indeed, each module (such as schedulers) may be developed in any language having a database access library.</p>
      <p noindent="true"><span class="smallcap" align="left">Functional Description</span>
OAR is a versatile resource and task manager (also called a batch scheduler) for HPC clusters, and other computing infrastructures (like distributed computing experimental testbeds where versatility is a key).</p>
      <p noindent="true">The OAR ecosystem also include several associated software tools that proved to be useful independently from OAR. Among theses, two softwares play a major role in the support our research studies. The first one is Kameleon (<ref xlink:href="http://kameleon.imag.fr" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>kameleon.<allowbreak/>imag.<allowbreak/>fr</ref>), a tool to help enhancing reproducibility of experiments by guarantee the ability to reproduce the complete used software stacks. The second one is Batsim (<ref xlink:href="https://gforge.inria.fr/projects/batsim" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>gforge.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>projects/<allowbreak/>batsim</ref>) a RJMS simulator based on SimGrid. Batsim simulates job execution taking into account the target platform hardware capabilities through SimGrid, while scheduling is performed by an actual job management system. A comprehensive API enables to easily plug into BatSim various job management systems like OAR.</p>
      <simplelist>
        <li id="uid34">
          <p noindent="true">Participants: Olivier Richard, Pierre Neyron, Salem Harrache and Bruno Bzeznik</p>
        </li>
        <li id="uid35">
          <p noindent="true">Partners: CIMENT - CNRS - Grid'5000 - LIG</p>
        </li>
        <li id="uid36">
          <p noindent="true">Contact: Olivier Richard</p>
        </li>
        <li id="uid37">
          <p noindent="true">URL: <ref xlink:href="http://oar.imag.fr" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>oar.<allowbreak/>imag.<allowbreak/>fr</ref></p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid38" level="1">
      <bodyTitle>FlowVR</bodyTitle>
      <p><span class="smallcap" align="left">Keywords:</span> HPC - In Situ Processing - Computational Steering</p>
      <p noindent="true"><span class="smallcap" align="left">Scientific Description</span>
FlowVR is an open source middelware to augment parallel simulations running on thousands of cores with in situ processing capabilities and live steering. FlowVR offers a very flexible environment while enabling high performance asynchronous in situ and in transit processing.</p>
      <p noindent="true"><span class="smallcap" align="left">Functional Description</span>
FlowVR adopts the "data-flow" paradigm, where your application is divided as a set of components exchanging messages (think of it as a directed graph). FlowVR enables to encapsulate existing codes in components, interconnect them through data channels, and deploy them on distributed computing resources. FlowVR takes care of all the heavy lifting such as application deployment and message exchange.</p>
      <simplelist>
        <li id="uid39">
          <p noindent="true">Participants: Bruno Raffin, Matthieu Dreher, Jérémy Jaussaud</p>
        </li>
        <li id="uid40">
          <p noindent="true">Contact: Bruno Raffin</p>
        </li>
        <li id="uid41">
          <p noindent="true">URL: <ref xlink:href="http://flowvr.sf.net" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>flowvr.<allowbreak/>sf.<allowbreak/>net</ref></p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid42" level="1">
      <bodyTitle>Platforms</bodyTitle>
      <subsection id="uid43" level="2">
        <bodyTitle>Grid'5000 (<ref xlink:href="https://www.grid5000.fr/" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>www.<allowbreak/>grid5000.<allowbreak/>fr/</ref>) and meso
center Ciment (<ref xlink:href="https://ciment.ujf-grenoble.fr" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>ciment.<allowbreak/>ujf-grenoble.<allowbreak/>fr</ref>) </bodyTitle>
        <p>We have been very active in promoting the factorization of compute
resources at a regional and national level. We have a three level
implication, locally to maintain a pool of very flexible
experimental machines (hundreds of cores), regionally through the
CIMENT meso center (Equipex Grant), and nationally by contributing
to the Grid'5000 platform, our local resources being included in
this platform. Olivier Richard is member of Grid'5000 scientific
committee and Pierre Neyron is member of the technical committee.
The OAR scheduler in particular is deployed on both infrastructures.
We are currently preparing proposals for the next generation machines within the context of the new university association (Univ. Grenoble-Alpes).</p>
      </subsection>
    </subsection>
  </logiciels>
  <resultats id="uid44">
    <bodyTitle>New Results</bodyTitle>
    <subsection id="uid45" level="1">
      <bodyTitle>In Situ Statistical Analysis for Parametric Studies</bodyTitle>
      <p>In situ processing proposes to reduce storage needs and I/O traffic by processing results of parallel simulations as soon as they are available in the memory of the compute processes. We focus in this paper <ref xlink:href="#datamove-2016-bid0" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> on computing in situ statistics on the results of N simulations from a parametric study. The classical approach consists in running various instances of the same simulation with different values of input parameters. Results are then saved to disks and statistics are computed post mortem, leading to very I/O intensive applications. Our solution is to develop Melissa, an in situ library running on staging nodes as a parallel server. When starting, simulations connect to Melissa and send the results of each time step to Melissa as soon as they are available. Melissa implements iterative versions of classical statistical operations, enabling to update results as soon as a new time step from a simulation is available. Once all statistics ar updated, the time step can be discarded. We also discuss two different approaches for scheduling simulation runs: the jobs-in-job and the multi-jobs approaches. Experiments run instances of the Computational Fluid Dynamics Open Source solver Code<formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mo>_</mo></math></formula>Saturne. They confirm that our approach enables one to avoid storing simulation results to disk or in memory.
</p>
    </subsection>
    <subsection id="uid46" level="1">
      <bodyTitle>Online Non-preemptive Scheduling in a Resource Augmentation Model based on Duality</bodyTitle>
      <p>Resource augmentation is a well-established model for analyzing algorithms, particularly in the
online setting. It has been successfully used for providing theoretical evidence for several heuristics
in scheduling with good performance in practice. According to this model, the algorithm is
applied to a more powerful environment than that of the adversary. Several types of resource
augmentation for scheduling problems have been proposed up to now, including speed augmentation,
machine augmentation and more recently rejection. In this paper <ref xlink:href="#datamove-2016-bid1" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, we present a framework
that unifies the various types of resource augmentation. Moreover, it allows generalize the notion
of resource augmentation for other types of resources. Our framework is based on mathematical
programming and it consists of extending the domain of feasible solutions for the algorithm with
respect to the domain of the adversary. This, in turn allows the natural concept of duality for
mathematical programming to be used as a tool for the analysis of the algorithm’s performance.
As an illustration of the above ideas, we apply this framework and we propose a primal-dual
algorithm for the online scheduling problem of minimizing the total weighted flow time of jobs on
unrelated machines when the preemption of jobs is not allowed. This is a well representative problem
for which no online algorithm with performance guarantee is known. Specifically, a strong
lower bound of <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mi>Ω</mi><mo>(</mo><mi>√</mi><mi>n</mi><mo>)</mo></mrow></math></formula> exists even for the offline unweighted version of the problem on a single
machine. In this paper, we first show a strong negative result even when speed augmentation is
used in the online setting. Then, using the generalized framework for resource augmentation and
by combining speed augmentation and rejection, we present an <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mo>(</mo><mn>1</mn><mo>+</mo><msub><mi>ϵ</mi><mi>s</mi></msub><mo>)</mo></mrow></math></formula>-speed <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mi>O</mi><mo>(</mo><mfrac><mn>1</mn><mrow><msub><mi>ϵ</mi><mi>s</mi></msub><msub><mi>ϵ</mi><mi>r</mi></msub></mrow></mfrac><mo>)</mo></mrow></math></formula>-competitive algorithm if we are allowed to reject jobs whose total weight is an <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msub><mi>ϵ</mi><mi>r</mi></msub></math></formula>-fraction of the weights of all jobs, for any <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msub><mi>ϵ</mi><mi>s</mi></msub></math></formula> &gt; 0 and <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><msub><mi>ϵ</mi><mi>r</mi></msub><mo>∈</mo><mrow><mo>(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>)</mo></mrow></mrow></math></formula>. Furthermore, we extend the idea for analysis of the above problem and we propose an <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mo>(</mo><mn>1</mn><mo>+</mo><msub><mi>ϵ</mi><mi>s</mi></msub><mo>)</mo></mrow></math></formula>-speed <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msub><mi>ϵ</mi><mi>r</mi></msub></math></formula>-rejection <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mi>O</mi><mo>(</mo><mfrac><msup><mi>k</mi><mfrac><mrow><mo>(</mo><mi>k</mi><mo>+</mo><mn>3</mn></mrow><mi>k</mi></mfrac></msup><mrow><msubsup><mi>ϵ</mi><mi>r</mi><mrow><mn>1</mn><mo>/</mo><mi>k</mi></mrow></msubsup><msubsup><mi>ϵ</mi><mi>s</mi><mfrac><mrow><mo>(</mo><mi>k</mi><mo>+</mo><mn>2</mn><mo>)</mo></mrow><mi>k</mi></mfrac></msubsup></mrow></mfrac><mo>)</mo></mrow></math></formula>-competitive algorithm for the more general objective of minimizing the weighted <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msub><mi>l</mi><mi>k</mi></msub></math></formula>-norm of the flow times of jobs.
</p>
    </subsection>
    <subsection id="uid47" level="1">
      <bodyTitle>Batsim: a Realistic Language-Independent Resources and Jobs Management Systems Simulator</bodyTitle>
      <p>As large scale computation systems are growing to exascale, Resources and Jobs Management Systems (RJMS) need to evolve to manage this scale modification. However, their study is problematic since they are critical production systems, where experimenting is extremely costly due to downtime and energy costs. Meanwhile, many scheduling algorithms emerging from theoretical studies have not been transferred to production tools for lack of realistic experimental validation. To tackle these problems we propose Batsim <ref xlink:href="#datamove-2016-bid2" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, an extendable, language-independent and scalable RJMS simulator. It allows researchers and engineers to test and compare any scheduling algorithm, using a simple event-based communication interface, which allows different levels of realism. In this paper we show that Batsim's behavior matches the one of the real RJMS OAR. Our evaluation process was made with reproducibility in mind and all the experiment material is freely available.
</p>
    </subsection>
  </resultats>
  <contrats id="uid48">
    <bodyTitle>Bilateral Contracts and Grants with Industry</bodyTitle>
    <subsection id="uid49" level="1">
      <bodyTitle>Bilateral Contracts with Industry</bodyTitle>
      <p><b>BULL-ATOS SE (2015-2018)</b>. Two PhD grants (David Glesser and Michael Mercier). Job and resource management algorithms.
</p>
    </subsection>
    <subsection id="uid50" level="1">
      <bodyTitle>Bilateral Grants with Industry</bodyTitle>
      <p><b>CEA DAM (2016-2018)</b>. PhD grant support contract (PhD of Estelle Dirand, funded by CEA). In situ analysis for Molecular Simulations.
</p>
    </subsection>
  </contrats>
  <partenariat id="uid51">
    <bodyTitle>Partnerships and Cooperations</bodyTitle>
    <subsection id="uid52" level="1">
      <bodyTitle>National Initiatives</bodyTitle>
      <subsection id="uid53" level="2">
        <bodyTitle>ANR</bodyTitle>
        <simplelist>
          <li id="uid54">
            <p noindent="true"><b>ANR grant MOEBIUS (2013-2017).</b> Multi-objective
scheduling for large computing platforms. Coordinator: Grenoble-INP
(DataMove). Partners: Grenoble-INP, Inria, BULL-ATOS .</p>
          </li>
        </simplelist>
      </subsection>
      <subsection id="uid55" level="2">
        <bodyTitle>Competitivity Clusters</bodyTitle>
        <simplelist>
          <li id="uid56">
            <p noindent="true"><b>PIA Avido (2015-2018)</b>. In situ analysis and visualization
for large scale numerical simulation. Coordinator: EDF SA. Partners: EDF SA, Total SA,
Kitware SAS , Université Pierre et Marie CURIE, Inria (DataMove).</p>
          </li>
          <li id="uid57">
            <p noindent="true"><b>FUI OverMind (2015-2017)</b>. Task planification and asset
management for the cartoon productions. Coordinator: Teamto Studio.
Partners: Teamto Studio, Folimage Studio, Ecole de Gobelins, Inria
(DataMove).</p>
          </li>
        </simplelist>
      </subsection>
    </subsection>
    <subsection id="uid58" level="1">
      <bodyTitle>European Initiatives</bodyTitle>
      <subsection id="uid59" level="2">
        <bodyTitle>FP7 &amp; H2020 Projects</bodyTitle>
        <subsection id="uid60" level="3">
          <bodyTitle>VELaSSCo</bodyTitle>
          <sanspuceslist>
            <li id="uid61">
              <p noindent="true">Title: Visualization For Extremely Large-Scale Scientific Computing</p>
            </li>
            <li id="uid62">
              <p noindent="true">Program: STREP (Specific Targeted Research Project)</p>
            </li>
            <li id="uid63">
              <p noindent="true">Duration: January 2014 - December 2016</p>
            </li>
            <li id="uid64">
              <p noindent="true">Coordinator: Centre Internacional de Metodes Numerics en Enginyeria (Spain)</p>
            </li>
            <li id="uid65">
              <p noindent="true">Partners: JOTNE (No.), SINTEF (No.), Fraunhofer IGD (D), ATOS (SP), Univ. Edinburgh (UK)</p>
            </li>
            <li id="uid66">
              <p noindent="true">Inria contact: Toan Nguyen, Bruno Raffin</p>
            </li>
            <li id="uid67">
              <p noindent="true">Abstract: VELaSSCo aims at developing a new concept of integrated end-user visual analysis methods with advanced management and post-processing algorithms for engineering modelling applications, scalable for real-time petabyte level simulations [59]. The interface will enable real- time interrogation of simulation data, generating key information for analysis. Main concerns have to do with handling of large amounts of data of a very specific kind intrinsically linked to geometrical properties; how to store, access, simplify and manipulate billion of records to extract the relevant information; how to represent information in a feasible and flexible way; and how to visualise and interactively inspect the huge quantity of information they produce taking into account end- user’s needs. VELaSSCo achieves this by putting together experts with relevant background in Big Data handling, advanced visualisation, engineering simulations, and a User Panel including research centres, SMEs and companies form key European industrial sectors such as aerospace, household products, chemical, pharmaceutical and civil engineering.</p>
            </li>
          </sanspuceslist>
        </subsection>
      </subsection>
    </subsection>
    <subsection id="uid68" level="1">
      <bodyTitle>International Initiatives</bodyTitle>
      <subsection id="uid69" level="2">
        <bodyTitle>Inria International Labs</bodyTitle>
        <subsection id="uid70" level="3">
          <bodyTitle>JLESC</bodyTitle>
          <sanspuceslist>
            <li id="uid71">
              <p noindent="true">Title: Joint Laboratory for Extreme-Scale-Computing.</p>
            </li>
            <li id="uid72">
              <p noindent="true">International Partners:</p>
              <sanspuceslist>
                <li id="uid73">
                  <p noindent="true">University of Illinois at Urbana Champaign (USA)</p>
                </li>
                <li id="uid74">
                  <p noindent="true">Argonne National Laboratory (USA),</p>
                </li>
                <li id="uid75">
                  <p noindent="true">Barcelona Supercomputing Center (Spain),</p>
                </li>
                <li id="uid76">
                  <p noindent="true">Jülich Supercomputing Centre (Germany)</p>
                </li>
                <li id="uid77">
                  <p noindent="true">Riken Advanced Institute for Computational Science (Japan)</p>
                </li>
              </sanspuceslist>
            </li>
            <li id="uid78">
              <p noindent="true">Start year: 2009</p>
            </li>
            <li id="uid79">
              <p noindent="true">See also: <ref xlink:href="https://jlesc.github.io/" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>jlesc.<allowbreak/>github.<allowbreak/>io/</ref></p>
            </li>
            <li id="uid80">
              <p noindent="true">The purpose of the Joint Laboratory for Extreme Scale Computing is to be an international, virtual organization whose goal is to enhance the ability of member organizations and investigators to make the bridge between Petascale and Extreme computing. The JLESC organizes a workshop every 6 months DataMove participates to. DataMove developed several collaborations related to in situ processing with Tom Peterka group (ANL) , the Argo exascale operating system with Swann Perarnau (ANL).</p>
            </li>
          </sanspuceslist>
        </subsection>
        <subsection id="uid81" level="3">
          <bodyTitle>ANOMALIES@EXASCALE</bodyTitle>
          <sanspuceslist>
            <li id="uid82">
              <p noindent="true">Title: Anomalies Detection and Handling towards Exascale Platforms</p>
            </li>
            <li id="uid83">
              <p noindent="true">International Partner:</p>
              <sanspuceslist>
                <li id="uid84">
                  <p noindent="true">University of Chicago (United States) - Argonne National Laboratory (ANL)</p>
                </li>
              </sanspuceslist>
            </li>
            <li id="uid85">
              <p noindent="true">Start year: 2014. End year: 2016.</p>
            </li>
            <li id="uid86">
              <p noindent="true">See also: <ref xlink:href="http://anomalies.imag.fr" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>anomalies.<allowbreak/>imag.<allowbreak/>fr</ref></p>
            </li>
            <li id="uid87">
              <p noindent="true">The Anomalies@exascale project intends to prospect new scheduling solutions for very large parallel computing platforms. In particular, we consider the new problems related to fault tolerance raising with the developments of exascale platforms. We expect to define new ways to detect both execution failures and more transient performance anomalies. Information gathered from the detectors will then be taken into account by schedulers to implement corrective measures. PI: Frederic Wagner</p>
            </li>
          </sanspuceslist>
        </subsection>
      </subsection>
      <subsection id="uid88" level="2">
        <bodyTitle>Inria Associate Teams Not Involved in an Inria International Labs</bodyTitle>
        <subsection id="uid89" level="3">
          <bodyTitle>ExaSE</bodyTitle>
          <sanspuceslist>
            <li id="uid90">
              <p noindent="true">Title: Exascale Computing Scheduling and Energy</p>
            </li>
            <li id="uid91">
              <p noindent="true">International Partners:</p>
              <sanspuceslist>
                <li id="uid92">
                  <p noindent="true">UFRGS, PUC Minas and UPS (Brazil)</p>
                </li>
              </sanspuceslist>
            </li>
            <li id="uid93">
              <p noindent="true">Duration: 2014 - 2016</p>
            </li>
            <li id="uid94">
              <p noindent="true">See also: <ref xlink:href="https://team.inria.fr/exase/" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>team.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>exase/</ref></p>
            </li>
            <li id="uid95">
              <p noindent="true">The main scientific context of this project is high performance computing on Exascale systems: large-scale machines with billions of processing cores and complex hierarchical structures. This project intends to explore the relationship between scheduling algorithms and techniques and the energy constraints present on such exascale systems. PI: Jean-Marc Vincent (Polaris)</p>
            </li>
          </sanspuceslist>
        </subsection>
      </subsection>
      <subsection id="uid96" level="2">
        <bodyTitle>Participation in Other International Programs</bodyTitle>
        <subsection id="uid97" level="3">
          <bodyTitle>LICIA</bodyTitle>
          <sanspuceslist>
            <li id="uid98">
              <p noindent="true">Title: International Laboratory in High Performance and Ubiquitous Computing</p>
            </li>
            <li id="uid99">
              <p noindent="true">International Partner (Institution - Laboratory - Researcher):</p>
              <sanspuceslist>
                <li id="uid100">
                  <p noindent="true">UFRGS (Brazil)</p>
                </li>
              </sanspuceslist>
            </li>
            <li id="uid101">
              <p noindent="true">Duration: 2011 - 2018</p>
            </li>
            <li id="uid102">
              <p noindent="true">See also: <ref xlink:href="http://licia-lab.org/" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>licia-lab.<allowbreak/>org/</ref></p>
            </li>
            <li id="uid103">
              <p noindent="true">The LICIA is an Internacional Laboratory and High Performance
and Ubiquitous Computing born in 2011 from the common desire of
members of Informatics Institute of the Federal University of Rio
Grande do Sul and of Laboratoire d’Informatique de Grenoble to
enhance and develop their scientific partnership that started by
the end of the 1970. LICIA is an Internacional Associated Lab of
the CNRS, a public french research institution. It has support from
several brazilian and french research funding agencies, such as
CNRS, Inria, ANR, European Union (from the french side) and CAPES,
CNPq, FAPERGS (from the Brazilian side). DataMove is deeply involved
in the animation of LICIA. Bruno Raffin is LICIA associate director.</p>
            </li>
          </sanspuceslist>
        </subsection>
        <subsection id="uid104" level="3">
          <bodyTitle>CAPES/COFECUB StarShip</bodyTitle>
          <sanspuceslist>
            <li id="uid105">
              <p noindent="true">Title: Scalable Tools and Algorithms para Resilient, Scalable, Hybrid Interactive Processing</p>
            </li>
            <li id="uid106">
              <p noindent="true">International Partner (Institution - Laboratory - Researcher):</p>
              <sanspuceslist>
                <li id="uid107">
                  <p noindent="true">UFRGS (Brazil)</p>
                </li>
              </sanspuceslist>
            </li>
            <li id="uid108">
              <p noindent="true">Duration: 2013 - 2016</p>
            </li>
            <li id="uid109">
              <p noindent="true">PI: Bruno Raffin (DataMove) and Alexandre Carissimi (UFRGS)</p>
            </li>
          </sanspuceslist>
        </subsection>
      </subsection>
    </subsection>
    <subsection id="uid110" level="1">
      <bodyTitle>International Research Visitors</bodyTitle>
      <subsection id="uid111" level="2">
        <bodyTitle>Internships</bodyTitle>
        <sanspuceslist>
          <li id="uid112">
            <p noindent="true">PhD in progress: Marcos Amaris Gonzalez, Performance Evaluation for GPU, USP (Sao Paulo, Brasil). 1 year "sandwich" visit. Local adviser: Denis Trystram</p>
          </li>
        </sanspuceslist>
      </subsection>
      <subsection id="uid113" level="2">
        <bodyTitle>Visits to International Teams</bodyTitle>
        <simplelist>
          <li id="uid114">
            <p noindent="true">Pierre François Dutot. Six month stay at University of Hawaii at Manoa (Sept. 2016 - Jan. 2017)</p>
          </li>
        </simplelist>
      </subsection>
    </subsection>
  </partenariat>
  <diffusion id="uid115">
    <bodyTitle>Dissemination</bodyTitle>
    <subsection id="uid116" level="1">
      <bodyTitle>Promoting Scientific Activities</bodyTitle>
      <subsection id="uid117" level="2">
        <bodyTitle>Scientific Events Organisation</bodyTitle>
        <subsection id="uid118" level="3">
          <bodyTitle>General Chair, Scientific Chair</bodyTitle>
          <simplelist>
            <li id="uid119">
              <p noindent="true">Euro-Par, Grenoble, August 2016: General chair and local organization.</p>
            </li>
            <li id="uid120">
              <p noindent="true">HCW'2016 (25th IEEE Heterogeneous Computing Workshop), Hyderabad, May 2016a: General Chair</p>
            </li>
          </simplelist>
        </subsection>
        <subsection id="uid121" level="3">
          <bodyTitle>Member of the Organizing Committees</bodyTitle>
          <simplelist>
            <li id="uid122">
              <p noindent="true">EGPGV (Eurographics Symposium on Parallel Rendering and Visualization): President of the steering committee.</p>
            </li>
          </simplelist>
        </subsection>
      </subsection>
      <subsection id="uid123" level="2">
        <bodyTitle>Scientific Events Selection</bodyTitle>
        <subsection id="uid124" level="3">
          <bodyTitle>Chair of Conference Program Committees</bodyTitle>
          <sanspuceslist>
            <li id="uid125">
              <p noindent="true">Euro-Par, Grenoble, August 2016: Topic chair.</p>
            </li>
          </sanspuceslist>
        </subsection>
        <subsection id="uid126" level="3">
          <bodyTitle>Member of the Conference Program Committees</bodyTitle>
          <sanspuceslist>
            <li id="uid127">
              <p noindent="true">ISAV 2016, November 2016, Salt Lake City, USA</p>
            </li>
            <li id="uid128">
              <p noindent="true">2nd IEEE BidDataSecurity, April 8-10 2016, New York, USA</p>
            </li>
            <li id="uid129">
              <p noindent="true">IPDPS 2016 (27th IEEE International Parallel &amp; Distributed Processing Symposium), May 23-27 2016, Chicago, USA</p>
            </li>
            <li id="uid130">
              <p noindent="true">CloudTech, My 24-26 2016, Marrakech, Marocco</p>
            </li>
            <li id="uid131">
              <p noindent="true">COMPAS, July 5-8 2016, Lorient, France</p>
            </li>
            <li id="uid132">
              <p noindent="true">PMAA'16 (10th internat. workshop on Parallel Matrix Algorithms and Applications), July 6-8 2016, Bordeaux, France</p>
            </li>
            <li id="uid133">
              <p noindent="true">ISPDC (15th Internat Symposium on Parallel and Distributed Computing), July 8-10 2016, Fuzhou, China</p>
            </li>
            <li id="uid134">
              <p noindent="true">EuroMPI, September 25-28 2016, Edinburgh, Scotland, UK</p>
            </li>
            <li id="uid135">
              <p noindent="true">28th SBAC-PAD, October 26-28 2016, Los Angeles, USA,</p>
            </li>
            <li id="uid136">
              <p noindent="true">Edu-HPC (Workshop on Education for High-Performance Computing), November 2016, Salt Lake city, USA</p>
            </li>
            <li id="uid137">
              <p noindent="true">CloudCom, December 12-15 2016, Luxemburg</p>
            </li>
          </sanspuceslist>
        </subsection>
      </subsection>
      <subsection id="uid138" level="2">
        <bodyTitle>Journal</bodyTitle>
        <subsection id="uid139" level="3">
          <bodyTitle>Member of the Editorial Boards</bodyTitle>
          <sanspuceslist>
            <li id="uid140">
              <p noindent="true">Associate Editor of the Parallel Computing journal PARCO.</p>
            </li>
            <li id="uid141">
              <p noindent="true">Member of the Editorial Board of JPDC.</p>
            </li>
            <li id="uid142">
              <p noindent="true">Member of the Editorial Board of Computational Methods in
Science and Technology.</p>
            </li>
            <li id="uid143">
              <p noindent="true">Member of the Editorial Board of ARIMA (revue africaine de
recherche en informatique et maths appliquées).</p>
            </li>
            <li id="uid144">
              <p noindent="true">Member of the Editorial Board of IEEE Trans. Parallel and
Distributed Systems TPDS.</p>
            </li>
          </sanspuceslist>
        </subsection>
      </subsection>
      <subsection id="uid145" level="2">
        <bodyTitle>Scientific Expertise</bodyTitle>
        <sanspuceslist>
          <li id="uid146">
            <p noindent="true">ANR project evaluation expert</p>
          </li>
          <li id="uid147">
            <p noindent="true">Nederlands e-science center expert</p>
          </li>
        </sanspuceslist>
      </subsection>
      <subsection id="uid148" level="2">
        <bodyTitle>Research Administration</bodyTitle>
        <sanspuceslist>
          <li id="uid149">
            <p noindent="true">Executive committee member of Mathematics and Computer Science Council of Univ. Grenoble-Alpes (Membre du directoire du Conseil du Pôle MSTIC de l'UGA)</p>
          </li>
          <li id="uid150">
            <p noindent="true">Mathematics and Computer Science Council of Univ. Grenoble-Alpes Members (Membre du Conseil du Pôle MSTIC de l'UGA)</p>
          </li>
          <li id="uid151">
            <p noindent="true">Steering commitee of Grid'5000</p>
          </li>
        </sanspuceslist>
      </subsection>
    </subsection>
    <subsection id="uid152" level="1">
      <bodyTitle>Teaching - Supervision - Juries</bodyTitle>
      <subsection id="uid153" level="2">
        <bodyTitle>Teaching</bodyTitle>
        <sanspuceslist>
          <li id="uid154">
            <p noindent="true">Master: Denis Trystram is responsible of the first year (M1) of the international
Master of Science in Informatics at Grenoble (MOSIG-M1).</p>
          </li>
          <li id="uid155">
            <p noindent="true">Master: D. Trystram, P.-F. Dutot, "Complexity, approximation theory and randomization" master course (M2) at Univ. Grenoble-Alpes</p>
          </li>
          <li id="uid156">
            <p noindent="true">Master: Pierre-François Dutot. 226 hours per year. Licence (first
and second year) at IUT2/UPMF (Institut Universitaire Technologique
de Univ. Grenoble-Alpes) and 9 hours Master M2R-ISC
Informatique-Systèmes-Communication at Univ. Grenoble-Alpes.</p>
          </li>
          <li id="uid157">
            <p noindent="true">Master: Grégory Mounié. 242 hours per year. Master (M1/2nd year and M2/3rd year) at
Engineering school ENSIMAG, Grenoble-INP.</p>
          </li>
          <li id="uid158">
            <p noindent="true">Master: Bruno Raffin. 28 hours per year. Parallel System. International
Master of Science in Informatics at Grenoble (MOSIG-M2).</p>
          </li>
          <li id="uid159">
            <p noindent="true">Master: Olivier Richard. 222 hours per year. Master at
Engineering school Polytech-Grenoble, Univ. Grenoble-Alpes.</p>
          </li>
          <li id="uid160">
            <p noindent="true">Master: Denis Trystram. 200 hours per year in average, mainly at first level of Engineering School ENSIMAG, Grenoble-INP.</p>
          </li>
          <li id="uid161">
            <p noindent="true">Master: Frédéric Wagner. 220 hours per year. Engineering
school ENSIMAG, Grenoble-INP (M1/2nd year and M2/3rd year)
(190h), Master DESS/M2-P SCCI Security (30h).</p>
          </li>
        </sanspuceslist>
      </subsection>
      <subsection id="uid162" level="2">
        <bodyTitle>Supervision</bodyTitle>
        <sanspuceslist>
          <li id="uid163">
            <p noindent="true">PhD: David Glesser, Energy Aware Resource Management for HPC, Univ. Grenoble-Alpes. Defended November 2016. Advisers: Denis Trystram and Yianis Georgiou (ATOS/BULL)</p>
          </li>
          <li id="uid164">
            <p noindent="true">PhD : Marwa Sridi, Un modèle de structure de données Cache-aware pour un parallélisme et un l'équilibrage dynamique de la charge, Univ Grenoble-Alpes. Defended April 2016. Advisers: Bruno Raffin, Vincent Faucher (CEA) and Thierry Gautier.</p>
          </li>
          <li id="uid165">
            <p noindent="true">PhD in progress : Julio Toss, Parallel Algorithms and Data
Structures for Physically Based Simulation of Deformable Objects, Univ. Grenoble-Alpes and UFRGS (co-tutelle). Started October 2013. Advisers: Bruno Raffin and Joao Comba (UFRGS).</p>
          </li>
          <li id="uid166">
            <p noindent="true">PhD in progress : Estelle Dirand, Integration of High-Performance Data Analytics and IOs for Molecular Dynamics on Exascale Computer, Univ. Grenoble-Alpes. Started January 2016. Advisers: Bruno Raffin and Laurent Colombet (CEA).</p>
          </li>
          <li id="uid167">
            <p noindent="true">PhD in progress: Michael Mercier, Resource Management and Job Scheduling in HPC–Cloud environments towards the Big Data era, Univ. Grenoble Alpes. Started October 2016. Advisers: Olivier Richard and Bruno Raffin.</p>
          </li>
          <li id="uid168">
            <p noindent="true">PhD in progress: Raphaël Bleuse, Affinity Scheduling, Univ. Grenoble-Alpes. Started October 2013. Adviser: Denis Trystram and Gregory Mounié.</p>
          </li>
          <li id="uid169">
            <p noindent="true">PhD in progress: Millian Poquet, Energy consumption optimization for high performance computing, Univ. Grenoble-Alpes. Started October 2014. Advisers: Denis Trystram and Pierre-François Dutot</p>
          </li>
          <li id="uid170">
            <p noindent="true">PhD in progress: Valentin Reis, Machine Learning for resource management, Univ. Grenoble-Alpes. Started October 2015. Advisers: Denis Trystram and Eric Gaussier</p>
          </li>
          <li id="uid171">
            <p noindent="true">PhD in progress: Abhinav Srivastav, Multi-objective Scheduling, Univ. Grenoble-Alpes. Started October 2015. Advisers: Denis Trystram and Oded Maler</p>
          </li>
          <li id="uid172">
            <p noindent="true">PhD in progress: Alessandro Kraemer, Scheduling in the Cloud, Univ Grenoble-Alpes and UFPR (co-tutelle). Started October 2014. Advisers: Olivier Richard and Denis Trystram.</p>
          </li>
          <li id="uid173">
            <p noindent="true">PhD in progress: Fernando Machado Mendonca, Locality Aware Scheduling, Univ. Grenoble-Alpes, Advisers: Frederic Wagner and Denis Trystram.</p>
          </li>
          <li id="uid174">
            <p noindent="true">PhD in progress: Mohammed Khatiri, Tasks scheduling on heterogeneous Multicore, Univ. Grenoble-Alpes and University Mohammed First (co-tutelle), Advisers: Denis Trystram, El Mostafa DAOUDI (University Mohammed First, Oujda, Maroc)</p>
          </li>
        </sanspuceslist>
      </subsection>
      <subsection id="uid175" level="2">
        <bodyTitle>Juries</bodyTitle>
        <sanspuceslist>
          <li id="uid176">
            <p noindent="true">PhD Defense of François Lehericey, 20th of September 2016. Jury Member. Ray Tracing Based Collision Detection: A quest for Performance. Université Bretagne Loire.</p>
          </li>
          <li id="uid177">
            <p noindent="true">PhD Defense of Rémy Dautriche, 20th of October 2016. Jury President. Multi-scale Interaction Techniques for the Interactive Visualization of Execution Traces. Univ Grenoble-Alpes.</p>
          </li>
          <li id="uid178">
            <p noindent="true">PhD Defense of Xiaohu Wu, 16th of February 2016. Reviewer. University of Nice.</p>
          </li>
          <li id="uid179">
            <p noindent="true">PhD Defense of Ziad Sultan, 17th of June 2016. Jury President. Parallel Generic and Adaptive Exacte Linear Algebra. Univ. Grenoble-Alpes</p>
          </li>
        </sanspuceslist>
      </subsection>
    </subsection>
    <subsection id="uid180" level="1">
      <bodyTitle>Popularization</bodyTitle>
      <sanspuceslist>
        <li id="uid181">
          <p noindent="true">In conjonction with the Polaris team, the team (Seniors and PhDs) participates to
several "computer science with hands" events, notably the "Fête de
la science", every year, but also some visits of classes from high
school of the area along the year at Inria Rhône-Alpes.</p>
        </li>
        <li id="uid182">
          <p noindent="true">Talk at the ISN conference organized by Inria, dedicated to
present the computer science to teachers of High School</p>
        </li>
      </sanspuceslist>
    </subsection>
  </diffusion>
  <biblio id="bibliography" html="bibliography" numero="10" titre="Bibliography">
    
    <biblStruct id="datamove-2016-bid13" type="phdthesis" rend="year" n="cite:glesser:tel-01425620">
      <identifiant type="hal" value="tel-01425620"/>
      <monogr>
        <title level="m">Road to exascale: Improving scheduling performances and reducing energy consumption with the help of end-users</title>
        <author>
          <persName key="moais-2014-idp124696">
            <foreName>David</foreName>
            <surname>Glesser</surname>
            <initial>D.</initial>
          </persName>
        </author>
        <imprint>
          <publisher>
            <orgName type="school">Univ. Grenoble Alpes</orgName>
          </publisher>
          <dateStruct>
            <month>October</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/tel-01425620" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>tel-01425620</ref>
        </imprint>
      </monogr>
      <note type="typdoc">Theses</note>
    </biblStruct>
    
    <biblStruct id="datamove-2016-bid14" type="phdthesis" rend="year" n="cite:sridi:tel-01430501">
      <identifiant type="hal" value="tel-01430501"/>
      <monogr>
        <title level="m">Cache Aware Dynamics Data Layout for Efficient Shared Memory Parallelisation</title>
        <author>
          <persName key="moais-2014-idp137136">
            <foreName>Marwa</foreName>
            <surname>Sridi</surname>
            <initial>M.</initial>
          </persName>
        </author>
        <imprint>
          <publisher>
            <orgName type="school">Université de Grenoble Alpes</orgName>
          </publisher>
          <dateStruct>
            <month>April</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/tel-01430501" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>tel-01430501</ref>
        </imprint>
      </monogr>
      <note type="typdoc">Theses</note>
    </biblStruct>
    
    <biblStruct id="datamove-2016-bid7" type="article" rend="year" n="cite:jansen:hal-01427256">
      <identifiant type="doi" value="10.1016/j.endm.2016.10.003"/>
      <identifiant type="hal" value="hal-01427256"/>
      <analytic>
        <title level="a">Scheduling parallel jobs on heterogeneous platforms</title>
        <author>
          <persName>
            <foreName>Klaus</foreName>
            <surname>Jansen</surname>
            <initial>K.</initial>
          </persName>
          <persName key="moais-2014-idp95616">
            <foreName>Denis</foreName>
            <surname>Trystram</surname>
            <initial>D.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid00521">
        <idno type="issn">1571-0653</idno>
        <title level="j">Electronic Notes in Discrete Mathematics</title>
        <imprint>
          <biblScope type="volume">55</biblScope>
          <dateStruct>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">9–12</biblScope>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01427256" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01427256</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="datamove-2016-bid5" type="article" rend="year" n="cite:toss:hal-01317549">
      <identifiant type="doi" value="10.1109/MCSE.2016.52"/>
      <identifiant type="hal" value="hal-01317549"/>
      <analytic>
        <title level="a">Parallel Voronoi Computation for Physics-Based Simulations</title>
        <author>
          <persName key="moais-2014-idp140888">
            <foreName>Julio</foreName>
            <surname>Toss</surname>
            <initial>J.</initial>
          </persName>
          <persName>
            <foreName>João</foreName>
            <surname>Comba</surname>
            <initial>J.</initial>
          </persName>
          <persName key="moais-2014-idp86304">
            <foreName>Bruno</foreName>
            <surname>Raffin</surname>
            <initial>B.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="no" x-international-audience="yes" id="rid00432">
        <idno type="issn">1521-9615</idno>
        <title level="j">Computing in Science and Engineering</title>
        <imprint>
          <biblScope type="volume">18</biblScope>
          <biblScope type="number">3</biblScope>
          <dateStruct>
            <month>May</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">88</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01317549" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01317549</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="datamove-2016-bid8" type="inproceedings" rend="year" n="cite:alessandro:hal-01432583">
      <identifiant type="hal" value="hal-01432583"/>
      <analytic>
        <title level="a">Reducing the Number of Response Time SLO Violations by a Cloud-HPC Convergence Scheduler</title>
        <author>
          <persName>
            <foreName>Kraemer</foreName>
            <surname>Alessandro</surname>
            <initial>K.</initial>
          </persName>
          <persName>
            <foreName>Carlos</foreName>
            <surname>Maziero</surname>
            <initial>C.</initial>
          </persName>
          <persName key="mescal-2014-idp71280">
            <foreName>Olivier</foreName>
            <surname>Richard</surname>
            <initial>O.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">2nd International Conference on Cloud Computing Technologies and Applications (CloudTech'16)</title>
        <loc>Marrakech, Morocco</loc>
        <imprint>
          <dateStruct>
            <month>May</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01432583" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01432583</ref>
        </imprint>
        <meeting id="cid624983">
          <title>International Conference of Cloud Computing Technologies and Applications</title>
          <num>2</num>
          <abbr type="sigle">CloudTech</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="datamove-2016-bid2" type="inproceedings" rend="year" n="cite:dutot:hal-01333471">
      <identifiant type="hal" value="hal-01333471"/>
      <analytic>
        <title level="a">Batsim: a Realistic Language-Independent Resources and Jobs Management Systems Simulator</title>
        <author>
          <persName key="moais-2014-idp90288">
            <foreName>Pierre-François</foreName>
            <surname>Dutot</surname>
            <initial>P.-F.</initial>
          </persName>
          <persName key="mescal-2014-idp79992">
            <foreName>Michael</foreName>
            <surname>Mercier</surname>
            <initial>M.</initial>
          </persName>
          <persName key="moais-2014-idp133408">
            <foreName>Millian</foreName>
            <surname>Poquet</surname>
            <initial>M.</initial>
          </persName>
          <persName key="mescal-2014-idp71280">
            <foreName>Olivier</foreName>
            <surname>Richard</surname>
            <initial>O.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">20th Workshop on Job Scheduling Strategies for Parallel Processing</title>
        <loc>Chicago, United States</loc>
        <imprint>
          <dateStruct>
            <month>May</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01333471" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01333471</ref>
        </imprint>
        <meeting id="cid624175">
          <title>Workshop on Job Scheduling Strategies for Parallel Processing</title>
          <num>20</num>
          <abbr type="sigle"/>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="datamove-2016-bid4" type="inproceedings" rend="year" n="cite:dutot:hal-01309052">
      <identifiant type="hal" value="hal-01309052"/>
      <analytic>
        <title level="a">Online Non-Preemptive Scheduling to Optimize Max Stretch on a Single Machine</title>
        <author>
          <persName key="moais-2014-idp90288">
            <foreName>Pierre-Francois</foreName>
            <surname>Dutot</surname>
            <initial>P.-F.</initial>
          </persName>
          <persName>
            <foreName>Erik</foreName>
            <surname>Saule</surname>
            <initial>E.</initial>
          </persName>
          <persName key="moais-2014-idp138376">
            <foreName>Abhinav</foreName>
            <surname>Srivastav</surname>
            <initial>A.</initial>
          </persName>
          <persName key="moais-2014-idp95616">
            <foreName>Denis</foreName>
            <surname>Trystram</surname>
            <initial>D.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="no" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">22nd International Computing and Combinatorics Conference (COCOON 2016)</title>
        <loc>Ho-Chi-Minh-Ville, Vietnam</loc>
        <imprint>
          <dateStruct>
            <month>August</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="http://hal.univ-grenoble-alpes.fr/hal-01309052" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>hal.<allowbreak/>univ-grenoble-alpes.<allowbreak/>fr/<allowbreak/>hal-01309052</ref>
        </imprint>
        <meeting id="cid106869">
          <title>International Computing and Combinatorics Conference</title>
          <num>22</num>
          <abbr type="sigle">COCOON</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="datamove-2016-bid1" type="inproceedings" rend="year" n="cite:lucarelli:hal-01334219">
      <identifiant type="doi" value="10.4230/LIPIcs.ESA.2016.63"/>
      <identifiant type="hal" value="hal-01334219"/>
      <analytic>
        <title level="a">Online Non-preemptive Scheduling in a Resource Augmentation Model based on Duality</title>
        <author>
          <persName key="moais-2014-idp105840">
            <foreName>Giorgio</foreName>
            <surname>Lucarelli</surname>
            <initial>G.</initial>
          </persName>
          <persName>
            <foreName>Nguyen</foreName>
            <surname>Kim Thang</surname>
            <initial>N.</initial>
          </persName>
          <persName key="moais-2014-idp138376">
            <foreName>Abhinav</foreName>
            <surname>Srivastav</surname>
            <initial>A.</initial>
          </persName>
          <persName key="moais-2014-idp95616">
            <foreName>Denis</foreName>
            <surname>Trystram</surname>
            <initial>D.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">European Symposium on Algorithms (ESA 2016)</title>
        <loc>Aarhus, Denmark</loc>
        <imprint>
          <biblScope type="volume">57</biblScope>
          <biblScope type="number">63</biblScope>
          <dateStruct>
            <month>August</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">1-17</biblScope>
          <ref xlink:href="http://hal.univ-grenoble-alpes.fr/hal-01334219" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>hal.<allowbreak/>univ-grenoble-alpes.<allowbreak/>fr/<allowbreak/>hal-01334219</ref>
        </imprint>
        <meeting id="cid30186">
          <title>Annual European Symposium on Algorithms</title>
          <num>24</num>
          <abbr type="sigle">ESA</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="datamove-2016-bid12" type="inproceedings" rend="year" n="cite:lucarelli:hal-01371023">
      <identifiant type="doi" value="10.1007/978-3-319-42634-1_41"/>
      <identifiant type="hal" value="hal-01371023"/>
      <analytic>
        <title level="a">From Preemptive to Non-preemptive Scheduling Using Rejections</title>
        <author>
          <persName key="moais-2014-idp105840">
            <foreName>Giorgio</foreName>
            <surname>Lucarelli</surname>
            <initial>G.</initial>
          </persName>
          <persName key="moais-2014-idp138376">
            <foreName>Abhinav</foreName>
            <surname>Srivastav</surname>
            <initial>A.</initial>
          </persName>
          <persName key="moais-2014-idp95616">
            <foreName>Denis</foreName>
            <surname>Trystram</surname>
            <initial>D.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">22nd International Computing and Combinatorics Conference (COCOON 2016)</title>
        <loc>Ho Chi Minh Ville, Vietnam</loc>
        <imprint>
          <biblScope type="volume">9797</biblScope>
          <dateStruct>
            <month>August</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">510-519</biblScope>
          <ref xlink:href="http://hal.univ-grenoble-alpes.fr/hal-01371023" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>hal.<allowbreak/>univ-grenoble-alpes.<allowbreak/>fr/<allowbreak/>hal-01371023</ref>
        </imprint>
        <meeting id="cid106869">
          <title>International Computing and Combinatorics Conference</title>
          <num>22</num>
          <abbr type="sigle">COCOON</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="datamove-2016-bid6" type="inproceedings" rend="year" n="cite:ngoko:hal-01427255">
      <identifiant type="doi" value="10.1109/IPDPSW.2016.68"/>
      <identifiant type="hal" value="hal-01427255"/>
      <analytic>
        <title level="a">An Automatic Tuning System for Solving NP-Hard Problems in Clouds</title>
        <author>
          <persName>
            <foreName>Yanik</foreName>
            <surname>Ngoko</surname>
            <initial>Y.</initial>
          </persName>
          <persName key="moais-2014-idp95616">
            <foreName>Denis</foreName>
            <surname>Trystram</surname>
            <initial>D.</initial>
          </persName>
          <persName key="moais-2015-idp96616">
            <foreName>Valentin</foreName>
            <surname>Reis</surname>
            <initial>V.</initial>
          </persName>
          <persName>
            <foreName>Christophe</foreName>
            <surname>Cérin</surname>
            <initial>C.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">IPDPSW 2016 - IEEE International Parallel and Distributed Processing Symposium Workshops</title>
        <loc>Chicago, United States</loc>
        <imprint>
          <dateStruct>
            <month>May</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">1443–1452</biblScope>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01427255" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01427255</ref>
        </imprint>
        <meeting id="cid87817">
          <title>IEEE International Parallel and Distributed Processing Symposium</title>
          <num>30</num>
          <abbr type="sigle">IPDPS</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="datamove-2016-bid10" type="inproceedings" rend="year" n="cite:sridi:hal-01420005">
      <identifiant type="doi" value="10.1016/j.procs.2016.05.413"/>
      <identifiant type="hal" value="hal-01420005"/>
      <analytic>
        <title level="a">Cache Aware Dynamics Data Layout for Efficient Shared Memory Parallelisation of EUROPLEXUS</title>
        <author>
          <persName key="moais-2014-idp137136">
            <foreName>Marwa</foreName>
            <surname>Sridi</surname>
            <initial>M.</initial>
          </persName>
          <persName key="moais-2014-idp86304">
            <foreName>Bruno</foreName>
            <surname>Raffin</surname>
            <initial>B.</initial>
          </persName>
          <persName>
            <foreName>Vincent</foreName>
            <surname>Faucher</surname>
            <initial>V.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">International Conference on Computational Science (ICCS)</title>
        <loc>San Diego, United States</loc>
        <title level="s">Procedia Computer Science</title>
        <imprint>
          <biblScope type="volume">80</biblScope>
          <dateStruct>
            <month>June</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">1083 - 1092</biblScope>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01420005" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01420005</ref>
        </imprint>
        <meeting id="cid115862">
          <title>International Conference on Computational Science</title>
          <num>13</num>
          <abbr type="sigle">ICCS</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="datamove-2016-bid0" type="inproceedings" rend="year" n="cite:terraz:hal-01383860">
      <identifiant type="hal" value="hal-01383860"/>
      <analytic>
        <title level="a">In Situ Statistical Analysis for Parametric Studies</title>
        <author>
          <persName key="hiepacs-2015-idp118504">
            <foreName>Théophile</foreName>
            <surname>Terraz</surname>
            <initial>T.</initial>
          </persName>
          <persName key="moais-2014-idp86304">
            <foreName>Bruno</foreName>
            <surname>Raffin</surname>
            <initial>B.</initial>
          </persName>
          <persName>
            <foreName>Alejandro</foreName>
            <surname>Ribes</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Yvan</foreName>
            <surname>Fournier</surname>
            <initial>Y.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">In Situ Infrastructures for Enabling Extreme-scale Analysis and Visualization (ISAV2016)</title>
        <loc>Salt Lake City, United States</loc>
        <imprint>
          <dateStruct>
            <month>November</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01383860" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01383860</ref>
        </imprint>
        <meeting id="cid624921">
          <title>Workshop on In Situ Infrastructures for Enabling Extreme-Scale Analysis and Visualization</title>
          <num>2</num>
          <abbr type="sigle"/>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="datamove-2016-bid3" type="techreport" rend="year" n="cite:bleuse:hal-01263100">
      <identifiant type="hal" value="hal-01263100"/>
      <monogr>
        <title level="m">Scheduling Independent Moldable Tasks on Multi-Cores with GPUs</title>
        <author>
          <persName key="moais-2014-idp118488">
            <foreName>Raphaël</foreName>
            <surname>Bleuse</surname>
            <initial>R.</initial>
          </persName>
          <persName>
            <foreName>Sascha</foreName>
            <surname>Hunold</surname>
            <initial>S.</initial>
          </persName>
          <persName>
            <foreName>Safia</foreName>
            <surname>Kedad-Sidhoum</surname>
            <initial>S.</initial>
          </persName>
          <persName key="moais-2014-idp130928">
            <foreName>Florence</foreName>
            <surname>Monna</surname>
            <initial>F.</initial>
          </persName>
          <persName key="moais-2014-idp92840">
            <foreName>Grégory</foreName>
            <surname>Mounié</surname>
            <initial>G.</initial>
          </persName>
          <persName key="moais-2014-idp95616">
            <foreName>Denis</foreName>
            <surname>Trystram</surname>
            <initial>D.</initial>
          </persName>
        </author>
        <imprint>
          <biblScope type="number">RR-8850</biblScope>
          <publisher>
            <orgName type="institution">Inria Grenoble Rhône-Alpes, Université de Grenoble</orgName>
          </publisher>
          <dateStruct>
            <month>January</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01263100" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01263100</ref>
        </imprint>
      </monogr>
      <note type="typdoc">Research Report</note>
    </biblStruct>
    
    <biblStruct id="datamove-2016-bid9" type="techreport" rend="year" n="cite:omidvartehrani:hal-01297763">
      <identifiant type="hal" value="hal-01297763"/>
      <monogr>
        <title level="m">Multi-Objective Group Discovery on the Social Web (Technical Report)</title>
        <author>
          <persName>
            <foreName>Behrooz</foreName>
            <surname>Omidvar-Tehrani</surname>
            <initial>B.</initial>
          </persName>
          <persName>
            <foreName>Sihem</foreName>
            <surname>Amer-Yahia</surname>
            <initial>S.</initial>
          </persName>
          <persName key="moais-2014-idp90288">
            <foreName>Pierre-Francois</foreName>
            <surname>Dutot</surname>
            <initial>P.-F.</initial>
          </persName>
          <persName key="moais-2014-idp95616">
            <foreName>Denis</foreName>
            <surname>Trystram</surname>
            <initial>D.</initial>
          </persName>
        </author>
        <imprint>
          <biblScope type="number">RR-LIG-052</biblScope>
          <publisher>
            <orgName type="institution">LIG</orgName>
          </publisher>
          <dateStruct>
            <month>April</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01297763" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01297763</ref>
        </imprint>
      </monogr>
      <note type="bnote">Les rapports de recherche du LIG - ISSN: 2105-0422</note>
      <note type="typdoc">Research Report</note>
    </biblStruct>
    
    <biblStruct id="datamove-2016-bid11" type="unpublished" rend="year" n="cite:amaris:hal-01420798">
      <identifiant type="hal" value="hal-01420798"/>
      <monogr>
        <title level="m">Generic algorithms for scheduling applications on hybrid multi-core machines</title>
        <author>
          <persName>
            <foreName>Marcos</foreName>
            <surname>Amaris</surname>
            <initial>M.</initial>
          </persName>
          <persName key="moais-2014-idp105840">
            <foreName>Giorgio</foreName>
            <surname>Lucarelli</surname>
            <initial>G.</initial>
          </persName>
          <persName key="datamove-2016-idp197648">
            <foreName>Clément</foreName>
            <surname>Mommessin</surname>
            <initial>C.</initial>
          </persName>
          <persName key="moais-2014-idp95616">
            <foreName>Denis</foreName>
            <surname>Trystram</surname>
            <initial>D.</initial>
          </persName>
        </author>
        <imprint>
          <dateStruct>
            <month>December</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01420798" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01420798</ref>
        </imprint>
      </monogr>
      <note type="bnote">working paper or preprint</note>
    </biblStruct>
  </biblio>
</raweb>
