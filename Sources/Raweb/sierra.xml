<?xml version="1.0" encoding="utf-8"?>
<raweb xmlns:xlink="http://www.w3.org/1999/xlink" xml:lang="en" year="2016">
  <identification id="sierra" isproject="true">
    <shortname>SIERRA</shortname>
    <projectName>Statistical Machine Learning and Parsimony</projectName>
    <theme-de-recherche>Optimization, machine learning and statistical methods</theme-de-recherche>
    <domaine-de-recherche>Applied Mathematics, Computation and Simulation</domaine-de-recherche>
    <urlTeam>http://www.di.ens.fr/sierra/</urlTeam>
    <structure_exterieure type="Labs">
      <libelle>Département d'Informatique de l'Ecole Normale Supérieure</libelle>
    </structure_exterieure>
    <structure_exterieure type="Organism">
      <libelle>CNRS</libelle>
    </structure_exterieure>
    <structure_exterieure type="Organism">
      <libelle>Ecole normale supérieure de Paris</libelle>
    </structure_exterieure>
    <header_dates_team>Creation of the Team: 2011 January 01, updated into Project-Team: 2012 January 01</header_dates_team>
    <LeTypeProjet>Project-Team</LeTypeProjet>
    <keywordsSdN>
      <term>1.2.8. - Network security</term>
      <term>3.4. - Machine learning and statistics</term>
      <term>5.4. - Computer vision</term>
      <term>6.2. - Scientific Computing, Numerical Analysis &amp; Optimization</term>
      <term>7.1. - Parallel and distributed algorithms</term>
      <term>7.3. - Optimization</term>
      <term>8.2. - Machine learning</term>
    </keywordsSdN>
    <keywordsSecteurs>
      <term>9.4.5. - Data science</term>
    </keywordsSecteurs>
    <UR name="Paris"/>
  </identification>
  <team id="uid1">
    <person key="sierra-2014-idm29296">
      <firstname>Francis</firstname>
      <lastname>Bach</lastname>
      <categoryPro>Chercheur</categoryPro>
      <research-centre>Paris</research-centre>
      <moreinfo>Team leader, Inria, Senior Researcher</moreinfo>
      <hdr>oui</hdr>
    </person>
    <person key="sierra-2015-idp82816">
      <firstname>Remi</firstname>
      <lastname>Leblond</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Paris</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="sierra-2014-idp65304">
      <firstname>Alexandre</firstname>
      <lastname>d'Aspremont</lastname>
      <categoryPro>Chercheur</categoryPro>
      <research-centre>Paris</research-centre>
      <moreinfo>CNRS, Senior Researcher</moreinfo>
      <hdr>oui</hdr>
    </person>
    <person key="sierra-2014-idp67776">
      <firstname>Anton</firstname>
      <lastname>Osokin</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Paris</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="sierra-2014-idp97752">
      <firstname>Fabian</firstname>
      <lastname>Pedregosa</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Paris</research-centre>
      <moreinfo>Chaire Havas Dauphine</moreinfo>
    </person>
    <person key="sierra-2016-idp123024">
      <firstname>Kevin</firstname>
      <lastname>Scaman</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Paris</research-centre>
      <moreinfo>Inria, from Nov 2016</moreinfo>
    </person>
    <person key="sierra-2014-idp84192">
      <firstname>Jean-Baptiste</firstname>
      <lastname>Alayrac</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Paris</research-centre>
      <moreinfo>Ecole Polytechnique</moreinfo>
    </person>
    <person key="sierra-2015-idp70512">
      <firstname>Dmitry</firstname>
      <lastname>Babichev</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Paris</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="sierra-2015-idp71752">
      <firstname>Anaël</firstname>
      <lastname>Bonneton</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Paris</research-centre>
      <moreinfo>ENS Paris</moreinfo>
    </person>
    <person key="sierra-2014-idp79056">
      <firstname>Alexandre</firstname>
      <lastname>Defossez</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Paris</research-centre>
      <moreinfo>CIFRE Facebook</moreinfo>
    </person>
    <person key="sierra-2014-idp85448">
      <firstname>Aymeric</firstname>
      <lastname>Dieuleveut</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Paris</research-centre>
      <moreinfo>ENS Paris</moreinfo>
    </person>
    <person key="sierra-2014-idp86672">
      <firstname>Christophe</firstname>
      <lastname>Dupuy</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Paris</research-centre>
      <moreinfo>CIFRE Technicolor</moreinfo>
    </person>
    <person key="sierra-2014-idp87896">
      <firstname>Nicolas</firstname>
      <lastname>Flammarion</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Paris</research-centre>
      <moreinfo>ENS Lyon</moreinfo>
    </person>
    <person key="sierra-2014-idp90344">
      <firstname>Damien</firstname>
      <lastname>Garreau</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Paris</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="sierra-2014-idp98984">
      <firstname>Anastasia</firstname>
      <lastname>Podosinnikova</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Paris</research-centre>
      <moreinfo>Inria, granted by Microsoft Research</moreinfo>
    </person>
    <person key="sierra-2015-idp85288">
      <firstname>Antoine</firstname>
      <lastname>Recanati</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Paris</research-centre>
      <moreinfo>CNRS</moreinfo>
    </person>
    <person key="sierra-2014-idp100216">
      <firstname>Vincent</firstname>
      <lastname>Roulet</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Paris</research-centre>
      <moreinfo>Ecole Polytechnique</moreinfo>
    </person>
    <person key="sierra-2015-idp88984">
      <firstname>Damien</firstname>
      <lastname>Scieur</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Paris</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="sierra-2015-idp90224">
      <firstname>Tatiana</firstname>
      <lastname>Shpakova</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Paris</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="sierra-2014-idp70264">
      <firstname>Amit</firstname>
      <lastname>Bermanis</lastname>
      <categoryPro>PostDoc</categoryPro>
      <research-centre>Paris</research-centre>
      <moreinfo>Inria, until Jul 2016</moreinfo>
    </person>
    <person key="sierra-2014-idp71528">
      <firstname>Nicolas</firstname>
      <lastname>Boumal</lastname>
      <categoryPro>PostDoc</categoryPro>
      <research-centre>Paris</research-centre>
      <moreinfo>Research in Paris, until Jan 2016</moreinfo>
    </person>
    <person key="sierra-2015-idp93952">
      <firstname>Pascal</firstname>
      <lastname>Germain</lastname>
      <categoryPro>PostDoc</categoryPro>
      <research-centre>Paris</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="sierra-2016-idp164528">
      <firstname>Robert</firstname>
      <lastname>Gower</lastname>
      <categoryPro>PostDoc</categoryPro>
      <research-centre>Paris</research-centre>
      <moreinfo>Inria, from Aug 2016</moreinfo>
    </person>
    <person key="sierra-2015-idp96448">
      <firstname>Balamurugan</firstname>
      <lastname>Palaniappan</lastname>
      <categoryPro>PostDoc</categoryPro>
      <research-centre>Paris</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="sierra-2016-idp169456">
      <firstname>Federico</firstname>
      <lastname>Vaggi</lastname>
      <categoryPro>PostDoc</categoryPro>
      <research-centre>Paris</research-centre>
      <moreinfo>ENS Paris, from May 2016</moreinfo>
    </person>
    <person key="sierra-2016-idp171936">
      <firstname>Chiranjib</firstname>
      <lastname>Bhattacharyya</lastname>
      <categoryPro>Visiteur</categoryPro>
      <research-centre>Paris</research-centre>
      <moreinfo>Associated Team Bigfoks2, Nov 2016</moreinfo>
    </person>
    <person key="sierra-2014-idp92792">
      <firstname>Remi</firstname>
      <lastname>Lajugie</lastname>
      <categoryPro>Visiteur</categoryPro>
      <research-centre>Paris</research-centre>
      <moreinfo>ENS Cachan, until Feb 2016</moreinfo>
    </person>
    <person key="sierra-2016-idp176896">
      <firstname>Reza</firstname>
      <lastname>Babanezhad Harikandeh</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Paris</research-centre>
      <moreinfo>Inria, from Jun 2016 until Sep 2016</moreinfo>
    </person>
    <person key="sierra-2014-idp89120">
      <firstname>Fajwel</firstname>
      <lastname>Fogel</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Paris</research-centre>
      <moreinfo>Chaire Havas Dauphine, until Mar 2016</moreinfo>
    </person>
    <person key="sierra-2016-idp181856">
      <firstname>Gauthier</firstname>
      <lastname>Gidel</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Paris</research-centre>
      <moreinfo>ENS Paris, from Apr 2016</moreinfo>
    </person>
    <person key="sierra-2016-idp184336">
      <firstname>Samy</firstname>
      <lastname>Jelassi</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Paris</research-centre>
      <moreinfo>Inria, from Feb 2016</moreinfo>
    </person>
    <person key="sierra-2014-idp91568">
      <firstname>Senanayak</firstname>
      <lastname>Karri</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Paris</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="sierra-2014-idm26584">
      <firstname>Simon</firstname>
      <lastname>Lacoste-Julien</lastname>
      <categoryPro>Chercheur</categoryPro>
      <research-centre>Paris</research-centre>
      <moreinfo>Inria, Starting Research Position, Until Aug 2016</moreinfo>
    </person>
    <person key="sierra-2016-idp191760">
      <firstname>Horia</firstname>
      <lastname>Mania</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Paris</research-centre>
      <moreinfo>Chaire Havas Dauphine, from May 2016 until Aug 2016</moreinfo>
    </person>
    <person key="sierra-2014-idp96488">
      <firstname>Guillaume</firstname>
      <lastname>Obozinski</lastname>
      <categoryPro>AutreCategorie</categoryPro>
      <research-centre>Paris</research-centre>
      <moreinfo>ENPC</moreinfo>
    </person>
    <person key="sierra-2014-idp77816">
      <firstname>Lindsay</firstname>
      <lastname>Polienor</lastname>
      <categoryPro>Assistant</categoryPro>
      <research-centre>Paris</research-centre>
    </person>
  </team>
  <presentation id="uid2">
    <bodyTitle>Overall Objectives</bodyTitle>
    <subsection id="uid3" level="1">
      <bodyTitle>Statement</bodyTitle>
      <p>Machine learning is a recent scientific domain, positioned between
applied mathematics, statistics and computer science. Its goals are
the optimization, control, and modelisation of complex systems from
examples. It applies to data from numerous engineering and scientific
fields (e.g., vision, bioinformatics, neuroscience, audio processing,
text processing, economy, finance, etc.), the ultimate goal being to
derive general theories and algorithms allowing advances in each of
these domains. Machine learning is characterized by the high quality
and quantity of the exchanges between theory, algorithms and
applications: interesting theoretical problems almost always emerge
from applications, while theoretical analysis allows the understanding
of why and when popular or successful algorithms do or do not work,
and leads to proposing significant improvements.</p>
      <p>Our academic positioning is exactly at the intersection between these
three aspects—algorithms, theory and applications—and our main
research goal is to make the link between theory and algorithms, and
between algorithms and high-impact applications in various engineering
and scientific fields, in particular computer vision, bioinformatics,
audio processing, text processing and neuro-imaging.</p>
      <p>Machine learning is now a vast field of research and the team focuses
on the following aspects: supervised learning (kernel methods,
calibration), unsupervised learning (matrix factorization, statistical
tests), parsimony (structured sparsity, theory and algorithms), and
optimization (convex optimization, bandit learning). These four
research axes are strongly interdependent, and the interplay between
them is key to successful practical applications.</p>
    </subsection>
  </presentation>
  <fondements id="uid4">
    <bodyTitle>Research Program</bodyTitle>
    <subsection id="uid5" level="1">
      <bodyTitle>Supervised Learning</bodyTitle>
      <p>This part of our research focuses on methods where, given a set of
examples of input/output pairs, the goal is to predict the output
for a new input, with research on kernel methods, calibration methods,
and multi-task learning.
</p>
    </subsection>
    <subsection id="uid6" level="1">
      <bodyTitle>Unsupervised Learning</bodyTitle>
      <p>We focus here on methods where no output is given and the goal is to
find structure of certain known types (e.g., discrete or
low-dimensional) in the data, with a focus on matrix factorization,
statistical tests, dimension reduction, and semi-supervised learning.
</p>
    </subsection>
    <subsection id="uid7" level="1">
      <bodyTitle>Parsimony</bodyTitle>
      <p>The concept of parsimony is central to many areas of science. In the
context of statistical machine learning, this takes the form of
variable or feature selection. The team focuses primarily on
structured sparsity, with theoretical and algorithmic contributions.
</p>
    </subsection>
    <subsection id="uid8" level="1">
      <bodyTitle>Optimization</bodyTitle>
      <p>Optimization in all its forms is central to machine learning, as many
of its theoretical frameworks are based at least in part on
empirical risk minimization. The team focuses primarily on convex and
bandit optimization, with a particular focus on large-scale optimization.
</p>
    </subsection>
  </fondements>
  <domaine id="uid9">
    <bodyTitle>Application Domains</bodyTitle>
    <subsection id="uid10" level="1">
      <bodyTitle>Application Domains</bodyTitle>
      <p>Machine learning research can be conducted from two main perspectives: the first one, which has been dominant in the last 30 years, is to design learning algorithms and theories which are as generic as possible, the goal being to make as few assumptions as possible regarding the problems to be solved and to let data speak for themselves. This has led to many interesting methodological developments and successful applications. However, we believe that this strategy has reached its limit for many application domains, such as computer vision, bioinformatics, neuro-imaging, text and audio processing, which leads to the second perspective our team is built on: Research in machine learning theory and algorithms should be driven by interdisciplinary collaborations, so that specific prior knowledge may be properly introduced into the learning process, in particular with the following fields:</p>
      <simplelist>
        <li id="uid11">
          <p noindent="true">Computer vision: object recognition, object detection, image segmentation, image/video processing, computational photography. In collaboration with the Willow project-team.</p>
        </li>
        <li id="uid12">
          <p noindent="true">Bioinformatics: cancer diagnosis, protein function prediction, virtual screening. In collaboration with Institut Curie.</p>
        </li>
        <li id="uid13">
          <p noindent="true">Text processing: document collection modeling, language models.</p>
        </li>
        <li id="uid14">
          <p noindent="true">Audio processing: source separation, speech/music processing.</p>
        </li>
        <li id="uid15">
          <p noindent="true">Neuro-imaging: brain-computer interface (fMRI, EEG, MEG).</p>
        </li>
      </simplelist>
    </subsection>
  </domaine>
  <logiciels id="uid16">
    <bodyTitle>New Software and Platforms</bodyTitle>
    <subsection id="uid17" level="1">
      <bodyTitle>DICA : Discrete Independent Component Analysis</bodyTitle>
      <p>
        <span class="smallcap" align="left">Functional Description</span>
      </p>
      <p>Moment Matching for Latent Dirichlet Allocation (LDA) and Discrete Independent Component Analysis (DICA).</p>
      <p>The DICA package contains Matlab and C++ (via Matlab mex files) implementations of estimation in the LDA and closely related DICA models.</p>
      <p>The implementation consists of two parts. One part contains the efficient implementation for construction of the moment/cumulant tensors, while the other part contains implementations of several so called joint diagonalization type algorithms used for matching the tensors. Any tensor type (see below) can be arbitrarily combined with one of the diagonalization algorithms (see below) leading, in total, to 6 algorithms.</p>
      <p>Two types of tensors are considered: (a) the LDA moments and (b) the DICA cumulants. The diagonalization algorithms include: (a) the orthogonal joint diagonalization algorithm based on iterative Jacobi rotations, (b) the spectral algorithm based on two eigen decompositions, and (c) the tensor power method.</p>
      <simplelist>
        <li id="uid18">
          <p noindent="true">Contact: Anastasia Podosinnikova</p>
        </li>
        <li id="uid19">
          <p noindent="true">URL: <ref xlink:href="https://github.com/anastasia-podosinnikova/dica" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>github.<allowbreak/>com/<allowbreak/>anastasia-podosinnikova/<allowbreak/>dica</ref></p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid20" level="1">
      <bodyTitle>LinearFW: Implementation of linearly convergent versions of Frank-Wolfe</bodyTitle>
      <p>
        <span class="smallcap" align="left">Functional Description</span>
      </p>
      <p>This is the code to reproduce all the experiments in the NIPS 2015 paper: "On the Global Linear Convergence of Frank-Wolfe Optimization Variants" by Simon Lacoste-Julien and Martin Jaggi, which covers the global linear convergence rate of Frank-Wolfe optimization variants for problems described as in Eq. (1) in the paper. It contains the implementation of Frank-Wolfe, away-steps Frank-Wolfe and pairwise Frank-Wolfe on two applications.</p>
      <simplelist>
        <li id="uid21">
          <p noindent="true">Contact: Simon Lacoste-Julien</p>
        </li>
        <li id="uid22">
          <p noindent="true">URL: <ref xlink:href="https://github.com/Simon-Lacoste-Julien/linearFW" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>github.<allowbreak/>com/<allowbreak/>Simon-Lacoste-Julien/<allowbreak/>linearFW</ref></p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid23" level="1">
      <bodyTitle>cnn_head_detection: Context-aware CNNs for person head detection</bodyTitle>
      <p>
        <span class="smallcap" align="left">Functional Description</span>
      </p>
      <p>Code for ICCV 2015 paper "Context-aware CNNs for person head detection":
Person detection is a key problem for many computer vision tasks. While face detection has reached maturity, detecting people under a full variation of camera view-points, human poses, lighting conditions and occlusions is still a difficult challenge. In this work we focus on detecting human heads in natural scenes. Starting from the recent local R-CNN object detector, we extend it with two types of contextual cues. First, we leverage person-scene relations and propose a Global CNN model trained to predict positions and scales of heads directly from the full image. Second, we explicitly model pairwise relations among objects and train a Pairwise CNN model using a structured-output surrogate loss. The Local, Global and Pairwise models are combined into a joint CNN framework. To train and test our full model, we introduce a large dataset composed of 369,846 human heads annotated in 224,740 movie frames. We evaluate our method and demonstrate improvements of person head detection against several recent baselines in three datasets. We also show improvements of the detection speed provided by our model.</p>
      <simplelist>
        <li id="uid24">
          <p noindent="true">Contact: Anton Osokin</p>
        </li>
        <li id="uid25">
          <p noindent="true">URL: <ref xlink:href="https://github.com/aosokin/cnn_head_detection" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>github.<allowbreak/>com/<allowbreak/>aosokin/<allowbreak/>cnn_head_detection</ref></p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid26" level="1">
      <bodyTitle>Lightning: large-scale linear classification, regression and ranking in Python</bodyTitle>
      <p>
        <span class="smallcap" align="left">Functional Description</span>
      </p>
      <p>Lightning is a Python library for large-scale machine learning. More specifically, the library focuses on linear models for classification, regression and ranking. Lightning is the first project to integrate scikit-learn-contrib, a repository of high-quality projects that follow the same API conventions as scikit-learn. Compared to scikit-learn, the main advantages of lightning are its scalability and its flexibility. Indeed, lightning implements cutting-edge optimization algorithms that allow to train models with millions of samples within seconds on commodity hardware. Furthermore, lightning can leverage prior knowledge thanks to so-called structured penalties, an area of research that has recently found applications in domains as diverse as biology, neuroimaging, finance or text processing. Lightning is available under the 3-clause BSD license at <ref xlink:href="http://contrib.scikit-learn.org/lightning/" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>contrib.<allowbreak/>scikit-learn.<allowbreak/>org/<allowbreak/>lightning/</ref>.</p>
      <simplelist>
        <li id="uid27">
          <p noindent="true">Contact: Fabian Pedregosa</p>
        </li>
        <li id="uid28">
          <p noindent="true">URL: <ref xlink:href="http://contrib.scikit-learn.org/lightning/" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>contrib.<allowbreak/>scikit-learn.<allowbreak/>org/<allowbreak/>lightning/</ref></p>
        </li>
      </simplelist>
    </subsection>
  </logiciels>
  <resultats id="uid29">
    <bodyTitle>New Results</bodyTitle>
    <subsection id="uid30" level="1">
      <bodyTitle>Regularized Nonlinear Acceleration</bodyTitle>
      <p>In <ref xlink:href="#sierra-2016-bid0" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, describe a convergence acceleration technique for generic optimization problems. Our scheme computes estimates of the optimum from a nonlinear average of the iterates produced by any optimization method. The weights in this average are computed via a simple linear system, whose solution can be updated online. This acceleration scheme runs in parallel to the base algorithm, providing improved estimates of the solution on the fly, while the original optimization method is running. Numerical experiments are detailed on classical classification problems.</p>
    </subsection>
    <subsection id="uid31" level="1">
      <bodyTitle>Harder, Better, Faster, Stronger Convergence Rates for Least-Squares Regression</bodyTitle>
      <p>In <ref xlink:href="#sierra-2016-bid1" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, we consider the optimization of a quadratic objective function whose gradients are only accessible through a stochastic oracle that returns the gradient at any given point plus a zero-mean finite variance random error. We present the first algorithm that achieves jointly the optimal prediction error rates for least-squares regression, both in terms of forgetting of initial conditions in <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mi>O</mi><mo>(</mo><mn>1</mn><mo>/</mo><msup><mi>n</mi><mn>2</mn></msup><mo>)</mo></mrow></math></formula>, and in terms of dependence on the noise and dimension <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>d</mi></math></formula> of the problem, as <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mi>O</mi><mo>(</mo><mi>d</mi><mo>/</mo><mi>n</mi><mo>)</mo></mrow></math></formula>. Our new algorithm is based on averaged accelerated regularized gradient descent, and may also be analyzed through finer assumptions on initial conditions and the Hessian matrix, leading to dimension-free quantities that may still be small while the " optimal " terms above are large. In order to characterize the tightness of these new bounds, we consider an application to non-parametric regression and use the known lower bounds on the statistical performance (without computational limits), which happen to match our bounds obtained from a single pass on the data and thus show optimality of our algorithm in a wide variety of particular trade-offs between bias and variance.</p>
    </subsection>
    <subsection id="uid32" level="1">
      <bodyTitle>Stochastic Variance Reduction Methods for Saddle-Point Problems</bodyTitle>
      <p>In <ref xlink:href="#sierra-2016-bid2" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, we consider convex-concave saddle-point problems where the objective functions may be split in many components, and extend recent stochastic variance reduction methods (such as SVRG or SAGA) to provide the first large-scale linearly convergent algorithms for this class of problems which are common in machine learning. While the algorithmic extension is straightforward, it comes with challenges and opportunities: (a) the convex minimization analysis does not apply and we use the notion of monotone operators to prove convergence, showing in particular that the same algorithm applies to a larger class of problems, such as variational inequalities, (b) there are two notions of splits, in terms of functions, or in terms of partial derivatives, (c) the split does need to be done with convex-concave terms, (d) non-uniform sampling is key to an efficient algorithm, both in theory and practice, and (e) these incremental algorithms can be easily accelerated using a simple extension of the “catalyst” framework, leading to an algorithm which is always superior to accelerated batch algorithms.</p>
    </subsection>
    <subsection id="uid33" level="1">
      <bodyTitle>Frank-Wolfe Algorithms for Saddle Point Problems</bodyTitle>
      <p>In <ref xlink:href="#sierra-2016-bid3" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, we extend the Frank-Wolfe (FW) optimization algorithm to solve constrained smooth
convex-concave saddle point (SP) problems. Remarkably, the method only requires access
to linear minimization oracles. Leveraging recent advances in FW optimization, we provide
the first proof of convergence of a FW-type saddle point solver over polytopes, thereby
partially answering a 30 year-old conjecture. We also survey other convergence results and
highlight gaps in the theoretical underpinnings of FW-style algorithms. Motivating
applications without known efficient alternatives are explored through structured predic-
tion with combinatorial penalties as well as games over matching polytopes involving an
exponential number of constraints.
</p>
    </subsection>
    <subsection id="uid34" level="1">
      <bodyTitle>Minding the Gaps for Block Frank-Wolfe Optimization of Structured SVM</bodyTitle>
      <p>In <ref xlink:href="#sierra-2016-bid4" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, we propose several improvements on the block-coordinate Frank-Wolfe (BCFW) algorithm from Lacoste-Julien et al. (2013) recently used to optimize the structured support vector machine (SSVM) objective in the context of structured prediction, though it has wider applications. The key intuition behind our improvements is that the estimates of block gaps maintained by BCFW reveal the block suboptimality that can be used as an adaptive criterion. First, we sample objects at each iteration of BCFW in an adaptive non-uniform way via gapbased sampling. Second, we incorporate pairwise and away-step variants of Frank-Wolfe into the block-coordinate setting. Third, we cache oracle calls with a cache-hit criterion based on the block gaps. Fourth, we provide the first method to compute an approximate regularization path for SSVM. Finally, we provide an exhaustive empirical evaluation of all our methods on four structured prediction datasets.
The associated SOFTWARE is here: <ref xlink:href="https://github.com/aosokin/gapBCFW" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>github.<allowbreak/>com/<allowbreak/>aosokin/<allowbreak/>gapBCFW</ref></p>
    </subsection>
    <subsection id="uid35" level="1">
      <bodyTitle>Asaga: Asynchronous Parallel Saga</bodyTitle>
      <p>In <ref xlink:href="#sierra-2016-bid5" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, we describe Asaga, an asynchronous parallel version of the incremental gradient algorithm Saga that enjoys fast linear convergence rates. We highlight a subtle but important technical issue present in a large fraction of the recent convergence rate proofs for asynchronous parallel optimization algorithms, and propose a simplification of the recently proposed “perturbed iterate” framework that resolves it. We thereby prove that Asaga can obtain a theoretical linear speedup on multi-core systems even without sparsity assumptions. We present results of an implementation on a 40-core architecture illustrating the practical speedup as well as the hardware overhead.</p>
    </subsection>
    <subsection id="uid36" level="1">
      <bodyTitle>Convergence Rate of Frank-Wolfe for Non-Convex Objectives</bodyTitle>
      <p>In <ref xlink:href="#sierra-2016-bid6" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, we give a simple proof that the Frank-Wolfe algorithm obtains a
stationary point at a rate of <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mi>O</mi><mo>(</mo><mn>1</mn><mo>/</mo><msqrt><mi>t</mi></msqrt><mo>)</mo></mrow></math></formula> on non-convex objectives
with a Lipschitz continuous gradient. Our analysis is affine invariant
and is the first, to the best of our knowledge, giving a similar rate
to what was already proven for projected gradient methods (though on
slightly different measures of stationarity).</p>
    </subsection>
    <subsection id="uid37" level="1">
      <bodyTitle>Highly-Smooth Zero-th Order Online Optimization</bodyTitle>
      <p>The minimization of convex functions which are only available through partial and noisy infor- mation is a key methodological problem in many disciplines. In <ref xlink:href="#sierra-2016-bid7" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, we consider convex optimization with noisy zero-th order information, that is noisy function evaluations at any desired point. We focus on problems with high degrees of smoothness, such as logistic regression. We show that as opposed to gradient-based algorithms, high-order smoothness may be used to improve estimation rates, with a precise dependence of our upper-bounds on the degree of smoothness. In particular, we show that for infinitely differentiable functions, we recover the same dependence on sample size as gradient-based algorithms, with an extra dimension-dependent factor. This is done for both convex and strongly-convex functions, with finite horizon and anytime algorithms. Finally, we also recover similar results in the online optimization setting.
</p>
    </subsection>
    <subsection id="uid38" level="1">
      <bodyTitle>Slice Inverse Regression with Score Functions</bodyTitle>
      <p>Non-linear regression and related problems such as non-linear classification are core important tasks
in machine learning and statistics.
We consider the problem of dimension reduction in non-linear regression, which is often formulated as a non-convex optimization problem.</p>
      <simplelist>
        <li id="uid39">
          <p noindent="true">We propose score function extensions to sliced inverse regression problems  <ref xlink:href="#sierra-2016-bid8" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>,  <ref xlink:href="#sierra-2016-bid9" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, both for the first-order and second-order score functions, which provably improve estimation in the population case over the non-sliced versions; we study finite sample estimators and study their consistency given the exact score functions.</p>
        </li>
        <li id="uid40">
          <p noindent="true">We propose also to learn the score function as well (using score matching technique  <ref xlink:href="#sierra-2016-bid10" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>) in two steps, i.e., first learning the score function and then learning the effective dimension reduction space, or directly, by solving a convex optimization problem regularized by the nuclear norm.</p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid41" level="1">
      <bodyTitle>Inference and learning for log-supermodular distributions</bodyTitle>
      <p>In <ref xlink:href="#sierra-2016-bid11" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, we consider log-supermodular models on binary variables, which are probabilistic models with negative log-densities which are submodular. These models provide probabilistic interpretations of common combinatorial optimization tasks such as image segmentation. We make the following contributions:</p>
      <simplelist>
        <li id="uid42">
          <p noindent="true">We review existing variational bounds for the log-partition function and show that the bound of
T. Hazan and T. Jaakkola (On the Partition Function and Random Maximum A-Posteriori Perturbations, Proc. ICML, 2012), based on “perturb-and-MAP” ideas, formally dominates the bounds proposed by J. Djolonga and A. Krause (From MAP to Marginals: Variational Inference in Bayesian Submodular Models, Adv. NIPS, 2014).</p>
        </li>
        <li id="uid43">
          <p noindent="true">We show that for parameter learning via maximum likelihood the existing bound of J. Djolonga and A. Krause typically leads to a degenerate solution while the one based on “perturb-and-MAP” ideas and logistic samples does not.</p>
        </li>
        <li id="uid44">
          <p noindent="true">Given that the bound based on “perturb-and-MAP” ideas is an expectation (over our own randomization), we propose to use a stochastic subgradient technique to maximize the lower-bound on the log-likelihood, which can also be extended to conditional maximum likelihood.</p>
        </li>
        <li id="uid45">
          <p noindent="true">We illustrate our new results on a set of experiments in binary image denoising, where we highlight the flexibility of a probabilistic model for learning with missing data.</p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid46" level="1">
      <bodyTitle>Beyond CCA: Moment Matching for Multi-View Models</bodyTitle>
      <p>In <ref xlink:href="#sierra-2016-bid12" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, we introduce three novel semi-parametric extensions of probabilistic canonical correlation analysis with identifiability guarantees. We consider moment matching techniques for estimation in these models. For that, by drawing explicit links between the new models and a discrete version of independent component analysis (DICA), we first extend the DICA cumulant tensors to the new discrete version of CCA. By further using a close connection with independent component analysis, we introduce generalized covariance matrices, which can replace the cumulant tensors in the moment matching framework, and, therefore, improve sample complexity and simplify derivations and algorithms significantly. As the tensor power method or orthogonal joint diagonalization are not applicable in the new setting, we use non-orthogonal joint diagonalization techniques for matching the cumulants. We demonstrate performance of the proposed models and estimation techniques on experiments with both synthetic and real datasets.</p>
    </subsection>
    <subsection id="uid47" level="1">
      <bodyTitle>PAC-Bayesian Theory Meets Bayesian Inference</bodyTitle>
      <p>In <ref xlink:href="#sierra-2016-bid13" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, we exhibit a strong link between frequentist PAC-Bayesian bounds and the Bayesian marginal likelihood. That is, for the negative log-likelihood loss function, we show that the minimization of PAC-Bayesian generalization bounds maximizes the Bayesian marginal likelihood. This provides an alternative explanation to the Bayesian Occam’s razor criteria, under the assumption that the data is generated by an <i>i.i.d.</i> distribution. Moreover, as the negative log-likelihood is an unbounded loss function, we motivate and propose a PAC-Bayesian theorem tailored for the sub-gamma loss family, and we show that our approach is sound on classical Bayesian linear regression tasks.</p>
    </subsection>
    <subsection id="uid48" level="1">
      <bodyTitle>A New PAC-Bayesian Perspective on Domain Adaptation</bodyTitle>
      <p>In <ref xlink:href="#sierra-2016-bid14" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, we study the issue of PAC-Bayesian domain adaptation: We want to learn, from a source domain, a majority vote model dedicated to a target one. Our theoretical contribution brings a new perspective by deriving an upper-bound on the target risk where the distributions’ divergence— expressed as a ratio—controls the trade-off between a source error measure and the target voters’ disagreement. Our bound suggests that one has to focus on regions where the source data is informative. From this result, we derive a PAC-Bayesian generalization bound, and specialize it to linear classifiers. Then, we infer a learning algorithm and perform experiments on real data.
</p>
    </subsection>
    <subsection id="uid49" level="1">
      <bodyTitle>PAC-Bayesian Bounds based on the Rényi Divergence</bodyTitle>
      <p>In <ref xlink:href="#sierra-2016-bid15" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, we propose a simplified proof process for PAC-Bayesian generalization bounds, that allows to divide the proof in four successive inequalities, easing the “customization” of PAC-Bayesian theorems. We also propose a family of PAC-Bayesian bounds based on the Rényi divergence between the prior and posterior distributions, whereas most PAC-Bayesian bounds are based on the Kullback-Leibler divergence. Finally, we present an empirical evaluation of the tightness of each inequality of the simplified proof, for both the classical PAC-Bayesian bounds and those based on the Rényi divergence.</p>
    </subsection>
    <subsection id="uid50" level="1">
      <bodyTitle>PAC-Bayesian theorems for multiview learning</bodyTitle>
      <p>In <ref xlink:href="#sierra-2016-bid16" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, we tackle the issue of multiview learning which aims to take advantages of multiple representations/views of the data. In this context, many machine learning algorithms exist. However, the majority of the theoretical studies focus on learning with exactly two representations. In this paper, we propose a general PAC-Bayesian theory for multiview learning with more than two views. We focus our study to binary classification models that take the form of a majority vote. We derive PAC-Bayesian generalization bounds allowing to consider different relations between empirical and true risks by taking into account a notion of diversity of the voters and views, and that can be naturally extended to semi-supervised learning.</p>
    </subsection>
    <subsection id="uid51" level="1">
      <bodyTitle>A spectral algorithm for fast <i>de novo</i> layout of uncorrected long nanopore reads</bodyTitle>
      <p>Seriation is an optimization problem that seeks to reconstruct an ordering between <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>n</mi></math></formula> variables from pairwise similarity information. It can be formulated as a combinatorial problem over permutations and several algorithms have been derived from relaxations of this problem. We make the link between the seriation framework and the task of de novo genome assembly, which consists of reconstructing a whole DNA sequence from small pieces of it that are oversampled so as to cover the full genome. To achieve this task, one has to find the layout of small pieces of DNA sequences (reads). This layout step can be cast as a seriation problem. We show that a spectral algorithm for seriation can be efficiently applied to a genome assembly scheme.</p>
      <p>New long read sequencers promise to transform sequencing and genome assembly by producing reads tens of kilobases long. However their high error rate significantly complicates assembly and requires expensive correction steps to layout the reads using standard assembly engines.</p>
      <p>We present an original and efficient spectral algorithm to layout the uncorrected nanopore reads, and its seamless integration into a straightforward overlap/layout/consensus (OLC) assembly scheme. The method is shown to assemble Oxford Nanopore reads from several bacterial genomes into good quality (<formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mo>∼</mo><mn>99</mn><mo>%</mo></mrow></math></formula> identity to the reference) genome-sized contigs, while yielding more fragmented assemblies from a <i>Sacharomyces cerevisiae</i> reference strain. See software in <ref xlink:href=" https://github.com/antrec/spectrassembler" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"> https://<allowbreak/>github.<allowbreak/>com/<allowbreak/>antrec/<allowbreak/>spectrassembler</ref>.</p>
    </subsection>
    <subsection id="uid52" level="1">
      <bodyTitle> Using Deep Learning and Generative Adversarial Networks to Study Large Scale GFP Screens</bodyTitle>
      <p>Fluorescent imaging of GFP tagged proteins is one of the most widely used
techniques to view the dynamics of proteins in live cells. By combining it
with different perturbations such as RNAi or drug treatments we can understand
how cells regulate complex processes such as mitosis or the cell cycle.</p>
      <p>However, GFP imaging has certain limitations. There are only a limited number
of different fluorescent proteins available, making imaging multiple proteins
at the same time very challenging and expensive. Finally, analyzing complex
screens can be very challenging: it's not always obvious a-priori what kind
of features will predict the phenotypes we are interested in.</p>
      <p>We discuss a new approach to studying large scale GFP screens using deep
convolutional networks. We show that by using convolutional neural networks,
we can greatly outperform traditional feature based approaches at different
kind of prediction tasks. The networks learn flexible representations, which
are suitable for multiple tasks, such as predicting the localization of Tea1
in fission yeast cells (blue signal, shown in image) in cells where only other
proteins are tagged.</p>
      <p>We then show that we can use generative adversarial neural networks to learn
highly compact latent representations. Those latent representations can then
be used to generate new realistic images, allowing us to simulate new
phenotypes, and to predict the outcome of new perturbations (joint work between Federico Vaggi, Anton Osokin, Theophile Dalens).</p>
    </subsection>
    <subsection id="uid53" level="1">
      <bodyTitle>SymPy: Symbolic computing in Python
</bodyTitle>
      <p>SymPy is an open source computer algebra system written in pure Python. It is built with a focus on extensibility and ease of use, through both interactive and programmatic applications. These characteristics have led SymPy to become the standard symbolic library for the scientific Python ecosystem. This paper <ref xlink:href="#sierra-2016-bid17" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> presents the architecture of SymPy, a description of its features, and a discussion of select domain specific submodules. The supplementary materials provide additional examples and further outline details of the architecture and features of SymPy. As for the software, I am one of the main authors of the lightning machine learning library, that you can include if you want.</p>
    </subsection>
    <subsection id="uid54" level="1">
      <bodyTitle>Robust Discriminative Clustering with Sparse Regularizers</bodyTitle>
      <p>Clustering high-dimensional data often requires some form of dimensionality reduction, where clustered variables are separated from "noise-looking" variables. In <ref xlink:href="#sierra-2016-bid18" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, we cast this problem as finding a low-dimensional projection of the data which is well-clustered. This yields a one-dimensional projection in the simplest situation with two clusters, and extends naturally to a multi-label scenario for more than two clusters. In this paper, (a) we first show that this joint clustering and dimension reduction formulation is equivalent to previously proposed discriminative clustering frameworks, thus leading to convex relaxations of the problem, (b) we propose a novel sparse extension, which is still cast as a convex relaxation and allows estimation in higher dimensions, (c) we propose a natural extension for the multi-label scenario, (d) we provide a new theoretical analysis of the performance of these formulations with a simple probabilistic model, leading to scalings over the form <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mi>d</mi><mo>=</mo><mi>O</mi><mo>(</mo><msqrt><mi>n</mi></msqrt><mo>)</mo></mrow></math></formula> for the affine invariant case and <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mi>d</mi><mo>=</mo><mi>O</mi><mo>(</mo><mi>n</mi><mo>)</mo></mrow></math></formula> for the sparse case, where <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>n</mi></math></formula> is the number of examples and <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>d</mi></math></formula> the ambient dimension, and finally, (e) we propose an efficient iterative algorithm with running-time complexity proportional to <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mi>O</mi><mo>(</mo><mi>n</mi><msup><mi>d</mi><mn>2</mn></msup><mo>)</mo></mrow></math></formula>, improving on earlier algorithms which had quadratic complexity in the number of examples.
</p>
    </subsection>
    <subsection id="uid55" level="1">
      <bodyTitle>Optimal Rates of Statistical Seriation</bodyTitle>
      <p>Given a matrix the seriation problem consists in permuting its rows in such way that all its columns have the same shape, for example, they are monotone increasing. In <ref xlink:href="#sierra-2016-bid19" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, we propose a statistical approach to this problem where the matrix of interest is observed with noise and study the corresponding minimax rate of estimation of the matrices. Specifically, when the columns are either unimodal or monotone, we show that the least squares estimator is optimal up to logarithmic factors and adapts to matrices with a certain natural structure. Finally, we propose a computationally efficient estimator in the monotonic case and study its performance both theoretically and experimentally. Our work is at the intersection of shape constrained estimation and recent work that involves permutation learning, such as graph denoising and ranking.</p>
    </subsection>
    <subsection id="uid56" level="1">
      <bodyTitle>Breaking Sticks and Ambiguities with Adaptive Skip-gram</bodyTitle>
      <p>Recently proposed Skip-gram model is a powerful method for learning high-dimensional word representations that capture rich semantic relationships between words. However, Skip-gram as well as most prior work on learning word representations does not take into account word ambiguity and maintain only single representation per word. Although a number of Skip-gram modifications were proposed to overcome this limitation and learn multi-prototype word representations, they either require a known number of word meanings or learn them using greedy heuristic approaches. In <ref xlink:href="#sierra-2016-bid20" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, we propose the Adaptive Skip-gram model which is a nonparametric Bayesian extension of Skip-gram capable to automatically learn the required number of representations for all words at desired semantic resolution. We derive efficient online variational learning algorithm for the model and empirically demonstrate its efficiency on word-sense induction task.</p>
    </subsection>
    <subsection id="uid57" level="1">
      <bodyTitle>Deep Part-Based Generative Shape Model with Latent Variables</bodyTitle>
      <p>The Shape Boltzmann Machine (SBM) and its multilabel version MSBM [5] have been recently introduced as deep generative models that capture the variations of an object shape. While being more flexible MSBM requires datasets with labeled parts of the objects for training. In <ref xlink:href="#sierra-2016-bid21" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, we present an algorithm for training MSBM using binary masks of objects and the seeds which approximately correspond to the locations of objects parts. The latter can be obtained from part-based detectors in an unsupervised manner. We derive a latent variable model and an EM-like training procedure for adjusting the weights of MSBM using a deep learning framework. We show that the model trained by our method outperforms SBM in the tasks related to binary shapes and is very close to the original MSBM in terms of quality of multilabel shapes.</p>
    </subsection>
    <subsection id="uid58" level="1">
      <bodyTitle>Unsupervised Learning from Narrated Instruction Videos</bodyTitle>
      <p>In <ref xlink:href="#sierra-2016-bid22" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, we address the problem of automatically learning the main steps to complete a certain task, such as changing a car tire, from a set of narrated instruction videos. The contributions of this paper are three-fold. First, we develop a new unsupervised learning approach that takes advantage of the complementary nature of the input video and the associated narration. The method solves two clustering problems, one in text and one in video, applied one after each other and linked by joint constraints to obtain a single coherent sequence of steps in both modalities. Second, we collect and annotate a new challenging dataset of real-world instruction videos from the Internet. The dataset contains about 800,000 frames for five different tasks that include complex interactions between people and objects, and are captured in a variety of indoor and outdoor settings. Third, we experimentally demonstrate that the proposed method can automatically discover, in an unsupervised manner, the main steps to achieve the task and locate the steps in the input videos.
The associated SOFTWARE is here: <ref xlink:href="https://github.com/jalayrac/instructionVideos" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>github.<allowbreak/>com/<allowbreak/>jalayrac/<allowbreak/>instructionVideos</ref></p>
    </subsection>
    <subsection id="uid59" level="1">
      <bodyTitle>Stochastic Optimization for Large-scale Optimal Transport</bodyTitle>
      <p>Optimal transport (OT) defines a powerful framework to compare probability distributions in a geometrically faithful way.
However, the practical impact of OT is still limited because of its computational burden.
In <ref xlink:href="#sierra-2016-bid23" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, we propose a new class of stochastic optimization algorithms to cope with large-scale OT problems. These methods can handle arbitrary distributions (either discrete or continuous) as long as one is able to draw samples from them, which is the typical setup in high-dimensional learning problems. This alleviates the need to discretize these densities, while giving access to provably convergent methods that output the correct distance without discretization error.
These algorithms rely on two main ideas: <i>(a)</i> the dual OT problem can be re-cast as the maximization of an expectation; <i>(b)</i> the entropic regularization of the primal OT problem yields a smooth dual optimization which can be addressed with algorithms that have a provably faster convergence.
We instantiate these ideas in three different setups: <i>(i)</i> when comparing a discrete distribution to another, we show that incremental stochastic optimization schemes can beat Sinkhorn's algorithm, the current state-of-the-art finite dimensional OT solver; <i>(ii)</i> when comparing a discrete distribution to a continuous density, a semi-discrete reformulation of the dual program is amenable to averaged stochastic gradient descent, leading to better performance than approximately solving the problem by discretization ; <i>(iii)</i> when dealing with two continuous densities, we propose a stochastic gradient descent over a reproducing kernel Hilbert space (RKHS). This is currently the only known method to solve this problem, apart from computing OT on finite samples. We backup these claims on a set of discrete, semi-discrete and continuous benchmark problems.</p>
    </subsection>
    <subsection id="uid60" level="1">
      <bodyTitle>Online but Accurate Inference for Latent Variable Models with Local Gibbs Sampling</bodyTitle>
      <p>We study parameter inference in large-scale latent variable models. We first propose a unified treatment of online inference for latent variable models from a non-canonical exponential family, and draw explicit links between several previously proposed frequentist or Bayesian methods. We then propose a novel inference method for the frequentist estimation of parameters, that adapts MCMC methods to online inference of latent variable models with the proper use of local Gibbs sampling. Then, for latent Dirichlet allocation,we provide an extensive set of experiments and comparisons with existing work, where our new approach outperforms all previously proposed methods.
In particular, using Gibbs sampling for latent variable inference is superior to variational inference in terms of test log-likelihoods. Moreover, Bayesian inference through variational methods perform poorly, sometimes leading to worse fits with latent variables of higher dimensionality.</p>
      <p>In <ref xlink:href="#sierra-2016-bid24" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, we focus on methods that make a single pass over the data to estimate parameters. We make the following contributions:</p>
      <orderedlist>
        <li id="uid61">
          <p noindent="true">We review and compare existing methods for online inference for latent variable models from a non-canonical exponential family, and draw explicit links between several previously proposed frequentist or Bayesian methods. Given the large number of existing methods, our unifying framework allows to understand differences and similarities between all of them.</p>
        </li>
        <li id="uid62">
          <p noindent="true">We propose a novel inference method for the frequentist estimation of parameters, that adapts MCMC methods to online inference of latent variable models with the proper use of “local” Gibbs sampling. In our online scheme, we apply Gibbs sampling to the current observation, which is “local”, as opposed to “global” batch schemes where Gibbs sampling is applied to the entire dataset.</p>
        </li>
        <li id="uid63">
          <p noindent="true">After formulating LDA as a non-canonical exponential family, we provide an extensive set of experiments, where our new approach outperforms all previously proposed methods. In particular, using Gibbs sampling for latent variable inference is superior to variational inference in terms of test log-likelihoods. Moreover, Bayesian inference through variational methods perform poorly, sometimes leading to worse fits with latent variables of higher dimensionality.</p>
        </li>
      </orderedlist>
    </subsection>
    <subsection id="uid64" level="1">
      <bodyTitle>Learning Determinantal Point Processes in Sublinear Time</bodyTitle>
      <p>In <ref xlink:href="#sierra-2016-bid25" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, we propose a new class of determinantal point processes (DPPs) which can be manipulated for inference and parameter learning in potentially sublinear time in the number of items. This class, based on a specific low-rank factorization of the marginal kernel, is particularly suited to a subclass of continuous DPPs and DPPs defined on exponentially many items. We apply this new class to modelling
text documents as sampling a DPP of sentences, and propose a conditional maximum likelihood formulation to model topic proportions, which is made possible with no approximation for our class of DPPs. We present an application to document summarization with a DPP on <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msup><mn>2</mn><mn>500</mn></msup></math></formula> items.</p>
      <p>We make the following contributions:</p>
      <simplelist>
        <label>–</label>
        <li id="uid65">
          <p noindent="true">We propose a new class of determinantal point processes (DPPs) which is based on a particular low-rank factorization of the marginal kernel. Through the availability of a particular second-moment matrix, the complexity for inference and learning tasks is polynomial in the rank of the factorization and thus often sublinear in the total number of items (with exact likelihood computations).</p>
        </li>
        <label>–</label>
        <li id="uid66">
          <p noindent="true">As shown in this work, these new DPPs are particularly suited to a subclass of continuous DPPs (infinite number of items), such as on <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msup><mrow><mo>[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>]</mo></mrow><mi>m</mi></msup></math></formula>, and DPPs defined on the <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>V</mi></math></formula>-dimensional hypercube, which has <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msup><mn>2</mn><mi>V</mi></msup></math></formula> elements.</p>
        </li>
        <label>–</label>
        <li id="uid67">
          <p noindent="true">We propose a model of documents as sampling a DPP of sentences, and propose a conditional maximum likelihood formulation to model topic proportions. We present an application to document summarization with a DPP on <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msup><mn>2</mn><mn>500</mn></msup></math></formula> items.</p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid68" level="1">
      <bodyTitle>Decentralized Topic Modelling with Latent Dirichlet Allocation</bodyTitle>
      <p>Privacy preserving networks can be modelled as decentralized networks (e.g., sensors, connected objects, smartphones), where communication between nodes of the network is not controlled by a master or central node. For this type of networks, the main issue is to gather/learn global information on the network (e.g., by optimizing a global cost function) while keeping the (sensitive) information at each node.
In this work, we focus on text information that agents do not want to share (e.g., text messages, emails, confidential reports). We use recent advances on decentralized optimization and topic models to infer topics from a graph with limited communication.
We propose a method to adapt latent Dirichlet allocation (LDA) model to decentralized optimization and show on synthetic data that we still recover similar parameters and similar performance at each node than with stochastic methods accessing to the whole information in the graph.</p>
      <p>In <ref xlink:href="#sierra-2016-bid26" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, we tackle the non-convex problem of topic modelling, where agents have sensitive text data at their disposal that they can not or do not want to share (e.g., text messages, emails, confidential reports).
More precisely, we adapt the particular Latent Dirichlet Allocation (LDA) model to decentralized networks. We combine recent work of <ref xlink:href="#sierra-2016-bid24" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> on online inference for latent variable models, which adapts online EM with local Gibbs sampling in the case of intractable latent variable models (such as LDA) and recent advances on decentralized optimization.</p>
    </subsection>
  </resultats>
  <contrats id="uid69">
    <bodyTitle>Bilateral Contracts and Grants with Industry</bodyTitle>
    <subsection id="uid70" level="1">
      <bodyTitle>Bilateral Contracts with Industry</bodyTitle>
      <p>Microsoft Research: “Structured Large-Scale Machine Learning”. Machine learning is now ubiquitous in industry, science, engineering, and personal life. While early successes were obtained by applying off-the- shelf techniques, there are two main challenges faced by machine learning in the “big data” era: structure and scale. The project proposes to explore three axes, from theoretical, algorithmic and practical perspectives: (1) large-scale convex optimization, (2) large-scale combinatorial optimization and (3) sequential decision making for structured data. The project involves two Inria sites (Paris and Grenoble) and four MSR sites (Cambridge, New England, Redmond, New York). Project website: <ref xlink:href="http://www.msr-inria.fr/projects/structured-large-scale-machine-learning/" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>www.<allowbreak/>msr-inria.<allowbreak/>fr/<allowbreak/>projects/<allowbreak/>structured-large-scale-machine-learning/</ref>.</p>
    </subsection>
    <subsection id="uid71" level="1">
      <bodyTitle>Bilateral Grants with Industry</bodyTitle>
      <simplelist>
        <li id="uid72">
          <p noindent="true">A. d’Aspremont: AXA, "mécénat scientifique, chaire Havas-Dauphine", machine learning.</p>
        </li>
        <li id="uid73">
          <p noindent="true">A. d’Aspremont: Société Générale - fondation ENS, "mécénat scientifique".</p>
        </li>
        <li id="uid74">
          <p noindent="true">A. d’Aspremont: Projet EMMA at Institut Louis Bachelier. Collaboration with Euroclear on REPO markets.</p>
        </li>
        <li id="uid75">
          <p noindent="true">S. Lacoste-Julien (with J. Sivic and I. Laptev in Willow project-team): Google Research Award
“Structured Learning from Video and Natural Language”.</p>
        </li>
        <li id="uid76">
          <p noindent="true">F. Bach: Gift from Facebook AI Research.</p>
        </li>
      </simplelist>
    </subsection>
  </contrats>
  <partenariat id="uid77">
    <bodyTitle>Partnerships and Cooperations</bodyTitle>
    <subsection id="uid78" level="1">
      <bodyTitle>European Initiatives</bodyTitle>
      <subsection id="uid79" level="2">
        <bodyTitle>FP7 &amp; H2020 Projects</bodyTitle>
        <subsection id="uid80" level="3">
          <bodyTitle>SIPA</bodyTitle>
          <sanspuceslist>
            <li id="uid81">
              <p noindent="true">Title: Semidefinite Programming with Applications in Statistical Learning</p>
            </li>
            <li id="uid82">
              <p noindent="true">Type: FP7</p>
            </li>
            <li id="uid83">
              <p noindent="true">Instrument: ERC Starting Grant Duration: May 2011 - May 2016 Coordinator: A. d’Aspremont (CNRS)</p>
            </li>
            <li id="uid84">
              <p noindent="true">Abstract: Interior point algorithms and a dramatic growth in computing power have revolutionized optimization in the last two decades. Highly nonlinear problems which were previously thought in- tractable are now routinely solved at reasonable scales. Semidefinite programs (i.e. linear programs on the cone of positive semidefinite matrices) are a perfect example of this trend: reasonably large, highly nonlinear but convex eigenvalue optimization problems are now solved efficiently by reliable numerical packages. This in turn means that a wide array of new applications for semidefinite pro- gramming have been discovered, mimicking the early development of linear programming. To cite only a few examples, semidefinite programs have been used to solve collaborative filtering problems (e.g. make personalized movie recommendations), approximate the solution of combinatorial pro- grams, optimize the mixing rate of Markov chains over networks, infer dependence patterns from multivariate time series or produce optimal kernels in classification problems. These new appli- cations also come with radically different algorithmic requirements. While interior point methods solve relatively small problems with a high precision, most recent applications of semidefinite pro- gramming in statistical learning for example form very large-scale problems with comparatively low precision targets, programs for which current algorithms cannot form even a single iteration. This proposal seeks to break this limit on problem size by deriving reliable first-order algorithms for solv- ing large-scale semidefinite programs with a significantly lower cost per iteration, using for example subsampling techniques to considerably reduce the cost of forming gradients. Beyond these algo- rithmic challenges, the proposed research will focus heavily on applications of convex programming to statistical learning and signal processing theory where optimization and duality results quantify the statistical performance of coding or variable selection algorithms for example. Finally, another central goal of this work will be to produce efficient, customized algorithms for some key problems arising in machine learning and statistics.</p>
            </li>
          </sanspuceslist>
        </subsection>
        <subsection id="uid85" level="3">
          <bodyTitle>MacSeNet</bodyTitle>
          <sanspuceslist>
            <li id="uid86">
              <p noindent="true">Title: Machine Sensing Training Network</p>
            </li>
            <li id="uid87">
              <p noindent="true">Type: H2020</p>
            </li>
            <li id="uid88">
              <p noindent="true">Instrument: Initial Training Network</p>
            </li>
            <li id="uid89">
              <p noindent="true">Duration: January 2015 - January 2019</p>
            </li>
            <li id="uid90">
              <p noindent="true">Coordinator: Mark Plumbley (University of Surrey)</p>
            </li>
            <li id="uid91">
              <p noindent="true">Inria contact: Francis Bach</p>
            </li>
            <li id="uid92">
              <p noindent="true">Abstract: The aim of this Innovative Training Network is to train a new generation of creative, entrepreneurial and innovative early stage researchers (ESRs) in the research area of measurement and estimation of signals using knowledge or data about the underlying structure.
We will develop new robust and efficient Machine Sensing theory and algorithms, together methods for a wide range of signals, including: advanced brain imaging; inverse imaging problems; audio and music signals; and non-traditional signals such as signals on graphs. We will apply these methods to real-world problems, through work with non-Academic partners, and disseminate the results of this research to a wide range of academic and non-academic audiences, including through publications, data, software and public engagement events.
MacSeNet is funded under the H2020-MSCA-ITN-2014 call and is part of the Marie Sklodowska- Curie Actions — Innovative Training Networks (ITN) funding scheme.</p>
            </li>
          </sanspuceslist>
        </subsection>
        <subsection id="uid93" level="3">
          <bodyTitle>Spartan</bodyTitle>
          <sanspuceslist>
            <li id="uid94">
              <p noindent="true">Title: Sparse Representations and Compressed Sensing Training Network Type: FP7</p>
            </li>
            <li id="uid95">
              <p noindent="true">Instrument: Initial Training Network</p>
            </li>
            <li id="uid96">
              <p noindent="true">Duration: October 2014 to October 2018</p>
            </li>
            <li id="uid97">
              <p noindent="true">Coordinator: Mark Plumbley (University of Surrey)</p>
            </li>
            <li id="uid98">
              <p noindent="true">Inria contact: Francis Bach</p>
            </li>
            <li id="uid99">
              <p noindent="true">Abstract: The SpaRTaN Initial Training Network will train a new generation of interdisciplinary researchers in sparse representations and compressed sensing, contributing to Europe’s leading role in scientific innovation.
By bringing together leading academic and industry groups with expertise in sparse representations, compressed sensing, machine learning and optimisation, and with an interest in applications such as hyperspectral imaging, audio signal processing and video analytics, this project will create an interdisciplinary, trans-national and inter-sectorial training network to enhance mobility and training of researchers in this area.
SpaRTaN is funded under the FP7-PEOPLE-2013-ITN call and is part of the Marie Curie Actions — Initial Training Networks (ITN) funding scheme: Project number - 607290</p>
            </li>
          </sanspuceslist>
        </subsection>
        <subsection id="uid100" level="3">
          <bodyTitle>SEQUOIA</bodyTitle>
          <sanspuceslist>
            <li id="uid101">
              <p noindent="true">Title: Robust algorithms for learning from modern data</p>
            </li>
            <li id="uid102">
              <p noindent="true">Programm: H2020</p>
            </li>
            <li id="uid103">
              <p noindent="true">Type: ERC</p>
            </li>
            <li id="uid104">
              <p noindent="true">Duration: 2017-202</p>
            </li>
            <li id="uid105">
              <p noindent="true">Coordinator: Inria</p>
            </li>
            <li id="uid106">
              <p noindent="true">Inria contact: Francis BACH</p>
            </li>
          </sanspuceslist>
        </subsection>
      </subsection>
    </subsection>
    <subsection id="uid107" level="1">
      <bodyTitle>International Initiatives</bodyTitle>
      <subsection id="uid108" level="2">
        <bodyTitle>Inria Associate Teams Not Involved in an Inria International Labs</bodyTitle>
        <subsection id="uid109" level="3">
          <bodyTitle>BigFOKS2</bodyTitle>
          <sanspuceslist>
            <li id="uid110">
              <p noindent="true">Title: Learning from Big Data: First-Order methods for Kernels and Submodular functions</p>
            </li>
            <li id="uid111">
              <p noindent="true">International Partner (Institution - Laboratory - Researcher):</p>
              <sanspuceslist>
                <li id="uid112">
                  <p noindent="true">IISc Bangalore (India)
- Computer Science Department - Chiranjib Bhattacharyya</p>
                </li>
              </sanspuceslist>
            </li>
            <li id="uid113">
              <p noindent="true">Start year: 2016</p>
            </li>
            <li id="uid114">
              <p noindent="true">See also: <ref xlink:href="http://mllab.csa.iisc.ernet.in/indo-french.html" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>mllab.<allowbreak/>csa.<allowbreak/>iisc.<allowbreak/>ernet.<allowbreak/>in/<allowbreak/>indo-french.<allowbreak/>html</ref></p>
            </li>
            <li id="uid115">
              <p noindent="true">Recent advances in sensor technologies have resulted in large amounts of data being generated in a wide array of scientific disciplines. Deriving models from such large datasets, often known as “Big Data”, is one of the important challenges facing many engineering and scientific disciplines. In this proposal we investigate the problem of learning supervised models from Big Data, which has immediate applications in Computational Biology, Computer vision, Natural language processing, Web, E-commerce, etc., where specific structure is often present and hard to take into account with current algorithms. Our focus will be on the algorithmic aspects. Often supervised learning problems can be cast as convex programs. The goal of this proposal will be to derive first-order methods which can be effective for solving such convex programs arising in the Big-Data setting. Keeping this broad goal in mind we investigate two foundational problems which are not well addressed in existing literature. The first problem investigates Stochastic Gradient Descent Algorithms in the context of First-order methods for designing algorithms for Kernel based prediction functions on Large Datasets. The second problem involves solving discrete optimization problems arising in Submodular formulations in Machine Learning, for which first-order methods have not reached the level of speed required for practical applications (notably in computer vision).</p>
            </li>
          </sanspuceslist>
        </subsection>
      </subsection>
    </subsection>
  </partenariat>
  <diffusion id="uid116">
    <bodyTitle>Dissemination</bodyTitle>
    <subsection id="uid117" level="1">
      <bodyTitle>Promoting Scientific Activities</bodyTitle>
      <subsection id="uid118" level="2">
        <bodyTitle>Scientific Events Organisation</bodyTitle>
        <sanspuceslist>
          <li id="uid119">
            <p noindent="true">Alexandre d'Aspremont: Workshop preparation for les Houches in Feb. 2016: “Optimization without borders”, to celebrate Y. Nesterov’s 60th birthday.</p>
          </li>
          <li id="uid120">
            <p noindent="true">Francis Bach: organization of a workshop at IHES (with S. Arlot and A. Celisse), March 2016.</p>
          </li>
          <li id="uid121">
            <p noindent="true">Francis Bach: co-organization of two NIPS workshops.</p>
          </li>
        </sanspuceslist>
      </subsection>
      <subsection id="uid122" level="2">
        <bodyTitle>Scientific Events Selection</bodyTitle>
        <subsection id="uid123" level="3">
          <bodyTitle>Member of the Conference Program Committees</bodyTitle>
          <sanspuceslist>
            <li id="uid124">
              <p noindent="true">Francis Bach: Area chair for ICML 2016</p>
            </li>
            <li id="uid125">
              <p noindent="true">Simon Lacoste-Julien: Area chair for ICML 2016</p>
            </li>
            <li id="uid126">
              <p noindent="true">Simon Lacoste-Julien: Area chair for NIPS 2016</p>
            </li>
          </sanspuceslist>
        </subsection>
      </subsection>
      <subsection id="uid127" level="2">
        <bodyTitle>Journal</bodyTitle>
        <subsection id="uid128" level="3">
          <bodyTitle>Member of Editorial Boards</bodyTitle>
          <sanspuceslist>
            <li id="uid129">
              <p noindent="true">Alexandre d'Aspremont: Associate Editor, SIAM Journal on Optimization (2013-...).</p>
            </li>
            <li id="uid130">
              <p noindent="true">F. Bach: Action Editor, Journal of Machine Learning Research.</p>
            </li>
            <li id="uid131">
              <p noindent="true">F. Bach: Information and Inference, Associate Editor.</p>
            </li>
            <li id="uid132">
              <p noindent="true">F. Bach: SIAM Journal on Imaging Sciences, Associate Editor.</p>
            </li>
            <li id="uid133">
              <p noindent="true">F. Bach: Electronic Journal of Statistics, Associate Editor.</p>
            </li>
          </sanspuceslist>
        </subsection>
      </subsection>
      <subsection id="uid134" level="2">
        <bodyTitle>Invited Talks</bodyTitle>
        <sanspuceslist>
          <li id="uid135">
            <p noindent="true">Alexandre d'Aspremont: Regularized Nonlinear Acceleration, BIRS workshop, Oaxaca, October 2016.</p>
          </li>
          <li id="uid136">
            <p noindent="true">Alexandre d'Aspremont: Optimal Affine Invariant Smooth Minimization Algorithms, Institut des hautes études scientifiques, June 2016.</p>
          </li>
          <li id="uid137">
            <p noindent="true">Alexandre d'Aspremont: Optimal Affine Invariant Smooth Minimization Algorithms, Nexus of Information and Computation Theories, Institut Henri Poincaré, March 2016.</p>
          </li>
          <li id="uid138">
            <p noindent="true">Alexandre d'Aspremont: Optimal Affine Invariant Smooth Minimization Algorithms, Workshop on Algorithms and Dynamics for Games and Optimization, Santiago Chile, January 2016.</p>
          </li>
          <li id="uid139">
            <p noindent="true">Francis Bach: Winter School on Signal processing, Bonn, January 2016.</p>
          </li>
          <li id="uid140">
            <p noindent="true">Francis Bach: "Optimization without borders", Les Houches, February 2016.</p>
          </li>
          <li id="uid141">
            <p noindent="true">Francis Bach: Oberwolfach, March 2016.</p>
          </li>
          <li id="uid142">
            <p noindent="true">Francis Bach: Dali meeting, Sestri Levante, Italy, March 2016.</p>
          </li>
          <li id="uid143">
            <p noindent="true">Francis Bach: ETH Computer Science Colloquium, April 2016.</p>
          </li>
          <li id="uid144">
            <p noindent="true">Francis Bach: Workshop San Servolo, May 2016.</p>
          </li>
          <li id="uid145">
            <p noindent="true">Francis Bach: Machine Learning summer school Cadiz, May 2016.</p>
          </li>
          <li id="uid146">
            <p noindent="true">Francis Bach: Summer school, Bangalore, July 2016.</p>
          </li>
          <li id="uid147">
            <p noindent="true">Francis Bach: ICCOPT conference, plenary speaker, August 2016.</p>
          </li>
          <li id="uid148">
            <p noindent="true">Francis Bach: Workshop, Haifa, Septembre 2016.</p>
          </li>
          <li id="uid149">
            <p noindent="true">Francis Bach: Statistics Seminar, Cambridge, October 2016.</p>
          </li>
          <li id="uid150">
            <p noindent="true">Francis Bach: BIRS Oaxaca, October 2016.</p>
          </li>
          <li id="uid151">
            <p noindent="true">Francis Bach: NIPS workshops (three presentations), December 2016.</p>
          </li>
          <li id="uid152">
            <p noindent="true">Damien Garreau: “Consistent multiple change-point detection with kernels”, Group meeting of Geometrica Inria project team, Saclay (February 18, 2016).</p>
          </li>
          <li id="uid153">
            <p noindent="true">Damien Garreau: “Consistent multiple change-point detection with kernels”, Inria Junior Seminar, Paris (March 15, 2016).</p>
          </li>
          <li id="uid154">
            <p noindent="true">Damien Garreau: “Consistent multiple change-point detection with kernels”, Colloque final de l'ANR Calibration, Nice (April 7, 2016).</p>
          </li>
          <li id="uid155">
            <p noindent="true">Damien Garreau: “Consistent multiple change-point detection with kernels”, Colloque Jeunes probabilistes et Statisticiens, Les Houches (April 18, 2016).</p>
          </li>
          <li id="uid156">
            <p noindent="true">Pascal Germain: “A Representation Learning Approach for Domain Adaptation”, Tao Seminars, Université Paris-Sud, Paris, France, March 2016.</p>
          </li>
          <li id="uid157">
            <p noindent="true">Pascal Germain: “A Representation Learning Approach for Domain Adaptation”, Data Intelligence Group Seminars, Université Jean-Monnet, Saint-Étienne, France, March 2016.</p>
          </li>
          <li id="uid158">
            <p noindent="true">Pascal Germain: “Variations on the PAC-Bayesian Bound”, Bayes in Paris Seminar at ENSAE, Paris, France, June 2016.</p>
          </li>
          <li id="uid159">
            <p noindent="true">Pascal Germain: “Variations on the PAC-Bayesian Bound”, Séminaires du département d'informatique et de génie logiciel, Université Laval, Quebec, Canada, July 2016.</p>
          </li>
          <li id="uid160">
            <p noindent="true">Simon Lacoste-Julien: "On the Global Linear Convergence of Frank-Wolfe Optimization
Variants", invited talk in the Conic and Polynomial Optimization
cluster at ICCOPT 2016, Tokyo, Japan, August 2016..</p>
          </li>
          <li id="uid161">
            <p noindent="true">Simon Lacoste-Julien: "On the Global Linear Convergence of Frank-Wolfe Optimization
Variants", invited talk in the Learning and Optimization workshop of
DALI meeting, Sestri Levante, Italy, April 2016.</p>
          </li>
          <li id="uid162">
            <p noindent="true">Simon Lacoste-Julien: "Modern Optimization for Structured Machine Learning", CS &amp; OR
Department Colloquium, Université de Montréal, Montreal, Canada,
February 2016.</p>
          </li>
          <li id="uid163">
            <p noindent="true">Antoine Recanati: Presentation at the group meeting of Mines ParisTech Centre for Computational Biology (CBIO) at Institut Curie, October, 18th 2016.</p>
          </li>
        </sanspuceslist>
      </subsection>
      <subsection id="uid164" level="2">
        <bodyTitle>Leadership within the Scientific Community</bodyTitle>
        <sanspuceslist>
          <li id="uid165">
            <p noindent="true">Alexandre d'Aspremont: Porteur de l'IRIS PSL “Science des données, données de la science”.</p>
          </li>
          <li id="uid166">
            <p noindent="true">Alexandre d'Aspremont: Co-scientific director of Master's program MASH (Mathématiques, Apprentissage et Sciences Humaines), with ENS - Paris Dauphine.</p>
          </li>
          <li id="uid167">
            <p noindent="true">Alexandre d'Aspremont: Scientific committee, programme Gaspard Monge pour l'Optimisation.</p>
          </li>
        </sanspuceslist>
      </subsection>
    </subsection>
    <subsection id="uid168" level="1">
      <bodyTitle>Teaching - Supervision - Juries</bodyTitle>
      <subsection id="uid169" level="2">
        <bodyTitle>Teaching</bodyTitle>
        <sanspuceslist>
          <li id="uid170">
            <p noindent="true">Master: A. d'Aspremont, M1 course on Optimization: ENS Paris, 21h</p>
          </li>
          <li id="uid171">
            <p noindent="true">Master: A. d'Aspremont, M2 course on Optimization: MVA, ENS Cachan, 21h</p>
          </li>
          <li id="uid172">
            <p noindent="true">Master : F. Bach (together with J.-P. Vert), “Apprentissage statistique”, 35h, M1, Ecole Normale Supérieure.</p>
          </li>
          <li id="uid173">
            <p noindent="true">Master : F. Bach (together with G. Obozinski), "Graphical models", 30h, M2 (MVA), ENS Cachan.</p>
          </li>
          <li id="uid174">
            <p noindent="true">Master : F. Bach , 20h, M2 (Mathématiques de l'aléatoire), Université Paris-Sud.</p>
          </li>
          <li id="uid175">
            <p noindent="true">Mastere (M1): S. Lacoste-Julien, F. Vogel, “Projets informatiques”,
10h, Université de Paris-Dauphine, Master M2: Mathématiques,
Apprentissage et Sciences Humaines (MASH)</p>
          </li>
          <li id="uid176">
            <p noindent="true">Master : A. Osokin (together with K. Alahari), “The introduction to discrete optimization”, 30h, M2, Centrale Supélec</p>
          </li>
          <li id="uid177">
            <p noindent="true">Master: Fabian Pedredoga, Machine learning with scikit-learn, Master Mathématiques, Apprentissage et Sciences Humaines (MASH), Paris Dauphine.</p>
          </li>
        </sanspuceslist>
      </subsection>
      <subsection id="uid178" level="2">
        <bodyTitle>Supervision</bodyTitle>
        <sanspuceslist>
          <li id="uid179">
            <p noindent="true">PhD: Anastasia Podosinnikova, November 2016, co-advised by Francis Bach and Simon Lacoste-Julien</p>
          </li>
          <li id="uid180">
            <p noindent="true">PhD: Thomas Schatz, September 2016, co-advised by and E. Dupoux (ENS, cognitive sciences).</p>
          </li>
          <li id="uid181">
            <p noindent="true">PhD: Sesh Kumar, September 2016, advised by F. Bach.</p>
          </li>
          <li id="uid182">
            <p noindent="true">PhD in progress : Nom du doctorant, titre (provisoire) du mémoire, date du début de la thèse, encadrant(s)</p>
          </li>
          <li id="uid183">
            <p noindent="true">PhD in progress : Jean-Baptiste Alayrac, co-advised by Simon Lacoste-Julien, Josef Sivic and Ivan Laptev, started Sept. 2014.</p>
          </li>
          <li id="uid184">
            <p noindent="true">PhD in progress : Rémi Leblond, advised by Simon Lacoste-Julien, started Sept. 2015.</p>
          </li>
          <li id="uid185">
            <p noindent="true">PhD in progress : Gauthier Gidel, advised by Simon Lacoste-Julien, started Sept. 2016.</p>
          </li>
          <li id="uid186">
            <p noindent="true">PhD in progress : Vincent Roulet, directed by Alexandre d'Aspremont, started as a PhD on Oct. 1 2014.</p>
          </li>
          <li id="uid187">
            <p noindent="true">PhD in progress : Nicolas Flammarion, co-directed by Alexandre d'Aspremont and Francis Bach, started Sept. 2013.</p>
          </li>
          <li id="uid188">
            <p noindent="true">PhD in progress : Damien Scieur, co-directed with Alexandre d'Aspremont and Francis Bach, started Sept. 2015.</p>
          </li>
          <li id="uid189">
            <p noindent="true">PhD in progress : Antoine Recanati, directed by Alexandre d'Aspremont, started Sept. 2015.</p>
          </li>
          <li id="uid190">
            <p noindent="true">PhD in progress: Rafael Rezende, September 2013, F. Bach, co-advised with J. Ponce.</p>
          </li>
          <li id="uid191">
            <p noindent="true">PhD in progress: PhD in progress: Christophe Dupuy, January 2014, co-advised by F. Bach and C. Diot (Technicolor).</p>
          </li>
          <li id="uid192">
            <p noindent="true">PhD in progress: Damien Garreau, September 2014, co-advised by S. Arlot and G. Biau.</p>
          </li>
          <li id="uid193">
            <p noindent="true">PhD in progress: Anaël Bonneton, December 2014, co- advised by F. Bach, located in Agence nationale de la sécurité des systèmes d’information (ANSSI).</p>
          </li>
          <li id="uid194">
            <p noindent="true">PhD in progress: Dmitry Babichev, September 2015, co-advised by F. Bach and A. Judistky (Univ. Grenoble).</p>
          </li>
          <li id="uid195">
            <p noindent="true">PhD in progress: Tatiana Shpakova, September 2015, advised by F. Bach.</p>
          </li>
        </sanspuceslist>
      </subsection>
      <subsection id="uid196" level="2">
        <bodyTitle>Juries</bodyTitle>
        <sanspuceslist>
          <li id="uid197">
            <p noindent="true">Alexandre d'Aspremont: PhD Committee for Igor Colin, Nov. 2016.</p>
          </li>
          <li id="uid198">
            <p noindent="true">Francis Bach: PhD Committee for Alain Durmus, Dec. 2016.</p>
          </li>
        </sanspuceslist>
      </subsection>
    </subsection>
  </diffusion>
  <biblio id="bibliography" html="bibliography" numero="10" titre="Bibliography">
    
    <biblStruct id="sierra-2016-bid32" type="article" rend="year" n="cite:fogel:hal-00907529">
      <identifiant type="hal" value="hal-00907529"/>
      <analytic>
        <title level="a">Phase retrieval for imaging problems</title>
        <author>
          <persName key="sierra-2014-idp89120">
            <foreName>Fajwel</foreName>
            <surname>Fogel</surname>
            <initial>F.</initial>
          </persName>
          <persName>
            <foreName>Irène</foreName>
            <surname>Waldspurger</surname>
            <initial>I.</initial>
          </persName>
          <persName>
            <foreName>Alexandre</foreName>
            <surname>D'Aspremont</surname>
            <initial>A.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid03116">
        <idno type="issn">1867-2949</idno>
        <title level="j">Mathematical Programming Computations</title>
        <imprint>
          <biblScope type="volume">8</biblScope>
          <biblScope type="number">3</biblScope>
          <dateStruct>
            <month>September</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">311-335</biblScope>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-00907529" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-00907529</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sierra-2016-bid22" type="inproceedings" rend="year" n="cite:alayrac:hal-01171193">
      <identifiant type="hal" value="hal-01171193"/>
      <analytic>
        <title level="a">Unsupervised Learning from Narrated Instruction Videos</title>
        <author>
          <persName key="sierra-2014-idp84192">
            <foreName>Jean-Baptiste</foreName>
            <surname>Alayrac</surname>
            <initial>J.-B.</initial>
          </persName>
          <persName key="willow-2014-idp114800">
            <foreName>Piotr</foreName>
            <surname>Bojanowski</surname>
            <initial>P.</initial>
          </persName>
          <persName key="willow-2015-idp100752">
            <foreName>Nishant</foreName>
            <surname>Agrawal</surname>
            <initial>N.</initial>
          </persName>
          <persName key="willow-2014-idp103392">
            <foreName>Josef</foreName>
            <surname>Sivic</surname>
            <initial>J.</initial>
          </persName>
          <persName key="willow-2014-idp101920">
            <foreName>Ivan</foreName>
            <surname>Laptev</surname>
            <initial>I.</initial>
          </persName>
          <persName key="sierra-2014-idm26584">
            <foreName>Simon</foreName>
            <surname>Lacoste-Julien</surname>
            <initial>S.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">CVPR2016 - 29th IEEE Conference on Computer Vision and Pattern Recognition</title>
        <loc>Las Vegas, United States</loc>
        <imprint>
          <dateStruct>
            <month>June</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01171193" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01171193</ref>
        </imprint>
        <meeting id="cid82398">
          <title>IEEE International Conference on Computer Vision and Pattern Recognition</title>
          <num>2016</num>
          <abbr type="sigle">CVPR</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sierra-2016-bid7" type="inproceedings" rend="year" n="cite:bach:hal-01321532">
      <identifiant type="hal" value="hal-01321532"/>
      <analytic>
        <title level="a">Highly-Smooth Zero-th Order Online Optimization Vianney Perchet</title>
        <author>
          <persName key="sierra-2014-idm29296">
            <foreName>Francis</foreName>
            <surname>Bach</surname>
            <initial>F.</initial>
          </persName>
          <persName key="sierra-2014-idp66480">
            <foreName>Vianney</foreName>
            <surname>Perchet</surname>
            <initial>V.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">Conference on Learning Theory (COLT)</title>
        <loc>New York, United States</loc>
        <imprint>
          <dateStruct>
            <month>June</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01321532" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01321532</ref>
        </imprint>
        <meeting id="cid29437">
          <title>Annual Conference on Learning Theory</title>
          <num>29</num>
          <abbr type="sigle">COLT</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sierra-2016-bid2" type="inproceedings" rend="year" n="cite:balamurugan:hal-01319293">
      <identifiant type="hal" value="hal-01319293"/>
      <analytic>
        <title level="a">Stochastic Variance Reduction Methods for Saddle-Point Problems</title>
        <author>
          <persName>
            <foreName>Palaniappan</foreName>
            <surname>Balamurugan</surname>
            <initial>P.</initial>
          </persName>
          <persName key="sierra-2014-idm29296">
            <foreName>Francis</foreName>
            <surname>Bach</surname>
            <initial>F.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="no" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">Neural Information Processing Systems (NIPS)</title>
        <loc>Barcelona, Spain</loc>
        <title level="s">Advances in Neural Information Processing Systems</title>
        <imprint>
          <dateStruct>
            <month>December</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01319293" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01319293</ref>
        </imprint>
        <meeting id="cid29560">
          <title>Annual Conference on Neural Information Processing Systems</title>
          <num>23</num>
          <abbr type="sigle">NIPS</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sierra-2016-bid20" type="inproceedings" rend="year" n="cite:bartunov:hal-01404056">
      <identifiant type="hal" value="hal-01404056"/>
      <analytic>
        <title level="a">Breaking Sticks and Ambiguities with Adaptive Skip-gram</title>
        <author>
          <persName>
            <foreName>Sergey</foreName>
            <surname>Bartunov</surname>
            <initial>S.</initial>
          </persName>
          <persName>
            <foreName>Dmitry</foreName>
            <surname>Kondrashkin</surname>
            <initial>D.</initial>
          </persName>
          <persName key="sierra-2014-idp67776">
            <foreName>Anton</foreName>
            <surname>Osokin</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Dmitry</foreName>
            <surname>Vetrov</surname>
            <initial>D.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">Proceedings of the 19th International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
        <loc>Cadiz, Spain</loc>
        <imprint>
          <dateStruct>
            <month>May</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">130–138</biblScope>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01404056" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01404056</ref>
        </imprint>
        <meeting id="cid388734">
          <title>International Conference on Artificial Intelligence and Statistics</title>
          <num>19</num>
          <abbr type="sigle">AISTATS</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sierra-2016-bid15" type="inproceedings" rend="year" n="cite:begin:hal-01384783">
      <identifiant type="hal" value="hal-01384783"/>
      <analytic>
        <title level="a">PAC-Bayesian Bounds based on the Rényi Divergence</title>
        <author>
          <persName>
            <foreName>Luc</foreName>
            <surname>Bégin</surname>
            <initial>L.</initial>
          </persName>
          <persName key="sierra-2015-idp93952">
            <foreName>Pascal</foreName>
            <surname>Germain</surname>
            <initial>P.</initial>
          </persName>
          <persName>
            <foreName>François</foreName>
            <surname>Laviolette</surname>
            <initial>F.</initial>
          </persName>
          <persName>
            <foreName>Jean-Francis</foreName>
            <surname>Roy</surname>
            <initial>J.-F.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="no" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">International Conference on Artificial Intelligence and Statistics (AISTATS 2016)</title>
        <loc>Cadiz, Spain</loc>
        <title level="s">Proceedings of the 19th International Conference on Artificial Intelligence and Statistics</title>
        <imprint>
          <dateStruct>
            <month>May</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01384783" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01384783</ref>
        </imprint>
        <meeting id="cid388734">
          <title>International Conference on Artificial Intelligence and Statistics</title>
          <num>19</num>
          <abbr type="sigle">AISTATS</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sierra-2016-bid26" type="inproceedings" rend="year" n="cite:colin:hal-01383111">
      <identifiant type="hal" value="hal-01383111"/>
      <analytic>
        <title level="a">Decentralized Topic Modelling with Latent Dirichlet Allocation</title>
        <author>
          <persName>
            <foreName>Igor</foreName>
            <surname>Colin</surname>
            <initial>I.</initial>
          </persName>
          <persName key="sierra-2014-idp86672">
            <foreName>Christophe</foreName>
            <surname>Dupuy</surname>
            <initial>C.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="no" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">NIPS 2016 - 30th Conference on Neural Information Processing Systems</title>
        <loc>Barcelone, Spain</loc>
        <imprint>
          <dateStruct>
            <month>December</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01383111" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01383111</ref>
        </imprint>
        <meeting id="cid29560">
          <title>Annual Conference on Neural Information Processing Systems</title>
          <num>30</num>
          <abbr type="sigle">NIPS</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sierra-2016-bid23" type="inproceedings" rend="year" n="cite:genevay:hal-01321664">
      <identifiant type="hal" value="hal-01321664"/>
      <analytic>
        <title level="a">Stochastic Optimization for Large-scale Optimal Transport</title>
        <author>
          <persName key="mokaplan-2014-idp65568">
            <foreName>Aude</foreName>
            <surname>Genevay</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Marco</foreName>
            <surname>Cuturi</surname>
            <initial>M.</initial>
          </persName>
          <persName key="mokaplan-2015-idp103960">
            <foreName>Gabriel</foreName>
            <surname>Peyré</surname>
            <initial>G.</initial>
          </persName>
          <persName key="sierra-2014-idm29296">
            <foreName>Francis</foreName>
            <surname>Bach</surname>
            <initial>F.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <editor role="editor">
          <persName>
            <foreName/>
            <surname>NIPS</surname>
            <initial/>
          </persName>
        </editor>
        <title level="m">NIPS 2016 - Thirtieth Annual Conference on Neural Information Processing System</title>
        <loc>Barcelona, Spain</loc>
        <title level="s">Proc. NIPS 2016</title>
        <imprint>
          <dateStruct>
            <month>December</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01321664" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01321664</ref>
        </imprint>
        <meeting id="cid29560">
          <title>Annual Conference on Neural Information Processing Systems</title>
          <num>30</num>
          <abbr type="sigle">NIPS</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sierra-2016-bid13" type="inproceedings" rend="year" n="cite:germain:hal-01324072">
      <identifiant type="hal" value="hal-01324072"/>
      <analytic>
        <title level="a">PAC-Bayesian Theory Meets Bayesian Inference</title>
        <author>
          <persName key="sierra-2015-idp93952">
            <foreName>Pascal</foreName>
            <surname>Germain</surname>
            <initial>P.</initial>
          </persName>
          <persName key="sierra-2014-idm29296">
            <foreName>Francis</foreName>
            <surname>Bach</surname>
            <initial>F.</initial>
          </persName>
          <persName>
            <foreName>Alexandre</foreName>
            <surname>Lacoste</surname>
            <initial>A.</initial>
          </persName>
          <persName key="sierra-2014-idm26584">
            <foreName>Simon</foreName>
            <surname>Lacoste-Julien</surname>
            <initial>S.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">Neural Information Processing Systems (NIPS 2016)</title>
        <loc>Barcelone, Spain</loc>
        <title level="s">Proceedings of the Neural Information Processing Systems Conference</title>
        <imprint>
          <dateStruct>
            <month>December</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01324072" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01324072</ref>
        </imprint>
        <meeting id="cid29560">
          <title>Annual Conference on Neural Information Processing Systems</title>
          <num>30</num>
          <abbr type="sigle">NIPS</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sierra-2016-bid14" type="inproceedings" rend="year" n="cite:germain:hal-01307045">
      <identifiant type="hal" value="hal-01307045"/>
      <analytic>
        <title level="a">A New PAC-Bayesian Perspective on Domain Adaptation</title>
        <author>
          <persName key="sierra-2015-idp93952">
            <foreName>Pascal</foreName>
            <surname>Germain</surname>
            <initial>P.</initial>
          </persName>
          <persName>
            <foreName>Amaury</foreName>
            <surname>Habrard</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>François</foreName>
            <surname>Laviolette</surname>
            <initial>F.</initial>
          </persName>
          <persName>
            <foreName>Emilie</foreName>
            <surname>Morvant</surname>
            <initial>E.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">33rd International Conference on Machine Learning (ICML 2016)</title>
        <loc>New York, NY, United States</loc>
        <title level="s">Proceedings of the 33rd International Conference on Machine Learning</title>
        <imprint>
          <dateStruct>
            <month>June</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01307045" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01307045</ref>
        </imprint>
        <meeting id="cid32516">
          <title>International Conference on Machine Learning</title>
          <num>33</num>
          <abbr type="sigle">ICML</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sierra-2016-bid34" type="inproceedings" rend="year" n="cite:goyal:hal-01329763">
      <identifiant type="hal" value="hal-01329763"/>
      <analytic>
        <title level="a">Théorèmes PAC-Bayésiens pour l'apprentissage multi-vues</title>
        <author>
          <persName>
            <foreName>Anil</foreName>
            <surname>Goyal</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Emilie</foreName>
            <surname>Morvant</surname>
            <initial>E.</initial>
          </persName>
          <persName key="sierra-2015-idp93952">
            <foreName>Pascal</foreName>
            <surname>Germain</surname>
            <initial>P.</initial>
          </persName>
          <persName>
            <foreName>Massih-Reza</foreName>
            <surname>Amini</surname>
            <initial>M.-R.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="no" x-proceedings="no" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">Conférence Francophone sur l'Apprentissage Automatique (CAp)</title>
        <loc>Marseille, France</loc>
        <imprint>
          <dateStruct>
            <month>July</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01329763" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01329763</ref>
        </imprint>
        <meeting id="cid50509">
          <title>Conférence Francophone sur l'Apprentissage Automatique</title>
          <num>2013</num>
          <abbr type="sigle">CAP</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sierra-2016-bid21" type="inproceedings" rend="year" n="cite:kirillov:hal-01404071">
      <identifiant type="hal" value="hal-01404071"/>
      <analytic>
        <title level="a">Deep Part-Based Generative Shape Model with Latent Variables</title>
        <author>
          <persName>
            <foreName>Alexander</foreName>
            <surname>Kirillov</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Mikhail</foreName>
            <surname>Gavrikov</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>Ekaterina</foreName>
            <surname>Lobacheva</surname>
            <initial>E.</initial>
          </persName>
          <persName key="sierra-2014-idp67776">
            <foreName>Anton</foreName>
            <surname>Osokin</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Dmitry</foreName>
            <surname>Vetrov</surname>
            <initial>D.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">27th British Machine Vision Conference (BMVC 2016)</title>
        <loc>York, United Kingdom</loc>
        <imprint>
          <dateStruct>
            <month>September</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01404071" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01404071</ref>
        </imprint>
        <meeting id="cid38519">
          <title>British Machine Vision Conference</title>
          <num>27</num>
          <abbr type="sigle">BMVC</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sierra-2016-bid33" type="inproceedings" rend="year" n="cite:lajugie:hal-01251018">
      <identifiant type="hal" value="hal-01251018"/>
      <analytic>
        <title level="a">A weakly-supervised discriminative model for audio-to-score alignment</title>
        <author>
          <persName key="sierra-2014-idp92792">
            <foreName>Rémi</foreName>
            <surname>Lajugie</surname>
            <initial>R.</initial>
          </persName>
          <persName key="willow-2014-idp114800">
            <foreName>Piotr</foreName>
            <surname>Bojanowski</surname>
            <initial>P.</initial>
          </persName>
          <persName key="mutant-2014-idp110360">
            <foreName>Philippe</foreName>
            <surname>Cuvillier</surname>
            <initial>P.</initial>
          </persName>
          <persName key="sierra-2014-idm27832">
            <foreName>Sylvain</foreName>
            <surname>Arlot</surname>
            <initial>S.</initial>
          </persName>
          <persName key="sierra-2014-idm29296">
            <foreName>Francis</foreName>
            <surname>Bach</surname>
            <initial>F.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">41st International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</title>
        <loc>Shanghai, China</loc>
        <title level="s">Proceedings of the 41st International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</title>
        <imprint>
          <dateStruct>
            <month>March</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01251018" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01251018</ref>
        </imprint>
        <meeting id="cid80145">
          <title>IEEE International Conference on Acoustics, Speech and Signal Processing</title>
          <num>41</num>
          <abbr type="sigle">ICASSP</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sierra-2016-bid29" type="inproceedings" rend="year" n="cite:landrieu:hal-01306786">
      <identifiant type="hal" value="hal-01306786"/>
      <analytic>
        <title level="a">Cut Pursuit: fast algorithms to learn piecewise constant functions</title>
        <author>
          <persName key="sierra-2014-idp94016">
            <foreName>Loic</foreName>
            <surname>Landrieu</surname>
            <initial>L.</initial>
          </persName>
          <persName key="sierra-2014-idp96488">
            <foreName>Guillaume</foreName>
            <surname>Obozinski</surname>
            <initial>G.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="no" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">19th International Conference on Artificial Intelligence and Statistics (AISTATS 2016)</title>
        <loc>Cadix, Spain</loc>
        <imprint>
          <dateStruct>
            <month>May</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01306786" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01306786</ref>
        </imprint>
        <meeting id="cid388734">
          <title>International Conference on Artificial Intelligence and Statistics</title>
          <num>19</num>
          <abbr type="sigle">AISTATS</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sierra-2016-bid4" type="inproceedings" rend="year" n="cite:osokin:hal-01323727">
      <identifiant type="hal" value="hal-01323727"/>
      <analytic>
        <title level="a">Minding the Gaps for Block Frank-Wolfe Optimization of Structured SVMs</title>
        <author>
          <persName key="sierra-2014-idp67776">
            <foreName>Anton</foreName>
            <surname>Osokin</surname>
            <initial>A.</initial>
          </persName>
          <persName key="sierra-2014-idp84192">
            <foreName>Jean-Baptiste</foreName>
            <surname>Alayrac</surname>
            <initial>J.-B.</initial>
          </persName>
          <persName key="sierra-2014-idp80352">
            <foreName>Isabella</foreName>
            <surname>Lukasewitz</surname>
            <initial>I.</initial>
          </persName>
          <persName key="galen-2014-idp78824">
            <foreName>Puneet K.</foreName>
            <surname>Dokania</surname>
            <initial>P. K.</initial>
          </persName>
          <persName key="sierra-2014-idm26584">
            <foreName>Simon</foreName>
            <surname>Lacoste-Julien</surname>
            <initial>S.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">International Conference on Machine Learning (ICML 2016)</title>
        <loc>New York, United States</loc>
        <imprint>
          <dateStruct>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01323727" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01323727</ref>
        </imprint>
        <meeting id="cid32516">
          <title>International Conference on Machine Learning</title>
          <num>33</num>
          <abbr type="sigle">ICML</abbr>
        </meeting>
      </monogr>
      <note type="bnote">Appears in Proceedings of the 33rd International Conference on Machine Learning (ICML 2016). 31 pages</note>
    </biblStruct>
    
    <biblStruct id="sierra-2016-bid11" type="inproceedings" rend="year" n="cite:shpakova:hal-01354789">
      <identifiant type="hal" value="hal-01354789"/>
      <analytic>
        <title level="a">Parameter Learning for Log-supermodular Distributions</title>
        <author>
          <persName key="sierra-2015-idp90224">
            <foreName>Tatiana</foreName>
            <surname>Shpakova</surname>
            <initial>T.</initial>
          </persName>
          <persName key="sierra-2014-idm29296">
            <foreName>Francis</foreName>
            <surname>Bach</surname>
            <initial>F.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">NIPS 2016 - Thirtieth Annual Conference on Neural Information Processing System</title>
        <loc>Barcelona, Spain</loc>
        <imprint>
          <dateStruct>
            <month>December</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01354789" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01354789</ref>
        </imprint>
        <meeting id="cid29560">
          <title>Annual Conference on Neural Information Processing Systems</title>
          <num>30</num>
          <abbr type="sigle">NIPS</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sierra-2016-bid35" type="techreport" rend="year" n="cite:germain:hal-01134246">
      <identifiant type="hal" value="hal-01134246"/>
      <monogr>
        <title level="m">PAC-Bayesian Theorems for Domain Adaptation with Specialization to Linear Classifiers</title>
        <author>
          <persName key="sierra-2015-idp93952">
            <foreName>Pascal</foreName>
            <surname>Germain</surname>
            <initial>P.</initial>
          </persName>
          <persName>
            <foreName>Amaury</foreName>
            <surname>Habrard</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>François</foreName>
            <surname>Laviolette</surname>
            <initial>F.</initial>
          </persName>
          <persName>
            <foreName>Emilie</foreName>
            <surname>Morvant</surname>
            <initial>E.</initial>
          </persName>
        </author>
        <imprint>
          <publisher>
            <orgName type="institution">Université Jean Monnet, Saint-Étienne (42) ; Département d'Informatique et de Génie Logiciel, Université Laval (Québec) ; ENS Paris ; IST Austria</orgName>
          </publisher>
          <dateStruct>
            <month>August</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01134246" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01134246</ref>
        </imprint>
      </monogr>
      <note type="bnote">This report is a long version of our paper entitled A PAC-Bayesian Approach for Domain Adaptation with Specialization to Linear Classifiers published in the proceedings of the International Conference on Machine Learning (ICML) 2013. We improved our main results, extended our experiments, and proposed an extension to multisource domain adaptation</note>
      <note type="typdoc">Research Report</note>
    </biblStruct>
    
    <biblStruct id="sierra-2016-bid36" type="unpublished" rend="year" n="cite:babichev:hal-01388498">
      <identifiant type="hal" value="hal-01388498"/>
      <monogr>
        <title level="m">Slice inverse regression with score functions</title>
        <author>
          <persName key="sierra-2015-idp70512">
            <foreName>Dmitry</foreName>
            <surname>Babichev</surname>
            <initial>D.</initial>
          </persName>
          <persName key="sierra-2014-idm29296">
            <foreName>Francis</foreName>
            <surname>Bach</surname>
            <initial>F.</initial>
          </persName>
        </author>
        <imprint>
          <dateStruct>
            <month>October</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01388498" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01388498</ref>
        </imprint>
      </monogr>
      <note type="bnote">working paper or preprint</note>
    </biblStruct>
    
    <biblStruct id="sierra-2016-bid27" type="unpublished" rend="year" n="cite:bach:hal-01222319">
      <identifiant type="hal" value="hal-01222319"/>
      <monogr>
        <title level="m">Submodular Functions: from Discrete to Continous Domains</title>
        <author>
          <persName key="sierra-2014-idm29296">
            <foreName>Francis</foreName>
            <surname>Bach</surname>
            <initial>F.</initial>
          </persName>
        </author>
        <imprint>
          <dateStruct>
            <month>February</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01222319" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01222319</ref>
        </imprint>
      </monogr>
      <note type="bnote">working paper or preprint</note>
    </biblStruct>
    
    <biblStruct id="sierra-2016-bid1" type="unpublished" rend="year" n="cite:dieuleveut:hal-01275431">
      <identifiant type="hal" value="hal-01275431"/>
      <monogr>
        <title level="m">Harder, Better, Faster, Stronger Convergence Rates for Least-Squares Regression</title>
        <author>
          <persName key="sierra-2014-idp85448">
            <foreName>Aymeric</foreName>
            <surname>Dieuleveut</surname>
            <initial>A.</initial>
          </persName>
          <persName key="sierra-2014-idp87896">
            <foreName>Nicolas</foreName>
            <surname>Flammarion</surname>
            <initial>N.</initial>
          </persName>
          <persName key="sierra-2014-idm29296">
            <foreName>Francis</foreName>
            <surname>Bach</surname>
            <initial>F.</initial>
          </persName>
        </author>
        <imprint>
          <dateStruct>
            <month>February</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01275431" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01275431</ref>
        </imprint>
      </monogr>
      <note type="bnote">working paper or preprint</note>
    </biblStruct>
    
    <biblStruct id="sierra-2016-bid25" type="unpublished" rend="year" n="cite:dupuy:hal-01383742">
      <identifiant type="hal" value="hal-01383742"/>
      <monogr>
        <title level="m">Learning Determinantal Point Processes in Sublinear Time</title>
        <author>
          <persName key="sierra-2014-idp86672">
            <foreName>Christophe</foreName>
            <surname>Dupuy</surname>
            <initial>C.</initial>
          </persName>
          <persName key="sierra-2014-idm29296">
            <foreName>Francis</foreName>
            <surname>Bach</surname>
            <initial>F.</initial>
          </persName>
        </author>
        <imprint>
          <dateStruct>
            <month>October</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01383742" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01383742</ref>
        </imprint>
      </monogr>
      <note type="bnote">Under review for AISTATS 2017</note>
    </biblStruct>
    
    <biblStruct subtype="nonparu-n" id="sierra-2016-bid24" type="unpublished" rend="year" n="cite:dupuy:hal-01284900">
      <identifiant type="hal" value="hal-01284900"/>
      <monogr>
        <title level="m">Online but Accurate Inference for Latent Variable Models with Local Gibbs Sampling</title>
        <author>
          <persName key="sierra-2014-idp86672">
            <foreName>Christophe</foreName>
            <surname>Dupuy</surname>
            <initial>C.</initial>
          </persName>
          <persName key="sierra-2014-idm29296">
            <foreName>Francis</foreName>
            <surname>Bach</surname>
            <initial>F.</initial>
          </persName>
        </author>
        <imprint>
          <dateStruct>
            <month>July</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01284900" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01284900</ref>
        </imprint>
      </monogr>
      <note type="bnote">Under submission in JMLR</note>
    </biblStruct>
    
    <biblStruct id="sierra-2016-bid19" type="unpublished" rend="year" n="cite:flammarion:hal-01405738">
      <identifiant type="hal" value="hal-01405738"/>
      <monogr>
        <title level="m">Optimal Rates of Statistical Seriation</title>
        <author>
          <persName key="sierra-2014-idp87896">
            <foreName>Nicolas</foreName>
            <surname>Flammarion</surname>
            <initial>N.</initial>
          </persName>
          <persName>
            <foreName>Cheng</foreName>
            <surname>Mao</surname>
            <initial>C.</initial>
          </persName>
          <persName>
            <foreName>Philippe</foreName>
            <surname>Rigollet</surname>
            <initial>P.</initial>
          </persName>
        </author>
        <imprint>
          <dateStruct>
            <month>November</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01405738" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01405738</ref>
        </imprint>
      </monogr>
      <note type="bnote">V2 corrects an error in Lemma A.1, v3 corrects appendix F on unimodal regression where the bounds now hold with polynomial probability rather than exponential</note>
    </biblStruct>
    
    <biblStruct id="sierra-2016-bid18" type="unpublished" rend="year" n="cite:flammarion:hal-01357666">
      <identifiant type="hal" value="hal-01357666"/>
      <monogr>
        <title level="m">Robust Discriminative Clustering with Sparse Regularizers</title>
        <author>
          <persName key="sierra-2014-idp87896">
            <foreName>Nicolas</foreName>
            <surname>Flammarion</surname>
            <initial>N.</initial>
          </persName>
          <persName key="sierra-2015-idp96448">
            <foreName>Balamurugan</foreName>
            <surname>Palaniappan</surname>
            <initial>B.</initial>
          </persName>
          <persName key="sierra-2014-idm29296">
            <foreName>Francis</foreName>
            <surname>Bach</surname>
            <initial>F.</initial>
          </persName>
        </author>
        <imprint>
          <dateStruct>
            <month>August</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01357666" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01357666</ref>
        </imprint>
      </monogr>
      <note type="bnote">working paper or preprint</note>
    </biblStruct>
    
    <biblStruct id="sierra-2016-bid38" type="unpublished" rend="year" n="cite:garreau:hal-01416704">
      <identifiant type="hal" value="hal-01416704"/>
      <monogr>
        <title level="m">Consistent change-point detection with kernels</title>
        <author>
          <persName key="sierra-2014-idp90344">
            <foreName>Damien</foreName>
            <surname>Garreau</surname>
            <initial>D.</initial>
          </persName>
          <persName key="sierra-2014-idm27832">
            <foreName>Sylvain</foreName>
            <surname>Arlot</surname>
            <initial>S.</initial>
          </persName>
        </author>
        <imprint>
          <dateStruct>
            <month>December</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01416704" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01416704</ref>
        </imprint>
      </monogr>
      <note type="bnote">working paper or preprint</note>
    </biblStruct>
    
    <biblStruct id="sierra-2016-bid3" type="unpublished" rend="year" n="cite:gidel:hal-01403348">
      <identifiant type="hal" value="hal-01403348"/>
      <monogr>
        <title level="m">Frank-Wolfe Algorithms for Saddle Point Problems</title>
        <author>
          <persName key="sierra-2016-idp181856">
            <foreName>Gauthier</foreName>
            <surname>Gidel</surname>
            <initial>G.</initial>
          </persName>
          <persName>
            <foreName>Tony</foreName>
            <surname>Jebara</surname>
            <initial>T.</initial>
          </persName>
          <persName key="sierra-2014-idm26584">
            <foreName>Simon</foreName>
            <surname>Lacoste-Julien</surname>
            <initial>S.</initial>
          </persName>
        </author>
        <imprint>
          <dateStruct>
            <month>October</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01403348" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01403348</ref>
        </imprint>
      </monogr>
      <note type="bnote">working paper or preprint</note>
    </biblStruct>
    
    <biblStruct id="sierra-2016-bid16" type="unpublished" rend="year" n="cite:goyal:hal-01336260">
      <identifiant type="hal" value="hal-01336260"/>
      <monogr>
        <title level="m">PAC-Bayesian Theorems for Multiview Learning</title>
        <author>
          <persName>
            <foreName>Anil</foreName>
            <surname>Goyal</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Emilie</foreName>
            <surname>Morvant</surname>
            <initial>E.</initial>
          </persName>
          <persName key="sierra-2015-idp93952">
            <foreName>Pascal</foreName>
            <surname>Germain</surname>
            <initial>P.</initial>
          </persName>
          <persName>
            <foreName>Massih-Reza</foreName>
            <surname>Amini</surname>
            <initial>M.-R.</initial>
          </persName>
        </author>
        <imprint>
          <dateStruct>
            <month>November</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01336260" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01336260</ref>
        </imprint>
      </monogr>
      <note type="bnote">working paper or preprint</note>
    </biblStruct>
    
    <biblStruct id="sierra-2016-bid6" type="unpublished" rend="year" n="cite:lacostejulien:hal-01415335">
      <identifiant type="hal" value="hal-01415335"/>
      <monogr>
        <title level="m">Convergence Rate of Frank-Wolfe for Non-Convex Objectives</title>
        <author>
          <persName key="sierra-2014-idm26584">
            <foreName>Simon</foreName>
            <surname>Lacoste-Julien</surname>
            <initial>S.</initial>
          </persName>
        </author>
        <imprint>
          <dateStruct>
            <month>June</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01415335" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01415335</ref>
        </imprint>
      </monogr>
      <note type="bnote">6 pages</note>
    </biblStruct>
    
    <biblStruct id="sierra-2016-bid5" type="unpublished" rend="year" n="cite:leblond:hal-01407833">
      <identifiant type="hal" value="hal-01407833"/>
      <monogr>
        <title level="m">Asaga: Asynchronous Parallel Saga</title>
        <author>
          <persName key="sierra-2015-idp82816">
            <foreName>Rémi</foreName>
            <surname>Leblond</surname>
            <initial>R.</initial>
          </persName>
          <persName key="sierra-2014-idp97752">
            <foreName>Fabian</foreName>
            <surname>Pedregosa</surname>
            <initial>F.</initial>
          </persName>
          <persName key="sierra-2014-idm26584">
            <foreName>Simon</foreName>
            <surname>Lacoste-Julien</surname>
            <initial>S.</initial>
          </persName>
        </author>
        <imprint>
          <dateStruct>
            <month>December</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01407833" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01407833</ref>
        </imprint>
      </monogr>
      <note type="bnote">working paper or preprint</note>
    </biblStruct>
    
    <biblStruct id="sierra-2016-bid17" type="unpublished" rend="year" n="cite:meurer:hal-01404156">
      <identifiant type="doi" value="10.7287/peerj.preprints.2083v3"/>
      <identifiant type="hal" value="hal-01404156"/>
      <monogr>
        <title level="m">SymPy: Symbolic computing in Python</title>
        <author>
          <persName>
            <foreName>Aaron</foreName>
            <surname>Meurer​</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Christopher P</foreName>
            <surname>Smith</surname>
            <initial>C. P.</initial>
          </persName>
          <persName>
            <foreName>Mateusz</foreName>
            <surname>Paprocki</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>Ondřej</foreName>
            <surname>Čertík</surname>
            <initial>O.</initial>
          </persName>
          <persName>
            <foreName>Sergey B</foreName>
            <surname>Kirpichev</surname>
            <initial>S. B.</initial>
          </persName>
          <persName>
            <foreName>Matthew</foreName>
            <surname>Rocklin</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>Amit</foreName>
            <surname>Kumar</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Sergiu</foreName>
            <surname>Ivanov</surname>
            <initial>S.</initial>
          </persName>
          <persName>
            <foreName>Jason K</foreName>
            <surname>Moore</surname>
            <initial>J. K.</initial>
          </persName>
          <persName>
            <foreName>Sartaj</foreName>
            <surname>Singh</surname>
            <initial>S.</initial>
          </persName>
          <persName>
            <foreName>Thilina</foreName>
            <surname>Rathnayake</surname>
            <initial>T.</initial>
          </persName>
          <persName>
            <foreName>Sean</foreName>
            <surname>Vig</surname>
            <initial>S.</initial>
          </persName>
          <persName>
            <foreName>Brian E</foreName>
            <surname>Granger</surname>
            <initial>B. E.</initial>
          </persName>
          <persName>
            <foreName>Richard P</foreName>
            <surname>Muller</surname>
            <initial>R. P.</initial>
          </persName>
          <persName>
            <foreName>Francesco</foreName>
            <surname>Bonazzi</surname>
            <initial>F.</initial>
          </persName>
          <persName>
            <foreName>Harsh</foreName>
            <surname>Gupta</surname>
            <initial>H.</initial>
          </persName>
          <persName>
            <foreName>Shivam</foreName>
            <surname>Vats</surname>
            <initial>S.</initial>
          </persName>
          <persName key="lfant-2014-idp78960">
            <foreName>Fredrik</foreName>
            <surname>Johansson</surname>
            <initial>F.</initial>
          </persName>
          <persName key="sierra-2014-idp97752">
            <foreName>Fabian</foreName>
            <surname>Pedregosa</surname>
            <initial>F.</initial>
          </persName>
          <persName>
            <foreName>Matthew J</foreName>
            <surname>Curry</surname>
            <initial>M. J.</initial>
          </persName>
          <persName>
            <foreName>Andy R</foreName>
            <surname>Terrel</surname>
            <initial>A. R.</initial>
          </persName>
          <persName>
            <foreName>Štěpán</foreName>
            <surname>Roučka</surname>
            <initial>Š.</initial>
          </persName>
          <persName>
            <foreName>Ashutosh</foreName>
            <surname>Saboo</surname>
            <initial>A.</initial>
          </persName>
          <persName>
            <foreName>Isuru</foreName>
            <surname>Fernando</surname>
            <initial>I.</initial>
          </persName>
          <persName>
            <foreName>Sumith</foreName>
            <surname>Kulal</surname>
            <initial>S.</initial>
          </persName>
          <persName>
            <foreName>Robert</foreName>
            <surname>Cimrman</surname>
            <initial>R.</initial>
          </persName>
          <persName>
            <foreName>Anthony</foreName>
            <surname>Scopatz</surname>
            <initial>A.</initial>
          </persName>
        </author>
        <imprint>
          <dateStruct>
            <month>May</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01404156" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01404156</ref>
        </imprint>
      </monogr>
      <note type="bnote">working paper or preprint</note>
    </biblStruct>
    
    <biblStruct id="sierra-2016-bid12" type="unpublished" rend="year" n="cite:podosinnikova:hal-01291060">
      <identifiant type="hal" value="hal-01291060"/>
      <monogr>
        <title level="m">Beyond CCA: Moment Matching for Multi-View Models</title>
        <author>
          <persName key="sierra-2014-idp98984">
            <foreName>Anastasia</foreName>
            <surname>Podosinnikova</surname>
            <initial>A.</initial>
          </persName>
          <persName key="sierra-2014-idm29296">
            <foreName>Francis</foreName>
            <surname>Bach</surname>
            <initial>F.</initial>
          </persName>
          <persName key="sierra-2014-idm26584">
            <foreName>Simon</foreName>
            <surname>Lacoste-Julien</surname>
            <initial>S.</initial>
          </persName>
        </author>
        <imprint>
          <dateStruct>
            <month>March</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01291060" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01291060</ref>
        </imprint>
      </monogr>
      <note type="bnote">working paper or preprint</note>
    </biblStruct>
    
    <biblStruct id="sierra-2016-bid30" type="unpublished" rend="year" n="cite:roulet:hal-01239305">
      <identifiant type="hal" value="hal-01239305"/>
      <monogr>
        <title level="m">Learning with Clustering Structure</title>
        <author>
          <persName key="sierra-2014-idp100216">
            <foreName>Vincent</foreName>
            <surname>Roulet</surname>
            <initial>V.</initial>
          </persName>
          <persName key="sierra-2014-idp89120">
            <foreName>Fajwel</foreName>
            <surname>Fogel</surname>
            <initial>F.</initial>
          </persName>
          <persName>
            <foreName>Alexandre</foreName>
            <surname>D'Aspremont</surname>
            <initial>A.</initial>
          </persName>
          <persName key="sierra-2014-idm29296">
            <foreName>Francis</foreName>
            <surname>Bach</surname>
            <initial>F.</initial>
          </persName>
        </author>
        <imprint>
          <dateStruct>
            <month>October</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01239305" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01239305</ref>
        </imprint>
      </monogr>
      <note type="bnote">working paper or preprint</note>
    </biblStruct>
    
    <biblStruct subtype="nonparu-n" id="sierra-2016-bid31" type="unpublished" rend="year" n="cite:schmidt:hal-00860051">
      <identifiant type="hal" value="hal-00860051"/>
      <monogr>
        <title level="m">Minimizing Finite Sums with the Stochastic Average Gradient</title>
        <author>
          <persName>
            <foreName>Mark</foreName>
            <surname>Schmidt</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>Nicolas</foreName>
            <surname>Le Roux</surname>
            <initial>N.</initial>
          </persName>
          <persName key="sierra-2014-idm29296">
            <foreName>Francis</foreName>
            <surname>Bach</surname>
            <initial>F.</initial>
          </persName>
        </author>
        <imprint>
          <dateStruct>
            <month>May</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-00860051" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-00860051</ref>
        </imprint>
      </monogr>
      <note type="bnote">Revision from January 2015 submission. Major changes: updated literature follow and discussion of subsequent work, additional Lemma showing the validity of one of the formulas, somewhat simplified presentation of Lyapunov bound, included code needed for checking proofs rather than the polynomials generated by the code, added error regions to the numerical experiments</note>
    </biblStruct>
    
    <biblStruct id="sierra-2016-bid0" type="unpublished" rend="year" n="cite:scieur:hal-01384682">
      <identifiant type="hal" value="hal-01384682"/>
      <monogr>
        <title level="m">Regularized Nonlinear Acceleration</title>
        <author>
          <persName key="sierra-2015-idp88984">
            <foreName>Damien</foreName>
            <surname>Scieur</surname>
            <initial>D.</initial>
          </persName>
          <persName>
            <foreName>Alexandre</foreName>
            <surname>D'Aspremont</surname>
            <initial>A.</initial>
          </persName>
          <persName key="sierra-2014-idm29296">
            <foreName>Francis</foreName>
            <surname>Bach</surname>
            <initial>F.</initial>
          </persName>
        </author>
        <imprint>
          <dateStruct>
            <month>November</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01384682" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01384682</ref>
        </imprint>
      </monogr>
      <note type="bnote">working paper or preprint</note>
    </biblStruct>
    
    <biblStruct id="sierra-2016-bid28" type="unpublished" rend="year" n="cite:seguin:hal-01255765">
      <identifiant type="hal" value="hal-01255765"/>
      <monogr>
        <title level="m">Instance-level video segmentation from object tracks</title>
        <author>
          <persName key="willow-2014-idp125896">
            <foreName>Guillaume</foreName>
            <surname>Seguin</surname>
            <initial>G.</initial>
          </persName>
          <persName key="willow-2014-idp114800">
            <foreName>Piotr</foreName>
            <surname>Bojanowski</surname>
            <initial>P.</initial>
          </persName>
          <persName key="sierra-2014-idp92792">
            <foreName>Rémi</foreName>
            <surname>Lajugie</surname>
            <initial>R.</initial>
          </persName>
          <persName key="willow-2014-idp101920">
            <foreName>Ivan</foreName>
            <surname>Laptev</surname>
            <initial>I.</initial>
          </persName>
        </author>
        <imprint>
          <dateStruct>
            <month>January</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01255765" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01255765</ref>
        </imprint>
      </monogr>
      <note type="bnote">working paper or preprint</note>
    </biblStruct>
    
    <biblStruct id="sierra-2016-bid37" type="unpublished" rend="year" n="cite:seshkumar:hal-01161759">
      <identifiant type="hal" value="hal-01161759"/>
      <monogr>
        <title level="m">Active-set Methods for Submodular Minimization Problems</title>
        <author>
          <persName>
            <foreName>K. S.</foreName>
            <surname>Sesh Kumar</surname>
            <initial>K. S.</initial>
          </persName>
          <persName key="sierra-2014-idm29296">
            <foreName>Francis</foreName>
            <surname>Bach</surname>
            <initial>F.</initial>
          </persName>
        </author>
        <imprint>
          <dateStruct>
            <month>November</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01161759" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01161759</ref>
        </imprint>
      </monogr>
      <note type="bnote">working paper or preprint</note>
    </biblStruct>
    
    <biblStruct id="sierra-2016-bid10" type="article" rend="foot" n="footcite:Hyvarinen">
      <analytic>
        <title level="a">Estimation of non-normalized statistical models by score matching</title>
        <author>
          <persName>
            <foreName>A.</foreName>
            <surname>Hyvärinen</surname>
            <initial>A.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">Journal of Machine Learning Research</title>
        <imprint>
          <biblScope type="volume">6</biblScope>
          <dateStruct>
            <year>2005</year>
          </dateStruct>
          <biblScope type="pages">695–709</biblScope>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sierra-2016-bid8" type="article" rend="foot" n="footcite:Li1991">
      <analytic>
        <title level="a">Sliced Inverse Regression for Dimensional Reduction</title>
        <author>
          <persName>
            <foreName>K.-C.</foreName>
            <surname>Li</surname>
            <initial>K.-C.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">Journal of the American Statistical Association</title>
        <imprint>
          <biblScope type="volume">86</biblScope>
          <dateStruct>
            <year>1991</year>
          </dateStruct>
          <biblScope type="pages">316–327</biblScope>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sierra-2016-bid9" type="article" rend="foot" n="footcite:Stoker1986">
      <analytic>
        <title level="a">Consistent estimation of scaled coefficients</title>
        <author>
          <persName>
            <foreName>T. M.</foreName>
            <surname>Stoker</surname>
            <initial>T. M.</initial>
          </persName>
        </author>
      </analytic>
      <monogr>
        <title level="j">Econometrica</title>
        <imprint>
          <biblScope type="volume">54</biblScope>
          <dateStruct>
            <year>1986</year>
          </dateStruct>
          <biblScope type="pages">1461–1481</biblScope>
        </imprint>
      </monogr>
    </biblStruct>
  </biblio>
</raweb>
