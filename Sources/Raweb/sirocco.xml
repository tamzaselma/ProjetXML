<?xml version="1.0" encoding="utf-8"?>
<raweb xmlns:xlink="http://www.w3.org/1999/xlink" xml:lang="en" year="2016">
  <identification id="sirocco" isproject="true">
    <shortname>SIROCCO</shortname>
    <projectName>Analysis representation, compression and communication of visual data</projectName>
    <theme-de-recherche>Vision, perception and multimedia interpretation</theme-de-recherche>
    <domaine-de-recherche>Perception, Cognition and Interaction</domaine-de-recherche>
    <urlTeam>http://team.inria.fr/sirocco</urlTeam>
    <structure_exterieure type="Labs">
      <libelle>Institut de recherche en informatique et syst√®mes al√©atoires (IRISA)</libelle>
    </structure_exterieure>
    <structure_exterieure type="Organism">
      <libelle>CNRS</libelle>
    </structure_exterieure>
    <structure_exterieure type="Organism">
      <libelle>Universit√© Rennes 1</libelle>
    </structure_exterieure>
    <header_dates_team>Creation of the Project-Team: 2012 January 01</header_dates_team>
    <LeTypeProjet>Project-Team</LeTypeProjet>
    <keywordsSdN>
      <term>5. - Interaction, multimedia and robotics</term>
      <term>5.3. - Image processing and analysis</term>
      <term>5.4. - Computer vision</term>
      <term>5.9. - Signal processing</term>
    </keywordsSdN>
    <keywordsSecteurs>
      <term>6. - IT and telecom</term>
    </keywordsSecteurs>
    <UR name="Rennes"/>
  </identification>
  <team id="uid1">
    <person key="sirocco-2014-idm8448">
      <firstname>Christine</firstname>
      <lastname>Guillemot</lastname>
      <categoryPro>Chercheur</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Team leader, Inria, Senior Researcher</moreinfo>
      <hdr>oui</hdr>
    </person>
    <person key="sirocco-2014-idm5560">
      <firstname>Thomas</firstname>
      <lastname>Maugey</lastname>
      <categoryPro>Chercheur</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Inria, Researcher</moreinfo>
    </person>
    <person key="sirocco-2014-idp86352">
      <firstname>Aline</firstname>
      <lastname>Roumy</lastname>
      <categoryPro>Chercheur</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Inria, 80%, Researcher</moreinfo>
    </person>
    <person key="sirocco-2014-idp87568">
      <firstname>Olivier</firstname>
      <lastname>Le Meur</lastname>
      <categoryPro>Enseignant</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Univ. Rennes I, Associate Professor</moreinfo>
      <hdr>oui</hdr>
    </person>
    <person key="hybrid-2015-idp77552">
      <firstname>Cedric</firstname>
      <lastname>Le Cam</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="sirocco-2014-idp105312">
      <firstname>Mikael</firstname>
      <lastname>Le Pendu</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="sirocco-2015-idp70528">
      <firstname>Xin</firstname>
      <lastname>Su</lastname>
      <categoryPro>Technique</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="sirocco-2014-idp95224">
      <firstname>Martin</firstname>
      <lastname>Alain</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Technicolor, granted by CIFRE, until Mar 2016</moreinfo>
    </person>
    <person key="sirocco-2015-idp74248">
      <firstname>Jean</firstname>
      <lastname>Begaint</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Technicolor, granted by CIFRE</moreinfo>
    </person>
    <person key="sirocco-2016-idp167056">
      <firstname>Pierre</firstname>
      <lastname>David</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Inria, from Oct 2016</moreinfo>
    </person>
    <person key="sirocco-2016-idp169504">
      <firstname>Elian</firstname>
      <lastname>Dib</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Inria, from Oct 2016</moreinfo>
    </person>
    <person key="sirocco-2015-idp76768">
      <firstname>Thierry</firstname>
      <lastname>Dumas</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="sirocco-2014-idp100272">
      <firstname>Julio Cesar</firstname>
      <lastname>Ferreira</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>co-tutelle between the Univ. of Rennes 1 and the Univ. of Uberlandia, until July 2016</moreinfo>
    </person>
    <person key="sirocco-2014-idp101552">
      <firstname>David</firstname>
      <lastname>Gommelet</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Cifre Ericsson, granted by CIFRE</moreinfo>
    </person>
    <person key="sirocco-2016-idp179216">
      <firstname>Fatma</firstname>
      <lastname>Hawary</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Technicolor, granted by CIFRE</moreinfo>
    </person>
    <person key="sirocco-2015-idp80512">
      <firstname>Matthieu</firstname>
      <lastname>Hog</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Technicolor, granted by CIFRE</moreinfo>
    </person>
    <person key="sirocco-2015-idp85520">
      <firstname>Hristina</firstname>
      <lastname>Hristova</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Univ. Rennes I, also with FRVsense</moreinfo>
    </person>
    <person key="sirocco-2016-idp186624">
      <firstname>Dmitry</firstname>
      <lastname>Kuzovkin</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Technicolor, granted by CIFRE, also with FRVSense</moreinfo>
    </person>
    <person key="sirocco-2016-idp189104">
      <firstname>Navid</firstname>
      <lastname>Mahmoudian Bidgoli</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Inria, from Oct 2016</moreinfo>
    </person>
    <person key="sirocco-2015-idp84272">
      <firstname>Mira</firstname>
      <lastname>Rizkallah</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Univ. Rennes I</moreinfo>
    </person>
    <person key="sirocco-2016-idp194000">
      <firstname>Maxime</firstname>
      <lastname>Rousselot</lastname>
      <categoryPro>PhD</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Harmonics, granted by CIFRE, also with FRVSense</moreinfo>
    </person>
    <person key="sirocco-2016-idp196480">
      <firstname>Oriel</firstname>
      <lastname>Frigo</lastname>
      <categoryPro>PostDoc</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Inria, from Dec 2016</moreinfo>
    </person>
    <person key="sirocco-2016-idp198976">
      <firstname>Xiaoran</firstname>
      <lastname>Jiang</lastname>
      <categoryPro>PostDoc</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
    <person key="sirocco-2015-idp86776">
      <firstname>Reuben</firstname>
      <lastname>Farrugia</lastname>
      <categoryPro>Visiteur</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>University of Malta, until Aug 2016</moreinfo>
    </person>
    <person key="sirocco-2016-idp203952">
      <firstname>Sheila S.</firstname>
      <lastname>Hemami</lastname>
      <categoryPro>Visiteur</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Univ. Rennes I, from May 2016 until Aug 2016</moreinfo>
    </person>
    <person key="serpico-2014-idp114008">
      <firstname>Huguette</firstname>
      <lastname>Bechu</lastname>
      <categoryPro>Assistant</categoryPro>
      <research-centre>Rennes</research-centre>
      <moreinfo>Inria</moreinfo>
    </person>
  </team>
  <presentation id="uid2">
    <bodyTitle>Overall Objectives</bodyTitle>
    <subsection id="uid3" level="1">
      <bodyTitle>Introduction</bodyTitle>
      <p>The goal of the SIROCCO project-team is the design and development of algorithms and practical solutions in the areas of analysis, modelling, coding, and communication of images and video signals. The objective is to cover several inter-dependent algorithmic problems of the end-to-end transmission chain from the capturing, compression, transmission to the rendering of the visual data. The project-team activities are structured and organized around the following inter-dependent research axes:</p>
      <simplelist>
        <li id="uid4">
          <p noindent="true">Analysis and modeling for compact representation and navigation <footnote id="uid5" id-text="1">By navigation we refer here to scene navigation by virtual view rendering, and to navigation across slices in volumic medical images.</footnote> in large volumes of visual data <footnote id="uid6" id-text="2">By visual data we refer to natural and medical images, videos, multi-view sequences as well as to visual cues or features extracted from video content.</footnote></p>
        </li>
        <li id="uid7">
          <p noindent="true">Rendering, inpainting and super-resolution of visual data</p>
        </li>
        <li id="uid8">
          <p noindent="true">Representation and compression of visual data</p>
        </li>
        <li id="uid9">
          <p noindent="true">Distributed processing and robust communication of visual data</p>
        </li>
      </simplelist>
      <p>Given the strong impact of standardization in the sector of networked multimedia, SIROCCO, in partnership with industrial companies, seeks to promote its results in standardization (<span class="smallcap" align="left">mpeg</span>). While aiming at generic approaches, some of the solutions developed are applied to practical problems in partnership with industry (Alcatel Lucent, Astrium, Orange labs., Technicolor, Thomson Video Networks) or in the framework of national projects (<span class="smallcap" align="left">ANR-ARSSO</span>, <span class="smallcap" align="left">ANR-PERSEE</span>). The application domains addressed by the project are networked visual applications taking into account their various requirements and needs in terms of compression, of resilience to channel noise and network adaptation, of advanced functionalities such as navigation, and of high quality rendering.</p>
    </subsection>
    <subsection id="uid10" level="1">
      <bodyTitle>Analysis and modeling for compact representation</bodyTitle>
      <p>Analysis and modeling of the visual data are crucial steps for a number of video processing problems: navigation in 3D scenes, compression, loss concealment, denoising, inpainting, editing, content summarization and navigation. The focus is on the extraction of different cues such as scene geometry, edge, texture and motion, on the extraction of high-level features (GIST-like or epitomes), and on the study of computational models of visual attention, useful for different visual processing tasks. In relation to the above problems, the project-team considers various types of image modalities (medical and satellite images, natural 2D still and moving images, multi-view and multi-view plus depth video content).</p>
    </subsection>
    <subsection id="uid11" level="1">
      <bodyTitle>Rendering, inpainting and super-resolution</bodyTitle>
      <p>This research axis addresses the problem of high quality reconstruction of various types of visual data after
decoding. Depending on the application and the corresponding type of content (2D, 3D), various issues are being addressed. For example, to be able to render 3D scenes, depth information is associated with each view as a depth map, and transmitted in order to perform virtual view generation. Given one view with its depth information, depth image-based rendering techniques have the ability to render views in any other spatial positions. However, the issue of intermediate view reconstruction remains a difficult ill-posed problem. Most errors in the view synthesis are caused by incorrect geometry information, inaccurate camera parameters, and occlusions/disocclusions. Efficient inpainting techniques are necessary to restore disocclusions areas. Inpainting techniques are also required in transmission scenarios, where packet losses result in missing data in the video after decoding. The design of efficient mono-view and multi-view super-resolution methods is also part of the project-team objectives to improve the rendering quality, as well as to trade-off quality against transmission rate.</p>
    </subsection>
    <subsection id="uid12" level="1">
      <bodyTitle>Representation and compression of visual data</bodyTitle>
      <p>The objective is to develop algorithmic tools for constructing low-dimensional representations of multi-view video plus depth data, of 2D image and video data, of visual features and of their descriptors. Our approach goes from the design of specific algorithmic tools to the development of complete compression algorithms. The algorithmic problems that we address include data dimensionality reduction, the design of compact representations for multi-view plus depth video content which allow high quality 3D rendering, the design of sparse representation methods and of dictionary learning techniques. The sparsity of the representation indeed depends on how well the dictionary is adapted to the data at hand. The problem of dictionary learning for data-adaptive representations, that goes beyond the concatenation of a few traditional bases, has thus become a key issue which we address for further progress in the area.</p>
      <p>Developing complete compression algorithms necessarily requires tackling visual processing topics beyond the issues of sparse data representation and dimensionality reduction. For example, problems of scalable, perceptual, and metadata-aided coding of 2D and 3D visual data, as well as of near lossless compression of medical image modalities (CT, MRI, virtual microscopy imaging) are tackled. Finally, methods for constructing rate-efficient feature digests allowing processing in lower-dimensional spaces, e.g. under stringent bandwidth constraints, also falls within the scope of this research axis.</p>
    </subsection>
    <subsection id="uid13" level="1">
      <bodyTitle>Distributed processing and robust communication</bodyTitle>
      <p>The goal is to develop theoretical and practical solutions for robust image and video transmission over heterogeneous and time-varying networks. The first objective is to construct coding tools that can adapt to heterogeneous networks. This includes the design of (i) sensing modules to measure network characteristics, of (ii) robust coding techniques and of (iii) error concealment methods for compensating for missing data at the decoder when erasures occur during the transmission. The first objective is thus to develop sensing and modeling methods which can recognize, model and predict the packets loss/delay end-to-end behaviour.
Given the estimated and predicted network conditions (e.g. Packet Error Rate (PER)), the objective is then to adapt the data coding, protection and transmission scheme.
However, the reliability of the estimated PER impacts the performance of FEC schemes. We investigate the problem of constructing codes which would be robust to channel uncertainty, i.e. which would perform well not only on a specific channel but also ‚Äúuniversally‚Äù, hence reducing the need for a feedback channel. This would be a significant advantage compared with rateless codes such as fountain codes which require a feedback channel.
Another problem which we address is error concealment. This refers to the problem of estimating lost symbols from the received ones by exploiting spatial and/or temporal correlation within the video signal.</p>
      <p>The availability of wireless camera sensors has also been spurring interest for a variety of applications ranging from scene interpretation, object tracking and security environment monitoring. In such camera sensor networks, communication energy and bandwidth are scarce resources, motivating the search for new distributed image processing and coding (Distributed Source Coding) solutions suitable for band and energy limited networking environments. In the past years, the team has developed a recognized expertise in the area of distributed source coding, which in theory allows for each sensor node to communicate losslessly at its conditional entropy rate without information exchange between the sensor nodes. However, distributed source coding (DSC) is still at the level of the proof of concept and many issues remain unresolved. The goal is thus to further address theoretical issues as the problem of modeling the correlation channel between sources, to further study the practicality of DSC in image coding and communication problems.</p>
    </subsection>
  </presentation>
  <fondements id="uid14">
    <bodyTitle>Research Program</bodyTitle>
    <subsection id="uid15" level="1">
      <bodyTitle>Introduction</bodyTitle>
      <p>The research activities on analysis, compression and communication of visual data mostly rely on tools and formalisms from the areas of statistical image modelling, of signal processing, of coding and information theory. However, the objective of better exploiting the Human Visual System (HVS) properties in the above goals also pertains to the areas of perceptual modelling and cognitive science. Some of the proposed research axes are also based on scientific foundations of computer vision (e.g. multi-view modelling and coding). We have limited this section to some tools which are central to the proposed research axes, but the design of complete compression and communication solutions obviously rely on a large number of other results in the areas of motion analysis, transform design, entropy code design, etc which cannot be all described here.
</p>
    </subsection>
    <subsection id="uid16" level="1">
      <bodyTitle>Parameter Estimation and Inference</bodyTitle>
      <p>Bayesian estimation, Expectation-Maximization, stochastic modelling
</p>
      <p>Parameter estimation is at the core of the processing tools studied and developed in the team. Applications range from the prediction of missing data or future data, to extracting some information about the data in order to perform efficient compression. More precisely, the data are assumed to be generated by a given stochastic data model, which is partially known. The set of possible models translates the a priori knowledge we have on the data and the best model has to be selected in this set. When the set of models or equivalently the set of probability laws is indexed by a parameter (scalar or vectorial), the model is said parametric and the model selection resorts to estimating the parameter. Estimation algorithms are therefore widely used at the encoder to analyze the data. In order to achieve high compression rates, the parameters are usually not sent and the decoder has to jointly select the model (i.e. estimate the model parameters) and extract the information of interest.</p>
    </subsection>
    <subsection id="uid17" level="1">
      <bodyTitle>Data Dimensionality Reduction</bodyTitle>
      <p>Manifolds, locally linear embedding, non-negative matrix factorization, principal component analysis
</p>
      <p>A fundamental problem in many data processing tasks (compression, classification, indexing) is to find a suitable representation of the data. It often aims at reducing the dimensionality of the input data so that tractable processing methods can then be applied. Well-known methods for data dimensionality reduction include principal component analysis (PCA) and independent component analysis (ICA). The methodologies which will be central to several proposed research problems will instead be based on sparse representations, on locally linear embedding (LLE) and on the ‚Äúnon negative matrix factorization‚Äù (NMF) framework.</p>
      <p>The objective of <i>sparse representations</i> is to find a sparse approximation of a given input data. In theory, given <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mi>A</mi><mo>‚àà</mo><msup><mi>‚Ñù</mi><mrow><mi>m</mi><mo>√ó</mo><mi>n</mi></mrow></msup></mrow></math></formula>, <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mi>m</mi><mo>&lt;</mo><mi>n</mi></mrow></math></formula>, and <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mi>ùêõ</mi><mo>‚àà</mo><msup><mi>‚Ñù</mi><mi>m</mi></msup></mrow></math></formula> with <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mi>m</mi><mo>&lt;</mo><mo>&lt;</mo><mi>n</mi></mrow></math></formula> and <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>A</mi></math></formula> is of full rank, one seeks the solution of <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mo movablelimits="true" form="prefix">min</mo><mo>{</mo><mo>‚à•</mo><mi>ùê±</mi><msub><mo>‚à•</mo><mn>0</mn></msub><mspace width="0.277778em"/><mo>:</mo><mspace width="0.277778em"/><mi>A</mi><mi>ùê±</mi><mo>=</mo><mi>ùêõ</mi><mo>}</mo><mo>,</mo></mrow></math></formula> where <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msub><mrow><mo>‚à•</mo><mi>ùê±</mi><mo>‚à•</mo></mrow><mn>0</mn></msub></math></formula> denotes the <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msub><mi>L</mi><mn>0</mn></msub></math></formula> norm of <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>x</mi></math></formula>, i.e. the number of non-zero components in <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>z</mi></math></formula>.
There exist many solutions <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>x</mi></math></formula> to <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mi>A</mi><mi>x</mi><mo>=</mo><mi>b</mi></mrow></math></formula>. The problem is to find the sparsest, the one for which <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>x</mi></math></formula> has the fewest non zero components. In practice, one actually seeks an approximate and thus even sparser solution which satisfies <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mo movablelimits="true" form="prefix">min</mo><mo>{</mo><mo>‚à•</mo><mi>ùê±</mi><msub><mo>‚à•</mo><mn>0</mn></msub><mspace width="0.277778em"/><mo>:</mo><mspace width="0.277778em"/><mo>‚à•</mo><mi>A</mi><mi>ùê±</mi><mo>-</mo><mi>ùêõ</mi><msub><mo>‚à•</mo><mi>p</mi></msub><mo>‚â§</mo><mi>œÅ</mi><mo>}</mo><mo>,</mo></mrow></math></formula> for some <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mi>œÅ</mi><mo>‚â•</mo><mn>0</mn></mrow></math></formula>, characterizing an admissible reconstruction error.
The norm <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>p</mi></math></formula> is usually 2, but could be 1 or <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>‚àû</mi></math></formula> as well. Except for the exhaustive combinatorial approach, there is no known method to find the exact solution under general conditions on the dictionary <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>A</mi></math></formula>. Searching for this sparsest representation is hence unfeasible and both problems are computationally intractable. Pursuit algorithms have been introduced as heuristic methods which aim at finding approximate solutions to the above problem with tractable complexity.</p>
      <p><i>Non negative matrix factorization</i> (NMF) is a non-negative approximate data representation <footnote id="uid18" id-text="3">D.D. Lee and H.S. Seung, ‚ÄúAlgorithms for non-negative matrix factorization‚Äù, Nature 401, 6755, (Oct. 1999), pp. 788-791.</footnote>. NMF aims at finding an approximate factorization of a non-negative input data matrix <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>V</mi></math></formula> into non-negative matrices <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>W</mi></math></formula> and <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>H</mi></math></formula>, where the columns of <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>W</mi></math></formula> can be seen as <i>basis vectors</i> and those of <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>H</mi></math></formula> as coefficients of the linear approximation of the input data. Unlike other linear representations like PCA and ICA, the non-negativity constraint makes the representation purely additive.
Classical data representation methods like PCA or Vector Quantization (VQ) can be placed in an NMF framework, the differences arising from different constraints being placed on the <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>W</mi></math></formula> and <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>H</mi></math></formula> matrices. In VQ, each column of <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>H</mi></math></formula> is constrained to be unitary with only one non-zero coefficient which is equal to 1. In PCA, the columns of <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>W</mi></math></formula> are constrained to be orthonormal and the rows of <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>H</mi></math></formula> to be orthogonal to each other. These methods of data-dependent dimensionality reduction will be at the core of our visual data analysis and compression activities.</p>
    </subsection>
    <subsection id="uid19" level="1">
      <bodyTitle>Perceptual Modelling</bodyTitle>
      <p>Saliency, visual attention, cognition
</p>
      <p>The human visual system (HVS) is not able to process all visual information of our visual field at once. To cope with this problem, our visual system must filter out irrelevant information and reduce redundant information. This feature of our visual system is driven by a selective sensing and analysis process. For instance, it is well known that the greatest visual acuity is provided by the fovea (center of the retina). Beyond this area, the acuity drops down with the eccentricity. Another example concerns the light that impinges on our retina. Only the visible light spectrum lying between 380 nm (violet) and 760 nm (red) is processed. To conclude on the selective sensing, it is important to mention that our sensitivity depends on a number of factors such as the spatial frequency, the orientation or the depth. These properties are modeled by a sensitivity function such as the Contrast Sensitivity Function (CSF).</p>
      <p>Our capacity of analysis is also related to our visual attention. Visual attention which is closely linked to eye movement (note that this attention is called <i>overt</i> while the covert attention does not involve eye movement) allows us to focus our biological resources on a particular area. It can be controlled by both top-down (i.e. goal-directed, intention) and bottom-up (stimulus-driven, data-dependent) sources of information <footnote id="uid20" id-text="4">L. Itti and C. Koch, ‚ÄúComputational Modelling of Visual Attention‚Äù , Nature Reviews Neuroscience, Vol. 2, No. 3, pp. 194-203, 2001.</footnote>. This detection is also influenced by prior knowledge about the environment of the scene <footnote id="uid21" id-text="5">J. Henderson, ‚ÄúRegarding scenes‚Äù, Directions in Psychological Science, vol. 16, pp. 219-222, 2007.</footnote>. Implicit assumptions related to prior knowledge or beliefs play an important role in our perception (see the example concerning the assumption that light comes from above-left). Our perception results from the combination of prior beliefs with data we gather from the environment. A Bayesian framework is an elegant solution to model these interactions <footnote id="uid22" id-text="6">L. Zhang, M. Tong, T. Marks, H. Shan, H. and G.W. Cottrell, ‚ÄúSUN: a Bayesian framework for saliency using natural statistics‚Äù, Journal of Vision, vol. 8, pp. 1-20, 2008.</footnote>. We define a vector <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msub><mover accent="true"><mi>v</mi><mo>‚Üí</mo></mover><mi>l</mi></msub></math></formula> of local measurements (contrast of color, orientation, etc.) and vector <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msub><mover accent="true"><mi>v</mi><mo>‚Üí</mo></mover><mi>c</mi></msub></math></formula> of global and contextual features (global features, prior locations, type of the scene, etc.). The salient locations <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>S</mi></math></formula> for a spatial position <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mover accent="true"><mi>x</mi><mo>‚Üí</mo></mover></math></formula> are then given by:</p>
      <formula id-text="1" id="uid23" textype="equation" type="display">
        <math xmlns="http://www.w3.org/1998/Math/MathML" mode="display" overflow="scroll">
          <mrow>
            <mi>S</mi>
            <mrow>
              <mo>(</mo>
              <mover accent="true">
                <mi>x</mi>
                <mo>‚Üí</mo>
              </mover>
              <mo>)</mo>
            </mrow>
            <mo>=</mo>
            <mfrac>
              <mn>1</mn>
              <mrow>
                <mi>p</mi>
                <mo>(</mo>
                <msub>
                  <mover accent="true">
                    <mi>v</mi>
                    <mo>‚Üí</mo>
                  </mover>
                  <mi>l</mi>
                </msub>
                <mfenced separators="" open="|" close="">
                  <msub>
                    <mover accent="true">
                      <mi>v</mi>
                      <mo>‚Üí</mo>
                    </mover>
                    <mi>c</mi>
                  </msub>
                </mfenced>
                <mo>)</mo>
              </mrow>
            </mfrac>
            <mo>√ó</mo>
            <mi>p</mi>
            <mrow>
              <mo>(</mo>
              <mi>s</mi>
              <mo>,</mo>
              <mover accent="true">
                <mi>x</mi>
                <mo>‚Üí</mo>
              </mover>
              <mfenced separators="" open="|" close="">
                <msub>
                  <mover accent="true">
                    <mi>v</mi>
                    <mo>‚Üí</mo>
                  </mover>
                  <mi>c</mi>
                </msub>
              </mfenced>
              <mo>)</mo>
            </mrow>
          </mrow>
        </math>
      </formula>
      <p>The first term represents the bottom-up salience. It is based on a kind of contrast detection, following the assumption that rare image features are more salient than frequent ones. Most of existing computational models of visual attention rely on this term. However, different approaches exist to extract the local visual features as well as the global ones. The second term is the contextual priors. For instance, given a scene, it indicates which parts of the scene are likely the most salient.</p>
    </subsection>
    <subsection id="uid24" level="1">
      <bodyTitle>Coding theory</bodyTitle>
      <p>OPTA limit (Optimum Performance Theoretically Attainable), Rate allocation,
Rate-Distortion optimization, lossy coding, joint source-channel coding
multiple description coding, channel modelization, oversampled frame
expansions, error correcting codes.
</p>
      <p>Source coding and channel coding theory <footnote id="uid25" id-text="7">T. M. Cover and J. A. Thomas, Elements of Information Theory, Second Edition, July 2006.</footnote> is central to our compression and communication activities, in particular to the design of entropy codes and of error correcting codes. Another field in coding theory which has emerged in the context of sensor networks is Distributed Source Coding (DSC). It refers to the compression of correlated signals captured by different sensors which do not communicate between themselves. All the signals captured are compressed independently and transmitted to a central base station which has the capability to decode them jointly. DSC finds its foundation in the seminal Slepian-Wolf <footnote id="uid26" id-text="8">D. Slepian and J. K. Wolf, ‚ÄúNoiseless coding of correlated information
sources.‚Äù IEEE Transactions on Information Theory, 19(4), pp. 471-480,
July 1973.</footnote> (SW) and Wyner-Ziv <footnote id="uid27" id-text="9">A. Wyner and J. Ziv, ‚ÄúThe rate-distortion function for source coding
ith side information at the decoder.‚Äù IEEE Transactions on Information Theory, pp. 1-10, January 1976.</footnote> (WZ) theorems. Let us consider two binary correlated sources <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>X</mi></math></formula> and <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>Y</mi></math></formula>. If the two coders communicate, it is well known from Shannon's theory that the minimum lossless rate for <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>X</mi></math></formula> and <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>Y</mi></math></formula> is given by the joint entropy <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mi>H</mi><mo>(</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo>)</mo></mrow></math></formula>. Slepian and Wolf have established in 1973 that this lossless compression rate bound can be approached with a vanishing error probability for long sequences, even if the two sources are coded separately, provided that they are decoded jointly and that their correlation is known to both the encoder and the decoder.</p>
      <p>In 1976, Wyner and Ziv considered the problem of coding of two correlated sources <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>X</mi></math></formula> and <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>Y</mi></math></formula>, with respect to a fidelity criterion. They have established the rate-distortion function <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mi>R</mi><msub><mo>*</mo><mrow><mi>X</mi><mo>|</mo><mi>Y</mi></mrow></msub><mrow><mo>(</mo><mi>D</mi><mo>)</mo></mrow></mrow></math></formula> for the case where the side information <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>Y</mi></math></formula> is perfectly known to the decoder only. For a given target distortion <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>D</mi></math></formula>, <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><mi>R</mi><msub><mo>*</mo><mrow><mi>X</mi><mo>|</mo><mi>Y</mi></mrow></msub><mrow><mo>(</mo><mi>D</mi><mo>)</mo></mrow></mrow></math></formula> in general verifies <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><msub><mi>R</mi><mrow><mi>X</mi><mo>|</mo><mi>Y</mi></mrow></msub><mrow><mo>(</mo><mi>D</mi><mo>)</mo></mrow><mo>‚â§</mo><mi>R</mi><msub><mo>*</mo><mrow><mi>X</mi><mo>|</mo><mi>Y</mi></mrow></msub><mrow><mo>(</mo><mi>D</mi><mo>)</mo></mrow><mo>‚â§</mo><msub><mi>R</mi><mi>X</mi></msub><mrow><mo>(</mo><mi>D</mi><mo>)</mo></mrow></mrow></math></formula>, where <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mrow><msub><mi>R</mi><mrow><mi>X</mi><mo>|</mo><mi>Y</mi></mrow></msub><mrow><mo>(</mo><mi>D</mi><mo>)</mo></mrow></mrow></math></formula> is the rate required to encode <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>X</mi></math></formula> if <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>Y</mi></math></formula> is available to both the encoder and the decoder, and <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><msub><mi>R</mi><mi>X</mi></msub></math></formula> is the minimal rate for encoding <formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mi>X</mi></math></formula> without SI.
These results give achievable rate bounds, however the design of codes and practical solutions for compression and communication applications remain a widely open issue.</p>
    </subsection>
  </fondements>
  <domaine id="uid28">
    <bodyTitle>Application Domains</bodyTitle>
    <subsection id="uid29" level="1">
      <bodyTitle>Introduction</bodyTitle>
      <p>The application domains addressed by the project are:</p>
      <simplelist>
        <li id="uid30">
          <p noindent="true">Compression with advanced functionalities of various image modalities (including multi-view, medical images such as MRI, CT, WSI, or satellite images);</p>
        </li>
        <li id="uid31">
          <p noindent="true">Networked multimedia applications taking into account their various needs in terms of image and 2D and 3D video compression, or in terms of network adaptation (e.g., resilience to channel noise);</p>
        </li>
        <li id="uid32">
          <p noindent="true">Content editing and post-production.</p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid33" level="1">
      <bodyTitle>Compression of emerging imaging modalities</bodyTitle>
      <p>Compression of images and of 2D video (including High Definition and Ultra High Definition) remains a widely-sought capability for a large number of applications. This is particularly true for mobile applications, as the need for wireless transmission capacity will significantly increase during the years to come. Hence, efficient compression tools are required to satisfy the trend towards mobile access to larger image resolutions and higher quality. A new impulse to research in video compression is also brought by the emergence of new formats beyond High Definition TV (HDTV) towards high dynamic range (higher bit depth, extended colorimetric space), super-resolution, formats for immersive displays allowing panoramic viewing and 3DTV.</p>
      <p>Different video data formats and technologies are envisaged for interactive and immersive 3D video applications using omni-directional videos, stereoscopic or multi-view videos. The "omni-directional video" set-up refers to 360-degree view from one single viewpoint or spherical video. Stereoscopic video is composed of two-view videos, the right and left images of the scene which, when combined, can recreate the depth aspect of the scene. A multi-view video refers to multiple video sequences captured by multiple video cameras and possibly by depth cameras. Associated with a view synthesis method, a multi-view video allows the generation of virtual views of the scene from any viewpoint. This property can be used in a large diversity of applications, including Three-Dimensional TV (3DTV), and Free Viewpoint Video (FTV). The notion of "free viewpoint video" refers to the possibility for the user to choose an arbitrary viewpoint and/or view direction within a visual scene, creating an immersive environment. Multi-view video generates a huge amount of redundant data which need to be compressed for storage and transmission. In parallel, the advent of a variety of heterogeneous delivery infrastructures has given momentum to extensive work on optimizing the end-to-end delivery QoS (Quality of Service). This encompasses compression capability but also capability for adapting the compressed streams to varying network conditions. The scalability of the video content compressed representation and its robustness to transmission impairments are thus important features for seamless adaptation to varying network conditions and to terminal capabilities.</p>
    </subsection>
    <subsection id="uid34" level="1">
      <bodyTitle>Networked visual applications</bodyTitle>
      <p><i>3D and Free Viewpoint TV:</i> The emergence of multi-view auto-stereoscopic displays has spurred a recent interest for broadcast or Internet delivery of 3D video to the home. Multiview video, with the help of depth information on the scene, allows scene rendering on immersive stereo or auto-stereoscopic displays for 3DTV applications. It also allows visualizing the scene from any viewpoint, for scene navigation and free-viewpoint TV (FTV) applications. However, the large volumes of data associated to multi-view video plus depth content raise new challenges in terms of compression and communication.</p>
      <p><i>Internet and mobile video:</i>
Broadband fixed (ADSL, ADSL2<formula type="inline"><math xmlns="http://www.w3.org/1998/Math/MathML" overflow="scroll"><mo>+</mo></math></formula>) and mobile access networks with different radio access technologies (RAT) (e.g. 3G/4G, GERAN, UTRAN, DVB-H), have enabled not only IPTV and Internet TV but also the emergence of mobile TV and mobile devices with internet capability. A major challenge for next internet TV or internet video remains to be able to deliver the increasing variety of media (including more and more bandwidth demanding media) with a sufficient end-to-end QoS (Quality of Service) and QoE (Quality of Experience).</p>
      <p><i>Mobile video retrieval:</i> The Internet has changed the ways of interacting with content. The user is shifting its media consumption from a passive to a more interactive mode, from linear broadcast (TV) to on demand content (YouTubes, iTunes, VoD), and to user-generated, searching for relevant, personalized content. New mobility and ubiquitous usage has also emerged. The increased power of mobile devices is making content search and retrieval applications using mobile phones possible. Quick access to content in mobile environments with restricted bandwidth resources will benefit from rate-efficient feature extraction and description.</p>
      <p><i>Wireless multi-camera vision systems:</i> Our activities on scene modelling, on rate-efficient feature description, distributed coding and compressed sensing should also lead to algorithmic building blocks relevant for wireless multi-camera vision systems, for applications such as visual surveillance and security.
</p>
    </subsection>
    <subsection id="uid35" level="1">
      <bodyTitle>Editing and post-production</bodyTitle>
      <p>Video editing and post-production are critical aspects in the audio-visual production process. Increased ways of ‚Äúconsuming‚Äù video content also highlight the need for content repurposing as well as for higher interaction and editing capabilities. Content captured at very high resolutions may need to be repurposed in order to be adapted to the requirements of actual users, to the transmission channel or to the terminal.
Content repurposing encompasses format conversion (retargeting), content summarization, and content editing. This processing requires powerful methods for extracting condensed video representations as well as powerful inpainting techniques. By providing advanced models, advanced video processing and image analysis tools, more visual effects, with more realism become possible. Other applications such as video annotation/retrieval, video restoration/stabilization, augmented reality, can also benefit from the proposed research.</p>
    </subsection>
  </domaine>
  <highlights id="uid36">
    <bodyTitle>Highlights of the Year</bodyTitle>
    <subsection id="uid37" level="1">
      <bodyTitle>Highlights of the Year</bodyTitle>
      <subsection id="uid38" level="2">
        <bodyTitle>Awards</bodyTitle>
        <p>C. Guillemot has been granted an ERC advanced grant for a project on computational light fields imaging.</p>
      </subsection>
    </subsection>
  </highlights>
  <logiciels id="uid39">
    <bodyTitle>New Software and Platforms</bodyTitle>
    <subsection id="uid40" level="1">
      <bodyTitle>Salient object extraction</bodyTitle>
      <p><span class="smallcap" align="left">Functional Description</span>
This software detects salient object in an input picture in an automatic manner. The detection is based on super-pixel segmentation and contrast of histogram. This software is dedicated to people working in image processing and post production.</p>
      <simplelist>
        <li id="uid41">
          <p noindent="true">Participants: Zhi Liu and Olivier Le Meur</p>
        </li>
        <li id="uid42">
          <p noindent="true">Contact: Olivier Le Meur</p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid43" level="1">
      <bodyTitle>VideoInpainting</bodyTitle>
      <p><span class="smallcap" align="left">Keywords:</span> Video Inpainting - Motion informations - Loss concealment - BMFI (Bilinear Motion Field Interpolation)</p>
      <p noindent="true"><span class="smallcap" align="left">Scientific Description</span>
From an input binary mask and a source picture, the software performs an examplar-based inpainting. The method is based on the combination of multiple inpainting applied on a low resolution of the input picture. Once the combination has been done, a single-image super-resolution method is applied to recover the details and the high frequency in the inpainted areas. The developments have been pursued in 2014, in particular by introducing a Poisson blending step in order to improve the visual quality of the inpainted video. This software is dedicated to people working in image processing and post production.</p>
      <simplelist>
        <li id="uid44">
          <p noindent="true">Participants: Ronan Le Boulch and Olivier Le Meur</p>
        </li>
        <li id="uid45">
          <p noindent="true">Contact: Olivier Le Meur</p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid46" level="1">
      <bodyTitle>Visual Fixation Analysis</bodyTitle>
      <p><span class="smallcap" align="left">Scientific Description</span>
From a set of fixation data and a picture, the software called Visual Fixation Analysis extracts from the input data a number of features (fixation duration, saccade length, orientation of saccade...) and computes a human saliency map. The software can also be used to assess the degree of similarity between a ground truth (eye fixation data) and a predicted saliency map. This software is dedicated to people working in cognitive science and computer vision.</p>
      <simplelist>
        <li id="uid47">
          <p noindent="true">Participants: Olivier Le Meur and Thierry Baccino</p>
        </li>
        <li id="uid48">
          <p noindent="true">Partner: Universit√© de Rennes 1</p>
        </li>
        <li id="uid49">
          <p noindent="true">Contact: Olivier Le Meur</p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid50" level="1">
      <bodyTitle>Saccadic model</bodyTitle>
      <p><span class="smallcap" align="left">Scientific Description</span>
The software called Scanpath Prediction aims at predicting the visual scanpath of an observer. The visual scanpath is a set of fixation points. The computational model is based on bottom-up saliency maps, viewing tendencies (that have been learned from eye tracking datasets) and inhibition-of-return. A presentation of this model is available on the following link: <ref xlink:href="http://fr.slideshare.net/OlivierLeMeur/saccadic-model-of-eye-movements-for-freeviewing-condition" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">http://<allowbreak/>fr.<allowbreak/>slideshare.<allowbreak/>net/<allowbreak/>OlivierLeMeur/<allowbreak/>saccadic-model-of-eye-movements-for-freeviewing-condition</ref>. This software is dedicated to people working in computer science, computer vision and cognitive science. This software is being registered at the APP (Agence de Protection des Programmes) under the number IDDN.FR.001.240029.000.S.P.2016.000.10000.</p>
      <simplelist>
        <li id="uid51">
          <p noindent="true">Participants: Olivier Le Meur</p>
        </li>
        <li id="uid52">
          <p noindent="true">Partner: Universit√© de Rennes 1</p>
        </li>
        <li id="uid53">
          <p noindent="true">Contact: Olivier Le Meur</p>
        </li>
      </simplelist>
    </subsection>
    <subsection id="uid54" level="1">
      <bodyTitle>ADT-ATep</bodyTitle>
      <p>The scientific and industrial community is nowadays exploring new multimedia applications using 3D data (beyond stereoscopy). In particular, Free Viewpoint Television (FTV) has attracted much attention in the recent years. In those systems, the user can choose in real time the view angle from which he wants to observe the scene.
Despite the great interest for FTV, the lack of realistic and ambitious datasets penalizes the research effort. The acquisition of such sequences is very costly in terms of hardware and working effort, which explains why no multi-view videos suitable for FTV has been made available yet.</p>
      <p>A project founded by Inriahub has recently started in the SIROCCO team. Called ATeP for ‚ÄúAcquisition, Traitement et Partage" (Acquisition, Processing and Sharing), it targets the acquisition of such dataset. Another interesting aspect of this project is that the acquisition system relies on omnidirectional cameras. The dataset will thus interest all the industries and scientists currently working on the development of efficient processing and coding tools for 360 videos.</p>
      <simplelist>
        <li id="uid55">
          <p noindent="true">Participants: C√©dric Le Cam, Thomas Maugey</p>
        </li>
        <li id="uid56">
          <p noindent="true">Partner: Inria</p>
        </li>
        <li id="uid57">
          <p noindent="true">Contact: Thomas Maugey</p>
        </li>
      </simplelist>
    </subsection>
  </logiciels>
  <resultats id="uid58">
    <bodyTitle>New Results</bodyTitle>
    <subsection id="uid59" level="1">
      <bodyTitle>Analysis and modeling for compact representation</bodyTitle>
      <p>3D modelling, multi-view plus depth videos, light-fields, 3D meshes, epitomes, image-based rendering, inpainting, view synthesis
</p>
      <subsection id="uid60" level="2">
        <bodyTitle>Visual attention</bodyTitle>
        <participants>
          <person key="sirocco-2014-idp87568">
            <firstname>Olivier</firstname>
            <lastname>Le Meur</lastname>
          </person>
        </participants>
        <p>Visual attention is the mechanism allowing to focus our visual processing resources on behaviorally relevant visual information. Two kinds of visual attention exist: one involves eye movements (overt orienting) whereas the other occurs without eye movements (covert orienting). Our research activities deal with the understanding and modeling of overt attention.</p>
        <p><b>Saccadic model:</b>
Previous research showed the existence of systematic tendencies in viewing behavior during scene exploration. For instance, saccades are known to follow a positively skewed, long-tailed distribution, and to be more frequently initiated in the horizontal or vertical directions. In 2016, we investigated the fact that these viewing biases are not universal, but are modulated by the semantic visual category of the stimulus. We showed that the joint distribution of saccade amplitudes and orientations significantly varies from one visual category to another. These joint distributions turn out to be, in addition, spatially variant within the scene frame. We demonstrated that a saliency model based on this better understanding of viewing behavioral biases and blind to any visual information outperforms well-established saliency models. We also proposed an extension of the saccadic model developed in 2015. The improvement consists in accounting for spatially-variant and context-dependent viewing biases. This model outperforms state-of-the-art saliency models, and provides scanpaths in close agreement with human behavior.</p>
        <p><b>Inference of age from eye movements:</b> We have presented evidence that information derived from eye gaze can be used to infer observers‚Äô age. From simple features extracted from the sequence of fixations and saccades, we predict the age of an observer. To reach this objective, we used the eye data from 101 observers split in 4 age groups (adults, 6-10 year-old, 4-6 year-old. and 2 year-old) to train a computational model. Participant‚Äôs eye movements were monitored while participants were instructed to explore color pictures taken from children books for 10 seconds. The analysis of eye gaze provided evidence of age-related differences in viewing patterns. Fixation durations decreased with age while saccades turned out to be shorter when comparing children with adults. We combine several features, such as fixation durations, saccade amplitudes, and learn a direct mapping from those features to age using Gentle AdaBoost classifiers. Experimental results show that the proposed method succeeds in predicting reasonably well the observer's age.</p>
        <object id="uid61">
          <table rend="inline">
            <tr style="">
              <td style="text-align:center;" halign="center">Input images</td>
              <td style="text-align:center;" halign="center"/>
              <td style="text-align:center;" halign="center"/>
              <td style="text-align:center;" halign="center"/>
            </tr>
            <tr style="">
              <td style="text-align:center;" halign="center">
and scribbles</td>
              <td style="text-align:center;" halign="center">GT labels</td>
              <td style="text-align:center;" halign="center">Results of Wanner et al.</td>
              <td style="text-align:center;" halign="center">Our results</td>
            </tr>
            <tr style="">
              <td style="text-align:center;" halign="center">
                <ressource xlink:href="IMG/budha_scribbles.png" type="inline" width="81.09052pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
              <td style="text-align:center;" halign="center">
                <ressource xlink:href="IMG/budha_label.png" type="inline" width="81.09052pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
              <td style="text-align:center;" halign="center">
                <ressource xlink:href="IMG/budha_labeling_wanner.png" type="inline" width="81.09052pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
              <td style="text-align:center;" halign="center">
                <ressource xlink:href="IMG/budha_labeling_ours.png" type="inline" width="81.09052pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
            <tr style="">
              <td style="text-align:center;" halign="center">
                <ressource xlink:href="IMG/butterfly_scribbles.png" type="inline" width="81.09052pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
              <td style="text-align:center;" halign="center">
                <ressource xlink:href="IMG/butterfly_label.png" type="inline" width="81.09052pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
              <td style="text-align:center;" halign="center">
                <ressource xlink:href="IMG/butterfly_labeling_wanner.png" type="inline" width="81.09052pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
              <td style="text-align:center;" halign="center">
                <ressource xlink:href="IMG/butterfly_labeling_ours.png" type="inline" width="81.09052pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
            <caption/>
          </table>
          <caption>Light-field segmentation results obtained with synthetic light-fields. From left to right, we show, the input central view with scribbles, the ground truth labelling, the results of Wanner et al. and our results.</caption>
        </object>
      </subsection>
      <subsection id="uid62" level="2">
        <bodyTitle>Graph structure in the rays space for fast light fields segmentation</bodyTitle>
        <participants>
          <person key="sirocco-2014-idm8448">
            <firstname>Christine</firstname>
            <lastname>Guillemot</lastname>
          </person>
          <person key="sirocco-2015-idp80512">
            <firstname>Matthieu</firstname>
            <lastname>Hog</lastname>
          </person>
        </participants>
        <p>In collaboration with Technicolor (Neus Sabater), we have introduced a novel graph representation for interactive light field segmentation using Markov Random Field (MRF). The greatest barrier to the adoption of MRF for light field processing is the large volume of input data. The proposed graph structure exploits the redundancy in the ray space in order to reduce the graph size, decreasing the running time of MRF-based optimisation tasks. The concepts of free rays
and ray bundles with corresponding neighbourhood relationships are defined to construct the simplified graph-based light field representation.
We have then developed a light field interactive segmentation algorithm using graph-cuts based on such ray space graph structure, that guarantees the segmentation consistency across all views. Our experiments with several datasets show results that are very close to the ground truth, competing with state of the art light field segmentation methods in terms of accuracy and with a significantly lower complexity. They also show that our method performs well on both densely and sparsely sampled light fields <ref xlink:href="#sirocco-2016-bid0" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> (see Figure <ref xlink:href="#uid61" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>).</p>
      </subsection>
    </subsection>
    <subsection id="uid63" level="1">
      <bodyTitle>Rendering, inpainting and super-resolution</bodyTitle>
      <p>image-based rendering, inpainting, view synthesis, super-resolution
</p>
      <subsection id="uid64" level="2">
        <bodyTitle>Joint color and gradient transfer through Multivariate Generalized Gaussian Distribution</bodyTitle>
        <participants>
          <person key="sirocco-2015-idp85520">
            <firstname>Hristina</firstname>
            <lastname>Hristova</lastname>
          </person>
          <person key="sirocco-2014-idp87568">
            <firstname>Olivier</firstname>
            <lastname>Le Meur</lastname>
          </person>
        </participants>
        <p>Multivariate generalized Gaussian distributions (MGGDs) have aroused a great interest in the image processing community thanks to their ability to describe accurately various image features, such as image gradient fields, wavelet coefficients, etc. However, so far their applicability has been limited by the lack of a transformation between two of these parametric distributions.
In collaboration with FRVSense (R√©mi Cozot and Kadi Bouatouch), we have proposed a novel transformation between MGGDs, consisting of an optimal transportation of the
second-order statistics and a stochastic-based shape parameter transformation. We employ the proposed transformation in both color and gradient transfers between images. We have also proposed a new simultaneous transfer of color and gradient.</p>
      </subsection>
      <subsection id="uid65" level="2">
        <bodyTitle>High-Dynamic-Range Image Recovery from Flash and Non-Flash Image Pairs</bodyTitle>
        <participants>
          <person key="sirocco-2015-idp85520">
            <firstname>Hristina</firstname>
            <lastname>Hristova</lastname>
          </person>
          <person key="sirocco-2014-idp87568">
            <firstname>Olivier</firstname>
            <lastname>Le Meur</lastname>
          </person>
        </participants>
        <p>In 2016, in collaboration with FRVSense (R√©mi Cozot and Kadi Bouatouch), we have proposed a novel method for creating High Dynamic Range (HDR) images from only two images - flash and non-flash images. The proposed method consists of two main steps, namely brightness gamma correction and bi-local chromatic adaptation transform (CAT). First, the brightness gamma correction performs series of increases and decreases of the brightness of the non-flash image and that way yields multiple images with various exposure values. Second, a proposed CAT method, called bi-local CAT enhances the quality of the computed images, by recovering details in the under-/over-exposed regions, using detail information from the flash image. The final multiple exposure images are then merged together to compute an HDR image. Evaluation shows that our HDR images, obtained by using only two LDR images, are close to HDR images, obtained by combining five manually taken multi-exposure images. The proposed method does not require the usage of a tripod and it is suitable for images of non-still objects, such as people, candle flames, etc. Figure¬†<ref xlink:href="#uid66" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> illustrates some results of the proposed method. The HDR-VDP-2 color-coded map (right-most image) shows the main luminance differences (the red areas) between our HDR result and the real HDR image. Snippets (a) and (b) show that the proposed method sharpens fine details, e.g. the net on the lamp. The net on the lamp of the real HDR image is blurry, due to a movement in the real multi-exposure images.</p>
        <object id="uid66">
          <table>
            <tr>
              <td>
                <ressource xlink:href="IMG/flashnoflashhdr.png" type="float" width="427.0pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
          </table>
          <caption>HDR image recovery from two input images, i.e. flash and non-flash images. Our HDR result and the real HDR image are tone-mapped for visualization on an LDR display.</caption>
        </object>
      </subsection>
      <subsection id="uid67" level="2">
        <bodyTitle>Depth inpainting</bodyTitle>
        <participants>
          <person key="sirocco-2014-idp87568">
            <firstname>Olivier</firstname>
            <lastname>Le Meur</lastname>
          </person>
        </participants>
        <p>To tackle the disocclusion inpainting of RGB-D images appearing when synthesizing new views of a scene by changing its viewpoint, in collaboration with Pierre Buyssens from the Greyc laboratory from the Caen University, we have developed a new examplar-based inpainting method of depth map. The proposed method is based on two main components. First, a novel algorithm to perform the depth-map disocclusion inpainting has been proposed. In particular, this intuitive approach is able to recover the lost structures of the objects and to inpaint the depth-map in a geometrically plausible manner. Then, a depth-guided patch-based inpainting method has been defined in order to fill-in the color image. Depth information coming from the reconstructed depth-map is added to each key step of the classical patch-based algorithm from Criminisi et al. in an intuitive manner. Relevant comparisons to state-of-the-art inpainting methods for the disocclusion inpainting of both depth and color images have illustrated the effectiveness of
the proposed algorithms.</p>
      </subsection>
      <subsection id="uid68" level="2">
        <bodyTitle>Super-resolution and inpainting for face recognition</bodyTitle>
        <participants>
          <person key="sirocco-2015-idp86776">
            <firstname>Reuben</firstname>
            <lastname>Farrugia</lastname>
          </person>
          <person key="sirocco-2014-idm8448">
            <firstname>Christine</firstname>
            <lastname>Guillemot</lastname>
          </person>
        </participants>
        <p>Most face super-resolution methods assume that low- and high-resolution manifolds have similar local geometrical
structure, hence learn local models on the low-resolution manifold
(e.g. sparse or locally linear embedding models), which are
then applied on the high-resolution manifold. However, the low-
resolution manifold is distorted by the one-to-many relationship
between low- and high- resolution patches.</p>
        <p>We have developed a method which learns linear models based on the local geometrical structure on the high-resolution manifold rather than on the low-resolution manifold. For this, in a first step, the low-resolution patch is used to derive a globally optimal estimate of the high-resolution patch. The approximated solution is shown to be close in Euclidean space to the ground-truth but is generally
smooth and lacks the texture details needed by state-of-the-art
face recognizers. Unlike existing methods, the sparse support
that best estimates the first approximated solution is found on
the high-resolution manifold. The derived support is then used
to extract the atoms from the coupled dictionaries that are most
suitable to learn an upscaling function between the low- and high-
resolution patches.</p>
        <p>The proposed solution has also been extended to
compute face super-resolution of non-frontal images.
Experimental results show that the proposed method out-
performs six face super-resolution and a state-of-the-art cross-
resolution face recognition method. These results also reveal
that the recognition and quality are significantly affected by the
method used for stitching all super-resolved patches together,
where quilting was found to better preserve the texture details
which helps to achieve higher recognition rates. The proposed
method was shown to be able to super-resolve facial images from
the IARPA Janus Benchmark A (IJB-A) dataset which considers
a wide range of poses and orientations.</p>
        <p>A method has also been developed to inpaint
occluded facial regions with unconstrained pose and orientation. This
approach first warps the facial region onto a reference model to synthesize a frontal view <ref xlink:href="#sirocco-2016-bid1" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. A modified Robust Principal Component Analysis
(RPCA) approach is then used to suppress warping errors. It then uses
a novel local patch-based face inpainting algorithm which hallucinates
missing pixels using a dictionary of face images which are pre-aligned to
the same reference model. The hallucinated region is then warped back
onto the original image to restore missing pixels.
Experimental results on synthetic occlusions demonstrate that the proposed face inpainting method has the best performance achieving PSNR gains of up to 0.74dB over the second-best method. Moreover, experiments on the COFW dataset and a number of real-world images show that the proposed method successfully restores occluded facial regions in the wild even for Closed-Circuit Television (CCTV) quality images.</p>
      </subsection>
      <subsection id="uid69" level="2">
        <bodyTitle>Light-field inpainting</bodyTitle>
        <participants>
          <person key="sirocco-2014-idm8448">
            <firstname>Christine</firstname>
            <lastname>Guillemot</lastname>
          </person>
          <person key="sirocco-2016-idp198976">
            <firstname>Xiaoran</firstname>
            <lastname>Jiang</lastname>
          </person>
          <person key="sirocco-2014-idp105312">
            <firstname>Mikael</firstname>
            <lastname>Le Pendu</lastname>
          </person>
        </participants>
        <p>Building up on the advances in low rank matrix completion,
we have developed a novel method for propagating
the inpainting of the central view of a light field to all the
other views. After generating a set of warped versions of
the inpainted central view with random homographies, both
the original light field views and the warped ones are vectorized
and concatenated into a matrix. Because of the redundancy
between the views, the matrix satisfies a low rank
assumption enabling us to fill the region to inpaint with low
rank matrix completion. To this end, a new matrix completion
algorithm, better suited to the inpainting application
than existing methods, has also been developed. Unlike
most of the existing light field inpainting algorithms,
our method does not require any depth prior. Another interesting
feature of the low rank approach is its ability to
cope with color and illumination variation between the input
views of the light field (see Fig.<ref xlink:href="#uid70" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>. As it can be seen in Figure
<ref xlink:href="#uid70" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, the proposed method yields inpainting consistency across views.</p>
        <object id="uid70">
          <table>
            <tr>
              <td>
                <ressource xlink:href="IMG/LFinpaint.png" type="float" width="427.0pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
          </table>
          <caption>Illustration of our inpainting propagation method : (a) Original central view. (b) Inpainted central view. (c) Another view of the light field inpainted
with a state-of-the-art 2D image inpainting method. (d) Propagated inpainting from central view to a different view with the developed low rank method.</caption>
        </object>
      </subsection>
    </subsection>
    <subsection id="uid71" level="1">
      <bodyTitle>Representation and compression of large volumes of visual data</bodyTitle>
      <p>Sparse representations, data dimensionality reduction, compression, scalability, perceptual coding, rate-distortion theory
</p>
      <subsection id="uid72" level="2">
        <bodyTitle>Graph-based multi-view video representation</bodyTitle>
        <participants>
          <person key="sirocco-2014-idm8448">
            <firstname>Christine</firstname>
            <lastname>Guillemot</lastname>
          </person>
          <person key="sirocco-2014-idm5560">
            <firstname>Thomas</firstname>
            <lastname>Maugey</lastname>
          </person>
          <person key="sirocco-2015-idp84272">
            <firstname>Mira</firstname>
            <lastname>Rizkallah</lastname>
          </person>
          <person key="sirocco-2015-idp70528">
            <firstname>Xin</firstname>
            <lastname>Su</lastname>
          </person>
        </participants>
        <p>One of the main open questions in multiview data processing is the design of representation methods for multiview data, where the challenge is to describe the scene content in a compact form that is robust to lossy data compression. Many approaches have been studied in the literature, such as the multiview and multiview plus depth formats, point clouds or mesh-based techniques. All these representations contain two types of data: i) the color or luminance information, which is classically described by 2D images; ii) the geometry information that describes the scene 3D characteristics, represented by 3D coordinates, depth maps or disparity vectors. Effective representation, coding and processing of multiview data partly rely on a proper representation of the geometry information. The multiview plus depth (MVD) format has become very popular in recent years for 3D data representation. However, this format induces very large volumes of data, hence the need for efficient compression schemes. On the other hand, lossy compression of depth information in general leads to annoying rendering artefacts especially along the contours of objects in the scene. Instead of lossy compression of depth maps, we consider the lossless transmission of a geometry representation that captures only the information needed for the required view reconstructions.</p>
        <p>The goal is thus to develop a Graph-Based Representation (GBR) for geometry information, where the geometry of the scene is represented as connections between corresponding pixels in different views. In this representation, two connected pixels are neighboring points in the 3D scene. The graph connections are derived from dense disparity maps and provide just enough geometry information to predict pixels in all the views that have to be synthesized. GBR drastically simplifies the geometry information to the bare minimum required for view prediction. This ‚Äútask-aware‚Äù geometry simplification allows us to control the view prediction accuracy before coding compared to baseline depth compression methods. In 2015, we have first considered multi-view configurations, in which cameras are parallel.</p>
        <p>In 2016, we have developed the extension of GBR to complex camera configurations. In <ref xlink:href="#sirocco-2016-bid2" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, Xin Su has implemented a generalized Graph-Based Representation handling two views with complex translations and rotations between them (Fig.¬†<ref xlink:href="#uid73" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>). The proposed approach uses the epipolar segments to have a row-wise description of the geometry that is as simple as for rectified views. This generalized GBR has been further extended to handle multiple views and scalable description of the geometry, <i>i.e.</i>, a geometry data that is coded as a function of the user navigation among the views.</p>
        <object id="uid73">
          <table>
            <tr>
              <td>
                <ressource xlink:href="IMG/GBR.png" type="float" width="213.5pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
          </table>
          <caption>The proposed GBR (i) provides edges describing the geometry information and (ii) link pixels that are neighbors in the 3D scene.</caption>
        </object>
        <p>The graph described above links neighboring pixels in the 3D scene as 3D meshes do. This meaningful structure might be used to code the color pixels lying on it. This can be done thanks to the new processing tools developed for signals lying on graphs. These tools rely however on covariance models that are assumed to be suited for the processed data. The PhD work of Mira Rizkallah is currently focussing on the effect of errors in the correlation models on the efficiency of the graph-based transforms.</p>
      </subsection>
      <subsection id="uid74" level="2">
        <bodyTitle>Sparse and low rank approximation of light fields</bodyTitle>
        <participants>
          <person key="sirocco-2014-idm8448">
            <firstname>Christine</firstname>
            <lastname>Guillemot</lastname>
          </person>
          <person key="sirocco-2016-idp198976">
            <firstname>Xiaoran</firstname>
            <lastname>Jiang</lastname>
          </person>
          <person key="sirocco-2014-idp105312">
            <firstname>Mikael</firstname>
            <lastname>Le Pendu</lastname>
          </person>
        </participants>
        <p>We have studied the problem of low rank approximation of light fields for compression. A homography-based approximation method has been proposed which jointly searches for homographies to align the different views of the light field together with the low rank approximation matrices. We have first considered a global homography per view and shown that depending on the variance of the disparity across views, the global homography is not sufficient to well-align the entire images. In a second step, we have thus considered multiple homographies, one per region, the region being extracted using depth information. We have first shown the benefit of the joint optimization of the homographies together with the low-rank approximation. The resulting compact representation compressed using HEVC yields compression performance significantly superior to those obtained by directly applying HEVC on the light field views re-structured as a video sequence.</p>
      </subsection>
      <subsection id="uid75" level="2">
        <bodyTitle>Deep learning, autoencoders and neural networks for sparse representation and compression</bodyTitle>
        <participants>
          <person key="sirocco-2015-idp76768">
            <firstname>Thierry</firstname>
            <lastname>Dumas</lastname>
          </person>
          <person key="sirocco-2014-idm8448">
            <firstname>Christine</firstname>
            <lastname>Guillemot</lastname>
          </person>
          <person key="sirocco-2014-idp86352">
            <firstname>Aline</firstname>
            <lastname>Roumy</lastname>
          </person>
        </participants>
        <p>Deep learning is a novel research area that attempts to extract high level abstractions from data by using a graph with multiple layers.
One could therefore expect that deep learning might allow efficient image compression based on these high level features. However, deep learning, as classical machine learning, consists in two phases: (i) build a graph that can make a good representation of the data (i.e. find an architecture usually made with neural nets), and (ii) learn the parameters of this architecture from large-scale data. As a consequence, neural nets are well suited for a specific task (text or image recognition) and require one training per task. The difficulty to apply machine learning approach to image compression is that it is important to deal with a large variety of patches, and with also various compression rates. To test the ability of neural networks to compress images, we studied shallow sparse autoencoders (AE) for image compression in <ref xlink:href="#sirocco-2016-bid3" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.
A performance analysis in terms of rate-distortion trade-off and complexity is conducted, comparing sparse AEs with LARS-Lasso, Coordinate Descent (CoD) and Orthogonal Matching Pursuit (OMP). A Winner Take All Auto-encoder (WTA AE) is proposed where image patches compete with one another when computing their sparse representation. This allows to spread the sparsity constraint on the whole image. Since the learning is made for this WTA AE, the neural network also learns to deal with various patches, which helps building a general-purpose AE. Finally, we showed that, WTA AE achieves the best rate-distortion trade-off, is robust to quantization noise and it is less complex than LARS-Lasso, CoD and OMP.</p>
      </subsection>
      <subsection id="uid76" level="2">
        <bodyTitle>Data geometry aware local basis selection</bodyTitle>
        <participants>
          <person key="sirocco-2014-idp100272">
            <firstname>Julio Cesar</firstname>
            <lastname>Ferreira</lastname>
          </person>
          <person key="sirocco-2014-idm8448">
            <firstname>Christine</firstname>
            <lastname>Guillemot</lastname>
          </person>
        </participants>
        <p>Local learning of sparse image models has proven to be very effective to solve a variety of inverse problems in many computer vision applications. To learn such models, the data samples are often clustered using the K-means algorithm with the Euclidean distance as a dissimilarity metric. However, the Euclidean distance
may not always be a good dissimilarity measure for comparing data samples lying on a manifold.</p>
        <p>In 2015, we have developed, in collaboration with Elif Vural (now Prof. at METU in Ankara, former postdoc in the team), two algorithms for determining a local subset of training samples from which a good local model can be computed for reconstructing a given input test sample, where we take into account the underlying
geometry of the data. The first algorithm, called Adaptive Geometry-driven Nearest Neighbor search (AGNN), is an adaptive scheme which can be seen as an out-of-sample extension of the replicator graph clustering method for local model learning. The second method, called Geometry-driven Overlapping Clusters (GOC), is a less complex nonadaptive alternative for training subset selection. The AGNN and GOC methods have
been evaluated in image super-resolution, deblurring and denoising applications and shown to outperform spectral clustering, soft clustering, and geodesic distance based subset selection in most settings. The selected patches are used for learning good local bases using the traditional PCA method. PCA is considered an efficient tool to recover the tangent space of the patch manifold when the manifold is sufficiently regular.</p>
        <p>However, when the patch manifold has high curvature, which is observed to be the case for images with high frequencies, PCA may not be suitable. It can be seen in Figure <ref xlink:href="#uid77" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> that the PCA basis with respect to a manifold fails to approximate the tangent space as the manifold bends over itself. In other words, PCA basis is not adapted when the curvature is too high.
On the other hand, it can be seen in Figure <ref xlink:href="#uid77" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/> that a union of subspaces with respect to a manifold might generate a local model that yields a more efficient local representation of data.</p>
        <p>In 2016, we have proposed a strategy to choose between these two kinds of bases locally depending on the local data geometry. This function is defined as the variability of the tangent space in each cluster.</p>
        <object id="uid77">
          <table>
            <tr>
              <td>
                <ressource xlink:href="IMG/pcavssot.png" type="float" height="113.81102pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
          </table>
          <caption>Subspaces computed with data sampled from a neighborhood on a manifold; (a): PCA basis which fails to approximate the subspace as the manifold curvature is too high; (b): union of subspaces generating a local model more coherent with the manifold geometry.</caption>
        </object>
      </subsection>
      <subsection id="uid78" level="2">
        <bodyTitle>Rate-distortion optimized tone curves for HDR video compression</bodyTitle>
        <participants>
          <person key="sirocco-2014-idp101552">
            <firstname>David</firstname>
            <lastname>Gommelet</lastname>
          </person>
          <person key="sirocco-2014-idm8448">
            <firstname>Christine</firstname>
            <lastname>Guillemot</lastname>
          </person>
          <person key="sirocco-2014-idp86352">
            <firstname>Aline</firstname>
            <lastname>Roumy</lastname>
          </person>
        </participants>
        <p>High Dynamic Range (HDR) images contain more intensity levels than traditional image formats. Instead of
8 or 10 bit integers, floating point values requiring much higher precision are used to represent the pixel data. These data thus need specific compression algorithms.
In collaboration with Ericsson <ref xlink:href="#sirocco-2016-bid4" location="biblio" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>, we have developed a novel compression algorithm that allows compatibility with the existing Low Dynamic Range (LDR) broadcast architecture in terms of display, compression algorithm and datarate, while delivering full HDR data to the users equipped with HDR display. The developed algorithm is thus a scalable video compression offering a base layer that corresponds to the LDR data and an enhancement layer, which together with the base layer corresponds to the HDR data. The novelty of the approach relies on the optimization of a mapping called Tone Mapping Operator (TMO) that maps efficiently the HDR data to the LDR data. The optimization has been carried out in a rate-distortion sense: the distortion of the HDR data is minimized under the constraint of minimum sum datarate (for the base and enhancement layer), while offering LDR data that are closed to some ‚Äúaesthetic‚Äù a priori.
Taking into account the aesthetic of the scene in video compression is indeed novel, since video compression is traditionally optimized to deliver the smallest distortion with the input data at the minimum datarate.</p>
      </subsection>
      <subsection id="uid79" level="2">
        <bodyTitle>Cloud-based image compression</bodyTitle>
        <participants>
          <person key="sirocco-2015-idp74248">
            <firstname>Jean</firstname>
            <lastname>Begaint</lastname>
          </person>
          <person key="sirocco-2014-idm8448">
            <firstname>Christine</firstname>
            <lastname>Guillemot</lastname>
          </person>
        </participants>
        <p>The emergence of cloud applications and web services has led to an increasing use of online resources. Image processing applications can benefit from this vast storage and distribution capacity. In collaboration with Technicolor, we investigate the use of this mass of redundant
data to enhance image compression schemes. A region-based registration algorithm has been developped to capture complex deformations between two images. The registration method is then used to exploit both global and local correspondences between pairs of images of the same scene. The region-based registration yields a better prediction (hence reduced prediction errors, see Fig.<ref xlink:href="#uid80" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>) which in turn yields a significant rate-distortion performance gain compared to current image coding solutions.</p>
        <object id="uid80">
          <table>
            <tr>
              <td>
                <ressource xlink:href="IMG/registration.png" type="float" width="427.0pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
          </table>
          <caption>Image registration with global and region-based homographies and corresponding prediction error.</caption>
        </object>
      </subsection>
    </subsection>
    <subsection id="uid81" level="1">
      <bodyTitle>Distributed processing and robust communication</bodyTitle>
      <p>Information theory, stochastic modelling, robust detection, maximum likelihood estimation, generalized likelihood ratio test, error and erasure resilient coding and decoding, multiple description coding, Slepian-Wolf coding, Wyner-Ziv coding, information theory, MAC channels
</p>
      <subsection id="uid82" level="2">
        <bodyTitle>Interactive Coding for Navigation in 3D scenes (ICON 3D)</bodyTitle>
        <participants>
          <person key="sirocco-2014-idm5560">
            <firstname>Thomas</firstname>
            <lastname>Maugey</lastname>
          </person>
          <person key="sirocco-2014-idp86352">
            <firstname>Aline</firstname>
            <lastname>Roumy</lastname>
          </person>
        </participants>
        <p>In order to have performing FTV systems, the data transmission has to take into account the interactivity of the user, <i>i.e.,</i> the viewpoint that is requested.
In other words, a FTV system transmits to the visualisation support only what needs to be updated when a user changes its viewpoint angle (<i>i.e.</i>, the new information appearing in its vision field).
The Sirocco has recently proposed some promising work using channel coding for interactive data coding. This coding scheme focusses on multi-view plus depth format only. In order to extend this approach to other formats, we have started a collaboration with the I3S laboratory in Nice, expert in 3D mesh compression.</p>
        <p>The project ICON 3D funded by the GdR-Isis will be divided into two parts. First, we will study and develop new geometry prediction algorithms for surface meshes. Given a part of a mesh, the prediction algorithm should be able to estimate a neighboring mesh subset corresponding to the one newly visible after user viewpoint angle change (Fig.¬†<ref xlink:href="#uid83" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>).
The prediction error will be characterized. Then, we will study the channel coding method that should be developed to correct this error.</p>
        <object id="uid83">
          <table>
            <tr>
              <td>
                <ressource xlink:href="IMG/icon3d.png" type="float" width="298.8987pt" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest" media="WEB"/>
              </td>
            </tr>
          </table>
          <caption>When a user changes his viewpoint angle, he discovers new part of the mesh that has to be transmitted.</caption>
        </object>
      </subsection>
    </subsection>
  </resultats>
  <contrats id="uid84">
    <bodyTitle>Bilateral Contracts and Grants with Industry</bodyTitle>
    <subsection id="uid85" level="1">
      <bodyTitle>Bilateral Grants with Industry</bodyTitle>
      <subsection id="uid86" level="2">
        <bodyTitle>Consulting contract with Enensys technologies</bodyTitle>
        <participants>
          <person key="sirocco-2014-idp86352">
            <firstname>Aline</firstname>
            <lastname>Roumy</lastname>
          </person>
        </participants>
        <simplelist>
          <li id="uid87">
            <p noindent="true">Title : Matrix inversion for video streaming.</p>
          </li>
          <li id="uid88">
            <p noindent="true">Research axis : REF AT 7.4. Distributed processing and robust communication</p>
          </li>
          <li id="uid89">
            <p noindent="true">Partners : Enensys, Inria-Rennes.</p>
          </li>
          <li id="uid90">
            <p noindent="true">Funding : Enensys.</p>
          </li>
          <li id="uid91">
            <p noindent="true">Period : Apr. 2016 - May 2016.</p>
          </li>
        </simplelist>
        <p>This contract with Enensys technologies aimed at studying solutions for reducing the complexity of matrix inversion used for encoding data in the context of video streaming. First a bibliographical study has been carried out related to the problem of matrix inversion in a finite field, then a novel solution has been proposed together with some recommendations regarding the algorithmic implementation.</p>
      </subsection>
      <subsection id="uid92" level="2">
        <bodyTitle>Google faculty research award</bodyTitle>
        <participants>
          <person key="sirocco-2014-idm8448">
            <firstname>Christine</firstname>
            <lastname>Guillemot</lastname>
          </person>
          <person key="sirocco-2016-idp198976">
            <firstname>Xiaoran</firstname>
            <lastname>Jiang</lastname>
          </person>
          <person key="sirocco-2014-idp105312">
            <firstname>Mikael</firstname>
            <lastname>Le Pendu</lastname>
          </person>
        </participants>
        <simplelist>
          <li id="uid93">
            <p noindent="true">Title ¬†: Light fields low rank and sparse approximation</p>
          </li>
          <li id="uid94">
            <p noindent="true">Research axis¬†: <ref xlink:href="#uid74" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
          </li>
          <li id="uid95">
            <p noindent="true">Partners¬†: Inria-Rennes.</p>
          </li>
          <li id="uid96">
            <p noindent="true">Funding¬†: Google.</p>
          </li>
          <li id="uid97">
            <p noindent="true">Period¬†: Oct.2015-Sept.2016.</p>
          </li>
        </simplelist>
        <p>The goal of the project was to study low-rank and sparse approximation models for light fields compression. A homography-based low-rank approximation has been developed showing significant PSNR-rate gains compared to a direct encoding of light field views with HEVC-inter coding.</p>
      </subsection>
      <subsection id="uid98" level="2">
        <bodyTitle>CIFRE contract with Envivio/ Ericsson on LDR compatible HDR video coding</bodyTitle>
        <participants>
          <person key="sirocco-2014-idm8448">
            <firstname>Christine</firstname>
            <lastname>Guillemot</lastname>
          </person>
          <person key="sirocco-2014-idp101552">
            <firstname>David</firstname>
            <lastname>Gommelet</lastname>
          </person>
          <person key="sirocco-2014-idp86352">
            <firstname>Aline</firstname>
            <lastname>Roumy</lastname>
          </person>
        </participants>
        <simplelist>
          <li id="uid99">
            <p noindent="true">Title ¬†: LDR-compatible coding of HDR video signals.</p>
          </li>
          <li id="uid100">
            <p noindent="true">Research axis¬†: ¬ß¬†<ref xlink:href="#uid78" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>.</p>
          </li>
          <li id="uid101">
            <p noindent="true">Partners¬†: Envivio.</p>
          </li>
          <li id="uid102">
            <p noindent="true">Funding¬†: Cifre Envivio.</p>
          </li>
          <li id="uid103">
            <p noindent="true">Period¬†: Oct.2014-Sept.2017.</p>
          </li>
        </simplelist>
        <p>The goal of this Cifre contract is to design solutions for LDR-compatible coding of HDR videos. This involves the study of rate-distortion optimized tone mapping operators taking into account constraints of temporal coherency to avoid the temporal flickering which results from a direct frame-by-frame application of classical tone mapping operators. The goal is also to design a coding
architecture which will build upon these operators, integrating coding tools tailored to the statistics of the HDR refinement signals.</p>
      </subsection>
      <subsection id="uid104" level="2">
        <bodyTitle>CIFRE contract with Harmonic on image analysis for HDR video compression</bodyTitle>
        <participants>
          <person key="sirocco-2016-idp194000">
            <firstname>Maxime</firstname>
            <lastname>Rousselot</lastname>
          </person>
          <person key="sirocco-2014-idp87568">
            <firstname>Olivier</firstname>
            <lastname>Le Meur</lastname>
          </person>
        </participants>
        <simplelist>
          <li id="uid105">
            <p noindent="true">Title : image and video analysis for HDR video compression</p>
          </li>
          <li id="uid106">
            <p noindent="true">Partners : Harmonic, Univ. Rennes 1</p>
          </li>
          <li id="uid107">
            <p noindent="true">Funding: Harmonic, ANRT</p>
          </li>
          <li id="uid108">
            <p noindent="true">Period: April 2016-April 2019</p>
          </li>
        </simplelist>
        <p>This project (in collaboration with R√©mi Cozot, FRVSense) aims to investigate two main axes. First, we want to assess whether the representation of High Dynamic Range signal has an impact on the coding efficiency. We will focus mainly on the Hybrid Log-Gamma (HLG) and Perceptual Quantizer (PQ) OETF (Opto-Electronic Transfer Function)approaches. The former defines a nonlinear transfer function which is display-independent and able to produce high quality images without compromising the director‚Äôs artistic intent. The latter approach is based on Just Noticeable Difference curve. If it turns out that this representation has an impact, the coding strategy should be adjusted with respect to the representation. In addition, specific preprocessing tools will be defined to deal with the limitations of PQ and HLG approaches.</p>
      </subsection>
      <subsection id="uid109" level="2">
        <bodyTitle>CIFRE contract with Technicolor on image collection analysis</bodyTitle>
        <participants>
          <person key="sirocco-2016-idp186624">
            <firstname>Dmitry</firstname>
            <lastname>Kuzovkin</lastname>
          </person>
          <person key="sirocco-2014-idp87568">
            <firstname>Olivier</firstname>
            <lastname>Le Meur</lastname>
          </person>
        </participants>
        <simplelist>
          <li id="uid110">
            <p noindent="true">Title : Spatiotemporal retargeting and recomposition based on artistic rules</p>
          </li>
          <li id="uid111">
            <p noindent="true">Partners : Technicolor, Univ. Rennes 1</p>
          </li>
          <li id="uid112">
            <p noindent="true">Funding: Technicolor, ANRT</p>
          </li>
          <li id="uid113">
            <p noindent="true">Period: Nov. 2015 ‚Äì Nov. 2018</p>
          </li>
        </simplelist>
        <p>The goal of the project (in collaboration with R√©mi Cozot, FRVSense) is to take advantage of the huge quantities of image and video data currently available - captured by both amateur and professional users - as well as the multiple copies of each scene that users often capture, to improve the aesthetic appeal of content. Additionally, given Technicolor's unique position, we propose to take advantage of insights as well as content from professional artists and colorists to learn how different content types can be enhanced.</p>
      </subsection>
      <subsection id="uid114" level="2">
        <bodyTitle>CIFRE contract with Technicolor on light fields editing</bodyTitle>
        <participants>
          <person key="sirocco-2014-idm8448">
            <firstname>Christine</firstname>
            <lastname>Guillemot</lastname>
          </person>
          <person key="sirocco-2015-idp80512">
            <firstname>Matthieu</firstname>
            <lastname>Hog</lastname>
          </person>
        </participants>
        <simplelist>
          <li id="uid115">
            <p noindent="true">Title ¬†: Light fields editing</p>
          </li>
          <li id="uid116">
            <p noindent="true">Research axis¬†: <ref xlink:href="#uid62" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
          </li>
          <li id="uid117">
            <p noindent="true">Partners¬†: Technicolor, Inria-Rennes.</p>
          </li>
          <li id="uid118">
            <p noindent="true">Funding¬†: Technicolor, ANRT.</p>
          </li>
          <li id="uid119">
            <p noindent="true">Period¬†: Oct.2015-Sept.2018.</p>
          </li>
        </simplelist>
        <p>Editing is quite common with classical imaging. Now, if we want light-fields cameras to be in the future as common as traditional cameras, this functionality should also be enabled with light-fields. The goal of the PhD
is to develop methods for light-field editing, and the work in 2016 has focused on the design of fast semi-supervised segmentation algorithms with
coherence constraints across sub-aperture images (see <ref xlink:href="#uid62" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/>).</p>
      </subsection>
      <subsection id="uid120" level="2">
        <bodyTitle>CIFRE contract with Technicolor on light fields compressed representation</bodyTitle>
        <participants>
          <person key="sirocco-2014-idm8448">
            <firstname>Christine</firstname>
            <lastname>Guillemot</lastname>
          </person>
          <person key="sirocco-2016-idp179216">
            <firstname>Fatma</firstname>
            <lastname>Hawary</lastname>
          </person>
        </participants>
        <simplelist>
          <li id="uid121">
            <p noindent="true">Title ¬†: Light fields compressed representation</p>
          </li>
          <li id="uid122">
            <p noindent="true">Partners¬†: Technicolor, Inria-Rennes.</p>
          </li>
          <li id="uid123">
            <p noindent="true">Funding¬†: Technicolor, ANRT.</p>
          </li>
          <li id="uid124">
            <p noindent="true">Period¬†: Feb.2016-Jan.2019.</p>
          </li>
        </simplelist>
        <p>The goal of this PhD is to study reconstruction algorithms from compressed measurements based on the assumption of sparsity in the Fourier domain. The goal is to apply these algorithms to scalable compression of light fields.</p>
      </subsection>
      <subsection id="uid125" level="2">
        <bodyTitle>CIFRE contract with Technicolor on cloud-based image compression</bodyTitle>
        <participants>
          <person key="sirocco-2015-idp74248">
            <firstname>Jean</firstname>
            <lastname>Begaint</lastname>
          </person>
          <person key="sirocco-2014-idm8448">
            <firstname>Christine</firstname>
            <lastname>Guillemot</lastname>
          </person>
        </participants>
        <simplelist>
          <li id="uid126">
            <p noindent="true">Title ¬†: Cloud-based image compression</p>
          </li>
          <li id="uid127">
            <p noindent="true">Research axis¬†: <ref xlink:href="#uid79" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
          </li>
          <li id="uid128">
            <p noindent="true">Partners¬†: Technicolor, Inria-Rennes.</p>
          </li>
          <li id="uid129">
            <p noindent="true">Funding¬†: Technicolor, ANRT.</p>
          </li>
          <li id="uid130">
            <p noindent="true">Period¬†: Nov.2015-Oct.2018.</p>
          </li>
        </simplelist>
        <p>The goal of this Cifre contract is to develop a novel image compression scheme exploiting similarity between
images in a cloud. The objective will therefore be to develop rate-distortion optimized affine or homographic
estimation and compensation methods which will allow us to construct prediction schemes and learn adapted
bases from most similar images retrieved by image descriptors. One issue to be addressed is the rate-distortion
trade-off induced by the need for transmitting image descriptors.</p>
      </subsection>
    </subsection>
  </contrats>
  <partenariat id="uid131">
    <bodyTitle>Partnerships and Cooperations</bodyTitle>
    <subsection id="uid132" level="1">
      <bodyTitle>Regional Initiatives</bodyTitle>
      <subsection id="uid133" level="2">
        <bodyTitle>CominLabs/InterCom project</bodyTitle>
        <participants>
          <person key="sirocco-2014-idp86352">
            <firstname>Aline</firstname>
            <lastname>Roumy</lastname>
          </person>
          <person key="sirocco-2014-idm5560">
            <firstname>Thomas</firstname>
            <lastname>Maugey</lastname>
          </person>
        </participants>
        <simplelist>
          <li id="uid134">
            <p noindent="true">Title : Interactive Communication (INTERCOM): Massive random access to subsets of compressed correlated data .</p>
          </li>
          <li id="uid135">
            <p noindent="true">Research axis : <ref xlink:href="#uid82" location="intern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest"/></p>
          </li>
          <li id="uid136">
            <p noindent="true">Partners : Inria-Rennes (Sirocco team and i4S team); LabSTICC, Telecom Bretagne, Signal &amp; Communications Department; External partner: Kieffer L2S, CentraleSupelec, Univ. Paris Sud.</p>
          </li>
          <li id="uid137">
            <p noindent="true">Funding : Labex CominLabs.</p>
          </li>
          <li id="uid138">
            <p noindent="true">Period : Oct. 2016 - Nov. 2019.</p>
          </li>
        </simplelist>
        <p>This project aims to develop novel compression techniques allowing massive random access to large databases. Indeed, we consider a database that is so large that, to be stored on a single server, the data have to be compressed efficiently, meaning that the redundancy/correlation between the data have to be exploited. The dataset is then stored on a server and made available to users that may want to access only a subset of the data. Such a request for a subset of the data is indeed random, since the choice of the subset is user-dependent. Finally, massive requests are made, meaning that, upon request, the server can only perform low complexity operations (such as bit extraction but no decompression/compression).
Algorithms for two emerging applications of this problem will be developed: Free-viewpoint Television (FTV) and massive requests to a database collecting data from a large-scale sensor network (such as Smart Cities).</p>
      </subsection>
    </subsection>
    <subsection id="uid139" level="1">
      <bodyTitle>European Initiatives</bodyTitle>
      <subsection id="uid140" level="2">
        <bodyTitle>FP7 &amp; H2020 Projects</bodyTitle>
        <subsection id="uid141" level="3">
          <bodyTitle>ERC-CLIM</bodyTitle>
          <participants>
            <person key="sirocco-2016-idp167056">
              <firstname>Pierre</firstname>
              <lastname>David</lastname>
            </person>
            <person key="sirocco-2016-idp169504">
              <firstname>Elian</firstname>
              <lastname>Dib</lastname>
            </person>
            <person key="sirocco-2014-idm8448">
              <firstname>Christine</firstname>
              <lastname>Guillemot</lastname>
            </person>
            <person key="sirocco-2015-idp70528">
              <firstname>Xin</firstname>
              <lastname>Su</lastname>
            </person>
          </participants>
          <p>Light fields yield a rich description of the scene ideally suited for advanced image creation capabilities from a single capture, such as simulating a capture with a different focus and a different depth of field, simulating lenses with different apertures, for creating images with different artistic intents or for producing 3D views. Light fields technology holds great promises for a number of application sectors, such as photography, augmented reality, light field microscopy, but also surveillance, to name only a few.</p>
          <p>The goal of the ERC-CLIM project is to develop algorithms for the entire static and video light fields processing chain, going from compact sparse and low rank representations and compression to restoration, high quality rendering and editing.</p>
        </subsection>
      </subsection>
    </subsection>
    <subsection id="uid142" level="1">
      <bodyTitle>International Initiatives</bodyTitle>
      <subsection id="uid143" level="2">
        <bodyTitle>Informal International Partners</bodyTitle>
        <simplelist>
          <li id="uid144">
            <p noindent="true">Reuben Farrugia, Prof. at the University of Malta has been one sabbatical year Sept. 2015-Aug. 2016) within the team, working on inverse problems (super-resolution, inpainting) for several applications.</p>
          </li>
          <li id="uid145">
            <p noindent="true">The study on guided image inpainting is carried out in collaboration with Prof. Pascal Frossard from EPFL (Ecole Polytechique Federal de Lausanne).</p>
          </li>
        </simplelist>
      </subsection>
    </subsection>
    <subsection id="uid146" level="1">
      <bodyTitle>International Research Visitors</bodyTitle>
      <subsection id="uid147" level="2">
        <bodyTitle>Visits of International Scientists</bodyTitle>
        <p>Sheila Hemami, Prof. at Northeastern University, Boston, has visited the team during three months (May 2016-July 2016), working on the problem of demultiplexing and decoding of micro-lenses based light fields.</p>
      </subsection>
    </subsection>
  </partenariat>
  <diffusion id="uid148">
    <bodyTitle>Dissemination</bodyTitle>
    <subsection id="uid149" level="1">
      <bodyTitle>Promoting Scientific Activities</bodyTitle>
      <subsection id="uid150" level="2">
        <bodyTitle>Scientific Events Organisation</bodyTitle>
        <subsection id="uid151" level="3">
          <bodyTitle>Member of Organizing Committees</bodyTitle>
          <simplelist>
            <li id="uid152">
              <p noindent="true">C. Guillemot has served as area chair for the European Signal Processing Conference (EUSIPCO), 2016.</p>
            </li>
            <li id="uid153">
              <p noindent="true">C. Guillemot co-organized (together with Joachim Weickert, Prof. Universit√§t des Saarlandes, and Thomas Pock, Prof. TU Graz, and Gerlind Plonka-Hoch, Prof. Universit√§t G√∂ttingen) a Schloss Dagstuhl seminar on inpainting-based image compression (Nov. 13-18, 2016).</p>
            </li>
          </simplelist>
        </subsection>
        <subsection id="uid154" level="3">
          <bodyTitle>Chair of Conference Program Committees</bodyTitle>
          <simplelist>
            <li id="uid155">
              <p noindent="true">C. Guillemot was technical co-chair of the IEEE image, video and multi-dimensional signal processing workshop, 11-12 July 2016</p>
            </li>
            <li id="uid156">
              <p noindent="true">C. Guillemot was general co-chair of the IEEE International workshop on Multimedia Signal Processing (IEEE-MMSP), Montreal, 21-23 Sept. 2016.</p>
            </li>
          </simplelist>
        </subsection>
        <subsection id="uid157" level="3">
          <bodyTitle>Member of Conference Program Committees</bodyTitle>
          <simplelist>
            <li id="uid158">
              <p noindent="true">O. Le Meur has been a member of technical program committees of international conferences: EUVIP 2016, QoMEX 2016</p>
            </li>
            <li id="uid159">
              <p noindent="true">A. Roumy has been a member of the technical program committee of the ACCV 2016 workshop on New Trends in Image Restoration and Enhancement.</p>
            </li>
          </simplelist>
        </subsection>
      </subsection>
      <subsection id="uid160" level="2">
        <bodyTitle>Journal</bodyTitle>
        <subsection id="uid161" level="3">
          <bodyTitle>Member of the Editorial Boards</bodyTitle>
          <simplelist>
            <li id="uid162">
              <p noindent="true">C. Guillemot has been associate editor of the Eurasip International Journal on Image Communication (2010-2016).</p>
            </li>
            <li id="uid163">
              <p noindent="true">C. Guillemot is senior area editor of the IEEE Trans. on Image Processing.</p>
            </li>
            <li id="uid164">
              <p noindent="true">C. Guillemot is associate editor of the International Journal on Mathematical Imaging and Vision.</p>
            </li>
            <li id="uid165">
              <p noindent="true">O. Le Meur is member of the editorial board of the IET Image Processing Journal.</p>
            </li>
            <li id="uid166">
              <p noindent="true">T. Maugey has been Guest editor for the Special Issue on Interactive Multi-view Video Services: from acquisition to Rendering, IEEE Multimedia Communication Technical Committee letters, Vol. 11(2), March 2016 (Guest Editors:
Erhan Ekmekcioglu, Thomas Maugey, Laura Toni)</p>
            </li>
            <li id="uid167">
              <p noindent="true">A. Roumy is associate editor of the Springer Annals of Telecommunications.</p>
            </li>
          </simplelist>
        </subsection>
      </subsection>
      <subsection id="uid168" level="2">
        <bodyTitle>Invited Talks</bodyTitle>
        <simplelist>
          <li id="uid169">
            <p noindent="true">O. Le Meur has been invited for a talk at Harmonic (Rennes) on ‚ÄúComputational Modelling of visual attention‚Äù</p>
          </li>
          <li id="uid170">
            <p noindent="true">O. Le Meur participated to the TechnoConference at Inria. The presentation dealt with spatio-temporal video inpainting.</p>
          </li>
          <li id="uid171">
            <p noindent="true">T. Maugey has been invited for a talk at I3S (Nice, France) on ‚ÄúEnabling user to interactively select viewpoint : a challenge for 3D data compression ", May 2016.</p>
          </li>
          <li id="uid172">
            <p noindent="true">C. Guillemot gave an overview talk on image compression at the Dagstuhl seminar (13-18 Nov. 2016).</p>
          </li>
        </simplelist>
      </subsection>
      <subsection id="uid173" level="2">
        <bodyTitle>Leadership within the Scientific Community</bodyTitle>
        <simplelist>
          <li id="uid174">
            <p noindent="true">C. Guillemot is member of the IEEE IVMSP technical committee</p>
          </li>
          <li id="uid175">
            <p noindent="true">C. Guillemot is senior member of the steering committee of IEEE Trans. on Multimedia (2016-2018).</p>
          </li>
        </simplelist>
      </subsection>
      <subsection id="uid176" level="2">
        <bodyTitle>Scientific Expertise</bodyTitle>
        <simplelist>
          <li id="uid177">
            <p noindent="true">C. Guillemot is member as scientific expert of the CCRRDT (Regional Committee of Research and Technological Development) of the Brittany region.</p>
          </li>
        </simplelist>
      </subsection>
      <subsection id="uid178" level="2">
        <bodyTitle>Research Administration</bodyTitle>
        <simplelist>
          <li id="uid179">
            <p noindent="true">C. Guillemot has been (Sept. 2015-Dec. 2016) vice-chair of Inria‚Äôs evaluation committee.</p>
          </li>
          <li id="uid180">
            <p noindent="true">C. Guillemot is member of the ‚Äúbureau du Comit√© des Projets‚Äù.</p>
          </li>
          <li id="uid181">
            <p noindent="true">T. Maugey has been member of the selection comitee for the assigment of the ministerial PhD grants at IRISA.</p>
          </li>
          <li id="uid182">
            <p noindent="true">A. Roumy served as a member of Board of Examiners (Comit√© de s√©lection) for an Associate Professor position (Maitre de Conf√©rences) at CREATIS Polytech Lyon (MCU61-46 2016).</p>
          </li>
          <li id="uid183">
            <p noindent="true">A. Roumy served as a member of Board of Examiners (Comit√© de s√©lection) for an Associate Professor position (Maitre de Conf√©rences) at ENSEIRB-MATMECA Bordeaux (MCF61-0127 2016).</p>
          </li>
          <li id="uid184">
            <p noindent="true">A. Roumy is a member of the Inria Joint Administrative Committee (CAP commission administrative paritaire).</p>
          </li>
        </simplelist>
      </subsection>
    </subsection>
    <subsection id="uid185" level="1">
      <bodyTitle>Teaching - Supervision - Juries</bodyTitle>
      <subsection id="uid186" level="2">
        <bodyTitle>Teaching</bodyTitle>
        <sanspuceslist>
          <li id="uid187">
            <p noindent="true">Master: C. Guillemot, Image and video compression, 8 hours, M2 computer science, Univ. of Rennes 1, France.</p>
          </li>
          <li id="uid188">
            <p noindent="true">Master: C. Guillemot, Image and video compression, 8 hours, M2 SISEA, Univ. of Rennes 1, France.</p>
          </li>
          <li id="uid189">
            <p noindent="true">Master: O. Le Meur, Selective visual attention, 6 hours, M2, Univ. of Paris 8, France.</p>
          </li>
          <li id="uid190">
            <p noindent="true">Master: O. Le Meur, Acquisition/Image Processing/Compression, 22 hours, M2 MITIC, Univ. of Rennes 1, France.</p>
          </li>
          <li id="uid191">
            <p noindent="true">Engineer degree: O. Le Meur, Image Processing, video analysis and compression, 54 hours, ESIR2, Univ. of Rennes 1, France.</p>
          </li>
          <li id="uid192">
            <p noindent="true">Engineer degree: O. Le Meur, Visual communication, 65 hours, ESIR3, Univ. of Rennes 1, France.</p>
          </li>
          <li id="uid193">
            <p noindent="true">Professional training: O. Le Meur, Image Processing and OpenCV, 42 hours, Technicolor Rennes.</p>
          </li>
          <li id="uid194">
            <p noindent="true">Engineering degree: A. Roumy, Compressive sensing, 17 hours, INSA Rennes, 5th year, Mathematical engineering, France.</p>
          </li>
          <li id="uid195">
            <p noindent="true">Master: A. Roumy, Compressive sensing, 15 hours, ENSAI Bruz, Master Big Data, France.</p>
          </li>
        </sanspuceslist>
      </subsection>
      <subsection id="uid196" level="2">
        <bodyTitle>Supervision of PhD defended during the year</bodyTitle>
        <sanspuceslist>
          <li id="uid197">
            <p noindent="true">PhD-VAE: P. Bordes, Adapting video compression to new formats, Univ. of Rennes 1, 18 Jan. 2016, C. Guillemot.</p>
          </li>
          <li id="uid198">
            <p noindent="true">PhD : M. Le Pendu, Backward compatible approaches for the compression of high dynamic range videos, Univ. of Rennes 1, 17 March 2016, C. Guillemot (Cifre contract with Technicolor).</p>
          </li>
          <li id="uid199">
            <p noindent="true">PhD : J. C. Ferreira, Algorithms for Super-resolution of Images based on Sparse Representation and Manifolds, co-tutelle Univ. of Rennes 1/ University of Uberlandia Brazil, 6 July 2016, C. Guillemot.</p>
          </li>
          <li id="uid200">
            <p noindent="true">PhD: C. Chamaret, Color Harmony: experimental and computational modeling, University of Rennes 1, 28th of April, 2016.</p>
          </li>
        </sanspuceslist>
      </subsection>
      <subsection id="uid201" level="2">
        <bodyTitle>Juries</bodyTitle>
        <simplelist>
          <li id="uid202">
            <p noindent="true">C. Guillemot has been member (rapporteur) of the PhD jury of:</p>
            <simplelist>
              <li id="uid203">
                <p noindent="true">T. Biatek, INSA-Rennes, Apr. 2016</p>
              </li>
              <li id="uid204">
                <p noindent="true">A. Akl, Univ. of Bordeaux, Feb. 2016</p>
              </li>
            </simplelist>
          </li>
          <li id="uid205">
            <p noindent="true">C. Guillemot has been member of the PhD jury of:</p>
            <simplelist>
              <li id="uid206">
                <p noindent="true">A. Gilles, INSA-Rennes, Sept. 2016</p>
              </li>
              <li id="uid207">
                <p noindent="true">L. LE Magoarou, Univ. Rennes 1, Nov. 2016</p>
              </li>
            </simplelist>
          </li>
          <li id="uid208">
            <p noindent="true">O. Le Meur has been member (rapporteur) of the jury of the PhD committee of:</p>
            <simplelist>
              <li id="uid209">
                <p noindent="true">Romain Cohendet, University Bretagne Loire, IRCCyN, 2016</p>
              </li>
              <li id="uid210">
                <p noindent="true">Andrea Helo, University Paris Descartes, 2016</p>
              </li>
              <li id="uid211">
                <p noindent="true">Ala Aboudib, Telecom Bretagne, 2016</p>
              </li>
              <li id="uid212">
                <p noindent="true">Franck Chi, Telecom Bretagne, 2016</p>
              </li>
            </simplelist>
          </li>
        </simplelist>
      </subsection>
    </subsection>
  </diffusion>
  <biblio id="bibliography" html="bibliography" numero="10" titre="Bibliography">
    
    <biblStruct id="sirocco-2016-bid16" type="phdthesis" rend="year" n="cite:alain:tel-01261590">
      <identifiant type="hal" value="tel-01261590"/>
      <monogr>
        <title level="m">A compact video representation format based on spatio-temporal linear embedding and epitome</title>
        <author>
          <persName key="sirocco-2014-idp95224">
            <foreName>Martin</foreName>
            <surname>Alain</surname>
            <initial>M.</initial>
          </persName>
        </author>
        <imprint>
          <publisher>
            <orgName type="school">Universit√© Rennes 1</orgName>
          </publisher>
          <dateStruct>
            <month>January</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/tel-01261590" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>tel-01261590</ref>
        </imprint>
      </monogr>
      <note type="typdoc">Theses</note>
    </biblStruct>
    
    <biblStruct id="sirocco-2016-bid22" type="phdthesis" rend="year" n="cite:ferreira:tel-01388977">
      <identifiant type="hal" value="tel-01388977"/>
      <monogr>
        <title level="m">Algorithms for super-resolution of images based on sparse representation and manifolds</title>
        <author>
          <persName key="sirocco-2014-idp100272">
            <foreName>J√∫lio C√©sar</foreName>
            <surname>Ferreira</surname>
            <initial>J. C.</initial>
          </persName>
        </author>
        <imprint>
          <publisher>
            <orgName type="school">Universit√© Rennes 1</orgName>
          </publisher>
          <dateStruct>
            <month>July</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://tel.archives-ouvertes.fr/tel-01388977" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>tel.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>tel-01388977</ref>
        </imprint>
      </monogr>
      <note type="typdoc">Theses</note>
    </biblStruct>
    
    <biblStruct id="sirocco-2016-bid5" type="phdthesis" rend="year" n="cite:lependu:tel-01312901">
      <identifiant type="hal" value="tel-01312901"/>
      <monogr>
        <title level="m">Backward compatible approaches for the compression of high dynamic range videos</title>
        <author>
          <persName key="sirocco-2014-idp105312">
            <foreName>Mika√´l</foreName>
            <surname>Le Pendu</surname>
            <initial>M.</initial>
          </persName>
        </author>
        <imprint>
          <publisher>
            <orgName type="school">Universit√© Rennes 1</orgName>
          </publisher>
          <dateStruct>
            <month>March</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://tel.archives-ouvertes.fr/tel-01312901" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>tel.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>tel-01312901</ref>
        </imprint>
      </monogr>
      <note type="typdoc">Theses</note>
    </biblStruct>
    
    <biblStruct id="sirocco-2016-bid12" type="article" rend="year" n="cite:buyssens:hal-01391065">
      <identifiant type="doi" value="10.1109/TIP.2016.2619263"/>
      <identifiant type="hal" value="hal-01391065"/>
      <analytic>
        <title level="a">Depth-guided disocclusion inpainting of synthesized RGB-D images</title>
        <author>
          <persName key="sirocco-2015-idp69272">
            <foreName>Pierre</foreName>
            <surname>Buyssens</surname>
            <initial>P.</initial>
          </persName>
          <persName key="sirocco-2014-idp87568">
            <foreName>Olivier</foreName>
            <surname>Le Meur</surname>
            <initial>O.</initial>
          </persName>
          <persName>
            <foreName>Maxime</foreName>
            <surname>Daisy</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>David</foreName>
            <surname>Tschumperl√©</surname>
            <initial>D.</initial>
          </persName>
          <persName>
            <foreName>Olivier</foreName>
            <surname>L√©zoray</surname>
            <initial>O.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid00728">
        <idno type="issn">1057-7149</idno>
        <title level="j">IEEE Transactions on Image Processing</title>
        <imprint>
          <biblScope type="volume">26</biblScope>
          <biblScope type="number">2</biblScope>
          <dateStruct>
            <year>2017</year>
          </dateStruct>
          <biblScope type="pages">525-538</biblScope>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01391065" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01391065</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sirocco-2016-bid13" type="article" rend="year" n="cite:ferreira:hal-01388955">
      <identifiant type="hal" value="hal-01388955"/>
      <analytic>
        <title level="a">Geometry-Aware Neighborhood Search for Learning Local Models for Image Superresolution</title>
        <author>
          <persName key="sirocco-2014-idp100272">
            <foreName>Julio Cesar</foreName>
            <surname>Ferreira</surname>
            <initial>J. C.</initial>
          </persName>
          <persName key="sirocco-2014-idp92728">
            <foreName>Elif</foreName>
            <surname>Vural</surname>
            <initial>E.</initial>
          </persName>
          <persName key="sirocco-2014-idm8448">
            <foreName>Christine</foreName>
            <surname>Guillemot</surname>
            <initial>C.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid00728">
        <idno type="issn">1057-7149</idno>
        <title level="j">IEEE Transactions on Image Processing</title>
        <imprint>
          <biblScope type="volume">25</biblScope>
          <biblScope type="number">3</biblScope>
          <dateStruct>
            <month>March</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">14</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01388955" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01388955</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sirocco-2016-bid9" type="article" rend="year" n="cite:gao:hal-01217115">
      <identifiant type="hal" value="hal-01217115"/>
      <analytic>
        <title level="a">Encoder-Driven Inpainting Strategy in Multiview Video Compression</title>
        <author>
          <persName>
            <foreName>Yu</foreName>
            <surname>Gao</surname>
            <initial>Y.</initial>
          </persName>
          <persName>
            <foreName>Gene</foreName>
            <surname>Cheung</surname>
            <initial>G.</initial>
          </persName>
          <persName key="sirocco-2014-idm5560">
            <foreName>Thomas</foreName>
            <surname>Maugey</surname>
            <initial>T.</initial>
          </persName>
          <persName>
            <foreName>Pascal</foreName>
            <surname>FROSSARD</surname>
            <initial>P.</initial>
          </persName>
          <persName>
            <foreName>Jie</foreName>
            <surname>Liang</surname>
            <initial>J.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid00728">
        <idno type="issn">1057-7149</idno>
        <title level="j">IEEE Transactions on Image Processing</title>
        <imprint>
          <biblScope type="volume">25</biblScope>
          <biblScope type="number">1</biblScope>
          <dateStruct>
            <month>January</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">134-149</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01217115" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01217115</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sirocco-2016-bid10" type="article" rend="year" n="cite:khattak:hal-01137927">
      <identifiant type="doi" value="10.1109/TCSVT.2015.2418631"/>
      <identifiant type="hal" value="hal-01137927"/>
      <analytic>
        <title level="a">Temporal and Inter-view Consistent Error Concealment Technique for Multiview plus Depth Video Broadcasting</title>
        <author>
          <persName>
            <foreName>Shadan</foreName>
            <surname>Khattak</surname>
            <initial>S.</initial>
          </persName>
          <persName key="sirocco-2014-idm5560">
            <foreName>Thomas</foreName>
            <surname>Maugey</surname>
            <initial>T.</initial>
          </persName>
          <persName>
            <foreName>Raouf</foreName>
            <surname>Hamzaoui</surname>
            <initial>R.</initial>
          </persName>
          <persName>
            <foreName>Shakeel</foreName>
            <surname>Ahmad</surname>
            <initial>S.</initial>
          </persName>
          <persName>
            <foreName>Pascal</foreName>
            <surname>FROSSARD</surname>
            <initial>P.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid00715">
        <idno type="issn">1051-8215</idno>
        <title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
        <imprint>
          <biblScope type="volume">26</biblScope>
          <biblScope type="number">5</biblScope>
          <dateStruct>
            <month>May</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">829-840</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01137927" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01137927</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sirocco-2016-bid6" type="article" rend="year" n="cite:lemeur:hal-01391745">
      <identifiant type="doi" value="10.1016/j.visres.2016.01.005"/>
      <identifiant type="hal" value="hal-01391745"/>
      <analytic>
        <title level="a">Introducing context-dependent and spatially-variant viewing biases in saccadic models</title>
        <author>
          <persName key="sirocco-2014-idp87568">
            <foreName>Olivier</foreName>
            <surname>Le Meur</surname>
            <initial>O.</initial>
          </persName>
          <persName>
            <foreName>Antoine</foreName>
            <surname>Coutrot</surname>
            <initial>A.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid01902">
        <idno type="issn">0042-6989</idno>
        <title level="j">Vision Research</title>
        <imprint>
          <biblScope type="volume">121</biblScope>
          <dateStruct>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">72 - 84</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01391745" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01391745</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sirocco-2016-bid14" type="article" rend="year" n="cite:lependu:hal-01388961">
      <identifiant type="doi" value="10.1109/TIP.2016.2571559"/>
      <identifiant type="hal" value="hal-01388961"/>
      <analytic>
        <title level="a">Inter-Layer Prediction of Color in High Dynamic Range Image Scalable Compression</title>
        <author>
          <persName key="sirocco-2014-idp105312">
            <foreName>Mika√´l</foreName>
            <surname>Le Pendu</surname>
            <initial>M.</initial>
          </persName>
          <persName key="sirocco-2014-idm8448">
            <foreName>Christine</foreName>
            <surname>Guillemot</surname>
            <initial>C.</initial>
          </persName>
          <persName>
            <foreName>Dominique</foreName>
            <surname>Thoreau</surname>
            <initial>D.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid00728">
        <idno type="issn">1057-7149</idno>
        <title level="j">IEEE Transactions on Image Processing</title>
        <imprint>
          <biblScope type="volume">25</biblScope>
          <dateStruct>
            <month>August</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">3585 - 3596</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01388961" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01388961</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sirocco-2016-bid7" type="incollection" rend="year" n="cite:mancas:hal-01393254">
      <identifiant type="doi" value="10.1007/978-1-4939-3435-5_18"/>
      <identifiant type="hal" value="hal-01393254"/>
      <analytic>
        <title level="a">Applications of Saliency Models</title>
        <author>
          <persName>
            <foreName>Matei</foreName>
            <surname>Mancas</surname>
            <initial>M.</initial>
          </persName>
          <persName key="sirocco-2014-idp87568">
            <foreName>Olivier</foreName>
            <surname>Le Meur</surname>
            <initial>O.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no">
        <editor role="editor">
          <persName>
            <foreName>Matei</foreName>
            <surname>Mancas</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>Vincent P.</foreName>
            <surname>Ferrera</surname>
            <initial>V. P.</initial>
          </persName>
          <persName>
            <foreName>Nicolas</foreName>
            <surname>Riche</surname>
            <initial>N.</initial>
          </persName>
          <persName>
            <foreName>John G.</foreName>
            <surname>Taylor</surname>
            <initial>J. G.</initial>
          </persName>
        </editor>
        <title level="m">From Human Attention to Computational Attention. A Multidisciplinary Approach</title>
        <title level="s">Springer Series in Cognitive and Neural Systems</title>
        <imprint>
          <biblScope type="volume">10</biblScope>
          <publisher>
            <orgName>Springer</orgName>
          </publisher>
          <dateStruct>
            <month>September</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">331-377</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01393254" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01393254</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sirocco-2016-bid15" type="article" rend="year" n="cite:maugey:hal-01262258">
      <identifiant type="hal" value="hal-01262258"/>
      <analytic>
        <title level="a">Reference view selection in DIBR-based multiview coding</title>
        <author>
          <persName key="sirocco-2014-idm5560">
            <foreName>Thomas</foreName>
            <surname>Maugey</surname>
            <initial>T.</initial>
          </persName>
          <persName key="serpico-2016-idp116432">
            <foreName>Giovanni</foreName>
            <surname>Petrazzuoli</surname>
            <initial>G.</initial>
          </persName>
          <persName>
            <foreName>Pascal</foreName>
            <surname>FROSSARD</surname>
            <initial>P.</initial>
          </persName>
          <persName>
            <foreName>Marco</foreName>
            <surname>Cagnazzo</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>Beatrice</foreName>
            <surname>Pesquet-Popescu</surname>
            <initial>B.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid00728">
        <idno type="issn">1057-7149</idno>
        <title level="j">IEEE Transactions on Image Processing</title>
        <imprint>
          <biblScope type="volume">25</biblScope>
          <biblScope type="number">4</biblScope>
          <dateStruct>
            <month>April</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">1808-1819</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01262258" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01262258</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sirocco-2016-bid20" type="article" rend="year" n="cite:roumy:hal-01394864">
      <identifiant type="hal" value="hal-01394864"/>
      <analytic>
        <title level="a">An Information theoretical problem in interactive Multi-View Video services</title>
        <author>
          <persName key="sirocco-2014-idp86352">
            <foreName>Aline</foreName>
            <surname>Roumy</surname>
            <initial>A.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="no" x-international-audience="yes" id="rid03136">
        <idno type="issn">I-NtFnd</idno>
        <title level="j">IEEE Communications Society Multimedia Communications Technical Committee (ComSoc MMTC) E-Letter</title>
        <imprint>
          <biblScope type="volume">11</biblScope>
          <biblScope type="number">2</biblScope>
          <dateStruct>
            <month>March</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">6</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01394864" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01394864</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sirocco-2016-bid11" type="article" rend="year" n="cite:vural:hal-01388959">
      <identifiant type="hal" value="hal-01388959"/>
      <analytic>
        <title level="a">Out-of-sample generalizations for supervised manifold learning for classification</title>
        <author>
          <persName key="sirocco-2014-idp92728">
            <foreName>Elif</foreName>
            <surname>Vural</surname>
            <initial>E.</initial>
          </persName>
          <persName key="sirocco-2014-idm8448">
            <foreName>Christine</foreName>
            <surname>Guillemot</surname>
            <initial>C.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" id="rid00728">
        <idno type="issn">1057-7149</idno>
        <title level="j">IEEE Transactions on Image Processing</title>
        <imprint>
          <biblScope type="volume">25</biblScope>
          <biblScope type="number">3</biblScope>
          <dateStruct>
            <month>March</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">15</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01388959" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01388959</ref>
        </imprint>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sirocco-2016-bid17" type="inproceedings" rend="year" n="cite:alain:hal-01317625">
      <identifiant type="doi" value="10.1109/ICIP.2016.7533151"/>
      <identifiant type="hal" value="hal-01317625"/>
      <analytic>
        <title level="a">Learning Clustering-Based Linear Mappings for Quantization Noise Removal</title>
        <author>
          <persName key="sirocco-2014-idp95224">
            <foreName>Martin</foreName>
            <surname>Alain</surname>
            <initial>M.</initial>
          </persName>
          <persName key="sirocco-2014-idm8448">
            <foreName>Christine</foreName>
            <surname>Guillemot</surname>
            <initial>C.</initial>
          </persName>
          <persName>
            <foreName>Dominique</foreName>
            <surname>Thoreau</surname>
            <initial>D.</initial>
          </persName>
          <persName>
            <foreName>Philippe</foreName>
            <surname>Guillotel</surname>
            <initial>P.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">IEEE International Conference on Image Processing (ICIP) 2016</title>
        <loc>Phoenix, United States</loc>
        <imprint>
          <dateStruct>
            <month>September</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01317625" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01317625</ref>
        </imprint>
        <meeting id="cid83858">
          <title>IEEE International Conference on Image Processing</title>
          <num>23</num>
          <abbr type="sigle">ICIP</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sirocco-2016-bid21" type="inproceedings" rend="year" n="cite:buyssens:hal-01320967">
      <identifiant type="hal" value="hal-01320967"/>
      <analytic>
        <title level="a">D√©soccultation de cartes de profondeurs pour la synth√®se de vues virtuelles</title>
        <author>
          <persName key="sirocco-2015-idp69272">
            <foreName>Pierre</foreName>
            <surname>Buyssens</surname>
            <initial>P.</initial>
          </persName>
          <persName key="sirocco-2014-idp87568">
            <foreName>Olivier</foreName>
            <surname>Le Meur</surname>
            <initial>O.</initial>
          </persName>
          <persName>
            <foreName>Maxime</foreName>
            <surname>Daisy</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>David</foreName>
            <surname>Tschumperl√©</surname>
            <initial>D.</initial>
          </persName>
          <persName>
            <foreName>Olivier</foreName>
            <surname>L√©zoray</surname>
            <initial>O.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="no" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">RFIA 2016</title>
        <loc>Clermont-Ferrand, France</loc>
        <imprint>
          <dateStruct>
            <month>June</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01320967" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01320967</ref>
        </imprint>
        <meeting id="cid54223">
          <title>Congr√®s Francophone de Reconnaissance des Formes et Intelligence Artificielle</title>
          <num>2016</num>
          <abbr type="sigle">RFIA</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sirocco-2016-bid3" type="inproceedings" rend="year" n="cite:dumas:hal-01377907">
      <identifiant type="doi" value="10.1109/ICMEW.2016.7574708"/>
      <identifiant type="hal" value="hal-01377907"/>
      <analytic>
        <title level="a">Shallow sparse autoencoders versus sparse coding algorithms for image compression</title>
        <author>
          <persName key="sirocco-2015-idp76768">
            <foreName>Thierry</foreName>
            <surname>Dumas</surname>
            <initial>T.</initial>
          </persName>
          <persName key="sirocco-2014-idp86352">
            <foreName>Aline</foreName>
            <surname>Roumy</surname>
            <initial>A.</initial>
          </persName>
          <persName key="sirocco-2014-idm8448">
            <foreName>Christine</foreName>
            <surname>Guillemot</surname>
            <initial>C.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">2016 IEEE International Conference on Multimedia and Expo (ICME 2016)</title>
        <loc>Seattle, WA, United States</loc>
        <imprint>
          <dateStruct>
            <month>July</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">1 - 6</biblScope>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01377907" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01377907</ref>
        </imprint>
        <meeting id="cid84585">
          <title>IEEE International Conference on Multimedia and Expo</title>
          <num>2016</num>
          <abbr type="sigle">ICME</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sirocco-2016-bid1" type="inproceedings" rend="year" n="cite:farrugia:hal-01388971">
      <identifiant type="hal" value="hal-01388971"/>
      <analytic>
        <title level="a">Model and Dictionary guided Face Inpainting in the Wild</title>
        <author>
          <persName key="sirocco-2015-idp86776">
            <foreName>Reuben</foreName>
            <surname>Farrugia</surname>
            <initial>R.</initial>
          </persName>
          <persName key="sirocco-2014-idm8448">
            <foreName>Christine</foreName>
            <surname>Guillemot</surname>
            <initial>C.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">ACCV workshop on New Trends in Image Restoration and Enhancement</title>
        <loc>Taipei, Taiwan</loc>
        <imprint>
          <dateStruct>
            <month>November</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">17</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01388971" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01388971</ref>
        </imprint>
        <meeting id="cid625486">
          <title>ACCV Workshop on New Trends in Image Restoration and Enhancement</title>
          <num>2016</num>
          <abbr type="sigle">NTIRE</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sirocco-2016-bid8" type="inproceedings" rend="year" n="cite:farrugia:hal-01388972">
      <identifiant type="hal" value="hal-01388972"/>
      <analytic>
        <title level="a">Robust Face Hallucination Using Quantization-Adaptive Dictionaries</title>
        <author>
          <persName key="sirocco-2015-idp86776">
            <foreName>Reuben</foreName>
            <surname>Farrugia</surname>
            <initial>R.</initial>
          </persName>
          <persName key="sirocco-2014-idm8448">
            <foreName>Christine</foreName>
            <surname>Guillemot</surname>
            <initial>C.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">IEEE International Conference on Image Processing</title>
        <loc>Phoenix, United States</loc>
        <imprint>
          <dateStruct>
            <month>September</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">5</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01388972" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01388972</ref>
        </imprint>
        <meeting id="cid83858">
          <title>IEEE International Conference on Image Processing</title>
          <num>15</num>
          <abbr type="sigle">ICIP</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sirocco-2016-bid4" type="inproceedings" rend="year" n="cite:gommelet:hal-01377701">
      <identifiant type="doi" value="10.1109/ICIP.2016.7532585"/>
      <identifiant type="hal" value="hal-01377701"/>
      <analytic>
        <title level="a">Rate-distortion optimization of a tone mapping with SDR quality constraint for backward-compatible high dynamic range compression</title>
        <author>
          <persName key="sirocco-2014-idp101552">
            <foreName>David</foreName>
            <surname>Gommelet</surname>
            <initial>D.</initial>
          </persName>
          <persName key="sirocco-2014-idp86352">
            <foreName>Aline</foreName>
            <surname>Roumy</surname>
            <initial>A.</initial>
          </persName>
          <persName key="sirocco-2014-idm8448">
            <foreName>Christine</foreName>
            <surname>Guillemot</surname>
            <initial>C.</initial>
          </persName>
          <persName>
            <foreName>Michael</foreName>
            <surname>Ropert</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>Julien</foreName>
            <surname>Letanou</surname>
            <initial>J.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">International Conference on Image Processing, ICIP</title>
        <loc>Phoenix, United States</loc>
        <title level="s">2016 IEEE International Conference on Image Processing (ICIP)</title>
        <imprint>
          <dateStruct>
            <month>September</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">1384-1388</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01377701" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01377701</ref>
        </imprint>
        <meeting id="cid83858">
          <title>IEEE International Conference on Image Processing</title>
          <num>17</num>
          <abbr type="sigle">ICIP</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sirocco-2016-bid0" type="inproceedings" rend="year" n="cite:hog:hal-01388965">
      <identifiant type="hal" value="hal-01388965"/>
      <analytic>
        <title level="a">Light Field Segmentation Using a Ray-Based Graph Structure</title>
        <author>
          <persName key="sirocco-2015-idp80512">
            <foreName>Matthieu</foreName>
            <surname>Hog</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>Neus</foreName>
            <surname>Sabater</surname>
            <initial>N.</initial>
          </persName>
          <persName key="sirocco-2014-idm8448">
            <foreName>Christine</foreName>
            <surname>Guillemot</surname>
            <initial>C.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">European Conference on Computer Vision - ECCV</title>
        <loc>Amsterdam, Netherlands</loc>
        <imprint>
          <dateStruct>
            <month>October</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">16</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01388965" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01388965</ref>
        </imprint>
        <meeting id="cid66293">
          <title>European Conference on Computer Vision</title>
          <num>11</num>
          <abbr type="sigle">ECCV</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sirocco-2016-bid19" type="inproceedings" rend="year" n="cite:lemeur:hal-01391750">
      <identifiant type="hal" value="hal-01391750"/>
      <analytic>
        <title level="a">How saccadic models help predict where we look during a visual task? Application to visual quality assessment</title>
        <author>
          <persName key="sirocco-2014-idp87568">
            <foreName>Olivier</foreName>
            <surname>Le Meur</surname>
            <initial>O.</initial>
          </persName>
          <persName>
            <foreName>Antoine</foreName>
            <surname>Coutrot</surname>
            <initial>A.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">SPIE Image Quality And System Performance</title>
        <loc>San Fransisco, United States</loc>
        <imprint>
          <dateStruct>
            <month>February</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01391750" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01391750</ref>
        </imprint>
        <meeting id="cid625487">
          <title>SPIE Image Quality And System Performance</title>
          <num>2016</num>
          <abbr type="sigle"/>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sirocco-2016-bid24" type="inproceedings" rend="year" n="cite:rizkallah:hal-01377834">
      <identifiant type="doi" value="10.1109/EUSIPCO.2016.7760378"/>
      <identifiant type="hal" value="hal-01377834"/>
      <analytic>
        <title level="a">Impact of Light Field Compression on Refocused and Extended Focus Images</title>
        <author>
          <persName key="sirocco-2015-idp84272">
            <foreName>Mira</foreName>
            <surname>Rizkallah</surname>
            <initial>M.</initial>
          </persName>
          <persName key="sirocco-2014-idm5560">
            <foreName>Thomas</foreName>
            <surname>Maugey</surname>
            <initial>T.</initial>
          </persName>
          <persName>
            <foreName>Charles</foreName>
            <surname>Yaacoub</surname>
            <initial>C.</initial>
          </persName>
          <persName key="sirocco-2014-idm8448">
            <foreName>Christine</foreName>
            <surname>Guillemot</surname>
            <initial>C.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">EUSIPCO 2016 - 24th European Signal Processing Conference</title>
        <loc>Budapest, Hungary</loc>
        <imprint>
          <dateStruct>
            <month>August</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01377834" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01377834</ref>
        </imprint>
        <meeting id="cid70310">
          <title>European Signal Processing Conference</title>
          <num>24</num>
          <abbr type="sigle">EUSIPCO</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sirocco-2016-bid2" type="inproceedings" rend="year" n="cite:su:hal-01378422">
      <identifiant type="doi" value="10.1109/ICIP.2016.7532619"/>
      <identifiant type="hal" value="hal-01378422"/>
      <analytic>
        <title level="a">Graph-based representation for multiview images with complex camera configurations</title>
        <author>
          <persName key="sirocco-2015-idp70528">
            <foreName>Xin</foreName>
            <surname>Su</surname>
            <initial>X.</initial>
          </persName>
          <persName key="sirocco-2014-idm5560">
            <foreName>Thomas</foreName>
            <surname>Maugey</surname>
            <initial>T.</initial>
          </persName>
          <persName key="sirocco-2014-idm8448">
            <foreName>Christine</foreName>
            <surname>Guillemot</surname>
            <initial>C.</initial>
          </persName>
        </author>
      </analytic>
      <monogr x-scientific-popularization="no" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no" x-editorial-board="yes">
        <title level="m">ICIP 2016 - IEEE International Conference on Image Processing</title>
        <loc>Phoenix, United States</loc>
        <imprint>
          <dateStruct>
            <month>September</month>
            <year>2016</year>
          </dateStruct>
          <biblScope type="pages">1554 - 1558</biblScope>
          <ref xlink:href="https://hal.inria.fr/hal-01378422" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01378422</ref>
        </imprint>
        <meeting id="cid83858">
          <title>IEEE International Conference on Image Processing</title>
          <num>23</num>
          <abbr type="sigle">ICIP</abbr>
        </meeting>
      </monogr>
    </biblStruct>
    
    <biblStruct id="sirocco-2016-bid18" type="misc" rend="year" n="cite:coutrot:hal-01391751">
      <identifiant type="hal" value="hal-01391751"/>
      <monogr x-scientific-popularization="no" x-editorial-board="yes" x-international-audience="yes" x-proceedings="yes" x-invited-conference="no">
        <title level="m">Visual attention saccadic models: taking into account global scene context and temporal aspects of gaze behaviour</title>
        <author>
          <persName>
            <foreName>Antoine</foreName>
            <surname>Coutrot</surname>
            <initial>A.</initial>
          </persName>
          <persName key="sirocco-2014-idp87568">
            <foreName>Olivier</foreName>
            <surname>Le Meur</surname>
            <initial>O.</initial>
          </persName>
        </author>
        <imprint>
          <dateStruct>
            <month>August</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.inria.fr/hal-01391751" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>inria.<allowbreak/>fr/<allowbreak/>hal-01391751</ref>
        </imprint>
      </monogr>
      <note type="howpublished">ECVP 2016 - European Conference on Visual Perception</note>
      <note type="bnote">Poster</note>
    </biblStruct>
    
    <biblStruct id="sirocco-2016-bid23" type="unpublished" rend="year" n="cite:hog:hal-01407852">
      <identifiant type="hal" value="hal-01407852"/>
      <monogr>
        <title level="m">Super-rays for Efficient Light Field Processing</title>
        <author>
          <persName key="sirocco-2015-idp80512">
            <foreName>Matthieu</foreName>
            <surname>Hog</surname>
            <initial>M.</initial>
          </persName>
          <persName>
            <foreName>Neus</foreName>
            <surname>Sabater</surname>
            <initial>N.</initial>
          </persName>
          <persName key="sirocco-2014-idm8448">
            <foreName>Christine</foreName>
            <surname>Guillemot</surname>
            <initial>C.</initial>
          </persName>
        </author>
        <imprint>
          <dateStruct>
            <month>December</month>
            <year>2016</year>
          </dateStruct>
          <ref xlink:href="https://hal.archives-ouvertes.fr/hal-01407852" location="extern" xlink:type="simple" xlink:show="replace" xlink:actuate="onRequest">https://<allowbreak/>hal.<allowbreak/>archives-ouvertes.<allowbreak/>fr/<allowbreak/>hal-01407852</ref>
        </imprint>
      </monogr>
      <note type="bnote">working paper or preprint</note>
    </biblStruct>
  </biblio>
</raweb>
